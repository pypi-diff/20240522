# Comparing `tmp/snowflake_ml_python-1.5.0-py3-none-any.whl.zip` & `tmp/snowflake_ml_python-1.5.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,382 +1,384 @@
-Zip file size: 1897128 bytes, number of entries: 380
--rw-r--r--  2.0 unx      360 b- defN 24-May-01 19:19 snowflake/cortex/__init__.py
--rw-r--r--  2.0 unx     1226 b- defN 24-May-01 19:19 snowflake/cortex/_complete.py
--rw-r--r--  2.0 unx     1358 b- defN 24-May-01 19:19 snowflake/cortex/_extract_answer.py
--rw-r--r--  2.0 unx     1149 b- defN 24-May-01 19:19 snowflake/cortex/_sentiment.py
--rw-r--r--  2.0 unx     1075 b- defN 24-May-01 19:19 snowflake/cortex/_summarize.py
--rw-r--r--  2.0 unx     1420 b- defN 24-May-01 19:19 snowflake/cortex/_translate.py
--rw-r--r--  2.0 unx     1557 b- defN 24-May-01 19:19 snowflake/cortex/_util.py
--rwxr-xr-x  2.0 unx       16 b- defN 24-May-01 19:20 snowflake/ml/version.py
--rw-r--r--  2.0 unx      161 b- defN 24-May-01 19:19 snowflake/ml/_internal/env.py
--rw-r--r--  2.0 unx    27629 b- defN 24-May-01 19:19 snowflake/ml/_internal/env_utils.py
--rw-r--r--  2.0 unx    13622 b- defN 24-May-01 19:19 snowflake/ml/_internal/file_utils.py
--rw-r--r--  2.0 unx     2696 b- defN 24-May-01 19:19 snowflake/ml/_internal/init_utils.py
--rw-r--r--  2.0 unx      161 b- defN 24-May-01 19:19 snowflake/ml/_internal/migrator_utils.py
--rw-r--r--  2.0 unx    22763 b- defN 24-May-01 19:19 snowflake/ml/_internal/telemetry.py
--rw-r--r--  2.0 unx     2168 b- defN 24-May-01 19:19 snowflake/ml/_internal/type_utils.py
--rw-r--r--  2.0 unx     3291 b- defN 24-May-01 19:19 snowflake/ml/_internal/container_services/image_registry/credential.py
--rw-r--r--  2.0 unx     5214 b- defN 24-May-01 19:19 snowflake/ml/_internal/container_services/image_registry/http_client.py
--rw-r--r--  2.0 unx    14537 b- defN 24-May-01 19:19 snowflake/ml/_internal/container_services/image_registry/imagelib.py
--rw-r--r--  2.0 unx     9115 b- defN 24-May-01 19:19 snowflake/ml/_internal/container_services/image_registry/registry_client.py
--rw-r--r--  2.0 unx      285 b- defN 24-May-01 19:19 snowflake/ml/_internal/exceptions/dataset_error_messages.py
--rw-r--r--  2.0 unx      762 b- defN 24-May-01 19:19 snowflake/ml/_internal/exceptions/dataset_errors.py
--rw-r--r--  2.0 unx     5453 b- defN 24-May-01 19:19 snowflake/ml/_internal/exceptions/error_codes.py
--rw-r--r--  2.0 unx       47 b- defN 24-May-01 19:19 snowflake/ml/_internal/exceptions/error_messages.py
--rw-r--r--  2.0 unx     1653 b- defN 24-May-01 19:19 snowflake/ml/_internal/exceptions/exceptions.py
--rw-r--r--  2.0 unx      419 b- defN 24-May-01 19:19 snowflake/ml/_internal/exceptions/fileset_error_messages.py
--rw-r--r--  2.0 unx     1040 b- defN 24-May-01 19:19 snowflake/ml/_internal/exceptions/fileset_errors.py
--rw-r--r--  2.0 unx      665 b- defN 24-May-01 19:19 snowflake/ml/_internal/exceptions/modeling_error_messages.py
--rw-r--r--  2.0 unx      804 b- defN 24-May-01 19:19 snowflake/ml/_internal/human_readable_id/adjectives.txt
--rw-r--r--  2.0 unx      837 b- defN 24-May-01 19:19 snowflake/ml/_internal/human_readable_id/animals.txt
--rw-r--r--  2.0 unx     1341 b- defN 24-May-01 19:19 snowflake/ml/_internal/human_readable_id/hrid_generator.py
--rw-r--r--  2.0 unx     4519 b- defN 24-May-01 19:19 snowflake/ml/_internal/human_readable_id/hrid_generator_base.py
--rw-r--r--  2.0 unx      214 b- defN 24-May-01 19:19 snowflake/ml/_internal/lineage/data_source.py
--rw-r--r--  2.0 unx     1712 b- defN 24-May-01 19:19 snowflake/ml/_internal/lineage/dataset_dataframe.py
--rw-r--r--  2.0 unx     3676 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/formatting.py
--rw-r--r--  2.0 unx    10925 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/identifier.py
--rw-r--r--  2.0 unx     2068 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/import_utils.py
--rw-r--r--  2.0 unx     1001 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/log_stream_processor.py
--rw-r--r--  2.0 unx     4534 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/parallelize.py
--rw-r--r--  2.0 unx     5857 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/pkg_version_utils.py
--rw-r--r--  2.0 unx    10668 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/query_result_checker.py
--rw-r--r--  2.0 unx     2405 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/result.py
--rw-r--r--  2.0 unx     1321 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/retryable_http.py
--rw-r--r--  2.0 unx     1727 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/session_token_manager.py
--rw-r--r--  2.0 unx     2927 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/snowflake_env.py
--rw-r--r--  2.0 unx     5403 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/snowpark_dataframe_utils.py
--rw-r--r--  2.0 unx     4292 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/spcs_attribution_utils.py
--rw-r--r--  2.0 unx     3270 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/sql_identifier.py
--rw-r--r--  2.0 unx     4384 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/table_manager.py
--rw-r--r--  2.0 unx     1402 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/temp_file_utils.py
--rw-r--r--  2.0 unx     2828 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/uri.py
--rw-r--r--  2.0 unx      236 b- defN 24-May-01 19:19 snowflake/ml/dataset/__init__.py
--rw-r--r--  2.0 unx    20863 b- defN 24-May-01 19:19 snowflake/ml/dataset/dataset.py
--rw-r--r--  2.0 unx     1712 b- defN 24-May-01 19:19 snowflake/ml/dataset/dataset_factory.py
--rw-r--r--  2.0 unx     3992 b- defN 24-May-01 19:19 snowflake/ml/dataset/dataset_metadata.py
--rw-r--r--  2.0 unx     8205 b- defN 24-May-01 19:19 snowflake/ml/dataset/dataset_reader.py
--rw-r--r--  2.0 unx      298 b- defN 24-May-01 19:19 snowflake/ml/feature_store/__init__.py
--rw-r--r--  2.0 unx     3378 b- defN 24-May-01 19:19 snowflake/ml/feature_store/entity.py
--rw-r--r--  2.0 unx    76062 b- defN 24-May-01 19:19 snowflake/ml/feature_store/feature_store.py
--rw-r--r--  2.0 unx    18624 b- defN 24-May-01 19:19 snowflake/ml/feature_store/feature_view.py
--rw-r--r--  2.0 unx     5740 b- defN 24-May-01 19:19 snowflake/ml/fileset/embedded_stage_fs.py
--rw-r--r--  2.0 unx    26201 b- defN 24-May-01 19:19 snowflake/ml/fileset/fileset.py
--rw-r--r--  2.0 unx     6884 b- defN 24-May-01 19:19 snowflake/ml/fileset/parquet_parser.py
--rw-r--r--  2.0 unx    15344 b- defN 24-May-01 19:19 snowflake/ml/fileset/sfcfs.py
--rw-r--r--  2.0 unx     6914 b- defN 24-May-01 19:19 snowflake/ml/fileset/snowfs.py
--rw-r--r--  2.0 unx    18408 b- defN 24-May-01 19:19 snowflake/ml/fileset/stage_fs.py
--rw-r--r--  2.0 unx     3849 b- defN 24-May-01 19:19 snowflake/ml/fileset/tf_dataset.py
--rw-r--r--  2.0 unx     2386 b- defN 24-May-01 19:19 snowflake/ml/fileset/torch_datapipe.py
--rw-r--r--  2.0 unx      393 b- defN 24-May-01 19:19 snowflake/ml/model/__init__.py
--rw-r--r--  2.0 unx    22329 b- defN 24-May-01 19:19 snowflake/ml/model/_api.py
--rw-r--r--  2.0 unx     8267 b- defN 24-May-01 19:19 snowflake/ml/model/custom_model.py
--rw-r--r--  2.0 unx      144 b- defN 24-May-01 19:19 snowflake/ml/model/deploy_platforms.py
--rw-r--r--  2.0 unx    29342 b- defN 24-May-01 19:19 snowflake/ml/model/model_signature.py
--rw-r--r--  2.0 unx    12705 b- defN 24-May-01 19:19 snowflake/ml/model/type_hints.py
--rw-r--r--  2.0 unx    13623 b- defN 24-May-01 19:19 snowflake/ml/model/_client/model/model_impl.py
--rw-r--r--  2.0 unx    17320 b- defN 24-May-01 19:19 snowflake/ml/model/_client/model/model_version_impl.py
--rw-r--r--  2.0 unx     4094 b- defN 24-May-01 19:19 snowflake/ml/model/_client/ops/metadata_ops.py
--rw-r--r--  2.0 unx    23490 b- defN 24-May-01 19:19 snowflake/ml/model/_client/ops/model_ops.py
--rw-r--r--  2.0 unx     5687 b- defN 24-May-01 19:19 snowflake/ml/model/_client/sql/model.py
--rw-r--r--  2.0 unx    14290 b- defN 24-May-01 19:19 snowflake/ml/model/_client/sql/model_version.py
--rw-r--r--  2.0 unx     1497 b- defN 24-May-01 19:19 snowflake/ml/model/_client/sql/stage.py
--rw-r--r--  2.0 unx     4646 b- defN 24-May-01 19:19 snowflake/ml/model/_client/sql/tag.py
--rw-r--r--  2.0 unx      302 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/image_builds/base_image_builder.py
--rw-r--r--  2.0 unx    10681 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/image_builds/client_image_builder.py
--rw-r--r--  2.0 unx     6129 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/image_builds/docker_context.py
--rw-r--r--  2.0 unx     1578 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/image_builds/gunicorn_run.sh
--rw-r--r--  2.0 unx    10095 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/image_builds/server_image_builder.py
--rw-r--r--  2.0 unx    11148 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/image_builds/inference_server/main.py
--rw-r--r--  2.0 unx     1783 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/image_builds/templates/dockerfile_template
--rw-r--r--  2.0 unx     1225 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/image_builds/templates/image_build_job_spec_template
--rw-r--r--  2.0 unx     3633 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/image_builds/templates/kaniko_shell_script_template
--rw-r--r--  2.0 unx    29286 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/snowservice/deploy.py
--rw-r--r--  2.0 unx     5989 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/snowservice/deploy_options.py
--rw-r--r--  2.0 unx      232 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/snowservice/instance_types.py
--rw-r--r--  2.0 unx      734 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/snowservice/templates/service_spec_template
--rw-r--r--  2.0 unx      540 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/snowservice/templates/service_spec_template_with_model
--rw-r--r--  2.0 unx     1696 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/utils/constants.py
--rw-r--r--  2.0 unx    11671 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/utils/snowservice_client.py
--rw-r--r--  2.0 unx     8358 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/warehouse/deploy.py
--rw-r--r--  2.0 unx     3011 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/warehouse/infer_template.py
--rw-r--r--  2.0 unx     7325 b- defN 24-May-01 19:19 snowflake/ml/model/_model_composer/model_composer.py
--rw-r--r--  2.0 unx     5667 b- defN 24-May-01 19:19 snowflake/ml/model/_model_composer/model_manifest/model_manifest.py
--rw-r--r--  2.0 unx     2530 b- defN 24-May-01 19:19 snowflake/ml/model/_model_composer/model_manifest/model_manifest_schema.py
--rw-r--r--  2.0 unx     1837 b- defN 24-May-01 19:19 snowflake/ml/model/_model_composer/model_method/function_generator.py
--rw-r--r--  2.0 unx     2291 b- defN 24-May-01 19:19 snowflake/ml/model/_model_composer/model_method/infer_function.py_template
--rw-r--r--  2.0 unx     2170 b- defN 24-May-01 19:19 snowflake/ml/model/_model_composer/model_method/infer_table_function.py_template
--rw-r--r--  2.0 unx     6682 b- defN 24-May-01 19:19 snowflake/ml/model/_model_composer/model_method/model_method.py
--rw-r--r--  2.0 unx     2642 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handler.py
--rw-r--r--  2.0 unx     5836 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_packager.py
--rw-r--r--  2.0 unx    18312 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_env/model_env.py
--rw-r--r--  2.0 unx     6033 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/_base.py
--rw-r--r--  2.0 unx     2622 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/_utils.py
--rw-r--r--  2.0 unx     8142 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/catboost.py
--rw-r--r--  2.0 unx     7325 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/custom.py
--rw-r--r--  2.0 unx    19985 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/huggingface_pipeline.py
--rw-r--r--  2.0 unx     8445 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/lightgbm.py
--rw-r--r--  2.0 unx    10772 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/llm.py
--rw-r--r--  2.0 unx     8993 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/mlflow.py
--rw-r--r--  2.0 unx     8033 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/pytorch.py
--rw-r--r--  2.0 unx     9051 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/sentence_transformers.py
--rw-r--r--  2.0 unx     8218 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/sklearn.py
--rw-r--r--  2.0 unx     7967 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/snowmlmodel.py
--rw-r--r--  2.0 unx     8179 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/tensorflow.py
--rw-r--r--  2.0 unx     8104 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/torchscript.py
--rw-r--r--  2.0 unx     8874 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/xgboost.py
--rw-r--r--  2.0 unx     1409 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers_migrator/base_migrator.py
--rwxr-xr-x  2.0 unx      274 b- defN 24-May-01 19:20 snowflake/ml/model/_packager/model_meta/_core_requirements.py
--rwxr-xr-x  2.0 unx       44 b- defN 24-May-01 19:20 snowflake/ml/model/_packager/model_meta/_packaging_requirements.py
--rw-r--r--  2.0 unx     1818 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_meta/model_blob_meta.py
--rw-r--r--  2.0 unx    17203 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_meta/model_meta.py
--rw-r--r--  2.0 unx     2380 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_meta/model_meta_schema.py
--rw-r--r--  2.0 unx     1311 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_meta_migrator/base_migrator.py
--rw-r--r--  2.0 unx     1041 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_meta_migrator/migrator_plans.py
--rw-r--r--  2.0 unx     2070 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_meta_migrator/migrator_v1.py
--rwxr-xr-x  2.0 unx      248 b- defN 24-May-01 19:20 snowflake/ml/model/_packager/model_runtime/_snowml_inference_alternative_requirements.py
--rw-r--r--  2.0 unx     5872 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_runtime/model_runtime.py
--rw-r--r--  2.0 unx     1304 b- defN 24-May-01 19:19 snowflake/ml/model/_signatures/base_handler.py
--rw-r--r--  2.0 unx     2706 b- defN 24-May-01 19:19 snowflake/ml/model/_signatures/builtins_handler.py
--rw-r--r--  2.0 unx    17972 b- defN 24-May-01 19:19 snowflake/ml/model/_signatures/core.py
--rw-r--r--  2.0 unx     5858 b- defN 24-May-01 19:19 snowflake/ml/model/_signatures/numpy_handler.py
--rw-r--r--  2.0 unx     8057 b- defN 24-May-01 19:19 snowflake/ml/model/_signatures/pandas_handler.py
--rw-r--r--  2.0 unx     4568 b- defN 24-May-01 19:19 snowflake/ml/model/_signatures/pytorch_handler.py
--rw-r--r--  2.0 unx     6036 b- defN 24-May-01 19:19 snowflake/ml/model/_signatures/snowpark_handler.py
--rw-r--r--  2.0 unx     6082 b- defN 24-May-01 19:19 snowflake/ml/model/_signatures/tensorflow_handler.py
--rw-r--r--  2.0 unx    12687 b- defN 24-May-01 19:19 snowflake/ml/model/_signatures/utils.py
--rw-r--r--  2.0 unx    10221 b- defN 24-May-01 19:19 snowflake/ml/model/models/huggingface_pipeline.py
--rw-r--r--  2.0 unx     3616 b- defN 24-May-01 19:19 snowflake/ml/model/models/llm.py
--rw-r--r--  2.0 unx       45 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/constants.py
--rw-r--r--  2.0 unx     9215 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/estimator_utils.py
--rw-r--r--  2.0 unx     4801 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/model_specifications.py
--rw-r--r--  2.0 unx      923 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/model_trainer.py
--rw-r--r--  2.0 unx     8422 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/model_trainer_builder.py
--rw-r--r--  2.0 unx     3364 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/model_transformer_builder.py
--rw-r--r--  2.0 unx     6482 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/transformer_protocols.py
--rw-r--r--  2.0 unx     7831 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/local_implementations/pandas_handlers.py
--rw-r--r--  2.0 unx     5737 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/local_implementations/pandas_trainer.py
--rw-r--r--  2.0 unx     5544 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/ml_runtime_implementations/ml_runtime_handlers.py
--rw-r--r--  2.0 unx     2535 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/ml_runtime_implementations/ml_runtime_trainer.py
--rw-r--r--  2.0 unx    54368 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/snowpark_implementations/distributed_hpo_trainer.py
--rw-r--r--  2.0 unx    14539 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_handlers.py
--rw-r--r--  2.0 unx    31819 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_trainer.py
--rw-r--r--  2.0 unx    17208 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/snowpark_implementations/xgboost_external_memory_trainer.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/calibration/__init__.py
--rwxr-xr-x  2.0 unx    51278 b- defN 24-May-01 19:20 snowflake/ml/modeling/calibration/calibrated_classifier_cv.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/__init__.py
--rwxr-xr-x  2.0 unx    49107 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/affinity_propagation.py
--rwxr-xr-x  2.0 unx    51144 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/agglomerative_clustering.py
--rwxr-xr-x  2.0 unx    49034 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/birch.py
--rwxr-xr-x  2.0 unx    51793 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/bisecting_k_means.py
--rwxr-xr-x  2.0 unx    49194 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/dbscan.py
--rwxr-xr-x  2.0 unx    51909 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/feature_agglomeration.py
--rwxr-xr-x  2.0 unx    51322 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/k_means.py
--rwxr-xr-x  2.0 unx    49404 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/mean_shift.py
--rwxr-xr-x  2.0 unx    52711 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/mini_batch_k_means.py
--rwxr-xr-x  2.0 unx    52508 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/optics.py
--rwxr-xr-x  2.0 unx    49403 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/spectral_biclustering.py
--rwxr-xr-x  2.0 unx    52595 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/spectral_clustering.py
--rwxr-xr-x  2.0 unx    48536 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/spectral_coclustering.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/compose/__init__.py
--rwxr-xr-x  2.0 unx    51374 b- defN 24-May-01 19:20 snowflake/ml/modeling/compose/column_transformer.py
--rwxr-xr-x  2.0 unx    49090 b- defN 24-May-01 19:20 snowflake/ml/modeling/compose/transformed_target_regressor.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/covariance/__init__.py
--rwxr-xr-x  2.0 unx    49428 b- defN 24-May-01 19:20 snowflake/ml/modeling/covariance/elliptic_envelope.py
--rwxr-xr-x  2.0 unx    47236 b- defN 24-May-01 19:20 snowflake/ml/modeling/covariance/empirical_covariance.py
--rwxr-xr-x  2.0 unx    49100 b- defN 24-May-01 19:20 snowflake/ml/modeling/covariance/graphical_lasso.py
--rwxr-xr-x  2.0 unx    50265 b- defN 24-May-01 19:20 snowflake/ml/modeling/covariance/graphical_lasso_cv.py
--rwxr-xr-x  2.0 unx    47374 b- defN 24-May-01 19:20 snowflake/ml/modeling/covariance/ledoit_wolf.py
--rwxr-xr-x  2.0 unx    48129 b- defN 24-May-01 19:20 snowflake/ml/modeling/covariance/min_cov_det.py
--rwxr-xr-x  2.0 unx    47015 b- defN 24-May-01 19:20 snowflake/ml/modeling/covariance/oas.py
--rwxr-xr-x  2.0 unx    47391 b- defN 24-May-01 19:20 snowflake/ml/modeling/covariance/shrunk_covariance.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/decomposition/__init__.py
--rwxr-xr-x  2.0 unx    52388 b- defN 24-May-01 19:20 snowflake/ml/modeling/decomposition/dictionary_learning.py
--rwxr-xr-x  2.0 unx    50051 b- defN 24-May-01 19:20 snowflake/ml/modeling/decomposition/factor_analysis.py
--rwxr-xr-x  2.0 unx    49985 b- defN 24-May-01 19:20 snowflake/ml/modeling/decomposition/fast_ica.py
--rwxr-xr-x  2.0 unx    48345 b- defN 24-May-01 19:20 snowflake/ml/modeling/decomposition/incremental_pca.py
--rwxr-xr-x  2.0 unx    52343 b- defN 24-May-01 19:20 snowflake/ml/modeling/decomposition/kernel_pca.py
--rwxr-xr-x  2.0 unx    53429 b- defN 24-May-01 19:20 snowflake/ml/modeling/decomposition/mini_batch_dictionary_learning.py
--rwxr-xr-x  2.0 unx    50694 b- defN 24-May-01 19:20 snowflake/ml/modeling/decomposition/mini_batch_sparse_pca.py
--rwxr-xr-x  2.0 unx    51612 b- defN 24-May-01 19:20 snowflake/ml/modeling/decomposition/pca.py
--rwxr-xr-x  2.0 unx    49499 b- defN 24-May-01 19:20 snowflake/ml/modeling/decomposition/sparse_pca.py
--rwxr-xr-x  2.0 unx    49122 b- defN 24-May-01 19:20 snowflake/ml/modeling/decomposition/truncated_svd.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/discriminant_analysis/__init__.py
--rwxr-xr-x  2.0 unx    51877 b- defN 24-May-01 19:20 snowflake/ml/modeling/discriminant_analysis/linear_discriminant_analysis.py
--rwxr-xr-x  2.0 unx    49659 b- defN 24-May-01 19:20 snowflake/ml/modeling/discriminant_analysis/quadratic_discriminant_analysis.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/__init__.py
--rwxr-xr-x  2.0 unx    50477 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/ada_boost_classifier.py
--rwxr-xr-x  2.0 unx    49368 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/ada_boost_regressor.py
--rwxr-xr-x  2.0 unx    51388 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/bagging_classifier.py
--rwxr-xr-x  2.0 unx    50624 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/bagging_regressor.py
--rwxr-xr-x  2.0 unx    56308 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/extra_trees_classifier.py
--rwxr-xr-x  2.0 unx    54912 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/extra_trees_regressor.py
--rwxr-xr-x  2.0 unx    57763 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/gradient_boosting_classifier.py
--rwxr-xr-x  2.0 unx    57356 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/gradient_boosting_regressor.py
--rwxr-xr-x  2.0 unx    57597 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/hist_gradient_boosting_classifier.py
--rwxr-xr-x  2.0 unx    56082 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/hist_gradient_boosting_regressor.py
--rwxr-xr-x  2.0 unx    50576 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/isolation_forest.py
--rwxr-xr-x  2.0 unx    56291 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/random_forest_classifier.py
--rwxr-xr-x  2.0 unx    54883 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/random_forest_regressor.py
--rwxr-xr-x  2.0 unx    50602 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/stacking_regressor.py
--rwxr-xr-x  2.0 unx    50155 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/voting_classifier.py
--rwxr-xr-x  2.0 unx    48681 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/voting_regressor.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/feature_selection/__init__.py
--rwxr-xr-x  2.0 unx    48018 b- defN 24-May-01 19:20 snowflake/ml/modeling/feature_selection/generic_univariate_select.py
--rwxr-xr-x  2.0 unx    47619 b- defN 24-May-01 19:20 snowflake/ml/modeling/feature_selection/select_fdr.py
--rwxr-xr-x  2.0 unx    47613 b- defN 24-May-01 19:20 snowflake/ml/modeling/feature_selection/select_fpr.py
--rwxr-xr-x  2.0 unx    47621 b- defN 24-May-01 19:20 snowflake/ml/modeling/feature_selection/select_fwe.py
--rwxr-xr-x  2.0 unx    47712 b- defN 24-May-01 19:20 snowflake/ml/modeling/feature_selection/select_k_best.py
--rwxr-xr-x  2.0 unx    47767 b- defN 24-May-01 19:20 snowflake/ml/modeling/feature_selection/select_percentile.py
--rwxr-xr-x  2.0 unx    50387 b- defN 24-May-01 19:20 snowflake/ml/modeling/feature_selection/sequential_feature_selector.py
--rwxr-xr-x  2.0 unx    47323 b- defN 24-May-01 19:20 snowflake/ml/modeling/feature_selection/variance_threshold.py
--rw-r--r--  2.0 unx    10203 b- defN 24-May-01 19:19 snowflake/ml/modeling/framework/_utils.py
--rw-r--r--  2.0 unx    31359 b- defN 24-May-01 19:19 snowflake/ml/modeling/framework/base.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/gaussian_process/__init__.py
--rwxr-xr-x  2.0 unx    53029 b- defN 24-May-01 19:20 snowflake/ml/modeling/gaussian_process/gaussian_process_classifier.py
--rwxr-xr-x  2.0 unx    52094 b- defN 24-May-01 19:20 snowflake/ml/modeling/gaussian_process/gaussian_process_regressor.py
--rw-r--r--  2.0 unx      298 b- defN 24-May-01 19:19 snowflake/ml/modeling/impute/__init__.py
--rwxr-xr-x  2.0 unx    53847 b- defN 24-May-01 19:20 snowflake/ml/modeling/impute/iterative_imputer.py
--rwxr-xr-x  2.0 unx    49563 b- defN 24-May-01 19:20 snowflake/ml/modeling/impute/knn_imputer.py
--rwxr-xr-x  2.0 unx    48420 b- defN 24-May-01 19:20 snowflake/ml/modeling/impute/missing_indicator.py
--rw-r--r--  2.0 unx    18499 b- defN 24-May-01 19:19 snowflake/ml/modeling/impute/simple_imputer.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/kernel_approximation/__init__.py
--rwxr-xr-x  2.0 unx    47451 b- defN 24-May-01 19:20 snowflake/ml/modeling/kernel_approximation/additive_chi2_sampler.py
--rwxr-xr-x  2.0 unx    49222 b- defN 24-May-01 19:20 snowflake/ml/modeling/kernel_approximation/nystroem.py
--rwxr-xr-x  2.0 unx    48470 b- defN 24-May-01 19:20 snowflake/ml/modeling/kernel_approximation/polynomial_count_sketch.py
--rwxr-xr-x  2.0 unx    47824 b- defN 24-May-01 19:20 snowflake/ml/modeling/kernel_approximation/rbf_sampler.py
--rwxr-xr-x  2.0 unx    47872 b- defN 24-May-01 19:20 snowflake/ml/modeling/kernel_approximation/skewed_chi2_sampler.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/kernel_ridge/__init__.py
--rwxr-xr-x  2.0 unx    49408 b- defN 24-May-01 19:20 snowflake/ml/modeling/kernel_ridge/kernel_ridge.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/lightgbm/__init__.py
--rwxr-xr-x  2.0 unx    48976 b- defN 24-May-01 19:20 snowflake/ml/modeling/lightgbm/lgbm_classifier.py
--rwxr-xr-x  2.0 unx    48479 b- defN 24-May-01 19:20 snowflake/ml/modeling/lightgbm/lgbm_regressor.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/__init__.py
--rwxr-xr-x  2.0 unx    49353 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/ard_regression.py
--rwxr-xr-x  2.0 unx    49769 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/bayesian_ridge.py
--rwxr-xr-x  2.0 unx    50341 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/elastic_net.py
--rwxr-xr-x  2.0 unx    51609 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/elastic_net_cv.py
--rwxr-xr-x  2.0 unx    49421 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/gamma_regressor.py
--rwxr-xr-x  2.0 unx    48618 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/huber_regressor.py
--rwxr-xr-x  2.0 unx    49841 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/lars.py
--rwxr-xr-x  2.0 unx    50062 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/lars_cv.py
--rwxr-xr-x  2.0 unx    49947 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/lasso.py
--rwxr-xr-x  2.0 unx    50732 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/lasso_cv.py
--rwxr-xr-x  2.0 unx    50977 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/lasso_lars.py
--rwxr-xr-x  2.0 unx    50938 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/lasso_lars_cv.py
--rwxr-xr-x  2.0 unx    50284 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/lasso_lars_ic.py
--rwxr-xr-x  2.0 unx    48160 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/linear_regression.py
--rwxr-xr-x  2.0 unx    54415 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/logistic_regression.py
--rwxr-xr-x  2.0 unx    55455 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/logistic_regression_cv.py
--rwxr-xr-x  2.0 unx    49632 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/multi_task_elastic_net.py
--rwxr-xr-x  2.0 unx    51270 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/multi_task_elastic_net_cv.py
--rwxr-xr-x  2.0 unx    49180 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/multi_task_lasso.py
--rwxr-xr-x  2.0 unx    50442 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/multi_task_lasso_cv.py
--rwxr-xr-x  2.0 unx    48787 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/orthogonal_matching_pursuit.py
--rwxr-xr-x  2.0 unx    52143 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/passive_aggressive_classifier.py
--rwxr-xr-x  2.0 unx    51210 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/passive_aggressive_regressor.py
--rwxr-xr-x  2.0 unx    51527 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/perceptron.py
--rwxr-xr-x  2.0 unx    49466 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/poisson_regressor.py
--rwxr-xr-x  2.0 unx    52585 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/ransac_regressor.py
--rwxr-xr-x  2.0 unx    51491 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/ridge.py
--rwxr-xr-x  2.0 unx    51879 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/ridge_classifier.py
--rwxr-xr-x  2.0 unx    49874 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/ridge_classifier_cv.py
--rwxr-xr-x  2.0 unx    50575 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/ridge_cv.py
--rwxr-xr-x  2.0 unx    56952 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/sgd_classifier.py
--rwxr-xr-x  2.0 unx    51750 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/sgd_one_class_svm.py
--rwxr-xr-x  2.0 unx    54423 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/sgd_regressor.py
--rwxr-xr-x  2.0 unx    49901 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/theil_sen_regressor.py
--rwxr-xr-x  2.0 unx    50857 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/tweedie_regressor.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/manifold/__init__.py
--rwxr-xr-x  2.0 unx    50084 b- defN 24-May-01 19:20 snowflake/ml/modeling/manifold/isomap.py
--rwxr-xr-x  2.0 unx    49304 b- defN 24-May-01 19:20 snowflake/ml/modeling/manifold/mds.py
--rwxr-xr-x  2.0 unx    50155 b- defN 24-May-01 19:20 snowflake/ml/modeling/manifold/spectral_embedding.py
--rwxr-xr-x  2.0 unx    53094 b- defN 24-May-01 19:20 snowflake/ml/modeling/manifold/tsne.py
--rw-r--r--  2.0 unx      524 b- defN 24-May-01 19:19 snowflake/ml/modeling/metrics/__init__.py
--rw-r--r--  2.0 unx    66499 b- defN 24-May-01 19:19 snowflake/ml/modeling/metrics/classification.py
--rw-r--r--  2.0 unx     4803 b- defN 24-May-01 19:19 snowflake/ml/modeling/metrics/correlation.py
--rw-r--r--  2.0 unx     4672 b- defN 24-May-01 19:19 snowflake/ml/modeling/metrics/covariance.py
--rw-r--r--  2.0 unx    13114 b- defN 24-May-01 19:19 snowflake/ml/modeling/metrics/metrics_utils.py
--rw-r--r--  2.0 unx    17569 b- defN 24-May-01 19:19 snowflake/ml/modeling/metrics/ranking.py
--rw-r--r--  2.0 unx    25845 b- defN 24-May-01 19:19 snowflake/ml/modeling/metrics/regression.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/mixture/__init__.py
--rwxr-xr-x  2.0 unx    54657 b- defN 24-May-01 19:20 snowflake/ml/modeling/mixture/bayesian_gaussian_mixture.py
--rwxr-xr-x  2.0 unx    52558 b- defN 24-May-01 19:20 snowflake/ml/modeling/mixture/gaussian_mixture.py
--rw-r--r--  2.0 unx      298 b- defN 24-May-01 19:19 snowflake/ml/modeling/model_selection/__init__.py
--rw-r--r--  2.0 unx    38125 b- defN 24-May-01 19:19 snowflake/ml/modeling/model_selection/grid_search_cv.py
--rw-r--r--  2.0 unx    38867 b- defN 24-May-01 19:19 snowflake/ml/modeling/model_selection/randomized_search_cv.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/multiclass/__init__.py
--rwxr-xr-x  2.0 unx    48160 b- defN 24-May-01 19:20 snowflake/ml/modeling/multiclass/one_vs_one_classifier.py
--rwxr-xr-x  2.0 unx    49094 b- defN 24-May-01 19:20 snowflake/ml/modeling/multiclass/one_vs_rest_classifier.py
--rwxr-xr-x  2.0 unx    48430 b- defN 24-May-01 19:20 snowflake/ml/modeling/multiclass/output_code_classifier.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/naive_bayes/__init__.py
--rwxr-xr-x  2.0 unx    48699 b- defN 24-May-01 19:20 snowflake/ml/modeling/naive_bayes/bernoulli_nb.py
--rwxr-xr-x  2.0 unx    49034 b- defN 24-May-01 19:20 snowflake/ml/modeling/naive_bayes/categorical_nb.py
--rwxr-xr-x  2.0 unx    48714 b- defN 24-May-01 19:20 snowflake/ml/modeling/naive_bayes/complement_nb.py
--rwxr-xr-x  2.0 unx    47843 b- defN 24-May-01 19:20 snowflake/ml/modeling/naive_bayes/gaussian_nb.py
--rwxr-xr-x  2.0 unx    48479 b- defN 24-May-01 19:20 snowflake/ml/modeling/naive_bayes/multinomial_nb.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/neighbors/__init__.py
--rwxr-xr-x  2.0 unx    51548 b- defN 24-May-01 19:20 snowflake/ml/modeling/neighbors/k_neighbors_classifier.py
--rwxr-xr-x  2.0 unx    51019 b- defN 24-May-01 19:20 snowflake/ml/modeling/neighbors/k_neighbors_regressor.py
--rwxr-xr-x  2.0 unx    49377 b- defN 24-May-01 19:20 snowflake/ml/modeling/neighbors/kernel_density.py
--rwxr-xr-x  2.0 unx    51955 b- defN 24-May-01 19:20 snowflake/ml/modeling/neighbors/local_outlier_factor.py
--rwxr-xr-x  2.0 unx    48037 b- defN 24-May-01 19:20 snowflake/ml/modeling/neighbors/nearest_centroid.py
--rwxr-xr-x  2.0 unx    49846 b- defN 24-May-01 19:20 snowflake/ml/modeling/neighbors/nearest_neighbors.py
--rwxr-xr-x  2.0 unx    51509 b- defN 24-May-01 19:20 snowflake/ml/modeling/neighbors/neighborhood_components_analysis.py
--rwxr-xr-x  2.0 unx    51961 b- defN 24-May-01 19:20 snowflake/ml/modeling/neighbors/radius_neighbors_classifier.py
--rwxr-xr-x  2.0 unx    50842 b- defN 24-May-01 19:20 snowflake/ml/modeling/neighbors/radius_neighbors_regressor.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/neural_network/__init__.py
--rwxr-xr-x  2.0 unx    48585 b- defN 24-May-01 19:20 snowflake/ml/modeling/neural_network/bernoulli_rbm.py
--rwxr-xr-x  2.0 unx    55921 b- defN 24-May-01 19:20 snowflake/ml/modeling/neural_network/mlp_classifier.py
--rwxr-xr-x  2.0 unx    55190 b- defN 24-May-01 19:20 snowflake/ml/modeling/neural_network/mlp_regressor.py
--rw-r--r--  2.0 unx      221 b- defN 24-May-01 19:19 snowflake/ml/modeling/parameters/disable_distributed_hpo.py
--rw-r--r--  2.0 unx      298 b- defN 24-May-01 19:19 snowflake/ml/modeling/pipeline/__init__.py
--rw-r--r--  2.0 unx    45360 b- defN 24-May-01 19:19 snowflake/ml/modeling/pipeline/pipeline.py
--rw-r--r--  2.0 unx      298 b- defN 24-May-01 19:19 snowflake/ml/modeling/preprocessing/__init__.py
--rw-r--r--  2.0 unx     6999 b- defN 24-May-01 19:19 snowflake/ml/modeling/preprocessing/binarizer.py
--rw-r--r--  2.0 unx    20970 b- defN 24-May-01 19:19 snowflake/ml/modeling/preprocessing/k_bins_discretizer.py
--rw-r--r--  2.0 unx     7405 b- defN 24-May-01 19:19 snowflake/ml/modeling/preprocessing/label_encoder.py
--rw-r--r--  2.0 unx     8768 b- defN 24-May-01 19:19 snowflake/ml/modeling/preprocessing/max_abs_scaler.py
--rw-r--r--  2.0 unx    12200 b- defN 24-May-01 19:19 snowflake/ml/modeling/preprocessing/min_max_scaler.py
--rw-r--r--  2.0 unx     6584 b- defN 24-May-01 19:19 snowflake/ml/modeling/preprocessing/normalizer.py
--rw-r--r--  2.0 unx    72322 b- defN 24-May-01 19:19 snowflake/ml/modeling/preprocessing/one_hot_encoder.py
--rw-r--r--  2.0 unx    33276 b- defN 24-May-01 19:19 snowflake/ml/modeling/preprocessing/ordinal_encoder.py
--rwxr-xr-x  2.0 unx    48511 b- defN 24-May-01 19:20 snowflake/ml/modeling/preprocessing/polynomial_features.py
--rw-r--r--  2.0 unx    12398 b- defN 24-May-01 19:19 snowflake/ml/modeling/preprocessing/robust_scaler.py
--rw-r--r--  2.0 unx    11395 b- defN 24-May-01 19:19 snowflake/ml/modeling/preprocessing/standard_scaler.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/semi_supervised/__init__.py
--rwxr-xr-x  2.0 unx    48936 b- defN 24-May-01 19:20 snowflake/ml/modeling/semi_supervised/label_propagation.py
--rwxr-xr-x  2.0 unx    49285 b- defN 24-May-01 19:20 snowflake/ml/modeling/semi_supervised/label_spreading.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/svm/__init__.py
--rwxr-xr-x  2.0 unx    51746 b- defN 24-May-01 19:20 snowflake/ml/modeling/svm/linear_svc.py
--rwxr-xr-x  2.0 unx    50099 b- defN 24-May-01 19:20 snowflake/ml/modeling/svm/linear_svr.py
--rwxr-xr-x  2.0 unx    52058 b- defN 24-May-01 19:20 snowflake/ml/modeling/svm/nu_svc.py
--rwxr-xr-x  2.0 unx    49137 b- defN 24-May-01 19:20 snowflake/ml/modeling/svm/nu_svr.py
--rwxr-xr-x  2.0 unx    52207 b- defN 24-May-01 19:20 snowflake/ml/modeling/svm/svc.py
--rwxr-xr-x  2.0 unx    49326 b- defN 24-May-01 19:20 snowflake/ml/modeling/svm/svr.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/tree/__init__.py
--rwxr-xr-x  2.0 unx    54483 b- defN 24-May-01 19:20 snowflake/ml/modeling/tree/decision_tree_classifier.py
--rwxr-xr-x  2.0 unx    53182 b- defN 24-May-01 19:20 snowflake/ml/modeling/tree/decision_tree_regressor.py
--rwxr-xr-x  2.0 unx    53825 b- defN 24-May-01 19:20 snowflake/ml/modeling/tree/extra_tree_classifier.py
--rwxr-xr-x  2.0 unx    52533 b- defN 24-May-01 19:20 snowflake/ml/modeling/tree/extra_tree_regressor.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/xgboost/__init__.py
--rwxr-xr-x  2.0 unx    59481 b- defN 24-May-01 19:20 snowflake/ml/modeling/xgboost/xgb_classifier.py
--rwxr-xr-x  2.0 unx    58980 b- defN 24-May-01 19:20 snowflake/ml/modeling/xgboost/xgb_regressor.py
--rwxr-xr-x  2.0 unx    59657 b- defN 24-May-01 19:20 snowflake/ml/modeling/xgboost/xgbrf_classifier.py
--rwxr-xr-x  2.0 unx    59183 b- defN 24-May-01 19:20 snowflake/ml/modeling/xgboost/xgbrf_regressor.py
--rw-r--r--  2.0 unx     7135 b- defN 24-May-01 19:19 snowflake/ml/monitoring/monitor.py
--rw-r--r--  2.0 unx     3597 b- defN 24-May-01 19:19 snowflake/ml/monitoring/shap.py
--rw-r--r--  2.0 unx       76 b- defN 24-May-01 19:19 snowflake/ml/registry/__init__.py
--rw-r--r--  2.0 unx     5316 b- defN 24-May-01 19:19 snowflake/ml/registry/_initial_schema.py
--rw-r--r--  2.0 unx     3166 b- defN 24-May-01 19:19 snowflake/ml/registry/_schema.py
--rw-r--r--  2.0 unx     4209 b- defN 24-May-01 19:19 snowflake/ml/registry/_schema_upgrade_plans.py
--rw-r--r--  2.0 unx     6922 b- defN 24-May-01 19:19 snowflake/ml/registry/_schema_version_manager.py
--rw-r--r--  2.0 unx    84795 b- defN 24-May-01 19:19 snowflake/ml/registry/model_registry.py
--rw-r--r--  2.0 unx    10949 b- defN 24-May-01 19:19 snowflake/ml/registry/registry.py
--rw-r--r--  2.0 unx     5809 b- defN 24-May-01 19:19 snowflake/ml/registry/_manager/model_manager.py
--rw-r--r--  2.0 unx     8005 b- defN 24-May-01 19:19 snowflake/ml/utils/connection_params.py
--rw-r--r--  2.0 unx     3817 b- defN 24-May-01 19:19 snowflake/ml/utils/sparse.py
--rw-r--r--  2.0 unx    11365 b- defN 24-May-01 19:20 snowflake_ml_python-1.5.0.dist-info/LICENSE.txt
--rw-r--r--  2.0 unx    50050 b- defN 24-May-01 19:20 snowflake_ml_python-1.5.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-May-01 19:20 snowflake_ml_python-1.5.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       10 b- defN 24-May-01 19:20 snowflake_ml_python-1.5.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    41165 b- defN 24-May-01 19:20 snowflake_ml_python-1.5.0.dist-info/RECORD
-380 files, 9604806 bytes uncompressed, 1829392 bytes compressed:  81.0%
+Zip file size: 1904067 bytes, number of entries: 382
+-rw-r--r--  2.0 unx      360 b- defN 24-May-22 17:53 snowflake/cortex/__init__.py
+-rw-r--r--  2.0 unx     1226 b- defN 24-May-22 17:53 snowflake/cortex/_complete.py
+-rw-r--r--  2.0 unx     1358 b- defN 24-May-22 17:53 snowflake/cortex/_extract_answer.py
+-rw-r--r--  2.0 unx     1149 b- defN 24-May-22 17:53 snowflake/cortex/_sentiment.py
+-rw-r--r--  2.0 unx     1075 b- defN 24-May-22 17:53 snowflake/cortex/_summarize.py
+-rw-r--r--  2.0 unx     1420 b- defN 24-May-22 17:53 snowflake/cortex/_translate.py
+-rw-r--r--  2.0 unx     1557 b- defN 24-May-22 17:53 snowflake/cortex/_util.py
+-rwxr-xr-x  2.0 unx       16 b- defN 24-May-22 17:54 snowflake/ml/version.py
+-rw-r--r--  2.0 unx      161 b- defN 24-May-22 17:53 snowflake/ml/_internal/env.py
+-rw-r--r--  2.0 unx    27758 b- defN 24-May-22 17:53 snowflake/ml/_internal/env_utils.py
+-rw-r--r--  2.0 unx    13622 b- defN 24-May-22 17:53 snowflake/ml/_internal/file_utils.py
+-rw-r--r--  2.0 unx     2696 b- defN 24-May-22 17:53 snowflake/ml/_internal/init_utils.py
+-rw-r--r--  2.0 unx      161 b- defN 24-May-22 17:53 snowflake/ml/_internal/migrator_utils.py
+-rw-r--r--  2.0 unx    22820 b- defN 24-May-22 17:53 snowflake/ml/_internal/telemetry.py
+-rw-r--r--  2.0 unx     2168 b- defN 24-May-22 17:53 snowflake/ml/_internal/type_utils.py
+-rw-r--r--  2.0 unx     3291 b- defN 24-May-22 17:53 snowflake/ml/_internal/container_services/image_registry/credential.py
+-rw-r--r--  2.0 unx     5214 b- defN 24-May-22 17:53 snowflake/ml/_internal/container_services/image_registry/http_client.py
+-rw-r--r--  2.0 unx    14537 b- defN 24-May-22 17:53 snowflake/ml/_internal/container_services/image_registry/imagelib.py
+-rw-r--r--  2.0 unx     9115 b- defN 24-May-22 17:53 snowflake/ml/_internal/container_services/image_registry/registry_client.py
+-rw-r--r--  2.0 unx      285 b- defN 24-May-22 17:53 snowflake/ml/_internal/exceptions/dataset_error_messages.py
+-rw-r--r--  2.0 unx      762 b- defN 24-May-22 17:53 snowflake/ml/_internal/exceptions/dataset_errors.py
+-rw-r--r--  2.0 unx     5453 b- defN 24-May-22 17:53 snowflake/ml/_internal/exceptions/error_codes.py
+-rw-r--r--  2.0 unx       47 b- defN 24-May-22 17:53 snowflake/ml/_internal/exceptions/error_messages.py
+-rw-r--r--  2.0 unx     1653 b- defN 24-May-22 17:53 snowflake/ml/_internal/exceptions/exceptions.py
+-rw-r--r--  2.0 unx      419 b- defN 24-May-22 17:53 snowflake/ml/_internal/exceptions/fileset_error_messages.py
+-rw-r--r--  2.0 unx     1040 b- defN 24-May-22 17:53 snowflake/ml/_internal/exceptions/fileset_errors.py
+-rw-r--r--  2.0 unx      665 b- defN 24-May-22 17:53 snowflake/ml/_internal/exceptions/modeling_error_messages.py
+-rw-r--r--  2.0 unx      804 b- defN 24-May-22 17:53 snowflake/ml/_internal/human_readable_id/adjectives.txt
+-rw-r--r--  2.0 unx      837 b- defN 24-May-22 17:53 snowflake/ml/_internal/human_readable_id/animals.txt
+-rw-r--r--  2.0 unx     1341 b- defN 24-May-22 17:53 snowflake/ml/_internal/human_readable_id/hrid_generator.py
+-rw-r--r--  2.0 unx     4519 b- defN 24-May-22 17:53 snowflake/ml/_internal/human_readable_id/hrid_generator_base.py
+-rw-r--r--  2.0 unx      214 b- defN 24-May-22 17:53 snowflake/ml/_internal/lineage/data_source.py
+-rw-r--r--  2.0 unx     3007 b- defN 24-May-22 17:53 snowflake/ml/_internal/lineage/lineage_utils.py
+-rw-r--r--  2.0 unx     3676 b- defN 24-May-22 17:53 snowflake/ml/_internal/utils/formatting.py
+-rw-r--r--  2.0 unx    10926 b- defN 24-May-22 17:53 snowflake/ml/_internal/utils/identifier.py
+-rw-r--r--  2.0 unx     2068 b- defN 24-May-22 17:53 snowflake/ml/_internal/utils/import_utils.py
+-rw-r--r--  2.0 unx     1001 b- defN 24-May-22 17:53 snowflake/ml/_internal/utils/log_stream_processor.py
+-rw-r--r--  2.0 unx     4534 b- defN 24-May-22 17:53 snowflake/ml/_internal/utils/parallelize.py
+-rw-r--r--  2.0 unx     5857 b- defN 24-May-22 17:53 snowflake/ml/_internal/utils/pkg_version_utils.py
+-rw-r--r--  2.0 unx    10668 b- defN 24-May-22 17:53 snowflake/ml/_internal/utils/query_result_checker.py
+-rw-r--r--  2.0 unx     2405 b- defN 24-May-22 17:53 snowflake/ml/_internal/utils/result.py
+-rw-r--r--  2.0 unx     1321 b- defN 24-May-22 17:53 snowflake/ml/_internal/utils/retryable_http.py
+-rw-r--r--  2.0 unx     1727 b- defN 24-May-22 17:53 snowflake/ml/_internal/utils/session_token_manager.py
+-rw-r--r--  2.0 unx     2927 b- defN 24-May-22 17:53 snowflake/ml/_internal/utils/snowflake_env.py
+-rw-r--r--  2.0 unx     5403 b- defN 24-May-22 17:53 snowflake/ml/_internal/utils/snowpark_dataframe_utils.py
+-rw-r--r--  2.0 unx     4292 b- defN 24-May-22 17:53 snowflake/ml/_internal/utils/spcs_attribution_utils.py
+-rw-r--r--  2.0 unx     3729 b- defN 24-May-22 17:53 snowflake/ml/_internal/utils/sql_identifier.py
+-rw-r--r--  2.0 unx     4384 b- defN 24-May-22 17:53 snowflake/ml/_internal/utils/table_manager.py
+-rw-r--r--  2.0 unx     1402 b- defN 24-May-22 17:53 snowflake/ml/_internal/utils/temp_file_utils.py
+-rw-r--r--  2.0 unx     2828 b- defN 24-May-22 17:53 snowflake/ml/_internal/utils/uri.py
+-rw-r--r--  2.0 unx      274 b- defN 24-May-22 17:53 snowflake/ml/dataset/__init__.py
+-rw-r--r--  2.0 unx    21013 b- defN 24-May-22 17:53 snowflake/ml/dataset/dataset.py
+-rw-r--r--  2.0 unx     1712 b- defN 24-May-22 17:53 snowflake/ml/dataset/dataset_factory.py
+-rw-r--r--  2.0 unx     3992 b- defN 24-May-22 17:53 snowflake/ml/dataset/dataset_metadata.py
+-rw-r--r--  2.0 unx     8061 b- defN 24-May-22 17:53 snowflake/ml/dataset/dataset_reader.py
+-rw-r--r--  2.0 unx      423 b- defN 24-May-22 17:53 snowflake/ml/feature_store/__init__.py
+-rw-r--r--  2.0 unx    10442 b- defN 24-May-22 17:53 snowflake/ml/feature_store/access_manager.py
+-rw-r--r--  2.0 unx     3378 b- defN 24-May-22 17:53 snowflake/ml/feature_store/entity.py
+-rw-r--r--  2.0 unx    78669 b- defN 24-May-22 17:53 snowflake/ml/feature_store/feature_store.py
+-rw-r--r--  2.0 unx    19274 b- defN 24-May-22 17:53 snowflake/ml/feature_store/feature_view.py
+-rw-r--r--  2.0 unx     5919 b- defN 24-May-22 17:53 snowflake/ml/fileset/embedded_stage_fs.py
+-rw-r--r--  2.0 unx    26201 b- defN 24-May-22 17:53 snowflake/ml/fileset/fileset.py
+-rw-r--r--  2.0 unx     6884 b- defN 24-May-22 17:53 snowflake/ml/fileset/parquet_parser.py
+-rw-r--r--  2.0 unx    15344 b- defN 24-May-22 17:53 snowflake/ml/fileset/sfcfs.py
+-rw-r--r--  2.0 unx     6948 b- defN 24-May-22 17:53 snowflake/ml/fileset/snowfs.py
+-rw-r--r--  2.0 unx    19532 b- defN 24-May-22 17:53 snowflake/ml/fileset/stage_fs.py
+-rw-r--r--  2.0 unx     3849 b- defN 24-May-22 17:53 snowflake/ml/fileset/tf_dataset.py
+-rw-r--r--  2.0 unx     2386 b- defN 24-May-22 17:53 snowflake/ml/fileset/torch_datapipe.py
+-rw-r--r--  2.0 unx      393 b- defN 24-May-22 17:53 snowflake/ml/model/__init__.py
+-rw-r--r--  2.0 unx    22329 b- defN 24-May-22 17:53 snowflake/ml/model/_api.py
+-rw-r--r--  2.0 unx     8267 b- defN 24-May-22 17:53 snowflake/ml/model/custom_model.py
+-rw-r--r--  2.0 unx      144 b- defN 24-May-22 17:53 snowflake/ml/model/deploy_platforms.py
+-rw-r--r--  2.0 unx    29342 b- defN 24-May-22 17:53 snowflake/ml/model/model_signature.py
+-rw-r--r--  2.0 unx    12705 b- defN 24-May-22 17:53 snowflake/ml/model/type_hints.py
+-rw-r--r--  2.0 unx    13655 b- defN 24-May-22 17:53 snowflake/ml/model/_client/model/model_impl.py
+-rw-r--r--  2.0 unx    17994 b- defN 24-May-22 17:53 snowflake/ml/model/_client/model/model_version_impl.py
+-rw-r--r--  2.0 unx     4890 b- defN 24-May-22 17:53 snowflake/ml/model/_client/ops/metadata_ops.py
+-rw-r--r--  2.0 unx    28714 b- defN 24-May-22 17:53 snowflake/ml/model/_client/ops/model_ops.py
+-rw-r--r--  2.0 unx     1252 b- defN 24-May-22 17:53 snowflake/ml/model/_client/sql/_base.py
+-rw-r--r--  2.0 unx     5736 b- defN 24-May-22 17:53 snowflake/ml/model/_client/sql/model.py
+-rw-r--r--  2.0 unx    15685 b- defN 24-May-22 17:53 snowflake/ml/model/_client/sql/model_version.py
+-rw-r--r--  2.0 unx      819 b- defN 24-May-22 17:53 snowflake/ml/model/_client/sql/stage.py
+-rw-r--r--  2.0 unx     4345 b- defN 24-May-22 17:53 snowflake/ml/model/_client/sql/tag.py
+-rw-r--r--  2.0 unx      302 b- defN 24-May-22 17:53 snowflake/ml/model/_deploy_client/image_builds/base_image_builder.py
+-rw-r--r--  2.0 unx    10681 b- defN 24-May-22 17:53 snowflake/ml/model/_deploy_client/image_builds/client_image_builder.py
+-rw-r--r--  2.0 unx     6129 b- defN 24-May-22 17:53 snowflake/ml/model/_deploy_client/image_builds/docker_context.py
+-rw-r--r--  2.0 unx     1578 b- defN 24-May-22 17:53 snowflake/ml/model/_deploy_client/image_builds/gunicorn_run.sh
+-rw-r--r--  2.0 unx    10095 b- defN 24-May-22 17:53 snowflake/ml/model/_deploy_client/image_builds/server_image_builder.py
+-rw-r--r--  2.0 unx    11148 b- defN 24-May-22 17:53 snowflake/ml/model/_deploy_client/image_builds/inference_server/main.py
+-rw-r--r--  2.0 unx     1783 b- defN 24-May-22 17:53 snowflake/ml/model/_deploy_client/image_builds/templates/dockerfile_template
+-rw-r--r--  2.0 unx     1225 b- defN 24-May-22 17:53 snowflake/ml/model/_deploy_client/image_builds/templates/image_build_job_spec_template
+-rw-r--r--  2.0 unx     3633 b- defN 24-May-22 17:53 snowflake/ml/model/_deploy_client/image_builds/templates/kaniko_shell_script_template
+-rw-r--r--  2.0 unx    29286 b- defN 24-May-22 17:53 snowflake/ml/model/_deploy_client/snowservice/deploy.py
+-rw-r--r--  2.0 unx     5989 b- defN 24-May-22 17:53 snowflake/ml/model/_deploy_client/snowservice/deploy_options.py
+-rw-r--r--  2.0 unx      232 b- defN 24-May-22 17:53 snowflake/ml/model/_deploy_client/snowservice/instance_types.py
+-rw-r--r--  2.0 unx      734 b- defN 24-May-22 17:53 snowflake/ml/model/_deploy_client/snowservice/templates/service_spec_template
+-rw-r--r--  2.0 unx      540 b- defN 24-May-22 17:53 snowflake/ml/model/_deploy_client/snowservice/templates/service_spec_template_with_model
+-rw-r--r--  2.0 unx     1696 b- defN 24-May-22 17:53 snowflake/ml/model/_deploy_client/utils/constants.py
+-rw-r--r--  2.0 unx    11671 b- defN 24-May-22 17:53 snowflake/ml/model/_deploy_client/utils/snowservice_client.py
+-rw-r--r--  2.0 unx     8358 b- defN 24-May-22 17:53 snowflake/ml/model/_deploy_client/warehouse/deploy.py
+-rw-r--r--  2.0 unx     3011 b- defN 24-May-22 17:53 snowflake/ml/model/_deploy_client/warehouse/infer_template.py
+-rw-r--r--  2.0 unx     7356 b- defN 24-May-22 17:53 snowflake/ml/model/_model_composer/model_composer.py
+-rw-r--r--  2.0 unx     5667 b- defN 24-May-22 17:53 snowflake/ml/model/_model_composer/model_manifest/model_manifest.py
+-rw-r--r--  2.0 unx     2530 b- defN 24-May-22 17:53 snowflake/ml/model/_model_composer/model_manifest/model_manifest_schema.py
+-rw-r--r--  2.0 unx     1837 b- defN 24-May-22 17:53 snowflake/ml/model/_model_composer/model_method/function_generator.py
+-rw-r--r--  2.0 unx     2291 b- defN 24-May-22 17:53 snowflake/ml/model/_model_composer/model_method/infer_function.py_template
+-rw-r--r--  2.0 unx     2170 b- defN 24-May-22 17:53 snowflake/ml/model/_model_composer/model_method/infer_table_function.py_template
+-rw-r--r--  2.0 unx     6682 b- defN 24-May-22 17:53 snowflake/ml/model/_model_composer/model_method/model_method.py
+-rw-r--r--  2.0 unx     2642 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_handler.py
+-rw-r--r--  2.0 unx     5836 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_packager.py
+-rw-r--r--  2.0 unx    18312 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_env/model_env.py
+-rw-r--r--  2.0 unx     6033 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_handlers/_base.py
+-rw-r--r--  2.0 unx     2622 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_handlers/_utils.py
+-rw-r--r--  2.0 unx     8142 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_handlers/catboost.py
+-rw-r--r--  2.0 unx     7325 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_handlers/custom.py
+-rw-r--r--  2.0 unx    19985 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_handlers/huggingface_pipeline.py
+-rw-r--r--  2.0 unx     8445 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_handlers/lightgbm.py
+-rw-r--r--  2.0 unx    10772 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_handlers/llm.py
+-rw-r--r--  2.0 unx     9022 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_handlers/mlflow.py
+-rw-r--r--  2.0 unx     8033 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_handlers/pytorch.py
+-rw-r--r--  2.0 unx     9051 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_handlers/sentence_transformers.py
+-rw-r--r--  2.0 unx     8218 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_handlers/sklearn.py
+-rw-r--r--  2.0 unx     7967 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_handlers/snowmlmodel.py
+-rw-r--r--  2.0 unx     8179 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_handlers/tensorflow.py
+-rw-r--r--  2.0 unx     8104 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_handlers/torchscript.py
+-rw-r--r--  2.0 unx     8874 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_handlers/xgboost.py
+-rw-r--r--  2.0 unx     1409 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_handlers_migrator/base_migrator.py
+-rwxr-xr-x  2.0 unx      274 b- defN 24-May-22 17:54 snowflake/ml/model/_packager/model_meta/_core_requirements.py
+-rwxr-xr-x  2.0 unx       44 b- defN 24-May-22 17:54 snowflake/ml/model/_packager/model_meta/_packaging_requirements.py
+-rw-r--r--  2.0 unx     1818 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_meta/model_blob_meta.py
+-rw-r--r--  2.0 unx    17203 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_meta/model_meta.py
+-rw-r--r--  2.0 unx     2380 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_meta/model_meta_schema.py
+-rw-r--r--  2.0 unx     1311 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_meta_migrator/base_migrator.py
+-rw-r--r--  2.0 unx     1041 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_meta_migrator/migrator_plans.py
+-rw-r--r--  2.0 unx     2070 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_meta_migrator/migrator_v1.py
+-rwxr-xr-x  2.0 unx      248 b- defN 24-May-22 17:54 snowflake/ml/model/_packager/model_runtime/_snowml_inference_alternative_requirements.py
+-rw-r--r--  2.0 unx     5872 b- defN 24-May-22 17:53 snowflake/ml/model/_packager/model_runtime/model_runtime.py
+-rw-r--r--  2.0 unx     1304 b- defN 24-May-22 17:53 snowflake/ml/model/_signatures/base_handler.py
+-rw-r--r--  2.0 unx     2706 b- defN 24-May-22 17:53 snowflake/ml/model/_signatures/builtins_handler.py
+-rw-r--r--  2.0 unx    17972 b- defN 24-May-22 17:53 snowflake/ml/model/_signatures/core.py
+-rw-r--r--  2.0 unx     5858 b- defN 24-May-22 17:53 snowflake/ml/model/_signatures/numpy_handler.py
+-rw-r--r--  2.0 unx     8057 b- defN 24-May-22 17:53 snowflake/ml/model/_signatures/pandas_handler.py
+-rw-r--r--  2.0 unx     4568 b- defN 24-May-22 17:53 snowflake/ml/model/_signatures/pytorch_handler.py
+-rw-r--r--  2.0 unx     6036 b- defN 24-May-22 17:53 snowflake/ml/model/_signatures/snowpark_handler.py
+-rw-r--r--  2.0 unx     6082 b- defN 24-May-22 17:53 snowflake/ml/model/_signatures/tensorflow_handler.py
+-rw-r--r--  2.0 unx    12687 b- defN 24-May-22 17:53 snowflake/ml/model/_signatures/utils.py
+-rw-r--r--  2.0 unx    10221 b- defN 24-May-22 17:53 snowflake/ml/model/models/huggingface_pipeline.py
+-rw-r--r--  2.0 unx     3616 b- defN 24-May-22 17:53 snowflake/ml/model/models/llm.py
+-rw-r--r--  2.0 unx       45 b- defN 24-May-22 17:53 snowflake/ml/modeling/_internal/constants.py
+-rw-r--r--  2.0 unx     9215 b- defN 24-May-22 17:53 snowflake/ml/modeling/_internal/estimator_utils.py
+-rw-r--r--  2.0 unx     4801 b- defN 24-May-22 17:53 snowflake/ml/modeling/_internal/model_specifications.py
+-rw-r--r--  2.0 unx      923 b- defN 24-May-22 17:53 snowflake/ml/modeling/_internal/model_trainer.py
+-rw-r--r--  2.0 unx     8422 b- defN 24-May-22 17:53 snowflake/ml/modeling/_internal/model_trainer_builder.py
+-rw-r--r--  2.0 unx     3364 b- defN 24-May-22 17:53 snowflake/ml/modeling/_internal/model_transformer_builder.py
+-rw-r--r--  2.0 unx     6482 b- defN 24-May-22 17:53 snowflake/ml/modeling/_internal/transformer_protocols.py
+-rw-r--r--  2.0 unx     7831 b- defN 24-May-22 17:53 snowflake/ml/modeling/_internal/local_implementations/pandas_handlers.py
+-rw-r--r--  2.0 unx     5737 b- defN 24-May-22 17:53 snowflake/ml/modeling/_internal/local_implementations/pandas_trainer.py
+-rw-r--r--  2.0 unx     5544 b- defN 24-May-22 17:53 snowflake/ml/modeling/_internal/ml_runtime_implementations/ml_runtime_handlers.py
+-rw-r--r--  2.0 unx     2535 b- defN 24-May-22 17:53 snowflake/ml/modeling/_internal/ml_runtime_implementations/ml_runtime_trainer.py
+-rw-r--r--  2.0 unx    55706 b- defN 24-May-22 17:53 snowflake/ml/modeling/_internal/snowpark_implementations/distributed_hpo_trainer.py
+-rw-r--r--  2.0 unx    14539 b- defN 24-May-22 17:53 snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_handlers.py
+-rw-r--r--  2.0 unx    35283 b- defN 24-May-22 17:53 snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_trainer.py
+-rw-r--r--  2.0 unx    17208 b- defN 24-May-22 17:53 snowflake/ml/modeling/_internal/snowpark_implementations/xgboost_external_memory_trainer.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-22 17:54 snowflake/ml/modeling/calibration/__init__.py
+-rwxr-xr-x  2.0 unx    51278 b- defN 24-May-22 17:54 snowflake/ml/modeling/calibration/calibrated_classifier_cv.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-22 17:54 snowflake/ml/modeling/cluster/__init__.py
+-rwxr-xr-x  2.0 unx    49107 b- defN 24-May-22 17:54 snowflake/ml/modeling/cluster/affinity_propagation.py
+-rwxr-xr-x  2.0 unx    51144 b- defN 24-May-22 17:54 snowflake/ml/modeling/cluster/agglomerative_clustering.py
+-rwxr-xr-x  2.0 unx    49034 b- defN 24-May-22 17:54 snowflake/ml/modeling/cluster/birch.py
+-rwxr-xr-x  2.0 unx    51793 b- defN 24-May-22 17:54 snowflake/ml/modeling/cluster/bisecting_k_means.py
+-rwxr-xr-x  2.0 unx    49194 b- defN 24-May-22 17:54 snowflake/ml/modeling/cluster/dbscan.py
+-rwxr-xr-x  2.0 unx    51909 b- defN 24-May-22 17:54 snowflake/ml/modeling/cluster/feature_agglomeration.py
+-rwxr-xr-x  2.0 unx    51322 b- defN 24-May-22 17:54 snowflake/ml/modeling/cluster/k_means.py
+-rwxr-xr-x  2.0 unx    49404 b- defN 24-May-22 17:54 snowflake/ml/modeling/cluster/mean_shift.py
+-rwxr-xr-x  2.0 unx    52711 b- defN 24-May-22 17:54 snowflake/ml/modeling/cluster/mini_batch_k_means.py
+-rwxr-xr-x  2.0 unx    52508 b- defN 24-May-22 17:54 snowflake/ml/modeling/cluster/optics.py
+-rwxr-xr-x  2.0 unx    49403 b- defN 24-May-22 17:54 snowflake/ml/modeling/cluster/spectral_biclustering.py
+-rwxr-xr-x  2.0 unx    52595 b- defN 24-May-22 17:54 snowflake/ml/modeling/cluster/spectral_clustering.py
+-rwxr-xr-x  2.0 unx    48536 b- defN 24-May-22 17:54 snowflake/ml/modeling/cluster/spectral_coclustering.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-22 17:54 snowflake/ml/modeling/compose/__init__.py
+-rwxr-xr-x  2.0 unx    51374 b- defN 24-May-22 17:54 snowflake/ml/modeling/compose/column_transformer.py
+-rwxr-xr-x  2.0 unx    49090 b- defN 24-May-22 17:54 snowflake/ml/modeling/compose/transformed_target_regressor.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-22 17:54 snowflake/ml/modeling/covariance/__init__.py
+-rwxr-xr-x  2.0 unx    49428 b- defN 24-May-22 17:54 snowflake/ml/modeling/covariance/elliptic_envelope.py
+-rwxr-xr-x  2.0 unx    47236 b- defN 24-May-22 17:54 snowflake/ml/modeling/covariance/empirical_covariance.py
+-rwxr-xr-x  2.0 unx    49100 b- defN 24-May-22 17:54 snowflake/ml/modeling/covariance/graphical_lasso.py
+-rwxr-xr-x  2.0 unx    50265 b- defN 24-May-22 17:54 snowflake/ml/modeling/covariance/graphical_lasso_cv.py
+-rwxr-xr-x  2.0 unx    47374 b- defN 24-May-22 17:54 snowflake/ml/modeling/covariance/ledoit_wolf.py
+-rwxr-xr-x  2.0 unx    48129 b- defN 24-May-22 17:54 snowflake/ml/modeling/covariance/min_cov_det.py
+-rwxr-xr-x  2.0 unx    47015 b- defN 24-May-22 17:54 snowflake/ml/modeling/covariance/oas.py
+-rwxr-xr-x  2.0 unx    47391 b- defN 24-May-22 17:54 snowflake/ml/modeling/covariance/shrunk_covariance.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-22 17:54 snowflake/ml/modeling/decomposition/__init__.py
+-rwxr-xr-x  2.0 unx    52388 b- defN 24-May-22 17:54 snowflake/ml/modeling/decomposition/dictionary_learning.py
+-rwxr-xr-x  2.0 unx    50051 b- defN 24-May-22 17:54 snowflake/ml/modeling/decomposition/factor_analysis.py
+-rwxr-xr-x  2.0 unx    49985 b- defN 24-May-22 17:54 snowflake/ml/modeling/decomposition/fast_ica.py
+-rwxr-xr-x  2.0 unx    48345 b- defN 24-May-22 17:54 snowflake/ml/modeling/decomposition/incremental_pca.py
+-rwxr-xr-x  2.0 unx    52343 b- defN 24-May-22 17:54 snowflake/ml/modeling/decomposition/kernel_pca.py
+-rwxr-xr-x  2.0 unx    53429 b- defN 24-May-22 17:54 snowflake/ml/modeling/decomposition/mini_batch_dictionary_learning.py
+-rwxr-xr-x  2.0 unx    50694 b- defN 24-May-22 17:54 snowflake/ml/modeling/decomposition/mini_batch_sparse_pca.py
+-rwxr-xr-x  2.0 unx    51612 b- defN 24-May-22 17:54 snowflake/ml/modeling/decomposition/pca.py
+-rwxr-xr-x  2.0 unx    49499 b- defN 24-May-22 17:54 snowflake/ml/modeling/decomposition/sparse_pca.py
+-rwxr-xr-x  2.0 unx    49122 b- defN 24-May-22 17:54 snowflake/ml/modeling/decomposition/truncated_svd.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-22 17:54 snowflake/ml/modeling/discriminant_analysis/__init__.py
+-rwxr-xr-x  2.0 unx    51877 b- defN 24-May-22 17:54 snowflake/ml/modeling/discriminant_analysis/linear_discriminant_analysis.py
+-rwxr-xr-x  2.0 unx    49659 b- defN 24-May-22 17:54 snowflake/ml/modeling/discriminant_analysis/quadratic_discriminant_analysis.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-22 17:54 snowflake/ml/modeling/ensemble/__init__.py
+-rwxr-xr-x  2.0 unx    50477 b- defN 24-May-22 17:54 snowflake/ml/modeling/ensemble/ada_boost_classifier.py
+-rwxr-xr-x  2.0 unx    49368 b- defN 24-May-22 17:54 snowflake/ml/modeling/ensemble/ada_boost_regressor.py
+-rwxr-xr-x  2.0 unx    51388 b- defN 24-May-22 17:54 snowflake/ml/modeling/ensemble/bagging_classifier.py
+-rwxr-xr-x  2.0 unx    50624 b- defN 24-May-22 17:54 snowflake/ml/modeling/ensemble/bagging_regressor.py
+-rwxr-xr-x  2.0 unx    56308 b- defN 24-May-22 17:54 snowflake/ml/modeling/ensemble/extra_trees_classifier.py
+-rwxr-xr-x  2.0 unx    54912 b- defN 24-May-22 17:54 snowflake/ml/modeling/ensemble/extra_trees_regressor.py
+-rwxr-xr-x  2.0 unx    57763 b- defN 24-May-22 17:54 snowflake/ml/modeling/ensemble/gradient_boosting_classifier.py
+-rwxr-xr-x  2.0 unx    57356 b- defN 24-May-22 17:54 snowflake/ml/modeling/ensemble/gradient_boosting_regressor.py
+-rwxr-xr-x  2.0 unx    57597 b- defN 24-May-22 17:54 snowflake/ml/modeling/ensemble/hist_gradient_boosting_classifier.py
+-rwxr-xr-x  2.0 unx    56082 b- defN 24-May-22 17:54 snowflake/ml/modeling/ensemble/hist_gradient_boosting_regressor.py
+-rwxr-xr-x  2.0 unx    50576 b- defN 24-May-22 17:54 snowflake/ml/modeling/ensemble/isolation_forest.py
+-rwxr-xr-x  2.0 unx    56291 b- defN 24-May-22 17:54 snowflake/ml/modeling/ensemble/random_forest_classifier.py
+-rwxr-xr-x  2.0 unx    54883 b- defN 24-May-22 17:54 snowflake/ml/modeling/ensemble/random_forest_regressor.py
+-rwxr-xr-x  2.0 unx    50602 b- defN 24-May-22 17:54 snowflake/ml/modeling/ensemble/stacking_regressor.py
+-rwxr-xr-x  2.0 unx    50155 b- defN 24-May-22 17:54 snowflake/ml/modeling/ensemble/voting_classifier.py
+-rwxr-xr-x  2.0 unx    48681 b- defN 24-May-22 17:54 snowflake/ml/modeling/ensemble/voting_regressor.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-22 17:54 snowflake/ml/modeling/feature_selection/__init__.py
+-rwxr-xr-x  2.0 unx    48018 b- defN 24-May-22 17:54 snowflake/ml/modeling/feature_selection/generic_univariate_select.py
+-rwxr-xr-x  2.0 unx    47619 b- defN 24-May-22 17:54 snowflake/ml/modeling/feature_selection/select_fdr.py
+-rwxr-xr-x  2.0 unx    47613 b- defN 24-May-22 17:54 snowflake/ml/modeling/feature_selection/select_fpr.py
+-rwxr-xr-x  2.0 unx    47621 b- defN 24-May-22 17:54 snowflake/ml/modeling/feature_selection/select_fwe.py
+-rwxr-xr-x  2.0 unx    47712 b- defN 24-May-22 17:54 snowflake/ml/modeling/feature_selection/select_k_best.py
+-rwxr-xr-x  2.0 unx    47767 b- defN 24-May-22 17:54 snowflake/ml/modeling/feature_selection/select_percentile.py
+-rwxr-xr-x  2.0 unx    50387 b- defN 24-May-22 17:54 snowflake/ml/modeling/feature_selection/sequential_feature_selector.py
+-rwxr-xr-x  2.0 unx    47323 b- defN 24-May-22 17:54 snowflake/ml/modeling/feature_selection/variance_threshold.py
+-rw-r--r--  2.0 unx    10203 b- defN 24-May-22 17:53 snowflake/ml/modeling/framework/_utils.py
+-rw-r--r--  2.0 unx    31439 b- defN 24-May-22 17:53 snowflake/ml/modeling/framework/base.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-22 17:54 snowflake/ml/modeling/gaussian_process/__init__.py
+-rwxr-xr-x  2.0 unx    53029 b- defN 24-May-22 17:54 snowflake/ml/modeling/gaussian_process/gaussian_process_classifier.py
+-rwxr-xr-x  2.0 unx    52094 b- defN 24-May-22 17:54 snowflake/ml/modeling/gaussian_process/gaussian_process_regressor.py
+-rw-r--r--  2.0 unx      298 b- defN 24-May-22 17:53 snowflake/ml/modeling/impute/__init__.py
+-rwxr-xr-x  2.0 unx    53847 b- defN 24-May-22 17:54 snowflake/ml/modeling/impute/iterative_imputer.py
+-rwxr-xr-x  2.0 unx    49563 b- defN 24-May-22 17:54 snowflake/ml/modeling/impute/knn_imputer.py
+-rwxr-xr-x  2.0 unx    48420 b- defN 24-May-22 17:54 snowflake/ml/modeling/impute/missing_indicator.py
+-rw-r--r--  2.0 unx    18499 b- defN 24-May-22 17:53 snowflake/ml/modeling/impute/simple_imputer.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-22 17:54 snowflake/ml/modeling/kernel_approximation/__init__.py
+-rwxr-xr-x  2.0 unx    47451 b- defN 24-May-22 17:54 snowflake/ml/modeling/kernel_approximation/additive_chi2_sampler.py
+-rwxr-xr-x  2.0 unx    49222 b- defN 24-May-22 17:54 snowflake/ml/modeling/kernel_approximation/nystroem.py
+-rwxr-xr-x  2.0 unx    48470 b- defN 24-May-22 17:54 snowflake/ml/modeling/kernel_approximation/polynomial_count_sketch.py
+-rwxr-xr-x  2.0 unx    47824 b- defN 24-May-22 17:54 snowflake/ml/modeling/kernel_approximation/rbf_sampler.py
+-rwxr-xr-x  2.0 unx    47872 b- defN 24-May-22 17:54 snowflake/ml/modeling/kernel_approximation/skewed_chi2_sampler.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-22 17:54 snowflake/ml/modeling/kernel_ridge/__init__.py
+-rwxr-xr-x  2.0 unx    49408 b- defN 24-May-22 17:54 snowflake/ml/modeling/kernel_ridge/kernel_ridge.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-22 17:54 snowflake/ml/modeling/lightgbm/__init__.py
+-rwxr-xr-x  2.0 unx    48976 b- defN 24-May-22 17:54 snowflake/ml/modeling/lightgbm/lgbm_classifier.py
+-rwxr-xr-x  2.0 unx    48479 b- defN 24-May-22 17:54 snowflake/ml/modeling/lightgbm/lgbm_regressor.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/__init__.py
+-rwxr-xr-x  2.0 unx    49353 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/ard_regression.py
+-rwxr-xr-x  2.0 unx    49769 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/bayesian_ridge.py
+-rwxr-xr-x  2.0 unx    50341 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/elastic_net.py
+-rwxr-xr-x  2.0 unx    51609 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/elastic_net_cv.py
+-rwxr-xr-x  2.0 unx    49421 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/gamma_regressor.py
+-rwxr-xr-x  2.0 unx    48618 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/huber_regressor.py
+-rwxr-xr-x  2.0 unx    49841 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/lars.py
+-rwxr-xr-x  2.0 unx    50062 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/lars_cv.py
+-rwxr-xr-x  2.0 unx    49947 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/lasso.py
+-rwxr-xr-x  2.0 unx    50732 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/lasso_cv.py
+-rwxr-xr-x  2.0 unx    50977 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/lasso_lars.py
+-rwxr-xr-x  2.0 unx    50938 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/lasso_lars_cv.py
+-rwxr-xr-x  2.0 unx    50284 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/lasso_lars_ic.py
+-rwxr-xr-x  2.0 unx    48160 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/linear_regression.py
+-rwxr-xr-x  2.0 unx    54415 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/logistic_regression.py
+-rwxr-xr-x  2.0 unx    55455 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/logistic_regression_cv.py
+-rwxr-xr-x  2.0 unx    49632 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/multi_task_elastic_net.py
+-rwxr-xr-x  2.0 unx    51270 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/multi_task_elastic_net_cv.py
+-rwxr-xr-x  2.0 unx    49180 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/multi_task_lasso.py
+-rwxr-xr-x  2.0 unx    50442 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/multi_task_lasso_cv.py
+-rwxr-xr-x  2.0 unx    48787 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/orthogonal_matching_pursuit.py
+-rwxr-xr-x  2.0 unx    52143 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/passive_aggressive_classifier.py
+-rwxr-xr-x  2.0 unx    51210 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/passive_aggressive_regressor.py
+-rwxr-xr-x  2.0 unx    51527 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/perceptron.py
+-rwxr-xr-x  2.0 unx    49466 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/poisson_regressor.py
+-rwxr-xr-x  2.0 unx    52585 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/ransac_regressor.py
+-rwxr-xr-x  2.0 unx    51491 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/ridge.py
+-rwxr-xr-x  2.0 unx    51879 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/ridge_classifier.py
+-rwxr-xr-x  2.0 unx    49874 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/ridge_classifier_cv.py
+-rwxr-xr-x  2.0 unx    50575 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/ridge_cv.py
+-rwxr-xr-x  2.0 unx    56952 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/sgd_classifier.py
+-rwxr-xr-x  2.0 unx    51750 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/sgd_one_class_svm.py
+-rwxr-xr-x  2.0 unx    54423 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/sgd_regressor.py
+-rwxr-xr-x  2.0 unx    49901 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/theil_sen_regressor.py
+-rwxr-xr-x  2.0 unx    50857 b- defN 24-May-22 17:54 snowflake/ml/modeling/linear_model/tweedie_regressor.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-22 17:54 snowflake/ml/modeling/manifold/__init__.py
+-rwxr-xr-x  2.0 unx    50084 b- defN 24-May-22 17:54 snowflake/ml/modeling/manifold/isomap.py
+-rwxr-xr-x  2.0 unx    49304 b- defN 24-May-22 17:54 snowflake/ml/modeling/manifold/mds.py
+-rwxr-xr-x  2.0 unx    50155 b- defN 24-May-22 17:54 snowflake/ml/modeling/manifold/spectral_embedding.py
+-rwxr-xr-x  2.0 unx    53094 b- defN 24-May-22 17:54 snowflake/ml/modeling/manifold/tsne.py
+-rw-r--r--  2.0 unx      524 b- defN 24-May-22 17:53 snowflake/ml/modeling/metrics/__init__.py
+-rw-r--r--  2.0 unx    66499 b- defN 24-May-22 17:53 snowflake/ml/modeling/metrics/classification.py
+-rw-r--r--  2.0 unx     4803 b- defN 24-May-22 17:53 snowflake/ml/modeling/metrics/correlation.py
+-rw-r--r--  2.0 unx     4672 b- defN 24-May-22 17:53 snowflake/ml/modeling/metrics/covariance.py
+-rw-r--r--  2.0 unx    13114 b- defN 24-May-22 17:53 snowflake/ml/modeling/metrics/metrics_utils.py
+-rw-r--r--  2.0 unx    17569 b- defN 24-May-22 17:53 snowflake/ml/modeling/metrics/ranking.py
+-rw-r--r--  2.0 unx    25845 b- defN 24-May-22 17:53 snowflake/ml/modeling/metrics/regression.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-22 17:54 snowflake/ml/modeling/mixture/__init__.py
+-rwxr-xr-x  2.0 unx    54657 b- defN 24-May-22 17:54 snowflake/ml/modeling/mixture/bayesian_gaussian_mixture.py
+-rwxr-xr-x  2.0 unx    52558 b- defN 24-May-22 17:54 snowflake/ml/modeling/mixture/gaussian_mixture.py
+-rw-r--r--  2.0 unx      298 b- defN 24-May-22 17:53 snowflake/ml/modeling/model_selection/__init__.py
+-rw-r--r--  2.0 unx    38125 b- defN 24-May-22 17:53 snowflake/ml/modeling/model_selection/grid_search_cv.py
+-rw-r--r--  2.0 unx    38867 b- defN 24-May-22 17:53 snowflake/ml/modeling/model_selection/randomized_search_cv.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-22 17:54 snowflake/ml/modeling/multiclass/__init__.py
+-rwxr-xr-x  2.0 unx    48160 b- defN 24-May-22 17:54 snowflake/ml/modeling/multiclass/one_vs_one_classifier.py
+-rwxr-xr-x  2.0 unx    49094 b- defN 24-May-22 17:54 snowflake/ml/modeling/multiclass/one_vs_rest_classifier.py
+-rwxr-xr-x  2.0 unx    48430 b- defN 24-May-22 17:54 snowflake/ml/modeling/multiclass/output_code_classifier.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-22 17:54 snowflake/ml/modeling/naive_bayes/__init__.py
+-rwxr-xr-x  2.0 unx    48699 b- defN 24-May-22 17:54 snowflake/ml/modeling/naive_bayes/bernoulli_nb.py
+-rwxr-xr-x  2.0 unx    49034 b- defN 24-May-22 17:54 snowflake/ml/modeling/naive_bayes/categorical_nb.py
+-rwxr-xr-x  2.0 unx    48714 b- defN 24-May-22 17:54 snowflake/ml/modeling/naive_bayes/complement_nb.py
+-rwxr-xr-x  2.0 unx    47843 b- defN 24-May-22 17:54 snowflake/ml/modeling/naive_bayes/gaussian_nb.py
+-rwxr-xr-x  2.0 unx    48479 b- defN 24-May-22 17:54 snowflake/ml/modeling/naive_bayes/multinomial_nb.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-22 17:54 snowflake/ml/modeling/neighbors/__init__.py
+-rwxr-xr-x  2.0 unx    51548 b- defN 24-May-22 17:54 snowflake/ml/modeling/neighbors/k_neighbors_classifier.py
+-rwxr-xr-x  2.0 unx    51019 b- defN 24-May-22 17:54 snowflake/ml/modeling/neighbors/k_neighbors_regressor.py
+-rwxr-xr-x  2.0 unx    49377 b- defN 24-May-22 17:54 snowflake/ml/modeling/neighbors/kernel_density.py
+-rwxr-xr-x  2.0 unx    51955 b- defN 24-May-22 17:54 snowflake/ml/modeling/neighbors/local_outlier_factor.py
+-rwxr-xr-x  2.0 unx    48037 b- defN 24-May-22 17:54 snowflake/ml/modeling/neighbors/nearest_centroid.py
+-rwxr-xr-x  2.0 unx    49846 b- defN 24-May-22 17:54 snowflake/ml/modeling/neighbors/nearest_neighbors.py
+-rwxr-xr-x  2.0 unx    51509 b- defN 24-May-22 17:54 snowflake/ml/modeling/neighbors/neighborhood_components_analysis.py
+-rwxr-xr-x  2.0 unx    51961 b- defN 24-May-22 17:54 snowflake/ml/modeling/neighbors/radius_neighbors_classifier.py
+-rwxr-xr-x  2.0 unx    50842 b- defN 24-May-22 17:54 snowflake/ml/modeling/neighbors/radius_neighbors_regressor.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-22 17:54 snowflake/ml/modeling/neural_network/__init__.py
+-rwxr-xr-x  2.0 unx    48585 b- defN 24-May-22 17:54 snowflake/ml/modeling/neural_network/bernoulli_rbm.py
+-rwxr-xr-x  2.0 unx    55921 b- defN 24-May-22 17:54 snowflake/ml/modeling/neural_network/mlp_classifier.py
+-rwxr-xr-x  2.0 unx    55190 b- defN 24-May-22 17:54 snowflake/ml/modeling/neural_network/mlp_regressor.py
+-rw-r--r--  2.0 unx      221 b- defN 24-May-22 17:53 snowflake/ml/modeling/parameters/disable_distributed_hpo.py
+-rw-r--r--  2.0 unx      298 b- defN 24-May-22 17:53 snowflake/ml/modeling/pipeline/__init__.py
+-rw-r--r--  2.0 unx    46223 b- defN 24-May-22 17:53 snowflake/ml/modeling/pipeline/pipeline.py
+-rw-r--r--  2.0 unx      298 b- defN 24-May-22 17:53 snowflake/ml/modeling/preprocessing/__init__.py
+-rw-r--r--  2.0 unx     6999 b- defN 24-May-22 17:53 snowflake/ml/modeling/preprocessing/binarizer.py
+-rw-r--r--  2.0 unx    20970 b- defN 24-May-22 17:53 snowflake/ml/modeling/preprocessing/k_bins_discretizer.py
+-rw-r--r--  2.0 unx     7405 b- defN 24-May-22 17:53 snowflake/ml/modeling/preprocessing/label_encoder.py
+-rw-r--r--  2.0 unx     8768 b- defN 24-May-22 17:53 snowflake/ml/modeling/preprocessing/max_abs_scaler.py
+-rw-r--r--  2.0 unx    12200 b- defN 24-May-22 17:53 snowflake/ml/modeling/preprocessing/min_max_scaler.py
+-rw-r--r--  2.0 unx     6584 b- defN 24-May-22 17:53 snowflake/ml/modeling/preprocessing/normalizer.py
+-rw-r--r--  2.0 unx    72322 b- defN 24-May-22 17:53 snowflake/ml/modeling/preprocessing/one_hot_encoder.py
+-rw-r--r--  2.0 unx    33276 b- defN 24-May-22 17:53 snowflake/ml/modeling/preprocessing/ordinal_encoder.py
+-rwxr-xr-x  2.0 unx    48511 b- defN 24-May-22 17:54 snowflake/ml/modeling/preprocessing/polynomial_features.py
+-rw-r--r--  2.0 unx    12398 b- defN 24-May-22 17:53 snowflake/ml/modeling/preprocessing/robust_scaler.py
+-rw-r--r--  2.0 unx    11395 b- defN 24-May-22 17:53 snowflake/ml/modeling/preprocessing/standard_scaler.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-22 17:54 snowflake/ml/modeling/semi_supervised/__init__.py
+-rwxr-xr-x  2.0 unx    48936 b- defN 24-May-22 17:54 snowflake/ml/modeling/semi_supervised/label_propagation.py
+-rwxr-xr-x  2.0 unx    49285 b- defN 24-May-22 17:54 snowflake/ml/modeling/semi_supervised/label_spreading.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-22 17:54 snowflake/ml/modeling/svm/__init__.py
+-rwxr-xr-x  2.0 unx    51746 b- defN 24-May-22 17:54 snowflake/ml/modeling/svm/linear_svc.py
+-rwxr-xr-x  2.0 unx    50099 b- defN 24-May-22 17:54 snowflake/ml/modeling/svm/linear_svr.py
+-rwxr-xr-x  2.0 unx    52058 b- defN 24-May-22 17:54 snowflake/ml/modeling/svm/nu_svc.py
+-rwxr-xr-x  2.0 unx    49137 b- defN 24-May-22 17:54 snowflake/ml/modeling/svm/nu_svr.py
+-rwxr-xr-x  2.0 unx    52207 b- defN 24-May-22 17:54 snowflake/ml/modeling/svm/svc.py
+-rwxr-xr-x  2.0 unx    49326 b- defN 24-May-22 17:54 snowflake/ml/modeling/svm/svr.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-22 17:54 snowflake/ml/modeling/tree/__init__.py
+-rwxr-xr-x  2.0 unx    54483 b- defN 24-May-22 17:54 snowflake/ml/modeling/tree/decision_tree_classifier.py
+-rwxr-xr-x  2.0 unx    53182 b- defN 24-May-22 17:54 snowflake/ml/modeling/tree/decision_tree_regressor.py
+-rwxr-xr-x  2.0 unx    53825 b- defN 24-May-22 17:54 snowflake/ml/modeling/tree/extra_tree_classifier.py
+-rwxr-xr-x  2.0 unx    52533 b- defN 24-May-22 17:54 snowflake/ml/modeling/tree/extra_tree_regressor.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-22 17:54 snowflake/ml/modeling/xgboost/__init__.py
+-rwxr-xr-x  2.0 unx    59481 b- defN 24-May-22 17:54 snowflake/ml/modeling/xgboost/xgb_classifier.py
+-rwxr-xr-x  2.0 unx    58980 b- defN 24-May-22 17:54 snowflake/ml/modeling/xgboost/xgb_regressor.py
+-rwxr-xr-x  2.0 unx    59657 b- defN 24-May-22 17:54 snowflake/ml/modeling/xgboost/xgbrf_classifier.py
+-rwxr-xr-x  2.0 unx    59183 b- defN 24-May-22 17:54 snowflake/ml/modeling/xgboost/xgbrf_regressor.py
+-rw-r--r--  2.0 unx     7135 b- defN 24-May-22 17:53 snowflake/ml/monitoring/monitor.py
+-rw-r--r--  2.0 unx     3597 b- defN 24-May-22 17:53 snowflake/ml/monitoring/shap.py
+-rw-r--r--  2.0 unx       76 b- defN 24-May-22 17:53 snowflake/ml/registry/__init__.py
+-rw-r--r--  2.0 unx     5316 b- defN 24-May-22 17:53 snowflake/ml/registry/_initial_schema.py
+-rw-r--r--  2.0 unx     3166 b- defN 24-May-22 17:53 snowflake/ml/registry/_schema.py
+-rw-r--r--  2.0 unx     4209 b- defN 24-May-22 17:53 snowflake/ml/registry/_schema_upgrade_plans.py
+-rw-r--r--  2.0 unx     6922 b- defN 24-May-22 17:53 snowflake/ml/registry/_schema_version_manager.py
+-rw-r--r--  2.0 unx    84795 b- defN 24-May-22 17:53 snowflake/ml/registry/model_registry.py
+-rw-r--r--  2.0 unx    10949 b- defN 24-May-22 17:53 snowflake/ml/registry/registry.py
+-rw-r--r--  2.0 unx     7126 b- defN 24-May-22 17:53 snowflake/ml/registry/_manager/model_manager.py
+-rw-r--r--  2.0 unx     8005 b- defN 24-May-22 17:53 snowflake/ml/utils/connection_params.py
+-rw-r--r--  2.0 unx     3817 b- defN 24-May-22 17:53 snowflake/ml/utils/sparse.py
+-rw-r--r--  2.0 unx    11365 b- defN 24-May-22 17:55 snowflake_ml_python-1.5.1.dist-info/LICENSE.txt
+-rw-r--r--  2.0 unx    52106 b- defN 24-May-22 17:55 snowflake_ml_python-1.5.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-May-22 17:55 snowflake_ml_python-1.5.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       10 b- defN 24-May-22 17:55 snowflake_ml_python-1.5.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    41358 b- defN 24-May-22 17:55 snowflake_ml_python-1.5.1.dist-info/RECORD
+382 files, 9639766 bytes uncompressed, 1836021 bytes compressed:  81.0%
```

## zipnote {}

```diff
@@ -90,15 +90,15 @@
 
 Filename: snowflake/ml/_internal/human_readable_id/hrid_generator_base.py
 Comment: 
 
 Filename: snowflake/ml/_internal/lineage/data_source.py
 Comment: 
 
-Filename: snowflake/ml/_internal/lineage/dataset_dataframe.py
+Filename: snowflake/ml/_internal/lineage/lineage_utils.py
 Comment: 
 
 Filename: snowflake/ml/_internal/utils/formatting.py
 Comment: 
 
 Filename: snowflake/ml/_internal/utils/identifier.py
 Comment: 
@@ -162,14 +162,17 @@
 
 Filename: snowflake/ml/dataset/dataset_reader.py
 Comment: 
 
 Filename: snowflake/ml/feature_store/__init__.py
 Comment: 
 
+Filename: snowflake/ml/feature_store/access_manager.py
+Comment: 
+
 Filename: snowflake/ml/feature_store/entity.py
 Comment: 
 
 Filename: snowflake/ml/feature_store/feature_store.py
 Comment: 
 
 Filename: snowflake/ml/feature_store/feature_view.py
@@ -225,14 +228,17 @@
 
 Filename: snowflake/ml/model/_client/ops/metadata_ops.py
 Comment: 
 
 Filename: snowflake/ml/model/_client/ops/model_ops.py
 Comment: 
 
+Filename: snowflake/ml/model/_client/sql/_base.py
+Comment: 
+
 Filename: snowflake/ml/model/_client/sql/model.py
 Comment: 
 
 Filename: snowflake/ml/model/_client/sql/model_version.py
 Comment: 
 
 Filename: snowflake/ml/model/_client/sql/stage.py
@@ -1119,23 +1125,23 @@
 
 Filename: snowflake/ml/utils/connection_params.py
 Comment: 
 
 Filename: snowflake/ml/utils/sparse.py
 Comment: 
 
-Filename: snowflake_ml_python-1.5.0.dist-info/LICENSE.txt
+Filename: snowflake_ml_python-1.5.1.dist-info/LICENSE.txt
 Comment: 
 
-Filename: snowflake_ml_python-1.5.0.dist-info/METADATA
+Filename: snowflake_ml_python-1.5.1.dist-info/METADATA
 Comment: 
 
-Filename: snowflake_ml_python-1.5.0.dist-info/WHEEL
+Filename: snowflake_ml_python-1.5.1.dist-info/WHEEL
 Comment: 
 
-Filename: snowflake_ml_python-1.5.0.dist-info/top_level.txt
+Filename: snowflake_ml_python-1.5.1.dist-info/top_level.txt
 Comment: 
 
-Filename: snowflake_ml_python-1.5.0.dist-info/RECORD
+Filename: snowflake_ml_python-1.5.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## snowflake/ml/version.py

```diff
@@ -1 +1 @@
-VERSION="1.5.0"
+VERSION="1.5.1"
```

## snowflake/ml/_internal/env_utils.py

```diff
@@ -549,14 +549,17 @@
     Raises:
         ValueError: Raised when the requirement has an unsupported or unknown type.
 
     Returns:
         A tuple of Dict of conda dependencies after validated, optional pip requirements if exist
         and a string 'major.minor.patchlevel' of python version.
     """
+    if not path.exists():
+        return collections.defaultdict(list), None, None
+
     with open(path, encoding="utf-8") as f:
         env = yaml.safe_load(stream=f)
 
     assert isinstance(env, dict)
 
     deps = []
     pip_deps = []
@@ -599,14 +602,17 @@
 
     Args:
         path: Path to the requirements.txt file.
 
     Returns:
         List of dependencies string after validated.
     """
+    if not path.exists():
+        return []
+
     with open(path, encoding="utf-8") as f:
         reqs = f.readlines()
 
     return validate_pip_requirement_string_list(reqs)
 
 
 # We have to use re to support MLFlow generated python string, which use = rather than ==
```

## snowflake/ml/_internal/telemetry.py

```diff
@@ -46,14 +46,15 @@
 @enum.unique
 class TelemetryField(enum.Enum):
     # constants
     NAME = "name"
     # types of telemetry
     TYPE_FUNCTION_USAGE = "function_usage"
     TYPE_SNOWML_SPCS_USAGE = "snowml_spcs_usage"
+    TYPE_SNOWML_PIPELINE_USAGE = "snowml_pipeline_usage"
     # message keys for telemetry
     KEY_PROJECT = "project"
     KEY_SUBPROJECT = "subproject"
     KEY_FUNC_NAME = "func_name"
     KEY_FUNC_PARAMS = "func_params"
     KEY_ERROR_INFO = "error_info"
     KEY_ERROR_CODE = "error_code"
```

## snowflake/ml/_internal/utils/identifier.py

```diff
@@ -152,15 +152,15 @@
             from the schema level object and 'others' are all the content post to the object.
 
     Raises:
         ValueError: If the id is invalid.
     """
     res = _SF_SCHEMA_LEVEL_OBJECT_RE.fullmatch(path)
     if not res:
-        raise ValueError(f"Invalid identifier. It should start with database.schema.stage. Getting {path}")
+        raise ValueError(f"Invalid identifier. It should start with database.schema.object. Getting {path}")
     return (
         res.group("db"),
         res.group("schema"),
         res.group("object"),
         res.group("others"),
     )
```

## snowflake/ml/_internal/utils/sql_identifier.py

```diff
@@ -1,8 +1,8 @@
-from typing import List
+from typing import List, Optional, Tuple
 
 from snowflake.ml._internal.utils import identifier
 
 
 class SqlIdentifier(str):
     """Represents an identifier in SQL. An identifier has 3 states:
         1. User input: this is the raw input string to initializer.
@@ -75,7 +75,20 @@
 
     def __hash__(self) -> int:
         return super().__hash__()
 
 
 def to_sql_identifiers(list_of_str: List[str], *, case_sensitive: bool = False) -> List[SqlIdentifier]:
     return [SqlIdentifier(val, case_sensitive=case_sensitive) for val in list_of_str]
+
+
+def parse_fully_qualified_name(
+    name: str,
+) -> Tuple[Optional[SqlIdentifier], Optional[SqlIdentifier], SqlIdentifier]:
+    db, schema, object, _ = identifier.parse_schema_level_object_identifier(name)
+
+    assert name is not None, f"Unable parse the input name `{name}` as fully qualified."
+    return (
+        SqlIdentifier(db) if db else None,
+        SqlIdentifier(schema) if schema else None,
+        SqlIdentifier(object),
+    )
```

## snowflake/ml/dataset/__init__.py

```diff
@@ -1,10 +1,11 @@
-from .dataset import Dataset
+from .dataset import Dataset, DatasetVersion
 from .dataset_factory import create_from_dataframe, load_dataset
 from .dataset_reader import DatasetReader
 
 __all__ = [
     "Dataset",
+    "DatasetVersion",
     "DatasetReader",
     "create_from_dataframe",
     "load_dataset",
 ]
```

## snowflake/ml/dataset/dataset.py

```diff
@@ -69,18 +69,19 @@
         if self._properties is None:
             sql_result = (
                 query_result_checker.SqlResultValidator(
                     self._session,
                     f"SHOW VERSIONS LIKE '{self._version}' IN DATASET {self._parent.fully_qualified_name}",
                     statement_params=_TELEMETRY_STATEMENT_PARAMS,
                 )
-                .has_dimensions(expected_rows=1)
+                .has_column(_DATASET_VERSION_NAME_COL, allow_empty=False)
                 .validate()
             )
-            self._properties = sql_result[0].as_dict(True)
+            (match_row,) = (r for r in sql_result if r[_DATASET_VERSION_NAME_COL] == self._version)
+            self._properties = match_row.as_dict(True)
         return self._properties.get(property_name, default)
 
     def _get_metadata(self) -> Optional[dataset_metadata.DatasetMetadata]:
         if self._raw_metadata is None:
             self._raw_metadata = json.loads(self._get_property("metadata", "{}"))
             try:
                 self._metadata = (
@@ -279,15 +280,15 @@
         Args:
             version: Dataset version name. Data contents are materialized to the Dataset entity.
             input_dataframe: A Snowpark DataFrame which yields the Dataset contents.
             shuffle: A boolean represents whether the data should be shuffled globally. Default to be false.
             exclude_cols: Name of column(s) in dataset to be excluded during training/testing (e.g. timestamp).
             label_cols: Name of column(s) in dataset that contains labels.
             properties: Custom metadata properties, saved under `DatasetMetadata.properties`
-            partition_by: Optional partitioning scheme within the new Dataset version.
+            partition_by: Optional SQL expression to use as the partitioning scheme within the new Dataset version.
             comment: A descriptive comment about this dataset.
 
         Returns:
             A Dataset object with the newly created version selected.
 
         Raises:
             SnowflakeMLException: The Dataset no longer exists.
```

## snowflake/ml/dataset/dataset_reader.py

```diff
@@ -1,14 +1,15 @@
 from typing import Any, List
 
 import pandas as pd
+from pyarrow import parquet as pq
 
 from snowflake import snowpark
 from snowflake.ml._internal import telemetry
-from snowflake.ml._internal.lineage import data_source, dataset_dataframe
+from snowflake.ml._internal.lineage import data_source, lineage_utils
 from snowflake.ml._internal.utils import import_utils
 from snowflake.ml.fileset import snowfs
 
 _PROJECT = "Dataset"
 _SUBPROJECT = "DatasetReader"
 TARGET_FILE_SIZE = 32 * 2**20  # The max file size for data loading.
 
@@ -181,22 +182,18 @@
             if only_feature_cols and source.exclude_cols:
                 df = df.drop(source.exclude_cols)
             dfs.append(df)
 
         combined_df = dfs[0]
         for df in dfs[1:]:
             combined_df = combined_df.union_all_by_name(df)
-        return dataset_dataframe.DatasetDataFrame.from_dataframe(combined_df, data_sources=self._sources, inplace=True)
+        return lineage_utils.patch_dataframe(combined_df, data_sources=self._sources, inplace=True)
 
     @telemetry.send_api_usage_telemetry(project=_PROJECT, subproject=_SUBPROJECT)
     def to_pandas(self) -> pd.DataFrame:
         """Retrieve the DatasetVersion contents as a Pandas Dataframe"""
         files = self._list_files()
         if not files:
             return pd.DataFrame()  # Return empty DataFrame
         self._fs.optimize_read(files)
-        pd_dfs = []
-        for file in files:
-            with self._fs.open(file) as fp:
-                pd_dfs.append(pd.read_parquet(fp))
-        pd_df = pd_dfs[0] if len(pd_dfs) == 1 else pd.concat(pd_dfs, ignore_index=True, copy=False)
-        return pd_df
+        pd_ds = pq.ParquetDataset(files, filesystem=self._fs)
+        return pd_ds.read_pandas().to_pandas()
```

## snowflake/ml/feature_store/__init__.py

```diff
@@ -1,9 +1,15 @@
 import os
 
 from snowflake.ml._internal import init_utils
 
+from .access_manager import setup_feature_store
+
 pkg_dir = os.path.dirname(os.path.abspath(__file__))
 pkg_name = __name__
 exportable_classes = init_utils.fetch_classes_from_modules_in_pkg_dir(pkg_dir=pkg_dir, pkg_name=pkg_name)
 for k, v in exportable_classes.items():
     globals()[k] = v
+
+__all__ = list(exportable_classes.keys()) + [
+    "setup_feature_store",
+]
```

## snowflake/ml/feature_store/feature_store.py

```diff
@@ -4,15 +4,27 @@
 import functools
 import json
 import logging
 import re
 import warnings
 from dataclasses import dataclass
 from enum import Enum
-from typing import Any, Callable, Dict, List, Optional, Tuple, TypeVar, Union, cast
+from typing import (
+    Any,
+    Callable,
+    Dict,
+    List,
+    Literal,
+    Optional,
+    Tuple,
+    TypeVar,
+    Union,
+    cast,
+    overload,
+)
 
 import packaging.version as pkg_version
 import snowflake.ml.version as snowml_version
 from pytimeparse.timeparse import timeparse
 from typing_extensions import Concatenate, ParamSpec
 
 from snowflake.ml import dataset
@@ -28,15 +40,15 @@
     to_sql_identifiers,
 )
 from snowflake.ml.dataset.dataset_metadata import FeatureStoreMetadata
 from snowflake.ml.feature_store.entity import _ENTITY_NAME_LENGTH_LIMIT, Entity
 from snowflake.ml.feature_store.feature_view import (
     _FEATURE_OBJ_TYPE,
     _FEATURE_VIEW_NAME_DELIMITER,
-    _TIMESTAMP_COL_PLACEHOLDER,
+    _LEGACY_TIMESTAMP_COL_PLACEHOLDER_VALS,
     FeatureView,
     FeatureViewSlice,
     FeatureViewStatus,
     FeatureViewVersion,
     _FeatureViewMetadata,
 )
 from snowflake.snowpark import DataFrame, Row, Session, functions as F
@@ -238,31 +250,24 @@
 
         self._check_database_exists_or_throw()
         if creation_mode == CreationMode.FAIL_IF_NOT_EXIST:
             self._check_internal_objects_exist_or_throw()
 
         else:
             try:
-                self._session.sql(f"CREATE SCHEMA IF NOT EXISTS {self._config.full_schema_path}").collect(
-                    statement_params=self._telemetry_stmp
-                )
-                for tag in to_sql_identifiers(
-                    [
-                        _FEATURE_VIEW_METADATA_TAG,
-                    ]
-                ):
+                # Explicitly check if schema exists first since we may not have CREATE SCHEMA privilege
+                if len(self._find_object("SCHEMAS", self._config.schema)) == 0:
+                    self._session.sql(f"CREATE SCHEMA IF NOT EXISTS {self._config.full_schema_path}").collect(
+                        statement_params=self._telemetry_stmp
+                    )
+                for tag in to_sql_identifiers([_FEATURE_VIEW_METADATA_TAG, _FEATURE_STORE_OBJECT_TAG]):
                     self._session.sql(f"CREATE TAG IF NOT EXISTS {self._get_fully_qualified_name(tag)}").collect(
                         statement_params=self._telemetry_stmp
                     )
-
-                self._session.sql(
-                    f"CREATE TAG IF NOT EXISTS {self._get_fully_qualified_name(_FEATURE_STORE_OBJECT_TAG)}"
-                ).collect(statement_params=self._telemetry_stmp)
             except Exception as e:
-                self.clear()
                 raise snowml_exceptions.SnowflakeMLException(
                     error_code=error_codes.INTERNAL_SNOWPARK_ERROR,
                     original_exception=RuntimeError(f"Failed to create feature store {name}: {e}."),
                 )
 
         # TODO: remove this after tag_ref_internal rollout
         self._use_optimized_tag_ref = self._tag_ref_internal_enabled()
@@ -746,15 +751,15 @@
 
         tag_name = self._get_fully_qualified_name(self._get_entity_name(name))
         try:
             self._session.sql(f"DROP TAG IF EXISTS {tag_name}").collect(statement_params=self._telemetry_stmp)
         except Exception as e:
             raise snowml_exceptions.SnowflakeMLException(
                 error_code=error_codes.INTERNAL_SNOWPARK_ERROR,
-                original_exception=RuntimeError(f"Failed to alter schema or drop tag: {e}."),
+                original_exception=RuntimeError(f"Failed to delete entity: {e}."),
             ) from e
         logger.info(f"Deleted Entity {name}.")
 
     @dispatch_decorator()
     def retrieve_feature_values(
         self,
         spine_df: DataFrame,
@@ -798,27 +803,60 @@
         )
 
         if exclude_columns is not None:
             df = self._exclude_columns(df, exclude_columns)
 
         return df
 
-    @dispatch_decorator()
+    @overload
     def generate_dataset(
         self,
         name: str,
         spine_df: DataFrame,
         features: List[Union[FeatureView, FeatureViewSlice]],
         version: Optional[str] = None,
         spine_timestamp_col: Optional[str] = None,
         spine_label_cols: Optional[List[str]] = None,
         exclude_columns: Optional[List[str]] = None,
         include_feature_view_timestamp_col: bool = False,
         desc: str = "",
+        output_type: Literal["dataset"] = "dataset",
     ) -> dataset.Dataset:
+        ...
+
+    @overload
+    def generate_dataset(
+        self,
+        name: str,
+        spine_df: DataFrame,
+        features: List[Union[FeatureView, FeatureViewSlice]],
+        output_type: Literal["table"],
+        version: Optional[str] = None,
+        spine_timestamp_col: Optional[str] = None,
+        spine_label_cols: Optional[List[str]] = None,
+        exclude_columns: Optional[List[str]] = None,
+        include_feature_view_timestamp_col: bool = False,
+        desc: str = "",
+    ) -> DataFrame:
+        ...
+
+    @dispatch_decorator()  # type: ignore[misc]
+    def generate_dataset(
+        self,
+        name: str,
+        spine_df: DataFrame,
+        features: List[Union[FeatureView, FeatureViewSlice]],
+        version: Optional[str] = None,
+        spine_timestamp_col: Optional[str] = None,
+        spine_label_cols: Optional[List[str]] = None,
+        exclude_columns: Optional[List[str]] = None,
+        include_feature_view_timestamp_col: bool = False,
+        desc: str = "",
+        output_type: Literal["dataset", "table"] = "dataset",
+    ) -> Union[dataset.Dataset, DataFrame]:
         """
         Generate dataset by given source table and feature views.
 
         Args:
             name: The name of the Dataset to be generated. Datasets are uniquely identified within a schema
                 by their name and version.
             spine_df: The fact table contains the raw dataset.
@@ -830,38 +868,37 @@
                 timestamp_col.
             spine_label_cols: Name of column(s) in spine_df that contains labels.
             exclude_columns: Column names to exclude from the result dataframe.
                 The underlying storage will still contain the columns.
             include_feature_view_timestamp_col: Generated dataset will include timestamp column of feature view
                 (if feature view has timestamp column) if set true. Default to false.
             desc: A description about this dataset.
+            output_type: The type of Snowflake storage to use for the generated training data.
 
         Returns:
-            A Dataset object.
+            If output_type is "dataset" (default), returns a Dataset object.
+            If output_type is "table", returns a Snowpark DataFrame representing the table.
 
         Raises:
-            SnowflakeMLException: [ValueError] spine_df contains more than one query.
             SnowflakeMLException: [ValueError] Dataset name/version already exists
             SnowflakeMLException: [ValueError] Snapshot creation failed.
+            SnowflakeMLException: [ValueError] Invalid output_type specified.
             SnowflakeMLException: [RuntimeError] Failed to create clone from table.
             SnowflakeMLException: [RuntimeError] Failed to find resources.
         """
+        if output_type not in {"table", "dataset"}:
+            raise snowml_exceptions.SnowflakeMLException(
+                error_code=error_codes.INVALID_ARGUMENT,
+                original_exception=ValueError(f"Invalid output_type: {output_type}."),
+            )
         if spine_timestamp_col is not None:
             spine_timestamp_col = SqlIdentifier(spine_timestamp_col)
         if spine_label_cols is not None:
             spine_label_cols = to_sql_identifiers(spine_label_cols)  # type: ignore[assignment]
 
-        if len(spine_df.queries["queries"]) != 1:
-            raise snowml_exceptions.SnowflakeMLException(
-                error_code=error_codes.INVALID_ARGUMENT,
-                original_exception=ValueError(
-                    f"spine_df must contain only one query. Got: {spine_df.queries['queries']}"
-                ),
-            )
-
         result_df, join_keys = self._join_features(
             spine_df, features, spine_timestamp_col, include_feature_view_timestamp_col
         )
 
         # Convert name to fully qualified name if not already fully qualified
         db_name, schema_name, object_name, _ = identifier.parse_schema_level_object_identifier(name)
         name = "{}.{}.{}".format(
@@ -871,41 +908,57 @@
         )
         version = version or datetime.datetime.now().strftime("%Y_%m_%d_%H_%M_%S")
 
         if exclude_columns is not None:
             result_df = self._exclude_columns(result_df, exclude_columns)
 
         fs_meta = FeatureStoreMetadata(
-            spine_query=spine_df.queries["queries"][0],
+            spine_query=spine_df.queries["queries"][-1],
             serialized_feature_views=[fv.to_json() for fv in features],
             spine_timestamp_col=spine_timestamp_col,
         )
 
         try:
-            ds: dataset.Dataset = dataset.create_from_dataframe(
-                self._session,
-                name,
-                version,
-                input_dataframe=result_df,
-                exclude_cols=[spine_timestamp_col],
-                label_cols=spine_label_cols,
-                properties=fs_meta,
-                comment=desc,
-            )
-            return ds
+            if output_type == "table":
+                table_name = f"{name}_{version}"
+                result_df.write.mode("errorifexists").save_as_table(table_name)  # type: ignore[call-overload]
+                ds_df = self._session.table(table_name)
+                return ds_df
+            else:
+                assert output_type == "dataset"
+                if not self._is_dataset_enabled():
+                    raise snowml_exceptions.SnowflakeMLException(
+                        error_code=error_codes.SNOWML_CREATE_FAILED,
+                        original_exception=RuntimeError(
+                            "Dataset is not enabled in your account. Ask your account admin to set"
+                            ' FEATURE_DATASET=ENABLED or set output_type="table" to generate the data'
+                            " as a Snowflake Table instead."
+                        ),
+                    )
+                ds: dataset.Dataset = dataset.create_from_dataframe(
+                    self._session,
+                    name,
+                    version,
+                    input_dataframe=result_df,
+                    exclude_cols=[spine_timestamp_col],
+                    label_cols=spine_label_cols,
+                    properties=fs_meta,
+                    comment=desc,
+                )
+                return ds
 
         except dataset_errors.DatasetExistError as e:
             raise snowml_exceptions.SnowflakeMLException(
                 error_code=error_codes.OBJECT_ALREADY_EXISTS,
-                original_exception=ValueError(str(e)),
+                original_exception=RuntimeError(str(e)),
             ) from e
         except SnowparkSQLException as e:
             raise snowml_exceptions.SnowflakeMLException(
                 error_code=error_codes.INTERNAL_SNOWPARK_ERROR,
-                original_exception=RuntimeError(f"An error occurred during Dataset generation: {e}."),
+                original_exception=RuntimeError(f"An error occurred during dataset generation: {e}."),
             ) from e
 
     @dispatch_decorator()
     def load_feature_views_from_dataset(self, ds: dataset.Dataset) -> List[Union[FeatureView, FeatureViewSlice]]:
         """
         Retrieve FeatureViews used during Dataset construction.
 
@@ -926,60 +979,55 @@
             or source_meta.properties.serialized_feature_views is None
         ):
             raise ValueError(f"Dataset {ds} does not contain valid feature view information.")
 
         return self._load_serialized_feature_objects(source_meta.properties.serialized_feature_views)
 
     @dispatch_decorator()
-    def clear(self) -> None:
+    def _clear(self, dryrun: bool = True) -> None:
         """
-        Clear all feature store internal objects including feature views, entities etc. Note feature store
-        instance (snowflake schema) won't be deleted. Use snowflake to delete feature store instance.
+        Clear all feature views and entities. Note Feature Store schema and metadata will NOT be purged
+        together. Use SQL to delete schema and metadata instead.
 
-        Raises:
-            SnowflakeMLException: [RuntimeError] Failed to clear feature store.
+        Args:
+            dryrun: Print a list of objects will be deleted but not actually perform the deletion when true.
         """
-        try:
-            result = self._session.sql(
-                f"""
-                SELECT *
-                FROM {self._config.database}.INFORMATION_SCHEMA.SCHEMATA
-                WHERE SCHEMA_NAME = '{self._config.schema.resolved()}'
-            """
-            ).collect()
-            if len(result) == 0:
-                return
+        warnings.warn(
+            "It will clear ALL feature views and entities in this Feature Store. Make sure your role"
+            " has sufficient access to all feature views and entities. Insufficient access to some feature"
+            " views or entities will leave Feature Store in an incomplete state.",
+            stacklevel=2,
+            category=UserWarning,
+        )
 
-            fs_obj_tag = self._find_object("TAGS", SqlIdentifier(_FEATURE_STORE_OBJECT_TAG))
-            if len(fs_obj_tag) == 0:
-                return
-
-            object_types = ["DYNAMIC TABLES", "DATASETS", "VIEWS", "TASKS"]
-            for obj_type in object_types:
-                all_object_rows = self._find_object(obj_type, None)
-                for row in all_object_rows:
-                    obj_name = self._get_fully_qualified_name(SqlIdentifier(row["name"], case_sensitive=True))
-                    self._session.sql(f"DROP {obj_type[:-1]} {obj_name}").collect()
-                    logger.info(f"Deleted {obj_type[:-1]}: {obj_name}.")
+        all_fvs_df = self.list_feature_views()
+        all_entities_df = self.list_entities()
+        all_fvs_rows = all_fvs_df.collect()
+        all_entities_rows = all_entities_df.collect()
+
+        if dryrun:
+            logger.info(
+                "Following feature views and entities will be deleted."
+                + " Set 'dryrun=False' to perform the actual deletion."
+            )
+            logger.info(f"Total {len(all_fvs_rows)} Feature views to be deleted:")
+            all_fvs_df.show(n=len(all_fvs_rows))
+            logger.info(f"\nTotal {len(all_entities_rows)} entities to be deleted:")
+            all_entities_df.show(n=len(all_entities_rows))
+            return
+
+        for fv_row in all_fvs_rows:
+            fv = self.get_feature_view(
+                SqlIdentifier(fv_row["NAME"], case_sensitive=True).identifier(), fv_row["VERSION"]
+            )
+            self.delete_feature_view(fv)
 
-            entity_tags = self._find_object("TAGS", SqlIdentifier(_ENTITY_TAG_PREFIX), prefix_match=True)
-            all_tags = [
-                _FEATURE_STORE_OBJECT_TAG,
-                _FEATURE_VIEW_METADATA_TAG,
-            ] + [SqlIdentifier(row["name"], case_sensitive=True) for row in entity_tags]
-            for tag_name in all_tags:
-                obj_name = self._get_fully_qualified_name(tag_name)
-                self._session.sql(f"DROP TAG IF EXISTS {obj_name}").collect()
-                logger.info(f"Deleted TAG: {obj_name}.")
+        for entity_row in all_entities_rows:
+            self.delete_entity(SqlIdentifier(entity_row["NAME"], case_sensitive=True).identifier())
 
-        except Exception as e:
-            raise snowml_exceptions.SnowflakeMLException(
-                error_code=error_codes.INTERNAL_SNOWPARK_ERROR,
-                original_exception=RuntimeError(f"Failed to clear feature store {self._config.full_schema_path}: {e}."),
-            ) from e
         logger.info(f"Feature store {self._config.full_schema_path} has been cleared.")
 
     def _get_feature_view_if_exists(self, name: str, version: str) -> FeatureView:
         existing_fv = self.get_feature_view(name, version)
         warnings.warn(
             f"FeatureView {name}/{version} already exists. Skip registration."
             + " Set `overwrite` to True if you want to replace existing FeatureView.",
@@ -1089,22 +1137,14 @@
     def _join_features(
         self,
         spine_df: DataFrame,
         features: List[Union[FeatureView, FeatureViewSlice]],
         spine_timestamp_col: Optional[SqlIdentifier],
         include_feature_view_timestamp_col: bool,
     ) -> Tuple[DataFrame, List[SqlIdentifier]]:
-        if len(spine_df.queries["queries"]) != 1:
-            raise snowml_exceptions.SnowflakeMLException(
-                error_code=error_codes.INVALID_ARGUMENT,
-                original_exception=ValueError(
-                    f"spine_df must contain only one query. Got: {spine_df.queries['queries']}"
-                ),
-            )
-
         for f in features:
             f = f.feature_view_ref if isinstance(f, FeatureViewSlice) else f
             if f.status == FeatureViewStatus.DRAFT:
                 raise snowml_exceptions.SnowflakeMLException(
                     error_code=error_codes.NOT_FOUND,
                     original_exception=ValueError(f"FeatureView {f.name} has not been registered."),
                 )
@@ -1118,15 +1158,15 @@
                             ),
                         )
 
         if self._asof_join_enabled is None:
             self._asof_join_enabled = self._is_asof_join_enabled()
 
         # TODO: leverage Snowpark dataframe for more concise syntax once it supports AsOfJoin
-        query = spine_df.queries["queries"][0]
+        query = spine_df.queries["queries"][-1]
         layer = 0
         for f in features:
             if isinstance(f, FeatureViewSlice):
                 cols = f.names
                 f = f.feature_view_ref
             else:
                 cols = f.feature_names
@@ -1176,15 +1216,23 @@
                         SELECT {join_keys_str}, {', '.join(cols)}
                         FROM {join_table_name}
                     ) r_{layer}
                     ON {' AND '.join([f'l_{layer}.{k} = r_{layer}.{k}' for k in join_keys])}
                 """
             layer += 1
 
-        return self._session.sql(query), join_keys
+        # TODO: construct result dataframe with datframe APIs once ASOF join is supported natively.
+        # Below code manually construct result dataframe from private members of spine dataframe, which
+        # likely will cause unintentional issues. This setp is needed because spine_df might contains
+        # prerequisite queries and post actions that must be carried over to result dataframe.
+        result_df = self._session.sql(query)
+        result_df._plan.queries = spine_df._plan.queries[:-1] + result_df._plan.queries
+        result_df._plan.post_actions = spine_df._plan.post_actions
+
+        return result_df, join_keys
 
     def _check_database_exists_or_throw(self) -> None:
         resolved_db_name = self._config.database.resolved()
         dbs = self._session.sql(
             f"""
             SHOW DATABASES LIKE '{resolved_db_name}' STARTS WITH '{resolved_db_name}'
         """
@@ -1513,68 +1561,71 @@
         m = re.match(_DT_OR_VIEW_QUERY_PATTERN, row["text"])
         if m is None:
             raise snowml_exceptions.SnowflakeMLException(
                 error_code=error_codes.INTERNAL_SNOWML_ERROR,
                 original_exception=RuntimeError(f"Failed to parse query text for FeatureView {name}/{version}: {row}."),
             )
 
+        fv_name = FeatureView._get_physical_name(name, version)
+        infer_schema_df = self._session.sql(f"SELECT * FROM {self._get_fully_qualified_name(fv_name)}")
+
         if m.group("obj_type") == "DYNAMIC TABLE":
             query = m.group("query")
             df = self._session.sql(query)
             desc = m.group("comment")
             fv_metadata = _FeatureViewMetadata.from_json(m.group("fv_metadata"))
             entities = [find_and_compose_entity(n) for n in fv_metadata.entities]
             ts_col = fv_metadata.timestamp_col
-            timestamp_col = ts_col if ts_col != _TIMESTAMP_COL_PLACEHOLDER else None
+            timestamp_col = ts_col if ts_col not in _LEGACY_TIMESTAMP_COL_PLACEHOLDER_VALS else None
 
             fv = FeatureView._construct_feature_view(
                 name=name,
                 entities=entities,
                 feature_df=df,
                 timestamp_col=timestamp_col,
                 desc=desc,
                 version=version,
                 status=FeatureViewStatus(row["scheduling_state"]),
-                feature_descs=self._fetch_column_descs(
-                    "DYNAMIC TABLE", SqlIdentifier(row["name"], case_sensitive=True)
-                ),
+                feature_descs=self._fetch_column_descs("DYNAMIC TABLE", fv_name),
                 refresh_freq=row["target_lag"],
                 database=self._config.database.identifier(),
                 schema=self._config.schema.identifier(),
                 warehouse=SqlIdentifier(row["warehouse"], case_sensitive=True).identifier(),
                 refresh_mode=row["refresh_mode"],
                 refresh_mode_reason=row["refresh_mode_reason"],
                 owner=row["owner"],
+                infer_schema_df=infer_schema_df,
             )
             return fv
         else:
             query = m.group("query")
             df = self._session.sql(query)
             desc = m.group("comment")
             fv_metadata = _FeatureViewMetadata.from_json(m.group("fv_metadata"))
             entities = [find_and_compose_entity(n) for n in fv_metadata.entities]
             ts_col = fv_metadata.timestamp_col
-            timestamp_col = ts_col if ts_col != _TIMESTAMP_COL_PLACEHOLDER else None
+            timestamp_col = ts_col if ts_col not in _LEGACY_TIMESTAMP_COL_PLACEHOLDER_VALS else None
 
             fv = FeatureView._construct_feature_view(
                 name=name,
                 entities=entities,
                 feature_df=df,
                 timestamp_col=timestamp_col,
                 desc=desc,
                 version=version,
                 status=FeatureViewStatus.STATIC,
-                feature_descs=self._fetch_column_descs("VIEW", SqlIdentifier(row["name"], case_sensitive=True)),
+                feature_descs=self._fetch_column_descs("VIEW", fv_name),
                 refresh_freq=None,
                 database=self._config.database.identifier(),
                 schema=self._config.schema.identifier(),
                 warehouse=None,
                 refresh_mode=None,
                 refresh_mode_reason=None,
                 owner=row["owner"],
+                infer_schema_df=infer_schema_df,
             )
             return fv
 
     def _fetch_column_descs(self, obj_type: str, obj_name: SqlIdentifier) -> Dict[str, str]:
         res = self._session.sql(f"DESC {obj_type} {self._get_fully_qualified_name(obj_name)}").collect(
             statement_params=self._telemetry_stmp
         )
@@ -1716,14 +1767,23 @@
                     ) LIMIT 1;
                 """
             ).collect()
             return True
         except Exception:
             return False
 
+    def _is_dataset_enabled(self) -> bool:
+        try:
+            self._session.sql(f"SHOW DATASETS IN SCHEMA {self._config.full_schema_path}").collect()
+            return True
+        except SnowparkSQLException as e:
+            if "'DATASETS' does not exist" in e.message:
+                return False
+            raise
+
     def _check_feature_store_object_versions(self) -> None:
         versions = self._collapse_object_versions()
         if len(versions) > 0 and pkg_version.parse(snowml_version.VERSION) < versions[0]:
             warnings.warn(
                 "The current snowflake-ml-python version out of date, package upgrade recommended "
                 + f"(current={snowml_version.VERSION}, recommended>={str(versions[0])})",
                 stacklevel=2,
```

## snowflake/ml/feature_store/feature_view.py

```diff
@@ -1,15 +1,15 @@
 from __future__ import annotations
 
 import json
 import re
 from collections import OrderedDict
 from dataclasses import asdict, dataclass
 from enum import Enum
-from typing import Dict, List, Optional
+from typing import Any, Dict, List, Optional
 
 from snowflake.ml._internal.exceptions import (
     error_codes,
     exceptions as snowml_exceptions,
 )
 from snowflake.ml._internal.utils.identifier import concat_names
 from snowflake.ml._internal.utils.sql_identifier import (
@@ -23,15 +23,16 @@
     StructType,
     TimestampType,
     TimeType,
     _NumericType,
 )
 
 _FEATURE_VIEW_NAME_DELIMITER = "$"
-_TIMESTAMP_COL_PLACEHOLDER = "FS_TIMESTAMP_COL_PLACEHOLDER_VAL"
+_LEGACY_TIMESTAMP_COL_PLACEHOLDER_VALS = ["FS_TIMESTAMP_COL_PLACEHOLDER_VAL", "NULL"]
+_TIMESTAMP_COL_PLACEHOLDER = "NULL"
 _FEATURE_OBJ_TYPE = "FEATURE_OBJ_TYPE"
 # Feature view version rule is aligned with dataset version rule in SQL.
 _FEATURE_VIEW_VERSION_RE = re.compile(r"^[a-zA-Z0-9][a-zA-Z0-9_.\-]*$")
 _FEATURE_VIEW_VERSION_MAX_LENGTH = 128
 
 
 @dataclass(frozen=True)
@@ -117,42 +118,45 @@
         self,
         name: str,
         entities: List[Entity],
         feature_df: DataFrame,
         timestamp_col: Optional[str] = None,
         refresh_freq: Optional[str] = None,
         desc: str = "",
+        **_kwargs: Any,
     ) -> None:
         """
         Create a FeatureView instance.
 
         Args:
-            name: name of the FeatureView. NOTE: FeatureView name will be capitalized.
+            name: name of the FeatureView. NOTE: following Snowflake identifier rule
             entities: entities that the FeatureView is associated with.
             feature_df: Snowpark DataFrame containing data source and all feature feature_df logics.
                 Final projection of the DataFrame should contain feature names, join keys and timestamp(if applicable).
             timestamp_col: name of the timestamp column for point-in-time lookup when consuming the
                 feature values.
             refresh_freq: Time unit defining how often the new feature data should be generated.
                 Valid args are { <num> { seconds | minutes | hours | days } | DOWNSTREAM | <cron expr> <time zone>}.
                 NOTE: Currently minimum refresh frequency is 1 minute.
                 NOTE: If refresh_freq is in cron expression format, there must be a valid time zone as well.
                     E.g. * * * * * UTC
                 NOTE: If refresh_freq is not provided, then FeatureView will be registered as View on Snowflake backend
                     and there won't be extra storage cost.
             desc: description of the FeatureView.
+            _kwargs: reserved kwargs for system generated args. NOTE: DO NOT USE.
         """
 
         self._name: SqlIdentifier = SqlIdentifier(name)
         self._entities: List[Entity] = entities
         self._feature_df: DataFrame = feature_df
         self._timestamp_col: Optional[SqlIdentifier] = (
             SqlIdentifier(timestamp_col) if timestamp_col is not None else None
         )
         self._desc: str = desc
+        self._infer_schema_df: DataFrame = _kwargs.get("_infer_schema_df", self._feature_df)
         self._query: str = self._get_query()
         self._version: Optional[FeatureViewVersion] = None
         self._status: FeatureViewStatus = FeatureViewStatus.DRAFT
         self._feature_desc: OrderedDict[SqlIdentifier, str] = OrderedDict((f, "") for f in self._get_feature_names())
         self._refresh_freq: Optional[str] = refresh_freq
         self._database: Optional[SqlIdentifier] = None
         self._schema: Optional[SqlIdentifier] = None
@@ -291,15 +295,15 @@
             raise RuntimeError(
                 f"Feature view {self.name}/{self.version} must be registered and non-static to update warehouse."
             )
         self._warehouse = SqlIdentifier(new_value)
 
     @property
     def output_schema(self) -> StructType:
-        return self._feature_df.schema
+        return self._infer_schema_df.schema
 
     @property
     def refresh_mode(self) -> Optional[str]:
         return self._refresh_mode
 
     @property
     def refresh_mode_reason(self) -> Optional[str]:
@@ -325,37 +329,37 @@
 
     def _validate(self) -> None:
         if _FEATURE_VIEW_NAME_DELIMITER in self._name:
             raise ValueError(
                 f"FeatureView name `{self._name}` contains invalid character `{_FEATURE_VIEW_NAME_DELIMITER}`."
             )
 
-        unescaped_df_cols = to_sql_identifiers(self._feature_df.columns)
+        unescaped_df_cols = to_sql_identifiers(self._infer_schema_df.columns)
         for e in self._entities:
             for k in e.join_keys:
                 if k not in unescaped_df_cols:
                     raise ValueError(
                         f"join_key {k} in Entity {e.name} is not found in input dataframe: {unescaped_df_cols}"
                     )
 
         if self._timestamp_col is not None:
             ts_col = self._timestamp_col
             if ts_col == SqlIdentifier(_TIMESTAMP_COL_PLACEHOLDER):
                 raise ValueError(f"Invalid timestamp_col name, cannot be {_TIMESTAMP_COL_PLACEHOLDER}.")
-            if ts_col not in to_sql_identifiers(self._feature_df.columns):
+            if ts_col not in to_sql_identifiers(self._infer_schema_df.columns):
                 raise ValueError(f"timestamp_col {ts_col} is not found in input dataframe.")
 
-            col_type = self._feature_df.schema[ts_col].datatype
+            col_type = self._infer_schema_df.schema[ts_col].datatype
             if not isinstance(col_type, (DateType, TimeType, TimestampType, _NumericType)):
                 raise ValueError(f"Invalid data type for timestamp_col {ts_col}: {col_type}.")
 
     def _get_feature_names(self) -> List[SqlIdentifier]:
         join_keys = [k for e in self._entities for k in e.join_keys]
         ts_col = [self._timestamp_col] if self._timestamp_col is not None else []
-        feature_names = to_sql_identifiers(self._feature_df.columns, case_sensitive=True)
+        feature_names = to_sql_identifiers(self._infer_schema_df.columns, case_sensitive=False)
         return [c for c in feature_names if c not in join_keys + ts_col]
 
     def __repr__(self) -> str:
         states = (f"{k}={v}" for k, v in vars(self).items())
         return f"{type(self).__name__}({', '.join(states)})"
 
     def __eq__(self, other: object) -> bool:
@@ -380,14 +384,17 @@
             and self._owner == other._owner
         )
 
     def _to_dict(self) -> Dict[str, str]:
         fv_dict = self.__dict__.copy()
         if "_feature_df" in fv_dict:
             fv_dict.pop("_feature_df")
+        if "_infer_schema_df" in fv_dict:
+            infer_schema_df = fv_dict.pop("_infer_schema_df")
+            fv_dict["_infer_schema_query"] = infer_schema_df.queries["queries"][0]
         fv_dict["_entities"] = [e._to_dict() for e in self._entities]
         fv_dict["_status"] = str(self._status)
         fv_dict["_name"] = str(self._name) if self._name is not None else None
         fv_dict["_version"] = str(self._version) if self._version is not None else None
         fv_dict["_database"] = str(self._database) if self._database is not None else None
         fv_dict["_schema"] = str(self._schema) if self._schema is not None else None
         fv_dict["_warehouse"] = str(self._warehouse) if self._warehouse is not None else None
@@ -436,14 +443,15 @@
             refresh_freq=json_dict["_refresh_freq"],
             database=json_dict["_database"],
             schema=json_dict["_schema"],
             warehouse=json_dict["_warehouse"],
             refresh_mode=json_dict["_refresh_mode"],
             refresh_mode_reason=json_dict["_refresh_mode_reason"],
             owner=json_dict["_owner"],
+            infer_schema_df=session.sql(json_dict.get("_infer_schema_query", None)),
         )
 
     @staticmethod
     def _get_physical_name(fv_name: SqlIdentifier, fv_version: FeatureViewVersion) -> SqlIdentifier:
         return SqlIdentifier(
             concat_names(
                 [
@@ -467,21 +475,23 @@
         refresh_freq: Optional[str],
         database: str,
         schema: str,
         warehouse: Optional[str],
         refresh_mode: Optional[str],
         refresh_mode_reason: Optional[str],
         owner: Optional[str],
+        infer_schema_df: Optional[DataFrame],
     ) -> FeatureView:
         fv = FeatureView(
             name=name,
             entities=entities,
             feature_df=feature_df,
             timestamp_col=timestamp_col,
             desc=desc,
+            _infer_schema_df=infer_schema_df,
         )
         fv._version = FeatureViewVersion(version) if version is not None else None
         fv._status = status
         fv._refresh_freq = refresh_freq
         fv._database = SqlIdentifier(database) if database is not None else None
         fv._schema = SqlIdentifier(schema) if schema is not None else None
         fv._warehouse = SqlIdentifier(warehouse) if warehouse is not None else None
```

## snowflake/ml/fileset/embedded_stage_fs.py

```diff
@@ -74,30 +74,34 @@
         """Fetch presigned urls for the given files."""
         # SnowURL requires full snow://<domain>/<entity>/versions/<version> as the stage path arg to get_presigned_Url
         versions_dict = defaultdict(list)
         for file in files:
             match = _SNOWURL_PATH_RE.fullmatch(file)
             assert match is not None and match.group("filepath") is not None
             versions_dict[match.group("version")].append(match.group("filepath"))
-        presigned_urls: List[Tuple[str, str]] = []
         try:
+            async_jobs: List[snowpark.AsyncJob] = []
             for version, version_files in versions_dict.items():
                 for file in version_files:
                     stage_loc = f"{self.stage_name}/versions/{version}"
-                    presigned_urls.extend(
-                        self._session.sql(
-                            f"select '{version}/{file}' as name,"
-                            f" get_presigned_url('{stage_loc}', '{file}', {url_lifetime}) as url"
-                        ).collect(
-                            statement_params=telemetry.get_function_usage_statement_params(
-                                project=stage_fs._PROJECT,
-                                api_calls=[snowpark.DataFrame.collect],
-                            ),
-                        )
+                    query_result = self._session.sql(
+                        f"select '{version}/{file}' as name,"
+                        f" get_presigned_url('{stage_loc}', '{file}', {url_lifetime}) as url"
+                    ).collect(
+                        block=False,
+                        statement_params=telemetry.get_function_usage_statement_params(
+                            project=stage_fs._PROJECT,
+                            api_calls=[snowpark.DataFrame.collect],
+                        ),
                     )
+                    async_jobs.append(query_result)
+            presigned_urls: List[Tuple[str, str]] = [
+                (r["NAME"], r["URL"]) for job in async_jobs for r in stage_fs._resolve_async_job(job)
+            ]
+            return presigned_urls
         except snowpark_exceptions.SnowparkClientException as e:
             if e.message.startswith(fileset_errors.ERRNO_DOMAIN_NOT_EXIST) or e.message.startswith(
                 fileset_errors.ERRNO_STAGE_NOT_EXIST
             ):
                 raise snowml_exceptions.SnowflakeMLException(
                     error_code=error_codes.SNOWML_NOT_FOUND,
                     original_exception=fileset_errors.StageNotFoundError(
@@ -105,15 +109,14 @@
                     ),
                 )
             else:
                 raise snowml_exceptions.SnowflakeMLException(
                     error_code=error_codes.INTERNAL_SNOWML_ERROR,
                     original_exception=fileset_errors.FileSetError(str(e)),
                 )
-        return presigned_urls
 
     @classmethod
     def _parent(cls, path: str) -> str:
         """Get parent of specified path up to minimally valid root path.
 
         For SnowURL, the minimum valid path is snow://<domain>/<entity>/versions/<version>
```

## snowflake/ml/fileset/snowfs.py

```diff
@@ -100,15 +100,16 @@
         if stage_name.startswith(protocol):
             stage_name = stage_name[len(protocol) :]
         abs_path = stage_name + "/" + path
         # FIXME(dhung): Temporary fix for bug in GS version 8.17
         if self._IS_BUGGED_VERSION:
             match = _SNOWURL_PATTERN.fullmatch(abs_path)
             assert match is not None
-            abs_path = abs_path.replace(match.group("relpath"), match.group("relpath").lstrip("/"))
+            if match.group("relpath"):
+                abs_path = abs_path.replace(match.group("relpath"), match.group("relpath").lstrip("/"))
         return abs_path
 
     @classmethod
     def _parse_file_path(cls, path: str) -> _SFFileEntityPath:  # type: ignore[override]
         """Parse a snowflake location path.
 
         The following properties will be extracted from the path input:
@@ -141,15 +142,15 @@
             name = identifier.get_schema_level_object_identifier(*parsed_name)
             filepath = snowurl_match.group("path")
             version = snowurl_match.group("version")
             relative_path = snowurl_match.group("relpath") or ""
             logging.debug(f"Parsed snow URL: {snowurl_match.groups()}")
             # FIXME(dhung): Temporary fix for bug in GS version 8.17
             if cls._IS_BUGGED_VERSION:
-                filepath = filepath.replace(f"{version}/", f"{version}//")
+                filepath = f"versions/{version}//{relative_path}"
             return _SFFileEntityPath(
                 domain=domain, name=name, version=version, relative_path=relative_path, filepath=filepath
             )
         except ValueError as e:
             raise snowml_exceptions.SnowflakeMLException(
                 error_code=error_codes.SNOWML_INVALID_STAGE,
                 original_exception=e,
```

## snowflake/ml/fileset/stage_fs.py

```diff
@@ -1,27 +1,28 @@
 import inspect
 import logging
 import time
 from dataclasses import dataclass
-from typing import Any, Dict, List, Optional, Tuple, Union
+from typing import Any, Dict, List, Optional, Tuple, Union, cast
 
 import fsspec
 from fsspec.implementations import http as httpfs
 
 from snowflake import snowpark
-from snowflake.connector import connection, errorcode
+from snowflake.connector import connection, errorcode, errors as snowpark_errors
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.exceptions import (
     error_codes,
     exceptions as snowml_exceptions,
     fileset_error_messages,
     fileset_errors,
 )
 from snowflake.snowpark import exceptions as snowpark_exceptions
 from snowflake.snowpark._internal import utils as snowpark_utils
+from snowflake.snowpark._internal.analyzer import snowflake_plan
 
 # The default length of how long a presigned url stays active in seconds.
 # Presigned url here is used to fetch file objects from Snowflake when SFStageFileSystem.open() is called.
 _PRESIGNED_URL_LIFETIME_SEC = 14400
 
 # The threshold of when the presigned url should get refreshed before its expiration.
 _PRESIGNED_URL_HEADROOM_SEC = 3600
@@ -163,15 +164,16 @@
         Raises:
             SnowflakeMLException: An error occurred when the given path points to a stage that cannot be found.
             SnowflakeMLException: An error occurred when Snowflake cannot list files in the given stage path.
         """
         try:
             loc = self.stage_name
             path = path.lstrip("/")
-            objects = self._session.sql(f"LIST '{loc}/{path}'").collect()
+            async_job: snowpark.AsyncJob = self._session.sql(f"LIST '{loc}/{path}'").collect(block=False)
+            objects: List[snowpark.Row] = _resolve_async_job(async_job)
         except snowpark_exceptions.SnowparkClientException as e:
             if e.message.startswith(fileset_errors.ERRNO_DOMAIN_NOT_EXIST):
                 raise snowml_exceptions.SnowflakeMLException(
                     error_code=error_codes.SNOWML_NOT_FOUND,
                     original_exception=fileset_errors.StageNotFoundError(
                         f"Stage {loc} does not exist or is not authorized."
                     ),
@@ -285,17 +287,15 @@
                 )
             else:
                 raise snowml_exceptions.SnowflakeMLException(
                     error_code=error_codes.INTERNAL_SNOWPARK_ERROR,
                     original_exception=e,
                 )
 
-    def _parse_list_result(
-        self, list_result: List[Tuple[str, int, str, str]], search_path: str
-    ) -> List[Dict[str, Any]]:
+    def _parse_list_result(self, list_result: List[snowpark.Row], search_path: str) -> List[Dict[str, Any]]:
         """Convert the result from LIST query to the expected format of fsspec ls() method.
 
         Note that Snowflake LIST query has different behavior with ls(). LIST query will return all the stage files
         whose path starts with the given search_path. However, ls() should only return files that either exactly
         match the given search_path or are under the given search_path directory.
 
         For example, both "mydir/hello_world.txt" and "mydir1/hello_world.txt" will appear as a result of "LIST mydir",
@@ -308,15 +308,16 @@
             search_path: The path that was used by the List query to get the list_result.
 
         Returns:
             A list of dict, where each dict contains key-value pairs as the properties of a file.
         """
         files: Dict[str, Dict[str, Any]] = {}
         search_path = search_path.strip("/")
-        for name, size, md5, last_modified in list_result:
+        for row in list_result:
+            name, size, md5, last_modified = row["name"], row["size"], row["md5"], row["last_modified"]
             obj_path = self._stage_path_to_relative_path(name)
             if obj_path == search_path:
                 # If there is a exact match, then the matched object will always be a file object.
                 self._add_file_info_helper(files, obj_path, size, "file", md5, last_modified)
                 continue
             elif search_path and not obj_path.startswith(search_path + "/"):
                 # If the path doesn't start with "<search_path>/", the object is not under the <search_path> directory.
@@ -404,7 +405,24 @@
         return presigned_urls
 
 
 def _match_error_code(ex: snowpark_exceptions.SnowparkSQLException, error_code: int) -> bool:
     # Snowpark writes error code to message instead of populating e.error_code
     error_code_str = str(error_code)
     return ex.error_code == error_code_str or error_code_str in ex.message
+
+
+@snowflake_plan.SnowflakePlan.Decorator.wrap_exception  # type: ignore[misc]
+def _resolve_async_job(async_job: snowpark.AsyncJob) -> List[snowpark.Row]:
+    # Make sure Snowpark exceptions are properly caught and converted by wrap_exception wrapper
+    try:
+        query_result = cast(List[snowpark.Row], async_job.result("row"))
+        return query_result
+    except snowpark_errors.DatabaseError as e:
+        # HACK: Snowpark surfaces a generic exception if query doesn't complete immediately
+        # assume it's due to FileNotFound
+        if type(e) is snowpark_errors.DatabaseError and "results are unavailable" in str(e):
+            raise snowml_exceptions.SnowflakeMLException(
+                error_code=error_codes.SNOWML_NOT_FOUND,
+                original_exception=fileset_errors.StageNotFoundError("Query failed."),
+            ) from e
+        raise
```

## snowflake/ml/model/_client/model/model_impl.py

```diff
@@ -1,13 +1,13 @@
-from typing import Dict, List, Optional, Tuple, Union
+from typing import Dict, List, Optional, Union
 
 import pandas as pd
 
 from snowflake.ml._internal import telemetry
-from snowflake.ml._internal.utils import identifier, sql_identifier
+from snowflake.ml._internal.utils import sql_identifier
 from snowflake.ml.model._client.model import model_version_impl
 from snowflake.ml.model._client.ops import model_ops
 
 _TELEMETRY_PROJECT = "MLOps"
 _TELEMETRY_SUBPROJECT = "ModelManagement"
 
 
@@ -41,15 +41,15 @@
     def name(self) -> str:
         """Return the name of the model that can be used to refer to it in SQL."""
         return self._model_name.identifier()
 
     @property
     def fully_qualified_name(self) -> str:
         """Return the fully qualified name of the model that can be used to refer to it in SQL."""
-        return self._model_ops._model_version_client.fully_qualified_model_name(self._model_name)
+        return self._model_ops._model_version_client.fully_qualified_object_name(None, None, self._model_name)
 
     @property
     @telemetry.send_api_usage_telemetry(
         project=_TELEMETRY_PROJECT,
         subproject=_TELEMETRY_SUBPROJECT,
     )
     def description(self) -> str:
@@ -72,14 +72,16 @@
     def comment(self) -> str:
         """The comment to the model."""
         statement_params = telemetry.get_statement_params(
             project=_TELEMETRY_PROJECT,
             subproject=_TELEMETRY_SUBPROJECT,
         )
         return self._model_ops.get_comment(
+            database_name=None,
+            schema_name=None,
             model_name=self._model_name,
             statement_params=statement_params,
         )
 
     @comment.setter
     @telemetry.send_api_usage_telemetry(
         project=_TELEMETRY_PROJECT,
@@ -88,14 +90,16 @@
     def comment(self, comment: str) -> None:
         statement_params = telemetry.get_statement_params(
             project=_TELEMETRY_PROJECT,
             subproject=_TELEMETRY_SUBPROJECT,
         )
         return self._model_ops.set_comment(
             comment=comment,
+            database_name=None,
+            schema_name=None,
             model_name=self._model_name,
             statement_params=statement_params,
         )
 
     @property
     @telemetry.send_api_usage_telemetry(
         project=_TELEMETRY_PROJECT,
@@ -105,15 +109,15 @@
         """The default version of the model."""
         statement_params = telemetry.get_statement_params(
             project=_TELEMETRY_PROJECT,
             subproject=_TELEMETRY_SUBPROJECT,
             class_name=self.__class__.__name__,
         )
         default_version_name = self._model_ops.get_default_version(
-            model_name=self._model_name, statement_params=statement_params
+            database_name=None, schema_name=None, model_name=self._model_name, statement_params=statement_params
         )
         return self.version(default_version_name)
 
     @default.setter
     @telemetry.send_api_usage_telemetry(
         project=_TELEMETRY_PROJECT,
         subproject=_TELEMETRY_SUBPROJECT,
@@ -125,15 +129,19 @@
             class_name=self.__class__.__name__,
         )
         if isinstance(version, str):
             version_name = sql_identifier.SqlIdentifier(version)
         else:
             version_name = version._version_name
         self._model_ops.set_default_version(
-            model_name=self._model_name, version_name=version_name, statement_params=statement_params
+            database_name=None,
+            schema_name=None,
+            model_name=self._model_name,
+            version_name=version_name,
+            statement_params=statement_params,
         )
 
     @telemetry.send_api_usage_telemetry(
         project=_TELEMETRY_PROJECT,
         subproject=_TELEMETRY_SUBPROJECT,
     )
     def version(self, version_name: str) -> model_version_impl.ModelVersion:
@@ -151,14 +159,16 @@
         """
         statement_params = telemetry.get_statement_params(
             project=_TELEMETRY_PROJECT,
             subproject=_TELEMETRY_SUBPROJECT,
         )
         version_id = sql_identifier.SqlIdentifier(version_name)
         if self._model_ops.validate_existence(
+            database_name=None,
+            schema_name=None,
             model_name=self._model_name,
             version_name=version_id,
             statement_params=statement_params,
         ):
             return model_version_impl.ModelVersion._ref(
                 self._model_ops,
                 model_name=self._model_name,
@@ -180,14 +190,16 @@
             A list of ModelVersion objects representing all versions in the model.
         """
         statement_params = telemetry.get_statement_params(
             project=_TELEMETRY_PROJECT,
             subproject=_TELEMETRY_SUBPROJECT,
         )
         version_names = self._model_ops.list_models_or_versions(
+            database_name=None,
+            schema_name=None,
             model_name=self._model_name,
             statement_params=statement_params,
         )
         return [
             model_version_impl.ModelVersion._ref(
                 self._model_ops,
                 model_name=self._model_name,
@@ -207,14 +219,16 @@
             A Pandas DataFrame showing information about all versions in the model.
         """
         statement_params = telemetry.get_statement_params(
             project=_TELEMETRY_PROJECT,
             subproject=_TELEMETRY_SUBPROJECT,
         )
         rows = self._model_ops.show_models_or_versions(
+            database_name=None,
+            schema_name=None,
             model_name=self._model_name,
             statement_params=statement_params,
         )
         return pd.DataFrame([row.as_dict() for row in rows])
 
     @telemetry.send_api_usage_telemetry(
         project=_TELEMETRY_PROJECT,
@@ -227,14 +241,16 @@
             version_name: The name of the version.
         """
         statement_params = telemetry.get_statement_params(
             project=_TELEMETRY_PROJECT,
             subproject=_TELEMETRY_SUBPROJECT,
         )
         self._model_ops.delete_model_or_version(
+            database_name=None,
+            schema_name=None,
             model_name=self._model_name,
             version_name=sql_identifier.SqlIdentifier(version_name),
             statement_params=statement_params,
         )
 
     @telemetry.send_api_usage_telemetry(
         project=_TELEMETRY_PROJECT,
@@ -246,37 +262,17 @@
         Returns:
             The model version object.
         """
         statement_params = telemetry.get_statement_params(
             project=_TELEMETRY_PROJECT,
             subproject=_TELEMETRY_SUBPROJECT,
         )
-        return self._model_ops.show_tags(model_name=self._model_name, statement_params=statement_params)
-
-    def _parse_tag_name(
-        self,
-        tag_name: str,
-    ) -> Tuple[sql_identifier.SqlIdentifier, sql_identifier.SqlIdentifier, sql_identifier.SqlIdentifier]:
-        _tag_db, _tag_schema, _tag_name, _ = identifier.parse_schema_level_object_identifier(tag_name)
-        if _tag_db is None:
-            tag_db_id = self._model_ops._model_client._database_name
-        else:
-            tag_db_id = sql_identifier.SqlIdentifier(_tag_db)
-
-        if _tag_schema is None:
-            tag_schema_id = self._model_ops._model_client._schema_name
-        else:
-            tag_schema_id = sql_identifier.SqlIdentifier(_tag_schema)
-
-        if _tag_name is None:
-            raise ValueError(f"Unable parse the tag name `{tag_name}` you input.")
-
-        tag_name_id = sql_identifier.SqlIdentifier(_tag_name)
-
-        return tag_db_id, tag_schema_id, tag_name_id
+        return self._model_ops.show_tags(
+            database_name=None, schema_name=None, model_name=self._model_name, statement_params=statement_params
+        )
 
     @telemetry.send_api_usage_telemetry(
         project=_TELEMETRY_PROJECT,
         subproject=_TELEMETRY_SUBPROJECT,
     )
     def get_tag(self, tag_name: str) -> Optional[str]:
         """Get the value of a tag attached to the model.
@@ -288,16 +284,18 @@
         Returns:
             The tag value as a string if the tag is attached, otherwise None.
         """
         statement_params = telemetry.get_statement_params(
             project=_TELEMETRY_PROJECT,
             subproject=_TELEMETRY_SUBPROJECT,
         )
-        tag_db_id, tag_schema_id, tag_name_id = self._parse_tag_name(tag_name)
+        tag_db_id, tag_schema_id, tag_name_id = sql_identifier.parse_fully_qualified_name(tag_name)
         return self._model_ops.get_tag_value(
+            database_name=None,
+            schema_name=None,
             model_name=self._model_name,
             tag_database_name=tag_db_id,
             tag_schema_name=tag_schema_id,
             tag_name=tag_name_id,
             statement_params=statement_params,
         )
 
@@ -313,16 +311,18 @@
                 the model will be used.
             tag_value: The value of the tag
         """
         statement_params = telemetry.get_statement_params(
             project=_TELEMETRY_PROJECT,
             subproject=_TELEMETRY_SUBPROJECT,
         )
-        tag_db_id, tag_schema_id, tag_name_id = self._parse_tag_name(tag_name)
+        tag_db_id, tag_schema_id, tag_name_id = sql_identifier.parse_fully_qualified_name(tag_name)
         self._model_ops.set_tag(
+            database_name=None,
+            schema_name=None,
             model_name=self._model_name,
             tag_database_name=tag_db_id,
             tag_schema_name=tag_schema_id,
             tag_name=tag_name_id,
             tag_value=tag_value,
             statement_params=statement_params,
         )
@@ -338,16 +338,18 @@
             tag_name: The name of the tag, can be fully qualified. If not fully qualified, the database or schema of
                 the model will be used.
         """
         statement_params = telemetry.get_statement_params(
             project=_TELEMETRY_PROJECT,
             subproject=_TELEMETRY_SUBPROJECT,
         )
-        tag_db_id, tag_schema_id, tag_name_id = self._parse_tag_name(tag_name)
+        tag_db_id, tag_schema_id, tag_name_id = sql_identifier.parse_fully_qualified_name(tag_name)
         self._model_ops.unset_tag(
+            database_name=None,
+            schema_name=None,
             model_name=self._model_name,
             tag_database_name=tag_db_id,
             tag_schema_name=tag_schema_id,
             tag_name=tag_name_id,
             statement_params=statement_params,
         )
 
@@ -361,19 +363,24 @@
         Args:
             model_name: The new model name.
         """
         statement_params = telemetry.get_statement_params(
             project=_TELEMETRY_PROJECT,
             subproject=_TELEMETRY_SUBPROJECT,
         )
-        db, schema, model, _ = identifier.parse_schema_level_object_identifier(model_name)
-        new_model_db = sql_identifier.SqlIdentifier(db) if db else None
-        new_model_schema = sql_identifier.SqlIdentifier(schema) if schema else None
-        new_model_id = sql_identifier.SqlIdentifier(model)
+        new_db, new_schema, new_model = sql_identifier.parse_fully_qualified_name(model_name)
+
         self._model_ops.rename(
+            database_name=None,
+            schema_name=None,
             model_name=self._model_name,
-            new_model_db=new_model_db,
-            new_model_schema=new_model_schema,
-            new_model_name=new_model_id,
+            new_model_db=new_db,
+            new_model_schema=new_schema,
+            new_model_name=new_model,
             statement_params=statement_params,
         )
-        self._model_name = new_model_id
+        self._model_ops = model_ops.ModelOperator(
+            self._model_ops._session,
+            database_name=new_db or self._model_ops._model_client._database_name,
+            schema_name=new_schema or self._model_ops._model_client._schema_name,
+        )
+        self._model_name = new_model
```

## snowflake/ml/model/_client/model/model_version_impl.py

```diff
@@ -68,15 +68,15 @@
     def version_name(self) -> str:
         """Return the name of the version to which the model version belongs, usable as a reference in SQL."""
         return self._version_name.identifier()
 
     @property
     def fully_qualified_model_name(self) -> str:
         """Return the fully qualified name of the model to which the model version belongs."""
-        return self._model_ops._model_version_client.fully_qualified_model_name(self._model_name)
+        return self._model_ops._model_version_client.fully_qualified_object_name(None, None, self._model_name)
 
     @property
     @telemetry.send_api_usage_telemetry(
         project=_TELEMETRY_PROJECT,
         subproject=_TELEMETRY_SUBPROJECT,
     )
     def description(self) -> str:
@@ -99,14 +99,16 @@
     def comment(self) -> str:
         """The comment to the model version."""
         statement_params = telemetry.get_statement_params(
             project=_TELEMETRY_PROJECT,
             subproject=_TELEMETRY_SUBPROJECT,
         )
         return self._model_ops.get_comment(
+            database_name=None,
+            schema_name=None,
             model_name=self._model_name,
             version_name=self._version_name,
             statement_params=statement_params,
         )
 
     @comment.setter
     @telemetry.send_api_usage_telemetry(
@@ -116,14 +118,16 @@
     def comment(self, comment: str) -> None:
         statement_params = telemetry.get_statement_params(
             project=_TELEMETRY_PROJECT,
             subproject=_TELEMETRY_SUBPROJECT,
         )
         return self._model_ops.set_comment(
             comment=comment,
+            database_name=None,
+            schema_name=None,
             model_name=self._model_name,
             version_name=self._version_name,
             statement_params=statement_params,
         )
 
     @telemetry.send_api_usage_telemetry(
         project=_TELEMETRY_PROJECT,
@@ -136,15 +140,19 @@
             A dictionary showing the metrics.
         """
         statement_params = telemetry.get_statement_params(
             project=_TELEMETRY_PROJECT,
             subproject=_TELEMETRY_SUBPROJECT,
         )
         return self._model_ops._metadata_ops.load(
-            model_name=self._model_name, version_name=self._version_name, statement_params=statement_params
+            database_name=None,
+            schema_name=None,
+            model_name=self._model_name,
+            version_name=self._version_name,
+            statement_params=statement_params,
         )["metrics"]
 
     @telemetry.send_api_usage_telemetry(
         project=_TELEMETRY_PROJECT,
         subproject=_TELEMETRY_SUBPROJECT,
     )
     def get_metric(self, metric_name: str) -> Any:
@@ -179,14 +187,16 @@
             project=_TELEMETRY_PROJECT,
             subproject=_TELEMETRY_SUBPROJECT,
         )
         metrics = self.show_metrics()
         metrics[metric_name] = value
         self._model_ops._metadata_ops.save(
             metadata_ops.ModelVersionMetadataSchema(metrics=metrics),
+            database_name=None,
+            schema_name=None,
             model_name=self._model_name,
             version_name=self._version_name,
             statement_params=statement_params,
         )
 
     @telemetry.send_api_usage_telemetry(
         project=_TELEMETRY_PROJECT,
@@ -207,25 +217,29 @@
         )
         metrics = self.show_metrics()
         if metric_name not in metrics:
             raise KeyError(f"Cannot find metric with name {metric_name}.")
         del metrics[metric_name]
         self._model_ops._metadata_ops.save(
             metadata_ops.ModelVersionMetadataSchema(metrics=metrics),
+            database_name=None,
+            schema_name=None,
             model_name=self._model_name,
             version_name=self._version_name,
             statement_params=statement_params,
         )
 
     def _get_functions(self) -> List[model_manifest_schema.ModelFunctionInfo]:
         statement_params = telemetry.get_statement_params(
             project=_TELEMETRY_PROJECT,
             subproject=_TELEMETRY_SUBPROJECT,
         )
         return self._model_ops.get_functions(
+            database_name=None,
+            schema_name=None,
             model_name=self._model_name,
             version_name=self._version_name,
             statement_params=statement_params,
         )
 
     @telemetry.send_api_usage_telemetry(
         project=_TELEMETRY_PROJECT,
@@ -305,14 +319,16 @@
         else:
             target_function_info = functions[0]
         return self._model_ops.invoke_method(
             method_name=sql_identifier.SqlIdentifier(target_function_info["name"]),
             method_function_type=target_function_info["target_method_function_type"],
             signature=target_function_info["signature"],
             X=X,
+            database_name=None,
+            schema_name=None,
             model_name=self._model_name,
             version_name=self._version_name,
             strict_input_validation=strict_input_validation,
             partition_column=partition_column,
             statement_params=statement_params,
         )
 
@@ -337,14 +353,16 @@
 
         target_local_path.mkdir(parents=False, exist_ok=True)
         statement_params = telemetry.get_statement_params(
             project=_TELEMETRY_PROJECT,
             subproject=_TELEMETRY_SUBPROJECT,
         )
         self._model_ops.download_files(
+            database_name=None,
+            schema_name=None,
             model_name=self._model_name,
             version_name=self._version_name,
             target_path=target_local_path,
             mode=export_mode.value,
             statement_params=statement_params,
         )
 
@@ -376,14 +394,16 @@
             project=_TELEMETRY_PROJECT,
             subproject=_TELEMETRY_SUBPROJECT,
         )
         if not force:
             with tempfile.TemporaryDirectory() as tmp_workspace_for_validation:
                 ws_path_for_validation = pathlib.Path(tmp_workspace_for_validation)
                 self._model_ops.download_files(
+                    database_name=None,
+                    schema_name=None,
                     model_name=self._model_name,
                     version_name=self._version_name,
                     target_path=ws_path_for_validation,
                     mode="minimal",
                     statement_params=statement_params,
                 )
                 pk_for_validation = model_composer.ModelComposer.load(
@@ -413,14 +433,16 @@
             category=RuntimeWarning,
             stacklevel=2,
         )
 
         # We need the folder to be existed.
         workspace = pathlib.Path(tempfile.mkdtemp())
         self._model_ops.download_files(
+            database_name=None,
+            schema_name=None,
             model_name=self._model_name,
             version_name=self._version_name,
             target_path=workspace,
             mode="model",
             statement_params=statement_params,
         )
         pk = model_composer.ModelComposer.load(workspace, meta_only=False, options=options)
```

## snowflake/ml/model/_client/ops/metadata_ops.py

```diff
@@ -57,49 +57,72 @@
         if not isinstance(loaded_metrics, dict):
             raise ValueError(f"Metrics in the metadata is expected to be a dictionary, getting {loaded_metrics}")
         return ModelVersionMetadataSchema(metrics=loaded_metrics)
 
     def _get_current_metadata_dict(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> Dict[str, Any]:
         version_info_list = self._model_client.show_versions(
-            model_name=model_name, version_name=version_name, statement_params=statement_params
+            database_name=database_name,
+            schema_name=schema_name,
+            model_name=model_name,
+            version_name=version_name,
+            statement_params=statement_params,
         )
         metadata_str = version_info_list[0][self._model_client.MODEL_VERSION_METADATA_COL_NAME]
         if not metadata_str:
             return {}
         res = json.loads(metadata_str)
         if not isinstance(res, dict):
             raise ValueError(f"Metadata is expected to be a dictionary, getting {res}")
         return res
 
     def load(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> ModelVersionMetadataSchema:
         metadata_dict = self._get_current_metadata_dict(
-            model_name=model_name, version_name=version_name, statement_params=statement_params
+            database_name=database_name,
+            schema_name=schema_name,
+            model_name=model_name,
+            version_name=version_name,
+            statement_params=statement_params,
         )
         return MetadataOperator._parse(metadata_dict)
 
     def save(
         self,
         metadata: ModelVersionMetadataSchema,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> None:
         metadata_dict = self._get_current_metadata_dict(
-            model_name=model_name, version_name=version_name, statement_params=statement_params
+            database_name=database_name,
+            schema_name=schema_name,
+            model_name=model_name,
+            version_name=version_name,
+            statement_params=statement_params,
         )
         metadata_dict.update({**metadata, "snowpark_ml_schema_version": MODEL_VERSION_METADATA_SCHEMA_VERSION})
         self._model_version_client.set_metadata(
-            metadata_dict, model_name=model_name, version_name=version_name, statement_params=statement_params
+            metadata_dict,
+            database_name=database_name,
+            schema_name=schema_name,
+            model_name=model_name,
+            version_name=version_name,
+            statement_params=statement_params,
         )
```

## snowflake/ml/model/_client/ops/model_ops.py

```diff
@@ -70,213 +70,288 @@
             return False
         return (
             self._stage_client == __value._stage_client
             and self._model_client == __value._model_client
             and self._model_version_client == __value._model_version_client
         )
 
-    def prepare_model_stage_path(self, *, statement_params: Optional[Dict[str, Any]] = None) -> str:
+    def prepare_model_stage_path(
+        self,
+        *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
+        statement_params: Optional[Dict[str, Any]] = None,
+    ) -> str:
         stage_name = sql_identifier.SqlIdentifier(
             snowpark_utils.random_name_for_temp_object(snowpark_utils.TempObjectType.STAGE)
         )
-        self._stage_client.create_tmp_stage(stage_name=stage_name, statement_params=statement_params)
-        return f"@{self._stage_client.fully_qualified_stage_name(stage_name)}/model"
+        self._stage_client.create_tmp_stage(
+            database_name=database_name,
+            schema_name=schema_name,
+            stage_name=stage_name,
+            statement_params=statement_params,
+        )
+        return f"@{self._stage_client.fully_qualified_object_name(database_name, schema_name, stage_name)}/model"
 
     def create_from_stage(
         self,
         composed_model: model_composer.ModelComposer,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> None:
         stage_path = str(composed_model.stage_path)
         if self.validate_existence(
+            database_name=database_name,
+            schema_name=schema_name,
             model_name=model_name,
             statement_params=statement_params,
         ):
             if self.validate_existence(
+                database_name=database_name,
+                schema_name=schema_name,
                 model_name=model_name,
                 version_name=version_name,
                 statement_params=statement_params,
             ):
                 raise ValueError(
-                    f"Model {self._model_version_client.fully_qualified_model_name(model_name)} "
-                    f"version {version_name} already existed."
+                    "Model "
+                    f"{self._model_version_client.fully_qualified_object_name(database_name, schema_name, model_name)}"
+                    f" version {version_name} already existed."
                 )
             else:
                 self._model_version_client.add_version_from_stage(
+                    database_name=database_name,
+                    schema_name=schema_name,
                     stage_path=stage_path,
                     model_name=model_name,
                     version_name=version_name,
                     statement_params=statement_params,
                 )
         else:
             self._model_version_client.create_from_stage(
+                database_name=database_name,
+                schema_name=schema_name,
                 stage_path=stage_path,
                 model_name=model_name,
                 version_name=version_name,
                 statement_params=statement_params,
             )
 
     def show_models_or_versions(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: Optional[sql_identifier.SqlIdentifier] = None,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> List[row.Row]:
         if model_name:
             return self._model_client.show_versions(
+                database_name=database_name,
+                schema_name=schema_name,
                 model_name=model_name,
                 validate_result=False,
                 statement_params=statement_params,
             )
         else:
             return self._model_client.show_models(
+                database_name=database_name,
+                schema_name=schema_name,
                 validate_result=False,
                 statement_params=statement_params,
             )
 
     def list_models_or_versions(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: Optional[sql_identifier.SqlIdentifier] = None,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> List[sql_identifier.SqlIdentifier]:
         res = self.show_models_or_versions(
+            database_name=database_name,
+            schema_name=schema_name,
             model_name=model_name,
             statement_params=statement_params,
         )
         if model_name:
             col_name = self._model_client.MODEL_VERSION_NAME_COL_NAME
         else:
             col_name = self._model_client.MODEL_NAME_COL_NAME
         return [sql_identifier.SqlIdentifier(row[col_name], case_sensitive=True) for row in res]
 
     def validate_existence(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: Optional[sql_identifier.SqlIdentifier] = None,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> bool:
         if version_name:
             res = self._model_client.show_versions(
+                database_name=database_name,
+                schema_name=schema_name,
                 model_name=model_name,
                 version_name=version_name,
                 validate_result=False,
                 statement_params=statement_params,
             )
         else:
             res = self._model_client.show_models(
+                database_name=database_name,
+                schema_name=schema_name,
                 model_name=model_name,
                 validate_result=False,
                 statement_params=statement_params,
             )
         return len(res) == 1
 
     def get_comment(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: Optional[sql_identifier.SqlIdentifier] = None,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> str:
         if version_name:
             res = self._model_client.show_versions(
+                database_name=database_name,
+                schema_name=schema_name,
                 model_name=model_name,
                 version_name=version_name,
                 statement_params=statement_params,
             )
             col_name = self._model_client.MODEL_VERSION_COMMENT_COL_NAME
         else:
             res = self._model_client.show_models(
+                database_name=database_name,
+                schema_name=schema_name,
                 model_name=model_name,
                 statement_params=statement_params,
             )
             col_name = self._model_client.MODEL_COMMENT_COL_NAME
         return cast(str, res[0][col_name])
 
     def set_comment(
         self,
         *,
         comment: str,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: Optional[sql_identifier.SqlIdentifier] = None,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> None:
         if version_name:
             self._model_version_client.set_comment(
                 comment=comment,
+                database_name=database_name,
+                schema_name=schema_name,
                 model_name=model_name,
                 version_name=version_name,
                 statement_params=statement_params,
             )
         else:
             self._model_client.set_comment(
                 comment=comment,
+                database_name=database_name,
+                schema_name=schema_name,
                 model_name=model_name,
                 statement_params=statement_params,
             )
 
     def set_default_version(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> None:
         if not self.validate_existence(
-            model_name=model_name, version_name=version_name, statement_params=statement_params
+            database_name=database_name,
+            schema_name=schema_name,
+            model_name=model_name,
+            version_name=version_name,
+            statement_params=statement_params,
         ):
             raise ValueError(f"You cannot set version {version_name} as default version as it does not exist.")
         self._model_version_client.set_default_version(
-            model_name=model_name, version_name=version_name, statement_params=statement_params
+            database_name=database_name,
+            schema_name=schema_name,
+            model_name=model_name,
+            version_name=version_name,
+            statement_params=statement_params,
         )
 
     def get_default_version(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> sql_identifier.SqlIdentifier:
-        res = self._model_client.show_models(model_name=model_name, statement_params=statement_params)[0]
+        res = self._model_client.show_models(
+            database_name=database_name,
+            schema_name=schema_name,
+            model_name=model_name,
+            statement_params=statement_params,
+        )[0]
         return sql_identifier.SqlIdentifier(
             res[self._model_client.MODEL_DEFAULT_VERSION_NAME_COL_NAME], case_sensitive=True
         )
 
     def get_tag_value(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
-        tag_database_name: sql_identifier.SqlIdentifier,
-        tag_schema_name: sql_identifier.SqlIdentifier,
+        tag_database_name: Optional[sql_identifier.SqlIdentifier],
+        tag_schema_name: Optional[sql_identifier.SqlIdentifier],
         tag_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> Optional[str]:
         r = self._tag_client.get_tag_value(
-            module_name=model_name,
+            database_name=database_name,
+            schema_name=schema_name,
+            model_name=model_name,
             tag_database_name=tag_database_name,
             tag_schema_name=tag_schema_name,
             tag_name=tag_name,
             statement_params=statement_params,
         )
         value = r.TAG_VALUE
         if value is None:
             return value
         return str(value)
 
     def show_tags(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> Dict[str, str]:
         tags_info = self._tag_client.get_tag_list(
-            module_name=model_name,
+            database_name=database_name,
+            schema_name=schema_name,
+            model_name=model_name,
             statement_params=statement_params,
         )
         res: Dict[str, str] = {
             identifier.get_schema_level_object_identifier(
                 sql_identifier.SqlIdentifier(r.TAG_DATABASE, case_sensitive=True),
                 sql_identifier.SqlIdentifier(r.TAG_SCHEMA, case_sensitive=True),
                 sql_identifier.SqlIdentifier(r.TAG_NAME, case_sensitive=True),
@@ -284,56 +359,68 @@
             for r in tags_info
         }
         return res
 
     def set_tag(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
-        tag_database_name: sql_identifier.SqlIdentifier,
-        tag_schema_name: sql_identifier.SqlIdentifier,
+        tag_database_name: Optional[sql_identifier.SqlIdentifier],
+        tag_schema_name: Optional[sql_identifier.SqlIdentifier],
         tag_name: sql_identifier.SqlIdentifier,
         tag_value: str,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> None:
         self._tag_client.set_tag_on_model(
+            database_name=database_name,
+            schema_name=schema_name,
             model_name=model_name,
             tag_database_name=tag_database_name,
             tag_schema_name=tag_schema_name,
             tag_name=tag_name,
             tag_value=tag_value,
             statement_params=statement_params,
         )
 
     def unset_tag(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
-        tag_database_name: sql_identifier.SqlIdentifier,
-        tag_schema_name: sql_identifier.SqlIdentifier,
+        tag_database_name: Optional[sql_identifier.SqlIdentifier],
+        tag_schema_name: Optional[sql_identifier.SqlIdentifier],
         tag_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> None:
         self._tag_client.unset_tag_on_model(
+            database_name=database_name,
+            schema_name=schema_name,
             model_name=model_name,
             tag_database_name=tag_database_name,
             tag_schema_name=tag_schema_name,
             tag_name=tag_name,
             statement_params=statement_params,
         )
 
     def get_model_version_manifest(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> model_manifest_schema.ModelManifestDict:
         with tempfile.TemporaryDirectory() as tmpdir:
             self._model_version_client.get_file(
+                database_name=database_name,
+                schema_name=schema_name,
                 model_name=model_name,
                 version_name=version_name,
                 file_path=pathlib.PurePosixPath(model_manifest.ModelManifest.MANIFEST_FILE_REL_PATH),
                 target_path=pathlib.Path(tmpdir),
                 statement_params=statement_params,
             )
             mm = model_manifest.ModelManifest(pathlib.Path(tmpdir))
@@ -358,27 +445,33 @@
                 ), f"Unable to match {target_method} in {sql_functions_names}."
             res[function_name] = target_method
         return res
 
     def get_functions(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> List[model_manifest_schema.ModelFunctionInfo]:
         raw_model_spec_res = self._model_client.show_versions(
+            database_name=database_name,
+            schema_name=schema_name,
             model_name=model_name,
             version_name=version_name,
             check_model_details=True,
             statement_params={**(statement_params or {}), "SHOW_MODEL_DETAILS_IN_SHOW_VERSIONS_IN_MODEL": True},
         )[0][self._model_client.MODEL_VERSION_MODEL_SPEC_COL_NAME]
         model_spec_dict = yaml.safe_load(raw_model_spec_res)
         model_spec = model_meta.ModelMetadata._validate_model_metadata(model_spec_dict)
         show_functions_res = self._model_version_client.show_functions(
+            database_name=database_name,
+            schema_name=schema_name,
             model_name=model_name,
             version_name=version_name,
             statement_params=statement_params,
         )
         function_names_and_types = []
         for r in show_functions_res:
             function_name = sql_identifier.SqlIdentifier(
@@ -415,14 +508,16 @@
     def invoke_method(
         self,
         *,
         method_name: sql_identifier.SqlIdentifier,
         method_function_type: str,
         signature: model_signature.ModelSignature,
         X: Union[type_hints.SupportedDataType, dataframe.DataFrame],
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
         strict_input_validation: bool = False,
         partition_column: Optional[sql_identifier.SqlIdentifier] = None,
         statement_params: Optional[Dict[str, str]] = None,
     ) -> Union[type_hints.SupportedDataType, dataframe.DataFrame]:
         identifier_rule = model_signature.SnowparkIdentifierRule.INFERRED
@@ -462,25 +557,29 @@
 
         if method_function_type == model_manifest_schema.ModelMethodFunctionTypes.FUNCTION.value:
             df_res = self._model_version_client.invoke_function_method(
                 method_name=method_name,
                 input_df=s_df,
                 input_args=input_args,
                 returns=returns,
+                database_name=database_name,
+                schema_name=schema_name,
                 model_name=model_name,
                 version_name=version_name,
                 statement_params=statement_params,
             )
         elif method_function_type == model_manifest_schema.ModelMethodFunctionTypes.TABLE_FUNCTION.value:
             df_res = self._model_version_client.invoke_table_function_method(
                 method_name=method_name,
                 input_df=s_df,
                 input_args=input_args,
                 partition_column=partition_column,
                 returns=returns,
+                database_name=database_name,
+                schema_name=schema_name,
                 model_name=model_name,
                 version_name=version_name,
                 statement_params=statement_params,
             )
 
         if keep_order:
             df_res = df_res.sort(
@@ -500,40 +599,50 @@
             return snowpark_handler.SnowparkDataFrameHandler.convert_to_df(df_res, features=signature.outputs)
         else:
             return df_res
 
     def delete_model_or_version(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: Optional[sql_identifier.SqlIdentifier] = None,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> None:
         if version_name:
             self._model_version_client.drop_version(
+                database_name=database_name,
+                schema_name=schema_name,
                 model_name=model_name,
                 version_name=version_name,
                 statement_params=statement_params,
             )
         else:
             self._model_client.drop_model(
+                database_name=database_name,
+                schema_name=schema_name,
                 model_name=model_name,
                 statement_params=statement_params,
             )
 
     def rename(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         new_model_db: Optional[sql_identifier.SqlIdentifier],
         new_model_schema: Optional[sql_identifier.SqlIdentifier],
         new_model_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> None:
         self._model_client.rename(
+            database_name=database_name,
+            schema_name=schema_name,
             model_name=model_name,
             new_model_db=new_model_db,
             new_model_schema=new_model_schema,
             new_model_name=new_model_name,
             statement_params=statement_params,
         )
 
@@ -550,35 +659,41 @@
         "model": {pathlib.PurePosixPath(model_composer.ModelComposer.MODEL_DIR_REL_PATH): True},
         "full": {pathlib.PurePosixPath(os.curdir): True},
     }
 
     def download_files(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
         target_path: pathlib.Path,
         mode: Literal["full", "model", "minimal"] = "model",
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> None:
         for remote_rel_path, is_dir in self.MODEL_FILE_DOWNLOAD_PATTERN[mode].items():
             list_file_res = self._model_version_client.list_file(
+                database_name=database_name,
+                schema_name=schema_name,
                 model_name=model_name,
                 version_name=version_name,
                 file_path=remote_rel_path,
                 is_dir=is_dir,
                 statement_params=statement_params,
             )
             file_list = [
                 pathlib.PurePosixPath(*pathlib.PurePosixPath(row.name).parts[2:])  # versions/<version_name>/...
                 for row in list_file_res
             ]
             for stage_file_path in file_list:
                 local_file_dir = target_path / stage_file_path.parent
                 local_file_dir.mkdir(parents=True, exist_ok=True)
                 self._model_version_client.get_file(
+                    database_name=database_name,
+                    schema_name=schema_name,
                     model_name=model_name,
                     version_name=version_name,
                     file_path=stage_file_path,
                     target_path=local_file_dir,
                     statement_params=statement_params,
                 )
```

## snowflake/ml/model/_client/sql/model.py

```diff
@@ -1,56 +1,36 @@
 from typing import Any, Dict, List, Optional
 
-from snowflake.ml._internal.utils import (
-    identifier,
-    query_result_checker,
-    sql_identifier,
-)
-from snowflake.snowpark import row, session
+from snowflake.ml._internal.utils import query_result_checker, sql_identifier
+from snowflake.ml.model._client.sql import _base
+from snowflake.snowpark import row
 
 
-class ModelSQLClient:
+class ModelSQLClient(_base._BaseSQLClient):
     MODEL_NAME_COL_NAME = "name"
     MODEL_COMMENT_COL_NAME = "comment"
     MODEL_DEFAULT_VERSION_NAME_COL_NAME = "default_version_name"
 
     MODEL_VERSION_NAME_COL_NAME = "name"
     MODEL_VERSION_COMMENT_COL_NAME = "comment"
     MODEL_VERSION_METADATA_COL_NAME = "metadata"
     MODEL_VERSION_MODEL_SPEC_COL_NAME = "model_spec"
 
-    def __init__(
-        self,
-        session: session.Session,
-        *,
-        database_name: sql_identifier.SqlIdentifier,
-        schema_name: sql_identifier.SqlIdentifier,
-    ) -> None:
-        self._session = session
-        self._database_name = database_name
-        self._schema_name = schema_name
-
-    def __eq__(self, __value: object) -> bool:
-        if not isinstance(__value, ModelSQLClient):
-            return False
-        return self._database_name == __value._database_name and self._schema_name == __value._schema_name
-
-    def fully_qualified_model_name(self, model_name: sql_identifier.SqlIdentifier) -> str:
-        return identifier.get_schema_level_object_identifier(
-            self._database_name.identifier(), self._schema_name.identifier(), model_name.identifier()
-        )
-
     def show_models(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: Optional[sql_identifier.SqlIdentifier] = None,
         validate_result: bool = True,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> List[row.Row]:
-        fully_qualified_schema_name = ".".join([self._database_name.identifier(), self._schema_name.identifier()])
+        actual_database_name = database_name or self._database_name
+        actual_schema_name = schema_name or self._schema_name
+        fully_qualified_schema_name = ".".join([actual_database_name.identifier(), actual_schema_name.identifier()])
         like_sql = ""
         if model_name:
             like_sql = f" LIKE '{model_name.resolved()}'"
 
         res = (
             query_result_checker.SqlResultValidator(
                 self._session,
@@ -65,28 +45,33 @@
             res = res.has_dimensions(expected_rows=1)
 
         return res.validate()
 
     def show_versions(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: Optional[sql_identifier.SqlIdentifier] = None,
         validate_result: bool = True,
         check_model_details: bool = False,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> List[row.Row]:
         like_sql = ""
         if version_name:
             like_sql = f" LIKE '{version_name.resolved()}'"
 
         res = (
             query_result_checker.SqlResultValidator(
                 self._session,
-                f"SHOW VERSIONS{like_sql} IN MODEL {self.fully_qualified_model_name(model_name)}",
+                (
+                    f"SHOW VERSIONS{like_sql} IN "
+                    f"MODEL {self.fully_qualified_object_name(database_name, schema_name, model_name)}"
+                ),
                 statement_params=statement_params,
             )
             .has_column(ModelSQLClient.MODEL_VERSION_NAME_COL_NAME, allow_empty=True)
             .has_column(ModelSQLClient.MODEL_VERSION_COMMENT_COL_NAME, allow_empty=True)
             .has_column(ModelSQLClient.MODEL_VERSION_METADATA_COL_NAME, allow_empty=True)
         )
         if validate_result and version_name:
@@ -95,49 +80,57 @@
             res = res.has_column(ModelSQLClient.MODEL_VERSION_MODEL_SPEC_COL_NAME, allow_empty=True)
 
         return res.validate()
 
     def set_comment(
         self,
         *,
-        comment: str,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
+        comment: str,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> None:
         query_result_checker.SqlResultValidator(
             self._session,
-            f"COMMENT ON MODEL {self.fully_qualified_model_name(model_name)} IS $${comment}$$",
+            (
+                f"COMMENT ON MODEL {self.fully_qualified_object_name(database_name, schema_name, model_name)}"
+                f" IS $${comment}$$"
+            ),
             statement_params=statement_params,
         ).has_dimensions(expected_rows=1, expected_cols=1).validate()
 
     def drop_model(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> None:
         query_result_checker.SqlResultValidator(
             self._session,
-            f"DROP MODEL {self.fully_qualified_model_name(model_name)}",
+            f"DROP MODEL {self.fully_qualified_object_name(database_name, schema_name, model_name)}",
             statement_params=statement_params,
         ).has_dimensions(expected_rows=1, expected_cols=1).validate()
 
     def rename(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         new_model_db: Optional[sql_identifier.SqlIdentifier],
         new_model_schema: Optional[sql_identifier.SqlIdentifier],
         new_model_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> None:
         # Use registry's database and schema if a non fully qualified new model name is provided.
-        new_fully_qualified_name = identifier.get_schema_level_object_identifier(
-            new_model_db.identifier() if new_model_db else self._database_name.identifier(),
-            new_model_schema.identifier() if new_model_schema else self._schema_name.identifier(),
-            new_model_name.identifier(),
-        )
+        new_fully_qualified_name = self.fully_qualified_object_name(new_model_db, new_model_schema, new_model_name)
         query_result_checker.SqlResultValidator(
             self._session,
-            f"ALTER MODEL {self.fully_qualified_model_name(model_name)} RENAME TO {new_fully_qualified_name}",
+            (
+                f"ALTER MODEL {self.fully_qualified_object_name(database_name, schema_name, model_name)}"
+                f" RENAME TO {new_fully_qualified_name}"
+            ),
             statement_params=statement_params,
         ).has_dimensions(expected_rows=1, expected_cols=1).validate()
```

## snowflake/ml/model/_client/sql/model_version.py

```diff
@@ -5,144 +5,140 @@
 from urllib.parse import ParseResult
 
 from snowflake.ml._internal.utils import (
     identifier,
     query_result_checker,
     sql_identifier,
 )
-from snowflake.snowpark import dataframe, functions as F, row, session, types as spt
+from snowflake.ml.model._client.sql import _base
+from snowflake.snowpark import dataframe, functions as F, row, types as spt
 from snowflake.snowpark._internal import utils as snowpark_utils
 
 
 def _normalize_url_for_sql(url: str) -> str:
     if url.startswith("'") and url.endswith("'"):
         url = url[1:-1]
     url = url.replace("'", "\\'")
     return f"'{url}'"
 
 
-class ModelVersionSQLClient:
+class ModelVersionSQLClient(_base._BaseSQLClient):
     FUNCTION_NAME_COL_NAME = "name"
     FUNCTION_RETURN_TYPE_COL_NAME = "return_type"
 
-    def __init__(
-        self,
-        session: session.Session,
-        *,
-        database_name: sql_identifier.SqlIdentifier,
-        schema_name: sql_identifier.SqlIdentifier,
-    ) -> None:
-        self._session = session
-        self._database_name = database_name
-        self._schema_name = schema_name
-
-    def __eq__(self, __value: object) -> bool:
-        if not isinstance(__value, ModelVersionSQLClient):
-            return False
-        return self._database_name == __value._database_name and self._schema_name == __value._schema_name
-
-    def fully_qualified_model_name(self, model_name: sql_identifier.SqlIdentifier) -> str:
-        return identifier.get_schema_level_object_identifier(
-            self._database_name.identifier(), self._schema_name.identifier(), model_name.identifier()
-        )
-
     def create_from_stage(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
         stage_path: str,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> None:
         query_result_checker.SqlResultValidator(
             self._session,
             (
-                f"CREATE MODEL {self.fully_qualified_model_name(model_name)} WITH VERSION {version_name.identifier()}"
-                f" FROM {stage_path}"
+                f"CREATE MODEL {self.fully_qualified_object_name(database_name, schema_name, model_name)}"
+                f" WITH VERSION {version_name.identifier()} FROM {stage_path}"
             ),
             statement_params=statement_params,
         ).has_dimensions(expected_rows=1, expected_cols=1).validate()
 
     # TODO(SNOW-987381): Merge with above when we have `create or alter module m [with] version v1 ...`
     def add_version_from_stage(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
         stage_path: str,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> None:
         query_result_checker.SqlResultValidator(
             self._session,
             (
-                f"ALTER MODEL {self.fully_qualified_model_name(model_name)} ADD VERSION {version_name.identifier()}"
-                f" FROM {stage_path}"
+                f"ALTER MODEL {self.fully_qualified_object_name(database_name, schema_name, model_name)}"
+                f" ADD VERSION {version_name.identifier()} FROM {stage_path}"
             ),
             statement_params=statement_params,
         ).has_dimensions(expected_rows=1, expected_cols=1).validate()
 
     def set_default_version(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> None:
         query_result_checker.SqlResultValidator(
             self._session,
             (
-                f"ALTER MODEL {self.fully_qualified_model_name(model_name)} "
+                f"ALTER MODEL {self.fully_qualified_object_name(database_name, schema_name, model_name)} "
                 f"SET DEFAULT_VERSION = {version_name.identifier()}"
             ),
             statement_params=statement_params,
         ).has_dimensions(expected_rows=1, expected_cols=1).validate()
 
     def list_file(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
         file_path: pathlib.PurePosixPath,
         is_dir: bool = False,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> List[row.Row]:
         # Workaround for snowURL bug.
         trailing_slash = "/" if is_dir else ""
 
         stage_location = (
             pathlib.PurePosixPath(
-                self.fully_qualified_model_name(model_name), "versions", version_name.resolved(), file_path
+                self.fully_qualified_object_name(database_name, schema_name, model_name),
+                "versions",
+                version_name.resolved(),
+                file_path,
             ).as_posix()
             + trailing_slash
         )
         stage_location_url = ParseResult(
             scheme="snow", netloc="model", path=stage_location, params="", query="", fragment=""
         ).geturl()
 
         return (
             query_result_checker.SqlResultValidator(
                 self._session,
                 f"List {_normalize_url_for_sql(stage_location_url)}",
                 statement_params=statement_params,
             )
-            .has_column("name")
+            .has_column("name", allow_empty=True)
             .validate()
         )
 
     def get_file(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
         file_path: pathlib.PurePosixPath,
         target_path: pathlib.Path,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> pathlib.Path:
         stage_location = pathlib.PurePosixPath(
-            self.fully_qualified_model_name(model_name), "versions", version_name.resolved(), file_path
+            self.fully_qualified_object_name(database_name, schema_name, model_name),
+            "versions",
+            version_name.resolved(),
+            file_path,
         ).as_posix()
         stage_location_url = ParseResult(
             scheme="snow", netloc="model", path=stage_location, params="", query="", fragment=""
         ).geturl()
         local_location = target_path.resolve().as_posix()
         local_location_url = f"file://{local_location}"
 
@@ -158,81 +154,90 @@
                 statement_params=statement_params,
             ).has_dimensions(expected_rows=1).validate()
         return target_path / file_path.name
 
     def show_functions(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> List[row.Row]:
         res = query_result_checker.SqlResultValidator(
             self._session,
             (
-                f"SHOW FUNCTIONS IN MODEL {self.fully_qualified_model_name(model_name)}"
+                f"SHOW FUNCTIONS IN MODEL {self.fully_qualified_object_name(database_name, schema_name, model_name)}"
                 f" VERSION {version_name.identifier()}"
             ),
             statement_params=statement_params,
         ).has_column(ModelVersionSQLClient.FUNCTION_NAME_COL_NAME, allow_empty=True)
 
         return res.validate()
 
     def set_comment(
         self,
         *,
-        comment: str,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
+        comment: str,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> None:
         query_result_checker.SqlResultValidator(
             self._session,
             (
-                f"ALTER MODEL {self.fully_qualified_model_name(model_name)} "
+                f"ALTER MODEL {self.fully_qualified_object_name(database_name, schema_name, model_name)} "
                 f"MODIFY VERSION {version_name.identifier()} SET COMMENT=$${comment}$$"
             ),
             statement_params=statement_params,
         ).has_dimensions(expected_rows=1, expected_cols=1).validate()
 
     def invoke_function_method(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
         method_name: sql_identifier.SqlIdentifier,
         input_df: dataframe.DataFrame,
         input_args: List[sql_identifier.SqlIdentifier],
         returns: List[Tuple[str, spt.DataType, sql_identifier.SqlIdentifier]],
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> dataframe.DataFrame:
         with_statements = []
         if len(input_df.queries["queries"]) == 1 and len(input_df.queries["post_actions"]) == 0:
             INTERMEDIATE_TABLE_NAME = "SNOWPARK_ML_MODEL_INFERENCE_INPUT"
             with_statements.append(f"{INTERMEDIATE_TABLE_NAME} AS ({input_df.queries['queries'][0]})")
         else:
+            actual_database_name = database_name or self._database_name
+            actual_schema_name = schema_name or self._schema_name
             tmp_table_name = snowpark_utils.random_name_for_temp_object(snowpark_utils.TempObjectType.TABLE)
             INTERMEDIATE_TABLE_NAME = identifier.get_schema_level_object_identifier(
-                self._database_name.identifier(),
-                self._schema_name.identifier(),
+                actual_database_name.identifier(),
+                actual_schema_name.identifier(),
                 tmp_table_name,
             )
             input_df.write.save_as_table(  # type: ignore[call-overload]
                 table_name=INTERMEDIATE_TABLE_NAME,
                 mode="errorifexists",
                 table_type="temporary",
                 statement_params=statement_params,
             )
 
         INTERMEDIATE_OBJ_NAME = "TMP_RESULT"
 
         module_version_alias = "MODEL_VERSION_ALIAS"
         with_statements.append(
             f"{module_version_alias} AS "
-            f"MODEL {self.fully_qualified_model_name(model_name)} VERSION {version_name.identifier()}"
+            f"MODEL {self.fully_qualified_object_name(database_name, schema_name, model_name)}"
+            f" VERSION {version_name.identifier()}"
         )
 
         args_sql_list = []
         for input_arg_value in input_args:
             args_sql_list.append(input_arg_value)
 
         args_sql = ", ".join(args_sql_list)
@@ -263,45 +268,50 @@
             output_df._statement_params = statement_params  # type: ignore[assignment]
 
         return output_df
 
     def invoke_table_function_method(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
         method_name: sql_identifier.SqlIdentifier,
         input_df: dataframe.DataFrame,
         input_args: List[sql_identifier.SqlIdentifier],
         returns: List[Tuple[str, spt.DataType, sql_identifier.SqlIdentifier]],
         partition_column: Optional[sql_identifier.SqlIdentifier],
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> dataframe.DataFrame:
         with_statements = []
         if len(input_df.queries["queries"]) == 1 and len(input_df.queries["post_actions"]) == 0:
             INTERMEDIATE_TABLE_NAME = "SNOWPARK_ML_MODEL_INFERENCE_INPUT"
             with_statements.append(f"{INTERMEDIATE_TABLE_NAME} AS ({input_df.queries['queries'][0]})")
         else:
+            actual_database_name = database_name or self._database_name
+            actual_schema_name = schema_name or self._schema_name
             tmp_table_name = snowpark_utils.random_name_for_temp_object(snowpark_utils.TempObjectType.TABLE)
             INTERMEDIATE_TABLE_NAME = identifier.get_schema_level_object_identifier(
-                self._database_name.identifier(),
-                self._schema_name.identifier(),
+                actual_database_name.identifier(),
+                actual_schema_name.identifier(),
                 tmp_table_name,
             )
             input_df.write.save_as_table(  # type: ignore[call-overload]
                 table_name=INTERMEDIATE_TABLE_NAME,
                 mode="errorifexists",
                 table_type="temporary",
                 statement_params=statement_params,
             )
 
         module_version_alias = "MODEL_VERSION_ALIAS"
         with_statements.append(
             f"{module_version_alias} AS "
-            f"MODEL {self.fully_qualified_model_name(model_name)} VERSION {version_name.identifier()}"
+            f"MODEL {self.fully_qualified_object_name(database_name, schema_name, model_name)}"
+            f" VERSION {version_name.identifier()}"
         )
 
         partition_by = partition_column.identifier() if partition_column is not None else "1"
 
         args_sql_list = []
         for input_arg_value in input_args:
             args_sql_list.append(input_arg_value)
@@ -340,33 +350,40 @@
 
         return output_df
 
     def set_metadata(
         self,
         metadata_dict: Dict[str, Any],
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> None:
         json_metadata = json.dumps(metadata_dict)
         query_result_checker.SqlResultValidator(
             self._session,
             (
-                f"ALTER MODEL {self.fully_qualified_model_name(model_name)} MODIFY VERSION {version_name.identifier()}"
-                f" SET METADATA=$${json_metadata}$$"
+                f"ALTER MODEL {self.fully_qualified_object_name(database_name, schema_name, model_name)}"
+                f" MODIFY VERSION {version_name.identifier()} SET METADATA=$${json_metadata}$$"
             ),
             statement_params=statement_params,
         ).has_dimensions(expected_rows=1, expected_cols=1).validate()
 
     def drop_version(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> None:
         query_result_checker.SqlResultValidator(
             self._session,
-            f"ALTER MODEL {self.fully_qualified_model_name(model_name)} DROP VERSION {version_name.identifier()}",
+            (
+                f"ALTER MODEL {self.fully_qualified_object_name(database_name, schema_name, model_name)}"
+                f" DROP VERSION {version_name.identifier()}"
+            ),
             statement_params=statement_params,
         ).has_dimensions(expected_rows=1, expected_cols=1).validate()
```

## snowflake/ml/model/_client/sql/stage.py

```diff
@@ -1,46 +1,20 @@
 from typing import Any, Dict, Optional
 
-from snowflake.ml._internal.utils import (
-    identifier,
-    query_result_checker,
-    sql_identifier,
-)
-from snowflake.snowpark import session
+from snowflake.ml._internal.utils import query_result_checker, sql_identifier
+from snowflake.ml.model._client.sql import _base
 
 
-class StageSQLClient:
-    def __init__(
-        self,
-        session: session.Session,
-        *,
-        database_name: sql_identifier.SqlIdentifier,
-        schema_name: sql_identifier.SqlIdentifier,
-    ) -> None:
-        self._session = session
-        self._database_name = database_name
-        self._schema_name = schema_name
-
-    def __eq__(self, __value: object) -> bool:
-        if not isinstance(__value, StageSQLClient):
-            return False
-        return self._database_name == __value._database_name and self._schema_name == __value._schema_name
-
-    def fully_qualified_stage_name(
-        self,
-        stage_name: sql_identifier.SqlIdentifier,
-    ) -> str:
-        return identifier.get_schema_level_object_identifier(
-            self._database_name.identifier(), self._schema_name.identifier(), stage_name.identifier()
-        )
-
+class StageSQLClient(_base._BaseSQLClient):
     def create_tmp_stage(
         self,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
         stage_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> None:
         query_result_checker.SqlResultValidator(
             self._session,
-            f"CREATE TEMPORARY STAGE {self.fully_qualified_stage_name(stage_name)}",
+            f"CREATE TEMPORARY STAGE {self.fully_qualified_object_name(database_name, schema_name, stage_name)}",
             statement_params=statement_params,
         ).has_dimensions(expected_rows=1, expected_cols=1).validate()
```

## snowflake/ml/model/_client/sql/tag.py

```diff
@@ -1,117 +1,93 @@
 from typing import Any, Dict, List, Optional
 
-from snowflake.ml._internal.utils import (
-    identifier,
-    query_result_checker,
-    sql_identifier,
-)
-from snowflake.snowpark import row, session
+from snowflake.ml._internal.utils import query_result_checker, sql_identifier
+from snowflake.ml.model._client.sql import _base
+from snowflake.snowpark import row
 
 
-class ModuleTagSQLClient:
-    def __init__(
-        self,
-        session: session.Session,
-        *,
-        database_name: sql_identifier.SqlIdentifier,
-        schema_name: sql_identifier.SqlIdentifier,
-    ) -> None:
-        self._session = session
-        self._database_name = database_name
-        self._schema_name = schema_name
-
-    def __eq__(self, __value: object) -> bool:
-        if not isinstance(__value, ModuleTagSQLClient):
-            return False
-        return self._database_name == __value._database_name and self._schema_name == __value._schema_name
-
-    def fully_qualified_module_name(
-        self,
-        module_name: sql_identifier.SqlIdentifier,
-    ) -> str:
-        return identifier.get_schema_level_object_identifier(
-            self._database_name.identifier(), self._schema_name.identifier(), module_name.identifier()
-        )
-
+class ModuleTagSQLClient(_base._BaseSQLClient):
     def set_tag_on_model(
         self,
-        model_name: sql_identifier.SqlIdentifier,
         *,
-        tag_database_name: sql_identifier.SqlIdentifier,
-        tag_schema_name: sql_identifier.SqlIdentifier,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
+        model_name: sql_identifier.SqlIdentifier,
+        tag_database_name: Optional[sql_identifier.SqlIdentifier],
+        tag_schema_name: Optional[sql_identifier.SqlIdentifier],
         tag_name: sql_identifier.SqlIdentifier,
         tag_value: str,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> None:
-        fq_model_name = self.fully_qualified_module_name(model_name)
-        fq_tag_name = identifier.get_schema_level_object_identifier(
-            tag_database_name.identifier(), tag_schema_name.identifier(), tag_name.identifier()
-        )
+        fq_model_name = self.fully_qualified_object_name(database_name, schema_name, model_name)
+        fq_tag_name = self.fully_qualified_object_name(tag_database_name, tag_schema_name, tag_name)
         query_result_checker.SqlResultValidator(
             self._session,
             f"ALTER MODEL {fq_model_name} SET TAG {fq_tag_name} = $${tag_value}$$",
             statement_params=statement_params,
         ).has_dimensions(expected_rows=1, expected_cols=1).validate()
 
     def unset_tag_on_model(
         self,
-        model_name: sql_identifier.SqlIdentifier,
         *,
-        tag_database_name: sql_identifier.SqlIdentifier,
-        tag_schema_name: sql_identifier.SqlIdentifier,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
+        model_name: sql_identifier.SqlIdentifier,
+        tag_database_name: Optional[sql_identifier.SqlIdentifier],
+        tag_schema_name: Optional[sql_identifier.SqlIdentifier],
         tag_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> None:
-        fq_model_name = self.fully_qualified_module_name(model_name)
-        fq_tag_name = identifier.get_schema_level_object_identifier(
-            tag_database_name.identifier(), tag_schema_name.identifier(), tag_name.identifier()
-        )
+        fq_model_name = self.fully_qualified_object_name(database_name, schema_name, model_name)
+        fq_tag_name = self.fully_qualified_object_name(tag_database_name, tag_schema_name, tag_name)
         query_result_checker.SqlResultValidator(
             self._session,
             f"ALTER MODEL {fq_model_name} UNSET TAG {fq_tag_name}",
             statement_params=statement_params,
         ).has_dimensions(expected_rows=1, expected_cols=1).validate()
 
     def get_tag_value(
         self,
-        module_name: sql_identifier.SqlIdentifier,
         *,
-        tag_database_name: sql_identifier.SqlIdentifier,
-        tag_schema_name: sql_identifier.SqlIdentifier,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
+        model_name: sql_identifier.SqlIdentifier,
+        tag_database_name: Optional[sql_identifier.SqlIdentifier],
+        tag_schema_name: Optional[sql_identifier.SqlIdentifier],
         tag_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> row.Row:
-        fq_module_name = self.fully_qualified_module_name(module_name)
-        fq_tag_name = identifier.get_schema_level_object_identifier(
-            tag_database_name.identifier(), tag_schema_name.identifier(), tag_name.identifier()
-        )
+        fq_model_name = self.fully_qualified_object_name(database_name, schema_name, model_name)
+        fq_tag_name = self.fully_qualified_object_name(tag_database_name, tag_schema_name, tag_name)
         return (
             query_result_checker.SqlResultValidator(
                 self._session,
-                f"SELECT SYSTEM$GET_TAG($${fq_tag_name}$$, $${fq_module_name}$$, 'MODULE') AS TAG_VALUE",
+                f"SELECT SYSTEM$GET_TAG($${fq_tag_name}$$, $${fq_model_name}$$, 'MODULE') AS TAG_VALUE",
                 statement_params=statement_params,
             )
             .has_dimensions(expected_rows=1, expected_cols=1)
             .has_column("TAG_VALUE")
             .validate()[0]
         )
 
     def get_tag_list(
         self,
-        module_name: sql_identifier.SqlIdentifier,
         *,
+        database_name: Optional[sql_identifier.SqlIdentifier],
+        schema_name: Optional[sql_identifier.SqlIdentifier],
+        model_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> List[row.Row]:
-        fq_module_name = self.fully_qualified_module_name(module_name)
+        fq_model_name = self.fully_qualified_object_name(database_name, schema_name, model_name)
+        actual_database_name = database_name or self._database_name
         return (
             query_result_checker.SqlResultValidator(
                 self._session,
                 f"""SELECT TAG_DATABASE, TAG_SCHEMA, TAG_NAME, TAG_VALUE
-FROM TABLE({self._database_name.identifier()}.INFORMATION_SCHEMA.TAG_REFERENCES($${fq_module_name}$$, 'MODULE'))""",
+FROM TABLE({actual_database_name.identifier()}.INFORMATION_SCHEMA.TAG_REFERENCES($${fq_model_name}$$, 'MODULE'))""",
                 statement_params=statement_params,
             )
             .has_column("TAG_DATABASE", allow_empty=True)
             .has_column("TAG_SCHEMA", allow_empty=True)
             .has_column("TAG_NAME", allow_empty=True)
             .has_column("TAG_VALUE", allow_empty=True)
             .validate()
```

## snowflake/ml/model/_model_composer/model_composer.py

```diff
@@ -7,15 +7,15 @@
 from typing import Any, Dict, List, Optional
 
 from absl import logging
 from packaging import requirements
 from typing_extensions import deprecated
 
 from snowflake.ml._internal import env as snowml_env, env_utils, file_utils
-from snowflake.ml._internal.lineage import data_source
+from snowflake.ml._internal.lineage import data_source, lineage_utils
 from snowflake.ml.model import model_signature, type_hints as model_types
 from snowflake.ml.model._model_composer.model_manifest import model_manifest
 from snowflake.ml.model._packager import model_packager
 from snowflake.snowpark import Session
 from snowflake.snowpark._internal import utils as snowpark_utils
 
 
@@ -176,11 +176,11 @@
         options: Optional[model_types.ModelLoadOption] = None,
     ) -> model_packager.ModelPackager:
         mp = model_packager.ModelPackager(str(workspace_path / ModelComposer.MODEL_DIR_REL_PATH))
         mp.load(meta_only=meta_only, options=options)
         return mp
 
     def _get_data_sources(self, model: model_types.SupportedModelType) -> Optional[List[data_source.DataSource]]:
-        data_sources = getattr(model, "_data_sources", None)
+        data_sources = getattr(model, lineage_utils.DATA_SOURCES_ATTR, None)
         if isinstance(data_sources, list) and all(isinstance(item, data_source.DataSource) for item in data_sources):
             return data_sources
         return None
```

## snowflake/ml/model/_packager/model_handlers/mlflow.py

```diff
@@ -1,8 +1,9 @@
 import os
+import pathlib
 import tempfile
 from typing import TYPE_CHECKING, Callable, Dict, Optional, Type, cast, final
 
 import pandas as pd
 from typing_extensions import TypeGuard, Unpack
 
 from snowflake.ml._internal import file_utils, type_utils
@@ -41,15 +42,15 @@
         conda_env_file_path = mlflow.pyfunc.get_model_dependencies(model_uri, format="conda")
     except (mlflow.MlflowException, OSError):
         raise ValueError("Cannot load MLFlow model dependencies.")
 
     if not os.path.exists(conda_env_file_path):
         raise ValueError("Cannot load MLFlow model dependencies.")
 
-    env.load_from_conda_file(conda_env_file_path)
+    env.load_from_conda_file(pathlib.Path(conda_env_file_path))
 
     return env
 
 
 @final
 class MLFlowHandler(_base.BaseModelHandler["mlflow.pyfunc.PyFuncModel"]):
     """Handler for MLFlow based model.
```

## snowflake/ml/modeling/_internal/snowpark_implementations/distributed_hpo_trainer.py

```diff
@@ -1,14 +1,14 @@
 import importlib
 import inspect
 import io
 import os
 import posixpath
 import sys
-from typing import Any, Dict, List, Optional, Tuple, Union
+from typing import Any, Dict, List, Optional, Set, Tuple, Union
 
 import cloudpickle as cp
 import numpy as np
 import numpy.typing as npt
 from sklearn import model_selection
 from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
 
@@ -150,15 +150,15 @@
             temp_dict["score_time"] = each_cv_result["mean_score_time"][0]
             out.append(temp_dict)
     first_test_score = out[0]["test_scores"]
     multimetric = isinstance(first_test_score, dict)
     return multimetric, estimator._format_results(param_grid, n_split, out)
 
 
-def construct_cv_results_new_implementation(
+def construct_cv_results_memory_efficient_version(
     estimator: Union[GridSearchCV, RandomizedSearchCV],
     n_split: int,
     param_grid: List[Dict[str, Any]],
     cv_results_raw_hex: List[Row],
     cross_validator_indices_length: int,
     parameter_grid_length: int,
 ) -> Tuple[Any, Dict[str, Any]]:
@@ -201,20 +201,43 @@
 
     for each_cv_result_hex in cv_results_raw_hex:
         # convert the hex string back to cv_results_
         hex_str = bytes.fromhex(each_cv_result_hex[0])
         with io.BytesIO(hex_str) as f_reload:
             out = cp.load(f_reload)
             all_out.extend(out)
+
+    # because original SearchCV is ranked by parameter first and cv second,
+    # to make the memory efficient, we implemented by fitting on cv first and parameter second
+    # when retrieving the results back, the ordering should revert back to remain the same result as original SearchCV
+    def generate_the_order_by_parameter_index(all_combination_length: int) -> List[int]:
+        pattern = []
+        for i in range(all_combination_length):
+            if i % parameter_grid_length == 0:
+                pattern.append(i)
+        for i in range(1, parameter_grid_length):
+            for j in range(all_combination_length):
+                if j % parameter_grid_length == i:
+                    pattern.append(j)
+        return pattern
+
+    def rerank_array(original_array: List[Any], pattern: List[int]) -> List[Any]:
+        reranked_array = []
+        for index in pattern:
+            reranked_array.append(original_array[index])
+        return reranked_array
+
+    pattern = generate_the_order_by_parameter_index(len(all_out))
+    reranked_all_out = rerank_array(all_out, pattern)
     first_test_score = all_out[0]["test_scores"]
-    return first_test_score, estimator._format_results(param_grid, n_split, all_out)
+    return first_test_score, estimator._format_results(param_grid, n_split, reranked_all_out)
 
 
 cp.register_pickle_by_value(inspect.getmodule(construct_cv_results))
-cp.register_pickle_by_value(inspect.getmodule(construct_cv_results_new_implementation))
+cp.register_pickle_by_value(inspect.getmodule(construct_cv_results_memory_efficient_version))
 
 
 class DistributedHPOTrainer(SnowparkModelTrainer):
     """
     A class for performing distributed hyperparameter optimization (HPO) using Snowpark.
 
     This class inherits from SnowparkModelTrainer and extends its functionality
@@ -657,15 +680,15 @@
         with open(os.path.join(local_estimator_path, sproc_export_file_name), mode="r+b") as result_file_obj:
             fit_estimator = cp.load(result_file_obj)
 
         cleanup_temp_files([local_estimator_path])
 
         return fit_estimator
 
-    def fit_search_snowpark_new_implementation(
+    def fit_search_snowpark_enable_efficient_memory_usage(
         self,
         param_grid: Union[model_selection.ParameterGrid, model_selection.ParameterSampler],
         dataset: DataFrame,
         session: Session,
         estimator: Union[model_selection.GridSearchCV, model_selection.RandomizedSearchCV],
         dependencies: List[str],
         udf_imports: List[str],
@@ -714,15 +737,15 @@
         udtf_statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=self._subproject,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[udtf],
-            custom_tags=dict([("hpo_udtf", True)]),
+            custom_tags=dict([("hpo_memory_efficient", True)]),
         )
 
         # Put locally serialized estimator on stage.
         session.file.put(
             estimator_file_name,
             temp_stage_name,
             auto_compress=False,
@@ -956,39 +979,43 @@
                     self.X = X
                     self.y = y
                     self.test_indices = indices
                     self.params_to_evaluate = params_to_evaluate
                     self.base_estimator = base_estimator
                     self.fit_and_score_kwargs = fit_and_score_kwargs
                     self.fit_score_params: List[Any] = []
-                    self.cached_train_test_indices = []
-                    # Calculate the full index here to avoid duplicate calculation (which consumes a lot of memory)
-                    full_index = np.arange(DATA_LENGTH)
-                    for i in range(n_splits):
-                        self.cached_train_test_indices.extend(
-                            [[np.setdiff1d(full_index, self.test_indices[i]), self.test_indices[i]]]
-                        )
+                    self.cv_indices_set: Set[int] = set()
 
                 def process(self, idx: int, params_idx: int, cv_idx: int) -> None:
                     self.fit_score_params.extend([[idx, params_idx, cv_idx]])
+                    self.cv_indices_set.add(cv_idx)
 
                 def end_partition(self) -> Iterator[Tuple[int, str]]:
                     from sklearn.base import clone
                     from sklearn.model_selection._validation import _fit_and_score
                     from sklearn.utils.parallel import Parallel, delayed
 
+                    cached_train_test_indices = {}
+                    # Calculate the full index here to avoid duplicate calculation (which consumes a lot of memory)
+                    full_index = np.arange(DATA_LENGTH)
+                    for i in self.cv_indices_set:
+                        cached_train_test_indices[i] = [
+                            np.setdiff1d(full_index, self.test_indices[i]),
+                            self.test_indices[i],
+                        ]
+
                     parallel = Parallel(n_jobs=_N_JOBS, pre_dispatch=_PRE_DISPATCH)
 
                     out = parallel(
                         delayed(_fit_and_score)(
                             clone(self.base_estimator),
                             self.X,
                             self.y,
-                            train=self.cached_train_test_indices[split_idx][0],
-                            test=self.cached_train_test_indices[split_idx][1],
+                            train=cached_train_test_indices[split_idx][0],
+                            test=cached_train_test_indices[split_idx][1],
                             parameters=self.params_to_evaluate[cand_idx],
                             split_progress=(split_idx, n_splits),
                             candidate_progress=(cand_idx, n_candidates),
                             **self.fit_and_score_kwargs,  # load sample weight here
                         )
                         for _, cand_idx, split_idx in self.fit_score_params
                     )
@@ -1001,31 +1028,33 @@
                     yield (
                         self.fit_score_params[0][0],
                         binary_cv_results,
                     )
 
             session.udtf.register(
                 SearchCV,
-                output_schema=StructType([StructField("IDX", IntegerType()), StructField("CV_RESULTS", StringType())]),
+                output_schema=StructType(
+                    [StructField("FIRST_IDX", IntegerType()), StructField("EACH_CV_RESULTS", StringType())]
+                ),
                 input_types=[IntegerType(), IntegerType(), IntegerType()],
                 name=random_udtf_name,
                 packages=required_deps,  # type: ignore[arg-type]
                 replace=True,
                 is_permanent=False,
                 imports=imports,  # type: ignore[arg-type]
                 statement_params=udtf_statement_params,
             )
 
             HP_TUNING = F.table_function(random_udtf_name)
 
             # param_indices is for the index for each parameter grid;
             # cv_indices is for the index for each cross_validator's fold;
             # param_cv_indices is for the index for the product of (len(param_indices) * len(cv_indices))
-            param_indices, cv_indices = zip(
-                *product(range(parameter_grid_length), range(cross_validator_indices_length))
+            cv_indices, param_indices = zip(
+                *product(range(cross_validator_indices_length), range(parameter_grid_length))
             )
 
             indices_info_pandas = pd.DataFrame(
                 {
                     "IDX": [i // _NUM_CPUs for i in range(parameter_grid_length * cross_validator_indices_length)],
                     "PARAM_IND": param_indices,
                     "CV_IND": cv_indices,
@@ -1038,19 +1067,19 @@
                 (
                     HP_TUNING(indices_info_sp["IDX"], indices_info_sp["PARAM_IND"], indices_info_sp["CV_IND"]).over(
                         partition_by="IDX"
                     )
                 ),
             )
 
-            first_test_score, cv_results_ = construct_cv_results_new_implementation(
+            first_test_score, cv_results_ = construct_cv_results_memory_efficient_version(
                 estimator,
                 n_splits,
                 list(param_grid),
-                HP_raw_results.select("CV_RESULTS").sort(F.col("IDX")).collect(),
+                HP_raw_results.select("EACH_CV_RESULTS").sort(F.col("FIRST_IDX")).collect(),
                 cross_validator_indices_length,
                 parameter_grid_length,
             )
 
             estimator.cv_results_ = cv_results_
             estimator.multimetric_ = isinstance(first_test_score, dict)
 
@@ -1159,15 +1188,15 @@
                 n_iter=self.estimator.n_iter,
                 random_state=self.estimator.random_state,
             )
         relaxed_dependencies = pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=model_spec.pkgDependencies, session=self.session
         )
         if ENABLE_EFFICIENT_MEMORY_USAGE:
-            return self.fit_search_snowpark_new_implementation(
+            return self.fit_search_snowpark_enable_efficient_memory_usage(
                 param_grid=param_grid,
                 dataset=self.dataset,
                 session=self.session,
                 estimator=self.estimator,
                 dependencies=relaxed_dependencies,
                 udf_imports=["sklearn"],
                 input_cols=self.input_cols,
```

## snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_trainer.py

```diff
@@ -41,14 +41,15 @@
 from snowflake.snowpark.stored_procedure import StoredProcedure
 
 cp.register_pickle_by_value(inspect.getmodule(get_temp_file_path))
 cp.register_pickle_by_value(inspect.getmodule(identifier.get_inferred_name))
 cp.register_pickle_by_value(inspect.getmodule(handle_inference_result))
 
 _PROJECT = "ModelDevelopment"
+_ENABLE_ANONYMOUS_SPROC = False
 
 
 class SnowparkModelTrainer:
     """
     A class for training models on Snowflake data using the Sproc.
 
     TODO (snandamuri): Introduce the concept of executor that would take the training function
@@ -247,14 +248,35 @@
 
             # Note: you can add something like  + "|" + str(df) to the return string
             # to pass debug information to the caller.
             return str(os.path.basename(local_result_file_name))
 
         return fit_wrapper_function
 
+    def _get_fit_wrapper_sproc_anonymous(self, statement_params: Dict[str, str]) -> StoredProcedure:
+        model_spec = ModelSpecificationsBuilder.build(model=self.estimator)
+        fit_sproc_name = random_name_for_temp_object(TempObjectType.PROCEDURE)
+
+        relaxed_dependencies = pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
+            pkg_versions=model_spec.pkgDependencies, session=self.session
+        )
+
+        fit_wrapper_sproc = self.session.sproc.register(
+            func=self._build_fit_wrapper_sproc(model_spec=model_spec),
+            is_permanent=False,
+            name=fit_sproc_name,
+            packages=["snowflake-snowpark-python"] + relaxed_dependencies,  # type: ignore[arg-type]
+            replace=True,
+            session=self.session,
+            statement_params=statement_params,
+            anonymous=True,
+        )
+
+        return fit_wrapper_sproc
+
     def _get_fit_wrapper_sproc(self, statement_params: Dict[str, str]) -> StoredProcedure:
         # If the sproc already exists, don't register.
         if not hasattr(self.session, "_FIT_WRAPPER_SPROCS"):
             self.session._FIT_WRAPPER_SPROCS: Dict[str, StoredProcedure] = {}  # type: ignore[attr-defined, misc]
 
         model_spec = ModelSpecificationsBuilder.build(model=self.estimator)
         fit_sproc_key = model_spec.__class__.__name__
@@ -506,14 +528,36 @@
                 quote_identifiers=False,
             )
 
             return str(os.path.basename(local_result_file_name))
 
         return fit_transform_wrapper_function
 
+    def _get_fit_predict_wrapper_sproc_anonymous(self, statement_params: Dict[str, str]) -> StoredProcedure:
+        model_spec = ModelSpecificationsBuilder.build(model=self.estimator)
+
+        fit_predict_sproc_name = random_name_for_temp_object(TempObjectType.PROCEDURE)
+
+        relaxed_dependencies = pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
+            pkg_versions=model_spec.pkgDependencies, session=self.session
+        )
+
+        fit_predict_wrapper_sproc = self.session.sproc.register(
+            func=self._build_fit_predict_wrapper_sproc(model_spec=model_spec),
+            is_permanent=False,
+            name=fit_predict_sproc_name,
+            packages=["snowflake-snowpark-python"] + relaxed_dependencies,  # type: ignore[arg-type]
+            replace=True,
+            session=self.session,
+            statement_params=statement_params,
+            anonymous=True,
+        )
+
+        return fit_predict_wrapper_sproc
+
     def _get_fit_predict_wrapper_sproc(self, statement_params: Dict[str, str]) -> StoredProcedure:
         # If the sproc already exists, don't register.
         if not hasattr(self.session, "_FIT_WRAPPER_SPROCS"):
             self.session._FIT_WRAPPER_SPROCS: Dict[str, StoredProcedure] = {}  # type: ignore[attr-defined, misc]
 
         model_spec = ModelSpecificationsBuilder.build(model=self.estimator)
         fit_predict_sproc_key = model_spec.__class__.__name__ + "_fit_predict"
@@ -541,14 +585,35 @@
 
         self.session._FIT_WRAPPER_SPROCS[  # type: ignore[attr-defined]
             fit_predict_sproc_key
         ] = fit_predict_wrapper_sproc
 
         return fit_predict_wrapper_sproc
 
+    def _get_fit_transform_wrapper_sproc_anonymous(self, statement_params: Dict[str, str]) -> StoredProcedure:
+        model_spec = ModelSpecificationsBuilder.build(model=self.estimator)
+
+        fit_transform_sproc_name = random_name_for_temp_object(TempObjectType.PROCEDURE)
+
+        relaxed_dependencies = pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
+            pkg_versions=model_spec.pkgDependencies, session=self.session
+        )
+
+        fit_transform_wrapper_sproc = self.session.sproc.register(
+            func=self._build_fit_transform_wrapper_sproc(model_spec=model_spec),
+            is_permanent=False,
+            name=fit_transform_sproc_name,
+            packages=["snowflake-snowpark-python"] + relaxed_dependencies,  # type: ignore[arg-type]
+            replace=True,
+            session=self.session,
+            statement_params=statement_params,
+            anonymous=True,
+        )
+        return fit_transform_wrapper_sproc
+
     def _get_fit_transform_wrapper_sproc(self, statement_params: Dict[str, str]) -> StoredProcedure:
         # If the sproc already exists, don't register.
         if not hasattr(self.session, "_FIT_WRAPPER_SPROCS"):
             self.session._FIT_WRAPPER_SPROCS: Dict[str, StoredProcedure] = {}  # type: ignore[attr-defined, misc]
 
         model_spec = ModelSpecificationsBuilder.build(model=self.estimator)
         fit_transform_sproc_key = model_spec.__class__.__name__ + "_fit_transform"
@@ -608,15 +673,18 @@
             project=_PROJECT,
             subproject=self._subproject,
             function_name=telemetry.get_statement_params_full_func_name(inspect.currentframe(), self._class_name),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]) if self._autogenerated else None,
         )
 
-        fit_wrapper_sproc = self._get_fit_wrapper_sproc(statement_params=statement_params)
+        if _ENABLE_ANONYMOUS_SPROC:
+            fit_wrapper_sproc = self._get_fit_wrapper_sproc_anonymous(statement_params=statement_params)
+        else:
+            fit_wrapper_sproc = self._get_fit_wrapper_sproc(statement_params=statement_params)
 
         try:
             sproc_export_file_name: str = fit_wrapper_sproc(
                 self.session,
                 queries,
                 stage_transform_file_name,
                 stage_result_file_name,
@@ -676,15 +744,19 @@
             project=_PROJECT,
             subproject=self._subproject,
             function_name=telemetry.get_statement_params_full_func_name(inspect.currentframe(), self._class_name),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]) if self._autogenerated else None,
         )
 
-        fit_predict_wrapper_sproc = self._get_fit_predict_wrapper_sproc(statement_params=statement_params)
+        if _ENABLE_ANONYMOUS_SPROC:
+            fit_predict_wrapper_sproc = self._get_fit_predict_wrapper_sproc_anonymous(statement_params=statement_params)
+        else:
+            fit_predict_wrapper_sproc = self._get_fit_predict_wrapper_sproc(statement_params=statement_params)
+
         fit_predict_result_name = random_name_for_temp_object(TempObjectType.TABLE)
 
         sproc_export_file_name: str = fit_predict_wrapper_sproc(
             self.session,
             queries,
             stage_transform_file_name,
             stage_result_file_name,
@@ -737,15 +809,21 @@
             project=_PROJECT,
             subproject=self._subproject,
             function_name=telemetry.get_statement_params_full_func_name(inspect.currentframe(), self._class_name),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]) if self._autogenerated else None,
         )
 
-        fit_transform_wrapper_sproc = self._get_fit_transform_wrapper_sproc(statement_params=statement_params)
+        if _ENABLE_ANONYMOUS_SPROC:
+            fit_transform_wrapper_sproc = self._get_fit_transform_wrapper_sproc_anonymous(
+                statement_params=statement_params
+            )
+        else:
+            fit_transform_wrapper_sproc = self._get_fit_transform_wrapper_sproc(statement_params=statement_params)
+
         fit_transform_result_name = random_name_for_temp_object(TempObjectType.TABLE)
 
         sproc_export_file_name: str = fit_transform_wrapper_sproc(
             self.session,
             queries,
             stage_transform_file_name,
             stage_result_file_name,
```

## snowflake/ml/modeling/framework/base.py

```diff
@@ -12,15 +12,15 @@
 from snowflake import snowpark
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.exceptions import (
     error_codes,
     exceptions,
     modeling_error_messages,
 )
-from snowflake.ml._internal.lineage import data_source, dataset_dataframe
+from snowflake.ml._internal.lineage import data_source, lineage_utils
 from snowflake.ml._internal.utils import identifier, parallelize
 from snowflake.ml.modeling.framework import _utils
 from snowflake.snowpark import functions as F
 
 PROJECT = "ModelDevelopment"
 SUBPROJECT = "Preprocessing"
 
@@ -426,16 +426,17 @@
 
     @telemetry.send_api_usage_telemetry(
         project=PROJECT,
         subproject=SUBPROJECT,
     )
     def fit(self, dataset: Union[snowpark.DataFrame, pd.DataFrame]) -> "BaseEstimator":
         """Runs universal logics for all fit implementations."""
-        if isinstance(dataset, dataset_dataframe.DatasetDataFrame):
-            self._data_sources = dataset._get_sources()
+        self._data_sources = getattr(dataset, lineage_utils.DATA_SOURCES_ATTR, None)
+        if self._data_sources:
+            assert all(isinstance(ds, data_source.DataSource) for ds in self._data_sources)
         return self._fit(dataset)
 
     @abstractmethod
     def _fit(self, dataset: Union[snowpark.DataFrame, pd.DataFrame]) -> "BaseEstimator":
         raise NotImplementedError()
 
     def _use_input_cols_only(self, dataset: pd.DataFrame) -> pd.DataFrame:
```

## snowflake/ml/modeling/pipeline/pipeline.py

```diff
@@ -111,25 +111,28 @@
         super().__init__()
         self.steps = steps
         self._is_final_step_estimator = Pipeline._is_estimator(steps[-1][1])
         self._is_fitted = False
         self._feature_names_in: List[np.ndarray[Any, np.dtype[Any]]] = []
         self._n_features_in: List[int] = []
         self._transformers_to_input_indices: Dict[str, List[int]] = {}
-        self._is_convertible_to_sklearn = True
+        self._modifies_label_or_sample_weight = True
 
         self._model_signature_dict: Optional[Dict[str, ModelSignature]] = None
 
         deps: Set[str] = {f"pandas=={pd.__version__}", f"scikit-learn=={skversion}"}
         for _, obj in steps:
             if isinstance(obj, base.BaseTransformer):
                 deps = deps | set(obj._get_dependencies())
         self._deps = list(deps)
         self._sklearn_object = None
         self.label_cols = self._get_label_cols()
+        self._is_convertible_to_sklearn = self._is_convertible_to_sklearn_object()
+
+        self._send_pipeline_configuration_telemetry()
 
     @staticmethod
     def _is_estimator(obj: object) -> bool:
         # TODO(SNOW-723770): Figure out a better way to identify estimator objects.
         return has_callable_attr(obj, "fit") and has_callable_attr(obj, "predict")
 
     @staticmethod
@@ -224,15 +227,15 @@
             estimator_step[1].get_label_cols()
             + ([] if not estimator_step[1].get_sample_weight_col() else [estimator_step[1].get_sample_weight_col()])
         )
 
         return [c for c in columns if c not in target_cols]
 
     def _append_step_feature_consumption_info(self, step_name: str, all_cols: List[str], input_cols: List[str]) -> None:
-        if self._is_convertible_to_sklearn:
+        if self._modifies_label_or_sample_weight:
             all_cols = self._get_sanitized_list_of_columns(all_cols)
             self._feature_names_in.append(np.asarray(all_cols, dtype=object))
             self._n_features_in.append(len(all_cols))
             self._transformers_to_input_indices[step_name] = _get_column_indices(
                 all_columns=all_cols, target_columns=input_cols
             )
 
@@ -244,15 +247,15 @@
             transformed_dataset = trans.transform(transformed_dataset)
         return transformed_dataset
 
     def _fit_transform_dataset(
         self, dataset: Union[snowpark.DataFrame, pd.DataFrame]
     ) -> Union[snowpark.DataFrame, pd.DataFrame]:
         self._reset()
-        self._is_convertible_to_sklearn = not self._is_pipeline_modifying_label_or_sample_weight()
+        self._modifies_label_or_sample_weight = not self._is_pipeline_modifying_label_or_sample_weight()
         transformed_dataset = dataset
         for name, trans in self._get_transformers():
             self._append_step_feature_consumption_info(
                 step_name=name, all_cols=transformed_dataset.columns[:], input_cols=trans.get_input_cols()
             )
             trans.fit(transformed_dataset)
             transformed_dataset = trans.transform(transformed_dataset)
@@ -421,15 +424,15 @@
         dataset = (
             snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(dataset)
             if isinstance(dataset, snowpark.DataFrame)
             else dataset
         )
 
         if self._can_be_trained_in_ml_runtime(dataset):
-            if not self._is_convertible_to_sklearn_object():
+            if not self._is_convertible_to_sklearn:
                 raise ValueError("This pipeline cannot be converted to an sklearn pipeline.")
             self._fit_ml_runtime(dataset)
 
         elif squash and isinstance(dataset, snowpark.DataFrame):
             session = dataset._session
             assert session is not None
             self._fit_snowpark_dataframe_within_one_sproc(session=session, dataset=dataset)
@@ -943,15 +946,15 @@
         """
         if not isinstance(dataset, snowpark.DataFrame):
             return False
 
         if not os.environ.get(IN_ML_RUNTIME_ENV_VAR):
             return False
 
-        return self._is_convertible_to_sklearn_object()
+        return self._is_convertible_to_sklearn
 
     @staticmethod
     def _wrap_transformer_in_column_transformer(
         transformer_name: str, transformer: base.BaseTransformer
     ) -> ColumnTransformer:
         """A helper function to convert a transformer object to an sklearn object and wrap in an sklearn
             ColumnTransformer.
@@ -999,15 +1002,15 @@
 
         return pipeline.Pipeline(sklearn_pipeline_steps)
 
     def _create_sklearn_object(self) -> pipeline.Pipeline:
         if not self._is_fitted:
             return self._create_unfitted_sklearn_object()
 
-        if not self._is_convertible_to_sklearn:
+        if not self._modifies_label_or_sample_weight:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=ValueError(
                     "The pipeline can't be converted to SKLearn equivalent because it modifies processing label or "
                     "sample_weight columns as part of pipeline preprocessing steps which is not allowed in SKLearn."
                 ),
             )
@@ -1105,11 +1108,28 @@
         """
         if self._is_fitted:
             if self._sklearn_object is not None:
                 return self._sklearn_object
             else:
                 return self._create_sklearn_object()
         else:
-            if self._is_convertible_to_sklearn_object():
+            if self._is_convertible_to_sklearn:
                 return self._create_unfitted_sklearn_object()
             else:
                 raise ValueError("This pipeline can not be converted to an sklearn pipeline.")
+
+    def _send_pipeline_configuration_telemetry(self) -> None:
+        """Track information about the pipeline setup. Currently, we want to track:
+        - Whether the pipeline is converible to an sklearn pipeline
+        - Whether the pipeline is being used in the SPCS ml runtime.
+        """
+
+        telemetry_data = {
+            "pipeline_is_convertible_to_sklearn": self._is_convertible_to_sklearn,
+            "in_spcs_ml_runtime": bool(os.environ.get(IN_ML_RUNTIME_ENV_VAR)),
+        }
+        telemetry.send_custom_usage(
+            project=_PROJECT,
+            subproject=_SUBPROJECT,
+            telemetry_type=telemetry.TelemetryField.TYPE_SNOWML_PIPELINE_USAGE.value,
+            data=telemetry_data,
+        )
```

## snowflake/ml/registry/_manager/model_manager.py

```diff
@@ -44,28 +44,37 @@
         signatures: Optional[Dict[str, model_signature.ModelSignature]] = None,
         sample_input_data: Optional[model_types.SupportedDataType] = None,
         code_paths: Optional[List[str]] = None,
         ext_modules: Optional[List[ModuleType]] = None,
         options: Optional[model_types.ModelSaveOption] = None,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> model_version_impl.ModelVersion:
-        model_name_id = sql_identifier.SqlIdentifier(model_name)
+        database_name_id, schema_name_id, model_name_id = sql_identifier.parse_fully_qualified_name(model_name)
 
         if not version_name:
             version_name = self._hrid_generator.generate()[1]
         version_name_id = sql_identifier.SqlIdentifier(version_name)
 
         if self._model_ops.validate_existence(
-            model_name=model_name_id, statement_params=statement_params
+            database_name=database_name_id,
+            schema_name=schema_name_id,
+            model_name=model_name_id,
+            statement_params=statement_params,
         ) and self._model_ops.validate_existence(
-            model_name=model_name_id, version_name=version_name_id, statement_params=statement_params
+            database_name=database_name_id,
+            schema_name=schema_name_id,
+            model_name=model_name_id,
+            version_name=version_name_id,
+            statement_params=statement_params,
         ):
             raise ValueError(f"Model {model_name} version {version_name} already existed.")
 
         stage_path = self._model_ops.prepare_model_stage_path(
+            database_name=database_name_id,
+            schema_name=schema_name_id,
             statement_params=statement_params,
         )
 
         logger.info("Start packaging and uploading your model. It might take some time based on the size of the model.")
 
         mc = model_composer.ModelComposer(self._model_ops._session, stage_path=stage_path)
         mc.save(
@@ -81,62 +90,78 @@
             options=options,
         )
 
         logger.info("Start creating MODEL object for you in the Snowflake.")
 
         self._model_ops.create_from_stage(
             composed_model=mc,
+            database_name=database_name_id,
+            schema_name=schema_name_id,
             model_name=model_name_id,
             version_name=version_name_id,
             statement_params=statement_params,
         )
 
         mv = model_version_impl.ModelVersion._ref(
-            self._model_ops,
+            model_ops.ModelOperator(
+                self._model_ops._session,
+                database_name=database_name_id or self._database_name,
+                schema_name=schema_name_id or self._schema_name,
+            ),
             model_name=model_name_id,
             version_name=version_name_id,
         )
 
         if comment:
             mv.comment = comment
 
         if metrics:
             self._model_ops._metadata_ops.save(
                 metadata_ops.ModelVersionMetadataSchema(metrics=metrics),
+                database_name=database_name_id,
+                schema_name=schema_name_id,
                 model_name=model_name_id,
                 version_name=version_name_id,
                 statement_params=statement_params,
             )
 
         return mv
 
     def get_model(
         self,
         model_name: str,
         *,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> model_impl.Model:
-        model_name_id = sql_identifier.SqlIdentifier(model_name)
+        database_name_id, schema_name_id, model_name_id = sql_identifier.parse_fully_qualified_name(model_name)
         if self._model_ops.validate_existence(
+            database_name=database_name_id,
+            schema_name=schema_name_id,
             model_name=model_name_id,
             statement_params=statement_params,
         ):
             return model_impl.Model._ref(
-                self._model_ops,
+                model_ops.ModelOperator(
+                    self._model_ops._session,
+                    database_name=database_name_id or self._database_name,
+                    schema_name=schema_name_id or self._schema_name,
+                ),
                 model_name=model_name_id,
             )
         else:
             raise ValueError(f"Unable to find model {model_name}")
 
     def models(
         self,
         *,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> List[model_impl.Model]:
         model_names = self._model_ops.list_models_or_versions(
+            database_name=None,
+            schema_name=None,
             statement_params=statement_params,
         )
         return [
             model_impl.Model._ref(
                 self._model_ops,
                 model_name=model_name,
             )
@@ -145,23 +170,27 @@
 
     def show_models(
         self,
         *,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> pd.DataFrame:
         rows = self._model_ops.show_models_or_versions(
+            database_name=None,
+            schema_name=None,
             statement_params=statement_params,
         )
         return pd.DataFrame([row.as_dict() for row in rows])
 
     def delete_model(
         self,
         model_name: str,
         *,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> None:
-        model_name_id = sql_identifier.SqlIdentifier(model_name)
+        database_name_id, schema_name_id, model_name_id = sql_identifier.parse_fully_qualified_name(model_name)
 
         self._model_ops.delete_model_or_version(
+            database_name=database_name_id,
+            schema_name=schema_name_id,
             model_name=model_name_id,
             statement_params=statement_params,
         )
```

## Comparing `snowflake_ml_python-1.5.0.dist-info/LICENSE.txt` & `snowflake_ml_python-1.5.1.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `snowflake_ml_python-1.5.0.dist-info/METADATA` & `snowflake_ml_python-1.5.1.dist-info/METADATA`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: snowflake-ml-python
-Version: 1.5.0
+Version: 1.5.1
 Summary: The machine learning client library that is used for interacting with Snowflake to build machine learning solutions.
 Author-email: "Snowflake, Inc" <support@snowflake.com>
 License: 
                                          Apache License
                                    Version 2.0, January 2004
                                 http://www.apache.org/licenses/
         
@@ -232,15 +232,14 @@
 Classifier: Topic :: Scientific/Engineering :: Information Analysis
 Requires-Python: <3.12,>=3.8
 Description-Content-Type: text/markdown
 License-File: LICENSE.txt
 Requires-Dist: absl-py <2,>=0.15
 Requires-Dist: anyio <4,>=3.5.0
 Requires-Dist: cachetools <6,>=3.1.1
-Requires-Dist: catboost <1.3,>=1.2.0
 Requires-Dist: cloudpickle >=2.0.0
 Requires-Dist: fsspec[http] <2024,>=2022.11
 Requires-Dist: importlib-resources <7,>=6.1.1
 Requires-Dist: numpy <2,>=1.23
 Requires-Dist: packaging <24,>=20.9
 Requires-Dist: pandas <3,>=1.0.0
 Requires-Dist: pyarrow
@@ -252,41 +251,44 @@
 Requires-Dist: scipy <2,>=1.9
 Requires-Dist: snowflake-connector-python[pandas] <4,>=3.5.0
 Requires-Dist: snowflake-snowpark-python !=1.12.0,<2,>=1.11.1
 Requires-Dist: sqlparse <1,>=0.4
 Requires-Dist: typing-extensions <5,>=4.1.0
 Requires-Dist: xgboost <2,>=1.7.3
 Provides-Extra: all
-Requires-Dist: lightgbm <4.2,>=3.3.5 ; extra == 'all'
+Requires-Dist: catboost <2,>=1.2.0 ; extra == 'all'
+Requires-Dist: lightgbm <5,>=3.3.5 ; extra == 'all'
 Requires-Dist: mlflow <2.4,>=2.1.0 ; extra == 'all'
 Requires-Dist: peft <1,>=0.5.0 ; extra == 'all'
 Requires-Dist: sentence-transformers <3,>=2.2.2 ; extra == 'all'
-Requires-Dist: sentencepiece <0.2,>=0.1.95 ; extra == 'all'
+Requires-Dist: sentencepiece <1,>=0.1.95 ; extra == 'all'
 Requires-Dist: shap ==0.42.1 ; extra == 'all'
 Requires-Dist: tensorflow <3,>=2.10 ; extra == 'all'
 Requires-Dist: tokenizers <1,>=0.10 ; extra == 'all'
 Requires-Dist: torch <3,>=2.0.1 ; extra == 'all'
 Requires-Dist: torchdata <1,>=0.4 ; extra == 'all'
 Requires-Dist: transformers <5,>=4.32.1 ; extra == 'all'
+Provides-Extra: catboost
+Requires-Dist: catboost <2,>=1.2.0 ; extra == 'catboost'
 Provides-Extra: lightgbm
-Requires-Dist: lightgbm <4.2,>=3.3.5 ; extra == 'lightgbm'
+Requires-Dist: lightgbm <5,>=3.3.5 ; extra == 'lightgbm'
 Provides-Extra: llm
 Requires-Dist: peft <1,>=0.5.0 ; extra == 'llm'
 Provides-Extra: mlflow
 Requires-Dist: mlflow <2.4,>=2.1.0 ; extra == 'mlflow'
 Provides-Extra: shap
 Requires-Dist: shap ==0.42.1 ; extra == 'shap'
 Provides-Extra: tensorflow
 Requires-Dist: tensorflow <3,>=2.10 ; extra == 'tensorflow'
 Provides-Extra: torch
 Requires-Dist: torch <3,>=2.0.1 ; extra == 'torch'
 Requires-Dist: torchdata <1,>=0.4 ; extra == 'torch'
 Provides-Extra: transformers
 Requires-Dist: sentence-transformers <3,>=2.2.2 ; extra == 'transformers'
-Requires-Dist: sentencepiece <0.2,>=0.1.95 ; extra == 'transformers'
+Requires-Dist: sentencepiece <1,>=0.1.95 ; extra == 'transformers'
 Requires-Dist: tokenizers <1,>=0.10 ; extra == 'transformers'
 Requires-Dist: transformers <5,>=4.32.1 ; extra == 'transformers'
 
 # Snowpark ML
 
 Snowpark ML is a set of tools including SDKs and underlying infrastructure to build and deploy machine learning models.
 With Snowpark ML, you can pre-process data, train, manage and deploy ML models all within Snowflake, using a single SDK,
@@ -367,14 +369,39 @@
 ```
 
 Note that until a `snowflake-ml-python` package version is available in the official Snowflake conda channel, there may
 be compatibility issues. Server-side functionality that `snowflake-ml-python` depends on may not yet be released.
 
 # Release History
 
+## 1.5.1
+
+### Bug Fixes
+
+- Dataset: Fix `snowflake.connector.errors.DataError: Query Result did not match expected number of rows` when accessing
+  DatasetVersion properties when case insensitive `SHOW VERSIONS IN DATASET` check matches multiple version names.
+- Dataset: Fix bug in SnowFS bulk file read when used with DuckDB
+- Registry: Fixed a bug when loading old models.
+- Lineage: Fix Dataset source lineage propagation through `snowpark.DataFrame` transformations
+
+### Behavior Changes
+
+- Feature Store: convert clear() into a private function. Also make it deletes feature views and entities only.
+- Feature Store: Use NULL as default value for timestamp tag value.
+
+### New Features
+
+- Feature Store: Added new `snowflake.ml.feature_store.setup_feature_store()` API to assist Feature Store RBAC setup.
+- Feature Store: Add `output_type` argument to `FeatureStore.generate_dataset()` to allow generating data snapshots
+  as Datasets or Tables.
+- Registry: `log_model`, `get_model`, `delete_model` now supports fully qualified name.
+- Modeling: Supports anonymous stored procedure during fit calls so that modeling would not require sufficient
+  permissions to operate on schema. Please call
+  `import snowflake.ml.modeling.parameters.enable_anonymous_sproc  # noqa: F401`
+
 ## 1.5.0
 
 ### Bug Fixes
 
 - Registry: Fix invalid parameter 'SHOW_MODEL_DETAILS_IN_SHOW_VERSIONS_IN_MODEL' error.
 
 ### Behavior Changes
@@ -407,20 +434,27 @@
 - `Dataset.owner` has been moved to `Dataset.selected_version.owner`
 - `Dataset.desc` has been moved to `DatasetVersion.selected_version.comment`
 - `Dataset.timestamp_col`, `Dataset.label_cols`, `Dataset.feature_store_metadata`, and
   `Dataset.schema_version` have been removed.
 
 #### Feature Store (PrPr)
 
-`FeatureStore.generate_dataset` argument list has been changed to match the new
+- `FeatureStore.generate_dataset` argument list has been changed to match the new
 `snowflake.ml.dataset.Dataset` definition
 
-- `materialized_table` has been removed and replaced with `name` and `version`.
-- `name` moved to first positional argument
-- `save_mode` has been removed as `merge` behavior is no longer supported. The new behavior is always `errorifexists`.
+  - `materialized_table` has been removed and replaced with `name` and `version`.
+  - `name` moved to first positional argument
+  - `save_mode` has been removed as `merge` behavior is no longer supported. The new behavior is always `errorifexists`.
+
+- Change feature view version type from str to `FeatureViewVersion`. It is a restricted string literal.
+
+- Remove as_dataframe arg from FeatureStore.list_feature_views(), now always returns result as DataFrame.
+
+- Combines few metadata tags into a new tag: SNOWML_FEATURE_VIEW_METADATA. This will make previously created feature views
+not readable by new SDK.
 
 ### New Features
 
 - Registry: Add `export` method to `ModelVersion` instance to export model files.
 - Registry: Add `load` method to `ModelVersion` instance to load the underlying object from the model.
 - Registry: Add `Model.rename` method to `Model` instance to rename or move a model.
 
@@ -428,25 +462,33 @@
 
 - Added Snowpark DataFrame integration using `Dataset.read.to_snowpark_dataframe()`
 - Added Pandas DataFrame integration using `Dataset.read.to_pandas()`
 - Added PyTorch and TensorFlow integrations using `Dataset.read.to_torch_datapipe()`
     and `Dataset.read.to_tf_dataset()` respectively.
 - Added `fsspec` style file integration using `Dataset.read.files()` and `Dataset.read.filesystem()`
 
+#### Feature Store
+
+- use new tag_reference_internal to speed up metadata lookup.
+
 ## 1.4.1 (2024-04-18)
 
 ### New Features
 
 - Registry: Add support for `catboost` model (`catboost.CatBoostClassifier`, `catboost.CatBoostRegressor`).
 - Registry: Add support for `lightgbm` model (`lightgbm.Booster`, `lightgbm.LightGBMClassifier`, `lightgbm.LightGBMRegressor`).
 
 ### Bug Fixes
 
 - Registry: Fix a bug that leads to relax_version option is not working.
 
+### Behavior changes
+
+- Feature Store: update_feature_view takes refresh_freq and warehouse as argument.
+
 ## 1.4.0 (2024-04-08)
 
 ### Bug Fixes
 
 - Registry: Fix a bug when multiple models are being called from the same query, models other than the first one will
   have incorrect result. This fix only works for newly logged model.
 - Modeling: When registering a model, only method(s) that is mentioned in `save_model` would be added to model signature
@@ -460,14 +502,16 @@
   is not available in the Snowflake conda channel
 - Modeling: Add sklearn as required dependency for LightGBM package.
 
 ### Behavior Changes
 
 - Registry: `apply` method is no longer by default logged when logging a xgboost model. If that is required, it could
   be specified manually when logging the model by `log_model(..., options={"target_methods": ["apply", ...]})`.
+- Feature Store: register_entity returns an entity object.
+- Feature Store: register_feature_view `block=true` becomes default.
 
 ### New Features
 
 - Registry: Add support for `sentence-transformers` model (`sentence_transformers.SentenceTransformer`).
 - Registry: Now version name is no longer required when logging a model. If not provided, a random human readable ID
   will be generated.
```

## Comparing `snowflake_ml_python-1.5.0.dist-info/RECORD` & `snowflake_ml_python-1.5.1.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,21 +1,21 @@
 snowflake/cortex/__init__.py,sha256=CAUk94eXmNBXXaiLg-yNodyM2FPHvacErKtdVQYqtRM,360
 snowflake/cortex/_complete.py,sha256=C2wLk5RMtg-d2bkdbQKou6U8nvR8g3vykpCkH9-gF9g,1226
 snowflake/cortex/_extract_answer.py,sha256=4tiz4pUisw035ZLmCQDcGuwoT-jFpuo5dzrQYhvYHCA,1358
 snowflake/cortex/_sentiment.py,sha256=7X_a8qJNFFgn-Y1tjwMDkyNJHz5yYl0PvnezVCc4TsM,1149
 snowflake/cortex/_summarize.py,sha256=DJRxUrPrTVmtQNgus0ZPF1z8nPmn4Rs5oL3U25CfXxQ,1075
 snowflake/cortex/_translate.py,sha256=JPMIXxHTgJPfJqT5Hw_WtYM6FZ8NuQufZ4XR-M8wnyo,1420
 snowflake/cortex/_util.py,sha256=0xDaDSctenhuj59atZenZp5q9zuhji0WQ77KPjqqNoc,1557
-snowflake/ml/version.py,sha256=3R9EThHATDpTgUWWErtoGeHWTDLDz1F9kgtQ7lMuM80,16
+snowflake/ml/version.py,sha256=WiE22xydACC6rXf2dqEDDHIINeQkQXbfF2V05DUpD-o,16
 snowflake/ml/_internal/env.py,sha256=kCrJTRnqQ97VGUVI1cWUPD8HuBWeL5vOOtwUR0NB9Mg,161
-snowflake/ml/_internal/env_utils.py,sha256=aqaF-bXPUvXxONfMRxIuH-JKyu3oWkRqyC_1jDEmK4Y,27629
+snowflake/ml/_internal/env_utils.py,sha256=HK5Ug5-gChiUv_z84BDjAuE9eHImrWRsX4Y7wJFApfk,27758
 snowflake/ml/_internal/file_utils.py,sha256=OyXHv-UcItiip1YgLnab6etonUQkYuyDtmplZA0CaoU,13622
 snowflake/ml/_internal/init_utils.py,sha256=U-oPOtyVf22hCwDH_CH2uDr9yuN6Mr3kwQ_yRAs1mcM,2696
 snowflake/ml/_internal/migrator_utils.py,sha256=k3erO8x3YJcX6nkKeyJAUNGg1qjE3RFmD-W6dtLzIH0,161
-snowflake/ml/_internal/telemetry.py,sha256=oM7dDcs1GKgxKP2UM7va1j1YfQGISFiGYyiT9zM7Yxc,22763
+snowflake/ml/_internal/telemetry.py,sha256=E8AEeXgmSKzYx709WYMaTBMWF8VAr259cgmsIFs3IAw,22820
 snowflake/ml/_internal/type_utils.py,sha256=0AjimiQoAPHGnpLV_zCR6vlMR5lJ8CkZkKFwiUHYDCo,2168
 snowflake/ml/_internal/container_services/image_registry/credential.py,sha256=nShNgIb2yNu9w6vceOY3aSgjpuOoi0spWWmvgEafPSk,3291
 snowflake/ml/_internal/container_services/image_registry/http_client.py,sha256=_zqPPp76Vk0jQ8eVK0OJ4mJgcWsdY4suUd1P7Orqmm8,5214
 snowflake/ml/_internal/container_services/image_registry/imagelib.py,sha256=Vh684uUZfwGGnxO-BZ4tRGa50l2uGM-4WfTg6QftlMY,14537
 snowflake/ml/_internal/container_services/image_registry/registry_client.py,sha256=Zic4bF67DMqEZbQMHffyeNoa83-FhswpZx02iBMjyrc,9115
 snowflake/ml/_internal/exceptions/dataset_error_messages.py,sha256=h7uGJbxBM6se-TW_64LKGGGdBCbwflzbBnmijWKX3Gc,285
 snowflake/ml/_internal/exceptions/dataset_errors.py,sha256=wZTPKZRJSYsfeTs0vDL8r4bFFSP_9ob8XinMgPi63RM,762
@@ -26,63 +26,65 @@
 snowflake/ml/_internal/exceptions/fileset_errors.py,sha256=ZJfkpeDgRIw3qA876fk9FIzxIrm-yZ8I9RXUbzaeM84,1040
 snowflake/ml/_internal/exceptions/modeling_error_messages.py,sha256=q1Nh7KvnUebdKCwwAPmotdAVS578CgAXcfDOfKoweVw,665
 snowflake/ml/_internal/human_readable_id/adjectives.txt,sha256=5o4MbVeHoELAqyLpyuKleOKR47jPjC_nKoziOIZMwT0,804
 snowflake/ml/_internal/human_readable_id/animals.txt,sha256=GDLzMwzxiL07PhIMxw4t89bhYqqg0bQfPiuQT8VNeME,837
 snowflake/ml/_internal/human_readable_id/hrid_generator.py,sha256=LYWB86qZgsVBvnc6Q5VjfDOmnGSQU3cTRKfId_nJSPY,1341
 snowflake/ml/_internal/human_readable_id/hrid_generator_base.py,sha256=D1yoVG1vmAFUhWQ5xCRRU6HCCBPbXHpOXagFd0jK0O8,4519
 snowflake/ml/_internal/lineage/data_source.py,sha256=D24FdR6Wq_PdUuCsBDvSMCr5CfHqpMamrc8-F5iZVJ0,214
-snowflake/ml/_internal/lineage/dataset_dataframe.py,sha256=qbEXgkHxzx6zZzJCGpIhFV7-OdAuA_qrO9AixxyxHSk,1712
+snowflake/ml/_internal/lineage/lineage_utils.py,sha256=4BNoyg3GFeUY5tDNdjDvN129rc6JymOt6PUdWP_Vhj4,3007
 snowflake/ml/_internal/utils/formatting.py,sha256=PswZ6Xas7sx3Ok1MBLoH2o7nfXOxaJqpUPg_UqXrQb8,3676
-snowflake/ml/_internal/utils/identifier.py,sha256=eokEDF7JIML2gm_3FfknPdPR9aBT3woweA5S4z_46-E,10925
+snowflake/ml/_internal/utils/identifier.py,sha256=7dV6dN_KAoupT-xJS8f19K69GVWa4069RmKVWMuWH9k,10926
 snowflake/ml/_internal/utils/import_utils.py,sha256=eexwIe7auT17s4aVxAns7se0_K15rcq3O17MkIvDpPI,2068
 snowflake/ml/_internal/utils/log_stream_processor.py,sha256=pBf8ycEamhHjEzUT55Rx_tFqSkYRpD5Dt71Mx9ZdaS8,1001
 snowflake/ml/_internal/utils/parallelize.py,sha256=Q6_-P2t4DoYNO8DyC1kOl7H3qNL-bUK6EgtlQ_b5ThY,4534
 snowflake/ml/_internal/utils/pkg_version_utils.py,sha256=tpu6B0HKpbT-svvU2Pbz7zNqzg-jgoSmwYvtTzXYyzw,5857
 snowflake/ml/_internal/utils/query_result_checker.py,sha256=h1nbUImdB9lSNCON3uIA0xCm8_JrS-TE-jQXJJs9WfU,10668
 snowflake/ml/_internal/utils/result.py,sha256=59Sz6MvhjakUNiONwg9oi2544AmORCJR3XyWTxY2vP0,2405
 snowflake/ml/_internal/utils/retryable_http.py,sha256=1GCuQkTGO4sX-VRbjy31e4_VgUjqsp5Lh2v5tSJjVK8,1321
 snowflake/ml/_internal/utils/session_token_manager.py,sha256=qXRlE7pyw-Gb0q_BmTdWZEu9pCq2oRNuJBoqfKD9QDQ,1727
 snowflake/ml/_internal/utils/snowflake_env.py,sha256=Mrov0v95pzVUeAe7r1e1PtlIco9ytj5SGAuUWORQaKs,2927
 snowflake/ml/_internal/utils/snowpark_dataframe_utils.py,sha256=HPyWxj-SwgvWUrYR38BkBtx813eMqz5wmQosgc1sce0,5403
 snowflake/ml/_internal/utils/spcs_attribution_utils.py,sha256=9XPKe1BDkWhnGuHDXBHE4FP-m3U22lTZnrQLsHGFhWU,4292
-snowflake/ml/_internal/utils/sql_identifier.py,sha256=CHTxr3qtc1ygNkA5oOQQa-XEoosw5sjfHe7J4WZlkDQ,3270
+snowflake/ml/_internal/utils/sql_identifier.py,sha256=ZcRjSfpovsqaY7S8bFB6z44z28XICncHGwOIzs8rLDI,3729
 snowflake/ml/_internal/utils/table_manager.py,sha256=jHGfl0YSqhFLL7DOOQkjUMzTmLkqFDIM7Gs0LBQw8BM,4384
 snowflake/ml/_internal/utils/temp_file_utils.py,sha256=7JNib0DvjxW7Eu3bimwAHibGosf0u8W49HEc49BghF8,1402
 snowflake/ml/_internal/utils/uri.py,sha256=pvskcWoeS0M66DaU2XlJzK9wce55z4J5dn5kTy_-Tqs,2828
-snowflake/ml/dataset/__init__.py,sha256=jyoLosJL9mTWTULTqJB3WaDS77-zsPcdoWDTNxyN-rY,236
-snowflake/ml/dataset/dataset.py,sha256=V6QeEN8WatRnpoXRss950ISKiE0hKrJg7PCFkHKmfYo,20863
+snowflake/ml/dataset/__init__.py,sha256=nESj7YEI2u90Oxyit_hKCQMWb7N1BlEM3Ho2Fm0MfHo,274
+snowflake/ml/dataset/dataset.py,sha256=LbiYP2S-dnw8a2ALswSLqZs7AittzSejMC9Hzipkpn0,21013
 snowflake/ml/dataset/dataset_factory.py,sha256=qdS6jX8uiCpW5TIKnZ-_2HRfWN3c_N1bZ6lBC1bLy5g,1712
 snowflake/ml/dataset/dataset_metadata.py,sha256=lvaYd1sNOgWcXD1q_-J7fQZ0ndOC8guR9IgKpChBcFA,3992
-snowflake/ml/dataset/dataset_reader.py,sha256=0Vgr_xV4YIBewPeymGNpaSCA7AVZcCgyq8pSuSwc2Ys,8205
-snowflake/ml/feature_store/__init__.py,sha256=dYtqk_GD_hAAZjGfH1maWlZQ30h4hu_KGaf-_y9_AD8,298
+snowflake/ml/dataset/dataset_reader.py,sha256=TKitOC7YBk3yZ9axL9nI1paSI2ooSqBn4zw5eOYpCGY,8061
+snowflake/ml/feature_store/__init__.py,sha256=VKBVkS050WNF8rcqNqwFfNXa_B9GZjcUpuibOGsUSls,423
+snowflake/ml/feature_store/access_manager.py,sha256=TiMHu5ds4ZsjvGTOPWum7zgb4A-m3LNFDSktBFNezdk,10442
 snowflake/ml/feature_store/entity.py,sha256=dCpzLC3jrt5wDHqFYJXbAYkMiZ0zEmiVDMGkks6MXkA,3378
-snowflake/ml/feature_store/feature_store.py,sha256=SrJkB_9Y6clbw6jCWmKvjGoZ78PU2hzUSnHTVnTfDKo,76062
-snowflake/ml/feature_store/feature_view.py,sha256=Epqmm5QiK6jdgrgE-x54Lq1B6VBocFLAVk5bt13eyYg,18624
-snowflake/ml/fileset/embedded_stage_fs.py,sha256=gzMX6RbU_K9jCy1zEfF0YZ7nZSVHW4b3UTVAuAn-etY,5740
+snowflake/ml/feature_store/feature_store.py,sha256=8SH806Dwy89TbKItTdnPi0pAn09pRObgjYSpmBnEjrI,78669
+snowflake/ml/feature_store/feature_view.py,sha256=B3oYaRuChQaLo8c8sdUF6McswVUryda5GWMK22b3Ipg,19274
+snowflake/ml/fileset/embedded_stage_fs.py,sha256=90nCRvRm2EZpDlx-Hu-NLI5s9fYbEFHdf0ggwjdrkQM,5919
 snowflake/ml/fileset/fileset.py,sha256=QRhxLeKf1QBqvXO4RyyRd1c8TixhYpHuBEII8Qi3C_M,26201
 snowflake/ml/fileset/parquet_parser.py,sha256=sjyRB59cGBzSzvbcYLvu_ApMPtrR-zwZsQkxekMR4FA,6884
 snowflake/ml/fileset/sfcfs.py,sha256=a77UJFz5Ve9s_26QpcOOoFNOBIKN91KmhYVTQkafn0c,15344
-snowflake/ml/fileset/snowfs.py,sha256=qwBbmMh5mGneO3scriERbdWJ8e717NfkUMXAz2Y4Dhg,6914
-snowflake/ml/fileset/stage_fs.py,sha256=yKMbPlc4chLG2svHKYy7tlefq6hy7kvnefHGunE7DPA,18408
+snowflake/ml/fileset/snowfs.py,sha256=RXCtZ43_e_Kq_vc-1tJABNo0stpwmHQI2MSCLeFhGfI,6948
+snowflake/ml/fileset/stage_fs.py,sha256=IebRjgPlJdwdAlpg_99DGbgIBD3XJb2p9N36O0tU3wI,19532
 snowflake/ml/fileset/tf_dataset.py,sha256=K8jafWBsyRaIYEmxaYAYNDj3dLApK82cg0Mlx52jX8I,3849
 snowflake/ml/fileset/torch_datapipe.py,sha256=O2irHckqLzPDnXemEbAEjc3ZCVnLufPdPbt9WKYiBp0,2386
 snowflake/ml/model/__init__.py,sha256=KgZmgLHXmkmEU5Q7pzYQlpfvIll4SRTSiT9s4RjeleI,393
 snowflake/ml/model/_api.py,sha256=u2VUcZ0OK4b8DtlqB_IMaT8EWt_onRVaw3VaWAm4OA4,22329
 snowflake/ml/model/custom_model.py,sha256=xvu7WZ1YmOdvuPePyAj6qMwKq-HNeVV9bNfkOT09CRI,8267
 snowflake/ml/model/deploy_platforms.py,sha256=r6cS3gTNWG9i4P00fHehY6Q8eBiNva6501OTyp_E5m0,144
 snowflake/ml/model/model_signature.py,sha256=UQSGieGJcnmC02V4feCYMdhMXnGoOUa9KBuDrbeivBM,29342
 snowflake/ml/model/type_hints.py,sha256=aUg_1xNtzdH2_kH48v918jbpEnHPNIn6MmfrwdvYvdg,12705
-snowflake/ml/model/_client/model/model_impl.py,sha256=vyjPVoiEUFLW_XGz2tMXHBOk0_yDI5DgnDq3RTmQuW0,13623
-snowflake/ml/model/_client/model/model_version_impl.py,sha256=92uLtim0rZAngg1D0GSFNhPH1QDwZaT6jxOg_rRKcL4,17320
-snowflake/ml/model/_client/ops/metadata_ops.py,sha256=XFNolmueu0nC3nAjb2Lj3v1NffDAhAq0JWMek9JVO38,4094
-snowflake/ml/model/_client/ops/model_ops.py,sha256=J96RRYHOxtO3ScQZYS176Mr4B1BDuWh7UwH-2cSIgng,23490
-snowflake/ml/model/_client/sql/model.py,sha256=dem_jSDQb16bW0U0PvtNbR48XEiceo-sKn86dNFoXBs,5687
-snowflake/ml/model/_client/sql/model_version.py,sha256=HKAiIWvqWtcADUSMXUtAISGj-z8AJFbxGSBackuo3N0,14290
-snowflake/ml/model/_client/sql/stage.py,sha256=4zP8aO6cv0IDrZEqhkheNWwy4qBuv1qyGLwMFSW-7EI,1497
-snowflake/ml/model/_client/sql/tag.py,sha256=RYvblBfQmK4xmLF0pz0BNUd9wddqlfHtEK1JRRpJGPE,4646
+snowflake/ml/model/_client/model/model_impl.py,sha256=hVtAHejB2pTDquWs4XNS7E7XZS1DI7nH7EILbd0btbc,13655
+snowflake/ml/model/_client/model/model_version_impl.py,sha256=Li9JtKwZvNqKjpAQM4qA52-F0fu-HASt0RWPDEJGFPE,17994
+snowflake/ml/model/_client/ops/metadata_ops.py,sha256=7cGx8zYzye2_cvZnyGxoukPtT6Q-Kexd-s4yeZmpmj8,4890
+snowflake/ml/model/_client/ops/model_ops.py,sha256=bn2dB9N_OHP1yMNoFwR4OFxID2_pFQytpWnfZ_195is,28714
+snowflake/ml/model/_client/sql/_base.py,sha256=pN5hxyC0gGzEJgZh2FBHLU0Y6iIoLcebHoE7wTpoUZQ,1252
+snowflake/ml/model/_client/sql/model.py,sha256=dKgrkYKuuAIaOcAC1K7_wxWgrtGF1r89sItcP00hUzY,5736
+snowflake/ml/model/_client/sql/model_version.py,sha256=eQsvfub4Vduy0t3NY4PTHzxwwgKl16nRVhzY7lq-ehk,15685
+snowflake/ml/model/_client/sql/stage.py,sha256=hrCh9P9F4l5R0hLr2r-wLDIEc4XYHMFdX1wNRveMVt0,819
+snowflake/ml/model/_client/sql/tag.py,sha256=pwwrcyPtSnkUfDzL3M8kqM0KSx7CaTtgty3HDhVC9vg,4345
 snowflake/ml/model/_deploy_client/image_builds/base_image_builder.py,sha256=clCaoO0DZam4X79UtQV1ZuMQtTezAJkhLu9ViAX18Xk,302
 snowflake/ml/model/_deploy_client/image_builds/client_image_builder.py,sha256=G74D9lV2B3d544YzFN-YrjPkaST7tbQeh-rM17dtoJc,10681
 snowflake/ml/model/_deploy_client/image_builds/docker_context.py,sha256=7uhAJsHsk7LbiZv_w3xOCE2O88rTUVnS3_B6OAz-JG4,6129
 snowflake/ml/model/_deploy_client/image_builds/gunicorn_run.sh,sha256=1pntXgqFthW4gdomqlyWx9CJF-Wqv8VMoLkgSiTHEJ0,1578
 snowflake/ml/model/_deploy_client/image_builds/server_image_builder.py,sha256=SNXqUBkI_tPAgdnLrQW10smG_7O_DGwAuK3dLFE-wJA,10095
 snowflake/ml/model/_deploy_client/image_builds/inference_server/main.py,sha256=Ltk7KrYsp-nrghMhbMWKqi3snU8inbqmKLHFFyBCeBY,11148
 snowflake/ml/model/_deploy_client/image_builds/templates/dockerfile_template,sha256=8jYNmQfGw7bJgHCEd3iK9Tj68ne_x5U0hWhgKqPxEXw,1783
@@ -93,15 +95,15 @@
 snowflake/ml/model/_deploy_client/snowservice/instance_types.py,sha256=YHI5D7UXNlEbV_Bzk0Nq6nrzfv2VUJfxwchLe7hY-lA,232
 snowflake/ml/model/_deploy_client/snowservice/templates/service_spec_template,sha256=hZX8XYPAlEU2R6JhZLj46js91g7XSfe2pysflCYH4HM,734
 snowflake/ml/model/_deploy_client/snowservice/templates/service_spec_template_with_model,sha256=2SUfeKVOSuZJgY6HZLi0m80ZrOzofjABbnusUl_JT1U,540
 snowflake/ml/model/_deploy_client/utils/constants.py,sha256=Ip_2GgsCYRXj_mD4MUdktQRlYGkqOXoznE49oignd7Y,1696
 snowflake/ml/model/_deploy_client/utils/snowservice_client.py,sha256=k0SulzWdttRvJkyuXM59aluEVgQg8Qd7XZUUpEBKuO4,11671
 snowflake/ml/model/_deploy_client/warehouse/deploy.py,sha256=yZR9M76oh6JbPQJHb6t3wGO3wuD04w0zLEXiEyZW_tg,8358
 snowflake/ml/model/_deploy_client/warehouse/infer_template.py,sha256=1THMd6JX1nW-OozECyxXbn9HJXDgNBUIdhfC9ODPDWY,3011
-snowflake/ml/model/_model_composer/model_composer.py,sha256=QSliEFs-wr9LguSTkQWIA9Nbw_9mOsHpUnanVsgV3Qs,7325
+snowflake/ml/model/_model_composer/model_composer.py,sha256=Ld11EWtryUMM0QhbLZmZEgNtyysEtxP1aG1Vfk-oSNk,7356
 snowflake/ml/model/_model_composer/model_manifest/model_manifest.py,sha256=5tMz0d7t9f0oJAEAOXC4BDDpMNAV4atKoK9C66ZHgvU,5667
 snowflake/ml/model/_model_composer/model_manifest/model_manifest_schema.py,sha256=PsRVrOt15Zr-t2K64_GK5aHjTWN4yLgixRqaYchY2rA,2530
 snowflake/ml/model/_model_composer/model_method/function_generator.py,sha256=2B-fykyanYlGWA4Ie2nOwXx2N5D2qZEvTbbPuSSreeI,1837
 snowflake/ml/model/_model_composer/model_method/infer_function.py_template,sha256=QpQXAIKDs9cotLOL0JdI6xLet1QJU7KtaF7O10nDQcs,2291
 snowflake/ml/model/_model_composer/model_method/infer_table_function.py_template,sha256=gex5if17PZ6t6fPcr2i_LO_3IRY03Ykcv_XAyKJt8pg,2170
 snowflake/ml/model/_model_composer/model_method/model_method.py,sha256=cr5soVDesBm19tjDG6lHLN6xrxj_uwPv1lKt8FgpM-c,6682
 snowflake/ml/model/_packager/model_handler.py,sha256=wMPGOegXx5GgiSA81gbKpfODosdj2mvD1bFbeN4OmNc,2642
@@ -110,15 +112,15 @@
 snowflake/ml/model/_packager/model_handlers/_base.py,sha256=-FfoDfULcfFRizya5ZHOjx48_w04Zy4eLEqOOrQIDHM,6033
 snowflake/ml/model/_packager/model_handlers/_utils.py,sha256=KKwS93yZnrUr2JERuRGWpzxCWwD6LOCCvR3ZfjZTnyQ,2622
 snowflake/ml/model/_packager/model_handlers/catboost.py,sha256=FC0Yw2QDknaR9jdzncTI4QckozT-y87hWSHsqQYHLTs,8142
 snowflake/ml/model/_packager/model_handlers/custom.py,sha256=y5CHdEeKWAO08uor2OtEob4-67zv1CVfRf1CLvBHN40,7325
 snowflake/ml/model/_packager/model_handlers/huggingface_pipeline.py,sha256=Z7vZ5zhZByLVPfNdSkhgzBot1Y8UBOM3ITj3Qfway3A,19985
 snowflake/ml/model/_packager/model_handlers/lightgbm.py,sha256=PWPdpOdden2vfloXZA5sA20b2dCBiGO1-NfJ8atH-Uc,8445
 snowflake/ml/model/_packager/model_handlers/llm.py,sha256=SgCgy9Ys5KivNymjF35ufCpPOtMtSby2Zu4Tllir8Mg,10772
-snowflake/ml/model/_packager/model_handlers/mlflow.py,sha256=Itw1fPiBdU2euOmjLU3P6Vyfj9Go3jSx1c-yHlQRYpU,8993
+snowflake/ml/model/_packager/model_handlers/mlflow.py,sha256=5Kyfg962x_kJQyabIQXf72bO0cAgNgj1vfy48RD9osw,9022
 snowflake/ml/model/_packager/model_handlers/pytorch.py,sha256=dSxKO530_IlF1OK3t9_UYpVntdPiszKy-x_7XGk0bzQ,8033
 snowflake/ml/model/_packager/model_handlers/sentence_transformers.py,sha256=JRPargMNEJaDFQIpzqEVvOml62G_UVVvJdqBH8Lhu_Y,9051
 snowflake/ml/model/_packager/model_handlers/sklearn.py,sha256=u4ino0SxjAMxEl2jgTqt6Mqs1dKGZmSE90mIp3qHErU,8218
 snowflake/ml/model/_packager/model_handlers/snowmlmodel.py,sha256=le4Y_dbiPlcjhiFpK1shla3pVgQ5UASdx2g7a70tYYY,7967
 snowflake/ml/model/_packager/model_handlers/tensorflow.py,sha256=ujBcbJ1-Ymv7ZeLfuxuDBe7QZ7KNU7x1p2k6OM_yi-0,8179
 snowflake/ml/model/_packager/model_handlers/torchscript.py,sha256=8s8sMWQ9ydJpK1Nk2uPQ-FVeB-xclfX5qzRDr9G1bdk,8104
 snowflake/ml/model/_packager/model_handlers/xgboost.py,sha256=x5bXz5DRzb3O7DMDOF535LBPGnydCa78JHP_7-vsnjY,8874
@@ -151,17 +153,17 @@
 snowflake/ml/modeling/_internal/model_trainer_builder.py,sha256=11cpEaxU1D7R7m79nVLcCA9dryUPsElS7YdlKZh850U,8422
 snowflake/ml/modeling/_internal/model_transformer_builder.py,sha256=Y6Y8XSr7X7xAy1FvjPuHTb9Opy7tnGoCuOUBc5WEBJ4,3364
 snowflake/ml/modeling/_internal/transformer_protocols.py,sha256=adbJH9BcD52Z1VbqoCE_9IexjIxERTXE8932Hz-gw3E,6482
 snowflake/ml/modeling/_internal/local_implementations/pandas_handlers.py,sha256=xrayZRLP8_qrnfLJE4uPZ1uz0z3xy4Y5HrJqM3c7MA4,7831
 snowflake/ml/modeling/_internal/local_implementations/pandas_trainer.py,sha256=MyTRkBV3zbDeO7HJaWrKYT3KkVqW51Q0AX2BbUtN4og,5737
 snowflake/ml/modeling/_internal/ml_runtime_implementations/ml_runtime_handlers.py,sha256=fgm1DpBBO0qUo2fXFwuN2uFAyTFhcIhT5_bC326VTVw,5544
 snowflake/ml/modeling/_internal/ml_runtime_implementations/ml_runtime_trainer.py,sha256=lM1vYwpJ1jgTh8vnuyMp4tnFibM6UFf50W1IpPWwUWE,2535
-snowflake/ml/modeling/_internal/snowpark_implementations/distributed_hpo_trainer.py,sha256=baETLCVGDcaaGXwiOx6QyhaMX_zQ1Kt7xGjotd_MSKo,54368
+snowflake/ml/modeling/_internal/snowpark_implementations/distributed_hpo_trainer.py,sha256=zBCEXRbO3w8BqL2ASpq09z9R_DXRTogOoml2Cdbggt0,55706
 snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_handlers.py,sha256=AgNupOLqXJytkbdQ1p5Nj1L5QShwi8PSUSYj506SxhM,14539
-snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_trainer.py,sha256=MYLvpNscLKI0AOuJIdKprtw3VinO0MtSVPj376k-ILo,31819
+snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_trainer.py,sha256=WiRekLxqcnnEJV3dHyjyU797tnKsgxj_g-ZAjmIVWVk,35283
 snowflake/ml/modeling/_internal/snowpark_implementations/xgboost_external_memory_trainer.py,sha256=VBYWGTy6ajQ-u2aiEvVU6NnKobEqJyz65oaHJS-ZjBs,17208
 snowflake/ml/modeling/calibration/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
 snowflake/ml/modeling/calibration/calibrated_classifier_cv.py,sha256=AGQub8A5L_xTB1gEJsbzTSZdsISnhdsAp3OmbEwRutw,51278
 snowflake/ml/modeling/cluster/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
 snowflake/ml/modeling/cluster/affinity_propagation.py,sha256=YeMtuQvE5f2InBc7CyAnlFf0hGy0Okz5X09AQ9C64bI,49107
 snowflake/ml/modeling/cluster/agglomerative_clustering.py,sha256=g_-psPSkpQt05ryMQFeS_GndA9hB8Dkg12ao2s7VNoQ,51144
 snowflake/ml/modeling/cluster/birch.py,sha256=0zAT_k-ZgnaGFfcZu5XGll7kAH8BZ8lFCTajWFYmV2o,49034
@@ -224,15 +226,15 @@
 snowflake/ml/modeling/feature_selection/select_fpr.py,sha256=QSF8zO9qzVv7auA4yBz1jkTqEA1mL_C3tSx93xUufMU,47613
 snowflake/ml/modeling/feature_selection/select_fwe.py,sha256=9gpQbytZwQjPBREld8r-p_bG7sZm3h7nQz4bgH7UL7k,47621
 snowflake/ml/modeling/feature_selection/select_k_best.py,sha256=wZyd-94tGP-FzK85b4dFPzncx0BpfbLME9Mjr5vw2ns,47712
 snowflake/ml/modeling/feature_selection/select_percentile.py,sha256=hnTQ5DtNot9OJah20V-tpkXAzLSbRfJjWhv2gxH11gc,47767
 snowflake/ml/modeling/feature_selection/sequential_feature_selector.py,sha256=-0ZEZboABCnbfguy1YmopbjVZ0-eut24E9XaPol97Sk,50387
 snowflake/ml/modeling/feature_selection/variance_threshold.py,sha256=KwTTDc1WLWz6SiXWz3Bks7u-zP53kdqPRLB3RHtvodw,47323
 snowflake/ml/modeling/framework/_utils.py,sha256=7k9iU5zAWa4ZpMZlg8KfSMi4vH3o69w5aAh5RTRNdZ4,10203
-snowflake/ml/modeling/framework/base.py,sha256=K6qW43lGX99a5v9qIhOTJptPrkMSzzZDa5sVgqii0dM,31359
+snowflake/ml/modeling/framework/base.py,sha256=ifv9wAOhPtd8kQT2TleIV0y3Ftw3xlULyvxGolyWn7w,31439
 snowflake/ml/modeling/gaussian_process/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
 snowflake/ml/modeling/gaussian_process/gaussian_process_classifier.py,sha256=1nDZ_nda4WBHqi4tW4avY2b0fNwV5ZF-DI1-BwQjtNg,53029
 snowflake/ml/modeling/gaussian_process/gaussian_process_regressor.py,sha256=zkHptpQKCEGJspDnZtNTBswzn-0wVDrOJRJ34ach8dM,52094
 snowflake/ml/modeling/impute/__init__.py,sha256=dYtqk_GD_hAAZjGfH1maWlZQ30h4hu_KGaf-_y9_AD8,298
 snowflake/ml/modeling/impute/iterative_imputer.py,sha256=Y3yPVO6EFqK-ZBxe_55Gp7To2jIt0N01HTyAyBkpVkI,53847
 snowflake/ml/modeling/impute/knn_imputer.py,sha256=0-YnL5I2aX9BlLDe0BcyUnYdZbbmNX4_Kxj-rcJ61DA,49563
 snowflake/ml/modeling/impute/missing_indicator.py,sha256=gWz2h46qVp7wVqpKa2pyQtO7Mw8bKzEy4M4gWOubiXc,48420
@@ -324,15 +326,15 @@
 snowflake/ml/modeling/neighbors/radius_neighbors_regressor.py,sha256=WS5xaHj91Ne6_jBDX8i79lRd4Xr0t_2YW7-c3PMd6xc,50842
 snowflake/ml/modeling/neural_network/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
 snowflake/ml/modeling/neural_network/bernoulli_rbm.py,sha256=sM6PA51sYbvJABIw8UiN7QrkcWIU9ajx7biKeswbIJU,48585
 snowflake/ml/modeling/neural_network/mlp_classifier.py,sha256=XYFnxnnEmMa3jlUfKbjDU6U5rTjYMRjfL3-BHmr5Bbs,55921
 snowflake/ml/modeling/neural_network/mlp_regressor.py,sha256=JYb6MziLoTHJvUvnInszDG9RlK17Nxw0SglJqydaP-k,55190
 snowflake/ml/modeling/parameters/disable_distributed_hpo.py,sha256=jyjlLPrtnDSQxlTTM0ayMjWKVL_IP3snd--yeXK5htY,221
 snowflake/ml/modeling/pipeline/__init__.py,sha256=dYtqk_GD_hAAZjGfH1maWlZQ30h4hu_KGaf-_y9_AD8,298
-snowflake/ml/modeling/pipeline/pipeline.py,sha256=KyA9a7CTTWYaONuB-BAn7dJ1PfXFKReNBKeM398qvi4,45360
+snowflake/ml/modeling/pipeline/pipeline.py,sha256=c-TbJKNlLJ7JSmFEmnVRkmrisqfvrVDhB62xHbgLEH0,46223
 snowflake/ml/modeling/preprocessing/__init__.py,sha256=dYtqk_GD_hAAZjGfH1maWlZQ30h4hu_KGaf-_y9_AD8,298
 snowflake/ml/modeling/preprocessing/binarizer.py,sha256=noHrlTqpI7RRzYbCSuCjKHxhL8NUCDKNw-kDNTwyY_U,6999
 snowflake/ml/modeling/preprocessing/k_bins_discretizer.py,sha256=g7kY0LHjnCaBzkslCkjdPV06eL2KRYwZuYKRmDef3ew,20970
 snowflake/ml/modeling/preprocessing/label_encoder.py,sha256=C35I9biWxefltNmXzqaJoqVgOP8eOnTNP7NIsnfR2xE,7405
 snowflake/ml/modeling/preprocessing/max_abs_scaler.py,sha256=xpuybHsjrL68u0qNe9DTrQOJsqzb8GOvHT0-_tIBzvM,8768
 snowflake/ml/modeling/preprocessing/min_max_scaler.py,sha256=agZt9B37PsVhmS8AkH8ix0bZFsf-EGapeTp6-OD1pwI,12200
 snowflake/ml/modeling/preprocessing/normalizer.py,sha256=iv3MgJZ4B9-X1fAlC0pWsrYuQvRz1iJrM0_f4XfZKc0,6584
@@ -366,15 +368,15 @@
 snowflake/ml/registry/__init__.py,sha256=XdPQK9ejYkSJVrSQ7HD3jKQO0hKq2mC4bPCB6qrtH3U,76
 snowflake/ml/registry/_initial_schema.py,sha256=KusBbu0vpgCh-dPHgC90xRSfP6Z79qC-eXTqT8GXpFI,5316
 snowflake/ml/registry/_schema.py,sha256=GOA427_mVKkq9RWRENHuqDimRS0SmmP4EWThNCu1Kz4,3166
 snowflake/ml/registry/_schema_upgrade_plans.py,sha256=LxZNXYGjGG-NmB7w7_SxgaJpZuXUO66XVMuh04oL6SI,4209
 snowflake/ml/registry/_schema_version_manager.py,sha256=-9wGH-7ELSZxp7-fW7hXTMqkJSIebXdSpwwgzdvnoYs,6922
 snowflake/ml/registry/model_registry.py,sha256=x42wR2lEyW99NnG8auNPOowg34bF87ksXQqrjMFd7Pw,84795
 snowflake/ml/registry/registry.py,sha256=RxEM0xLWdF3kIPf5upJffaPPP9liNMMZOnVeSyYNIb8,10949
-snowflake/ml/registry/_manager/model_manager.py,sha256=LYX_nS_egwum7F_LCbz_a3hibIHOTDK8LO1DPOWxPrE,5809
+snowflake/ml/registry/_manager/model_manager.py,sha256=OOXPAOL8XZGWY0YiKD8vFfJHxJl1DXjk-oISig-bxmQ,7126
 snowflake/ml/utils/connection_params.py,sha256=JRpQppuWRk6bhdLzVDhMfz3Y6yInobFNLHmIBaXD7po,8005
 snowflake/ml/utils/sparse.py,sha256=XqDQkw39Ml6YIknswdkvFIwUwBk_GBXAbP8IACfPENg,3817
-snowflake_ml_python-1.5.0.dist-info/LICENSE.txt,sha256=PdEp56Av5m3_kl21iFkVTX_EbHJKFGEdmYeIO1pL_Yk,11365
-snowflake_ml_python-1.5.0.dist-info/METADATA,sha256=4_GHqJoiNYXF-WBWL6qwV6wA13ZlB6ySiGQZBLCPwRY,50050
-snowflake_ml_python-1.5.0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-snowflake_ml_python-1.5.0.dist-info/top_level.txt,sha256=TY0gFSHKDdZy3THb0FGomyikWQasEGldIR1O0HGOHVw,10
-snowflake_ml_python-1.5.0.dist-info/RECORD,,
+snowflake_ml_python-1.5.1.dist-info/LICENSE.txt,sha256=PdEp56Av5m3_kl21iFkVTX_EbHJKFGEdmYeIO1pL_Yk,11365
+snowflake_ml_python-1.5.1.dist-info/METADATA,sha256=R4CXn-UPNIGbbSaVg1CXioUMX9yIXVNeLII-aGVEzuE,52106
+snowflake_ml_python-1.5.1.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+snowflake_ml_python-1.5.1.dist-info/top_level.txt,sha256=TY0gFSHKDdZy3THb0FGomyikWQasEGldIR1O0HGOHVw,10
+snowflake_ml_python-1.5.1.dist-info/RECORD,,
```

