# Comparing `tmp/teradatamlspk-20.0.0.0-py3-none-any.whl.zip` & `tmp/teradatamlspk-20.0.0.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,44 +1,44 @@
-Zip file size: 191950 bytes, number of entries: 42
--rw-r--r--  2.0 unx    99421 b- defN 24-Mar-22 17:55 teradatamlspk/LICENSE-3RD-PARTY.pdf
--rw-r--r--  2.0 unx    66696 b- defN 24-Mar-22 17:55 teradatamlspk/LICENSE.pdf
--rw-r--r--  2.0 unx     5098 b- defN 24-Mar-22 17:55 teradatamlspk/README.md
--rw-r--r--  2.0 unx      554 b- defN 24-Mar-22 17:55 teradatamlspk/__init__.py
--rw-r--r--  2.0 unx       20 b- defN 24-Mar-22 17:55 teradatamlspk/_version.py
--rw-r--r--  2.0 unx     1150 b- defN 24-Mar-22 17:55 teradatamlspk/conf.py
--rw-r--r--  2.0 unx     4527 b- defN 24-Mar-22 17:55 teradatamlspk/context.py
--rw-r--r--  2.0 unx      511 b- defN 24-Mar-22 17:55 teradatamlspk/exceptions.py
--rw-r--r--  2.0 unx     1385 b- defN 24-Mar-22 17:55 teradatamlspk/storagelevel.py
--rw-r--r--  2.0 unx        0 b- defN 24-Mar-22 17:55 teradatamlspk/common/__init__.py
--rw-r--r--  2.0 unx     7672 b- defN 24-Mar-22 17:55 teradatamlspk/common/constants.py
--rw-r--r--  2.0 unx    24347 b- defN 24-Mar-22 17:55 teradatamlspk/converter/__init__.py
--rw-r--r--  2.0 unx    32122 b- defN 24-Mar-22 17:55 teradatamlspk/converter/user_guide.json
--rw-r--r--  2.0 unx        0 b- defN 24-Mar-22 17:55 teradatamlspk/ml/__init__.py
--rw-r--r--  2.0 unx    33646 b- defN 24-Mar-22 17:55 teradatamlspk/ml/constants.py
--rw-r--r--  2.0 unx    48208 b- defN 24-Mar-22 17:55 teradatamlspk/ml/util.py
--rw-r--r--  2.0 unx     3397 b- defN 24-Mar-22 17:55 teradatamlspk/ml/classification/__init__.py
--rw-r--r--  2.0 unx     1397 b- defN 24-Mar-22 17:55 teradatamlspk/ml/clustering/__init__.py
--rw-r--r--  2.0 unx     4272 b- defN 24-Mar-22 17:55 teradatamlspk/ml/evaluation/__init__.py
--rw-r--r--  2.0 unx     4624 b- defN 24-Mar-22 17:55 teradatamlspk/ml/feature/__init__.py
--rw-r--r--  2.0 unx    15432 b- defN 24-Mar-22 17:55 teradatamlspk/ml/param/__init__.py
--rw-r--r--  2.0 unx     1961 b- defN 24-Mar-22 17:55 teradatamlspk/ml/regression/__init__.py
--rw-r--r--  2.0 unx      128 b- defN 24-Mar-22 17:55 teradatamlspk/ml/stat/__init__.py
--rw-r--r--  2.0 unx      695 b- defN 24-Mar-22 17:55 teradatamlspk/sql/__init__.py
--rw-r--r--  2.0 unx     2082 b- defN 24-Mar-22 17:55 teradatamlspk/sql/catalog.py
--rw-r--r--  2.0 unx     8021 b- defN 24-Mar-22 17:55 teradatamlspk/sql/column.py
--rw-r--r--  2.0 unx     2525 b- defN 24-Mar-22 17:55 teradatamlspk/sql/constants.py
--rw-r--r--  2.0 unx     2757 b- defN 24-Mar-22 17:55 teradatamlspk/sql/context.py
--rw-r--r--  2.0 unx    41070 b- defN 24-Mar-22 17:55 teradatamlspk/sql/dataframe.py
--rw-r--r--  2.0 unx     2088 b- defN 24-Mar-22 17:55 teradatamlspk/sql/dataframe_utils.py
--rw-r--r--  2.0 unx    29264 b- defN 24-Mar-22 17:55 teradatamlspk/sql/functions.py
--rw-r--r--  2.0 unx     5824 b- defN 24-Mar-22 17:55 teradatamlspk/sql/group.py
--rw-r--r--  2.0 unx     4207 b- defN 24-Mar-22 17:55 teradatamlspk/sql/readwriter.py
--rw-r--r--  2.0 unx     4432 b- defN 24-Mar-22 17:55 teradatamlspk/sql/session.py
--rw-r--r--  2.0 unx    41272 b- defN 24-Mar-22 17:55 teradatamlspk/sql/types.py
--rw-r--r--  2.0 unx     1679 b- defN 24-Mar-22 17:55 teradatamlspk/sql/utils.py
--rw-r--r--  2.0 unx     2772 b- defN 24-Mar-22 17:55 teradatamlspk/sql/window.py
--rw-r--r--  2.0 unx     5860 b- defN 24-Mar-22 17:55 teradatamlspk-20.0.0.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Mar-22 17:55 teradatamlspk-20.0.0.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       14 b- defN 24-Mar-22 17:55 teradatamlspk-20.0.0.0.dist-info/top_level.txt
--rw-r--r--  2.0 unx        1 b- defN 24-Mar-22 17:55 teradatamlspk-20.0.0.0.dist-info/zip-safe
-?rw-rw-r--  2.0 unx     3655 b- defN 24-Mar-22 17:55 teradatamlspk-20.0.0.0.dist-info/RECORD
-42 files, 514878 bytes uncompressed, 186088 bytes compressed:  63.9%
+Zip file size: 198348 bytes, number of entries: 42
+-rw-r--r--  2.0 unx    99421 b- defN 24-May-17 04:24 teradatamlspk/LICENSE-3RD-PARTY.pdf
+-rw-r--r--  2.0 unx    66850 b- defN 24-May-17 04:24 teradatamlspk/LICENSE.pdf
+-rw-r--r--  2.0 unx     7261 b- defN 24-May-17 04:24 teradatamlspk/README.md
+-rw-r--r--  2.0 unx      554 b- defN 24-May-17 04:24 teradatamlspk/__init__.py
+-rw-r--r--  2.0 unx       20 b- defN 24-May-17 04:24 teradatamlspk/_version.py
+-rw-r--r--  2.0 unx     1150 b- defN 24-May-17 04:24 teradatamlspk/conf.py
+-rw-r--r--  2.0 unx     4527 b- defN 24-May-17 04:24 teradatamlspk/context.py
+-rw-r--r--  2.0 unx      511 b- defN 24-May-17 04:24 teradatamlspk/exceptions.py
+-rw-r--r--  2.0 unx     1385 b- defN 24-May-17 04:24 teradatamlspk/storagelevel.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-17 04:24 teradatamlspk/common/__init__.py
+-rw-r--r--  2.0 unx     6080 b- defN 24-May-17 04:24 teradatamlspk/common/constants.py
+-rw-r--r--  2.0 unx    35057 b- defN 24-May-17 04:24 teradatamlspk/converter/__init__.py
+-rw-r--r--  2.0 unx    34165 b- defN 24-May-17 04:24 teradatamlspk/converter/user_guide.json
+-rw-r--r--  2.0 unx        0 b- defN 24-May-17 04:24 teradatamlspk/ml/__init__.py
+-rw-r--r--  2.0 unx    33648 b- defN 24-May-17 04:24 teradatamlspk/ml/constants.py
+-rw-r--r--  2.0 unx    47820 b- defN 24-May-17 04:24 teradatamlspk/ml/util.py
+-rw-r--r--  2.0 unx     3397 b- defN 24-May-17 04:24 teradatamlspk/ml/classification/__init__.py
+-rw-r--r--  2.0 unx     1397 b- defN 24-May-17 04:24 teradatamlspk/ml/clustering/__init__.py
+-rw-r--r--  2.0 unx     4472 b- defN 24-May-17 04:24 teradatamlspk/ml/evaluation/__init__.py
+-rw-r--r--  2.0 unx     4624 b- defN 24-May-17 04:24 teradatamlspk/ml/feature/__init__.py
+-rw-r--r--  2.0 unx    15432 b- defN 24-May-17 04:24 teradatamlspk/ml/param/__init__.py
+-rw-r--r--  2.0 unx     1961 b- defN 24-May-17 04:24 teradatamlspk/ml/regression/__init__.py
+-rw-r--r--  2.0 unx      128 b- defN 24-May-17 04:24 teradatamlspk/ml/stat/__init__.py
+-rw-r--r--  2.0 unx      695 b- defN 24-May-17 04:24 teradatamlspk/sql/__init__.py
+-rw-r--r--  2.0 unx     2082 b- defN 24-May-17 04:24 teradatamlspk/sql/catalog.py
+-rw-r--r--  2.0 unx     9847 b- defN 24-May-17 04:24 teradatamlspk/sql/column.py
+-rw-r--r--  2.0 unx     4475 b- defN 24-May-17 04:24 teradatamlspk/sql/constants.py
+-rw-r--r--  2.0 unx     2757 b- defN 24-May-17 04:24 teradatamlspk/sql/context.py
+-rw-r--r--  2.0 unx    43124 b- defN 24-May-17 04:24 teradatamlspk/sql/dataframe.py
+-rw-r--r--  2.0 unx     3441 b- defN 24-May-17 04:24 teradatamlspk/sql/dataframe_utils.py
+-rw-r--r--  2.0 unx    26448 b- defN 24-May-17 04:24 teradatamlspk/sql/functions.py
+-rw-r--r--  2.0 unx     5538 b- defN 24-May-17 04:24 teradatamlspk/sql/group.py
+-rw-r--r--  2.0 unx     9861 b- defN 24-May-17 04:24 teradatamlspk/sql/readwriter.py
+-rw-r--r--  2.0 unx     4571 b- defN 24-May-17 04:24 teradatamlspk/sql/session.py
+-rw-r--r--  2.0 unx    41272 b- defN 24-May-17 04:24 teradatamlspk/sql/types.py
+-rw-r--r--  2.0 unx     1683 b- defN 24-May-17 04:24 teradatamlspk/sql/utils.py
+-rw-r--r--  2.0 unx     2772 b- defN 24-May-17 04:24 teradatamlspk/sql/window.py
+-rw-r--r--  2.0 unx     8023 b- defN 24-May-17 04:24 teradatamlspk-20.0.0.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-May-17 04:24 teradatamlspk-20.0.0.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       14 b- defN 24-May-17 04:24 teradatamlspk-20.0.0.1.dist-info/top_level.txt
+-rw-r--r--  2.0 unx        1 b- defN 24-May-17 04:24 teradatamlspk-20.0.0.1.dist-info/zip-safe
+?rw-rw-r--  2.0 unx     3655 b- defN 24-May-17 04:24 teradatamlspk-20.0.0.1.dist-info/RECORD
+42 files, 540211 bytes uncompressed, 192486 bytes compressed:  64.4%
```

## zipnote {}

```diff
@@ -105,23 +105,23 @@
 
 Filename: teradatamlspk/sql/utils.py
 Comment: 
 
 Filename: teradatamlspk/sql/window.py
 Comment: 
 
-Filename: teradatamlspk-20.0.0.0.dist-info/METADATA
+Filename: teradatamlspk-20.0.0.1.dist-info/METADATA
 Comment: 
 
-Filename: teradatamlspk-20.0.0.0.dist-info/WHEEL
+Filename: teradatamlspk-20.0.0.1.dist-info/WHEEL
 Comment: 
 
-Filename: teradatamlspk-20.0.0.0.dist-info/top_level.txt
+Filename: teradatamlspk-20.0.0.1.dist-info/top_level.txt
 Comment: 
 
-Filename: teradatamlspk-20.0.0.0.dist-info/zip-safe
+Filename: teradatamlspk-20.0.0.1.dist-info/zip-safe
 Comment: 
 
-Filename: teradatamlspk-20.0.0.0.dist-info/RECORD
+Filename: teradatamlspk-20.0.0.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## teradatamlspk/LICENSE.pdf

### teradatamlspk/LICENSE.pdf

 * *Document info*

```diff
@@ -1,4 +1,5 @@
-CreationDate: "D:20240312180904+00'00'"
+CreationDate: "D:20240412095922+00'00'"
 Creator: 'Chromium'
-ModDate: "D:20240312180904+00'00'"
-Producer: 'Skia/PDF m122'
+ModDate: "D:20240412095922+00'00'"
+Producer: 'Skia/PDF m123'
+Title: 'teradatamlspk License File - Analytics Languages Libraries - Confluence'
```

### pdftotext {} -

```diff
@@ -1,9 +1,9 @@
 teradatamlspk License File
-teradatamlspk Version 20.00.00.00
+teradatamlspk Version 20.00.00.01
 Copyright (c) 2024 Teradata. All rights reserved.
 LICENSE AGREEMENT
 PRODUCT: teradatamlspk
 IMPORTANT - READ THIS AGREEMENT CAREFULLY BEFORE INSTALLING OR USING THE SOFTWARE. TERADATA WILL LICENSE
 THE SOFTWARE TO YOU ONLY IF YOU ACCEPT THE TERMS AND CONDITIONS OF THIS AGREEMENT AND MEET THE
 CONDITIONS FOR USING THE SOFTWARE DESCRIBED BELOW. BY DOWNLOADING, INSTALLING OR USING THE SOFTWARE,
 YOU (1) AGREE TO THE TERMS AND CONDITIONS OF THIS AGREEMENT, AND (2) REPRESENT AND WARRANT THAT YOU
```

## teradatamlspk/README.md

```diff
@@ -12,21 +12,46 @@
 * [Release Notes](#release-notes)
 * [Installation and Requirements](#installation-and-requirements)
 * [Using the Teradata Python Package](#using-the-teradata-Python-package)
 * [Documentation](#documentation)
 * [License](#license)
 
 ## Release Notes:
+#### teradatamlspk 20.00.00.01
+* ##### teradatamlspk DataFrame
+  * `write()` - Supports writing the DataFrame to local file system or to Vantage or to cloud storage.
+  * `writeTo()` - Supports writing the DataFrame to a Vantage table.
+  * `rdd` - Returns the same DataFrame.
+* ##### teradatamlspk DataFrameColumn a.k.a. ColumnExpression
+  * `desc_nulls_first` - Returns a sort expression based on the descending order of the given column name, and null values appear before non-null values.
+  * `desc_nulls_last` - Returns a sort expression based on the descending order of the given column name, and null values appear after non-null values.
+  * `asc_nulls_first` - Returns a sort expression based on the ascending order of the given column name, and null values appear before non-null values.
+  * `asc_nulls_last` - Returns a sort expression based on the ascending order of the given column name, and null values appear after non-null values.
+* ##### Updates
+  * `DataFrame.fillna()` and `DataFrame.na.fill()` now supports input arguments of the same data type or their types must be compatible. 
+  * `DataFrame.agg()` and `GroupedData.agg()` function supports Column as input and '*' for 'count'.
+  * `DataFrameColumn.cast()` and `DataFrameColumn.alias()` now accepts string literal which are case insensitive.
+  * Optimised performance for `DataFrame.show()`  
+  * Classification Summary, TrainingSummary object and MulticlassClassificationEvaluator now supports `weightedTruePositiveRate` and `weightedFalsePositiveRate` metric.
+  * Arithmetic operations can be performed on window aggregates.
+* ##### Bug Fixes
+  * `DataFrame.head()` returns a list when n is 1.
+  * `DataFrame.union()` and `DataFrame.unionAll()` now performs union of rows based on columns position.
+  * `DataFrame.groupBy()` and `DataFrame.groupby()` now accepts columns as positional arguments as well, for example `df.groupBy("col1", "col2")`.
+  * MLlib Functions attribute `numClasses` and `intercept` now return value.
+  * Appropriate error is raised if invalid file is passed to `pyspark2teradataml`.
+  * `when` function accepts Column also along with literal for `value` argument. 
+
 #### teradatamlspk 20.0.0.0
 * `teradatamlspk 20.0.0.0` is the initial release version. Please refer to the teradatamlspk User Guide for the available API's and their functionality.
 
 ## Installation and Requirements
 
 ### Package Requirements:
-* Python 3.5 or later
+* Python 3.8 or later
 
 Note: 32-bit Python is not supported.
 
 ### Minimum System Requirements:
 * Windows 7 (64Bit) or later
 * macOS 10.9 (64Bit) or later
 * Red Hat 7 or later versions
```

## teradatamlspk/_version.py

```diff
@@ -1 +1 @@
-version = "20.0.0.0"
+version = "20.0.0.1"
```

## teradatamlspk/common/constants.py

```diff
@@ -24,28 +24,14 @@
     dataframe_func = lambda x: dataframe.DataFrame(data=x)
     dataframe_grouped_func = lambda x, y: group.GroupedData(data=x, non_grouped_data=y)
     reset_pandas_df_index = lambda pdf: pdf.reset_index() if pdf.index.name is not None else pdf
     to_row_ = lambda recs: (Row(**rec._asdict()) for rec in recs)
     shape = lambda x: x[0]
 
     @staticmethod
-    def head_func(df):
-        """
-        :param df: teradataml DataFrame.
-        :return: Row, list of Rows or None.
-        """
-        recs = [Row(**(rec._asdict())) for rec in df.itertuples()]
-        if len(recs) == 1:
-            return recs[0]
-        elif len(recs) == 0:
-            return None
-        else:
-            return recs
-
-    @staticmethod
     def tail_func(df):
         return [rec for rec in df.itertuples()]
 
     @staticmethod
     def limit_func(df):
         return _DataFrameReturnDF.dataframe_func(df.drop(["sampleid"], axis=1))
 
@@ -86,24 +72,14 @@
                        "func_params": {"subset": "column_names"},
                        "return_func": _DataFrameReturnDF.dataframe_func
                        },
     "drop_duplicates": {"tdml_func": "drop_duplicate",
                         "func_params": {"subset": "column_names"},
                         "return_func": _DataFrameReturnDF.dataframe_func
                         },
-    "groupBy": {"tdml_func": "groupby",
-                "func_params": {"cols": "columns_expr"},
-                "return_func": _DataFrameReturnDF.dataframe_grouped_func
-                # TODO: cols accepts Column object also along with str. tdml do not accept Column object.
-                #   will be addressed with ELE-5975.
-                },
-    "groupby": {"tdml_func": "groupby",
-                "func_params": {"cols": "columns_expr"},
-                "return_func": _DataFrameReturnDF.dataframe_grouped_func
-                },
     "orderBy": {"tdml_func": "sort",
                 "func_params": {"cols": "columns", "ascending": "ascending"},
                 "return_func": _DataFrameReturnDF.dataframe_func
                 # TODO: cols accepts Column object also along with str. tdml do not accept Column object.
                 #   Will be addressed with ELE-5976.
                 },
     "where": {"tdml_func": "__getitem__",
@@ -114,51 +90,36 @@
              "func_params": {"num": "n"},
              "return_func": _DataFrameReturnDF.tail_func
              },
     "toPandas": {"tdml_func": "to_pandas",
                  "func_params": {"all_rows": True},
                  "return_func": _DataFrameReturnDF.reset_pandas_df_index
                  },
-    "unionAll": {"tdml_func": "concat",
-                 "func_params": {"other": "other"},
-                 "return_func": _DataFrameReturnDF.dataframe_func
-                 },
     "_tdml_concat": {"tdml_func": "concat",
                      "func_params": {"other": "other", "allow_duplicates": False},
                      "return_func": _DataFrameReturnDF.dataframe_func
                      },
-    "union": {"tdml_func": "concat",
-              "func_params": {"other": "other", "allow_duplicates": False},
-              "return_func": _DataFrameReturnDF.dataframe_func
-             },
     "cov": {"tdml_func": "assign",
             # No mapping to tdml assign exists. Uses these columns to build _SQLColumnExpression.
             "func_params": {"col1": None, "col2": None},
             "default_tdml_values": {"drop_columns": True},
             "column_expressions": [{"left": "col1", "right": "col2", "operation": "covar_samp"}],
             "return_func": _DataFrameReturnDF.value_func
             },
     "corr": {"tdml_func": "assign",
              # No mapping to tdml assign exists. Uses these columns to build _SQLColumnExpression.
              "func_params": {"col1": None, "col2": None, "method": None},
              "default_tdml_values": {"drop_columns": True},
              "column_expressions": [{"left": "col1", "right": "col2", "operation": "corr"}],
              "return_func": _DataFrameReturnDF.value_func
              },
-    "head": {"tdml_func": "head",
-             "func_params": {"n": "n"},
-             "return_func": _DataFrameReturnDF.head_func
-             },
     "toLocalIterator": {"tdml_func": "itertuples",
              "func_params": {},
              "return_func": _DataFrameReturnDF.to_row_
              },
     "_summary": {"tdml_func": "describe",
                  "func_params": {"percentile": "percentiles", "statistics": "statistics", "columns": "columns"}
              },
     "_tdml_filter": {"tdml_func": "filter",
                      "func_params": {"regex":"regex"},
-                     "return_func": _DataFrameReturnDF.dataframe_func},
-    "fillna": {"tdml_func": "fillna",
-               "func_params": {"value": "value", "subset": "columns"},
-               "return_func": _DataFrameReturnDF.dataframe_func}
+                     "return_func": _DataFrameReturnDF.dataframe_func}
 }
```

## teradatamlspk/converter/__init__.py

```diff
@@ -5,15 +5,34 @@
 
 
 class UserNoteType(Enum):
     NOT_SUPPORTED = 1
     PARTIALLY_SUPPORTED = 2
     NO_ACTION = 3
 
-
+functions_with_default_values ={
+    'LinearSVC': ['featuresCol', 'setFeaturesCol'],
+    'KMeans': ['featuresCol', 'setFeaturesCol'],
+    'GaussianMixture': ['featuresCol', 'setFeaturesCol'],
+    'LinearRegression': ['featuresCol', 'setFeaturesCol'],
+    'LogisticRegression': ['featuresCol', 'setFeaturesCol'],
+    'MultilayerPerceptronClassifier': ['featuresCol', 'setFeaturesCol'],
+    'DecisionTreeClassifier': ['featuresCol', 'setFeaturesCol'],
+    'DecisionTreeRegressor': ['featuresCol', 'setFeaturesCol'],
+    'VarianceThresholdSelector': ['featuresCol', 'setFeaturesCol'],
+    'GBTClassifier': ['featuresCol', 'setFeaturesCol'],
+    'RandomForestClassifier': ['featuresCol', 'setFeaturesCol'],
+    'IsotonicRegression': ['featuresCol', 'setFeaturesCol'],
+    'RandomForestRegressor': ['featuresCol', 'setFeaturesCol'],
+    'NaiveBayes': ['featuresCol', 'setFeaturesCol'],
+    'OneVsRest': ['featuresCol', 'setFeaturesCol'],
+    'GBTRegressor': ['featuresCol', 'setFeaturesCol'],
+    'ClusteringEvaluator': ['featuresCol', 'setFeaturesCol'],
+    'UnivariateFeatureSelector': ['featuresCol', 'setFeaturesCol']
+}
 
 class UserNotes:
     def __init__(self, start_line_no, end_line_no,  python_object_name, user_notes, note_type):
         self.start_line_no = start_line_no
         self.end_line_no = end_line_no
         self.object_name = python_object_name
         self.user_notes = user_notes
@@ -234,14 +253,17 @@
                   <span class="imp_notes">Important Notes: </span>
                   <ul>
                   
                     <li>Refer user guide and supportability matrix for ML functions. </li>
 					<li>Some functions are not supported however they are supported with manual code changes. Look at the section 'Examples' in the user guide to know more details.  </li>
 					<li>ML Functions accepts multiple columns for arguments. Hence, no need to pass vectors, update the script to pass multiple columns. </li>
 					<li>RDD API's are not applicable to Vantage. Make use of DataFrame API's. </li>
+					<li>Columns are case sensitive in teradatamlspk and they are case insensitive in PySpark. Convert the column names to appropriate case while converting the code. </li>
+                    <li>DataFrame.rdd returns same DataFrame as RDD is not applicable to Vantage. Hence use DataFrame API's and do not use RDD API's. </li>
+                    <li>pyspark2teradataml does not modify the SQL statements. Users are advised to manually update the SQL statements if the corresponding SQL statement is not valid in Vantage. </li>
                   </ul>
                   <h3>Text in below table in any of below three colors. Every color has significance as explained below: </h3>
                   <ul>
                   
                     <li><span style="color: red; text-decoration: underline;">red</span>: These API's do not have functionality in teradatamlspk. User need to achieve the functionality through some other way. </li>
                     <li><span style="color: blue; text-decoration: underline;">blue </span>: These API's have functionality but there may be some difference in functionality when compared with PySpark. Notes specifies what is the exact difference so user should change it manually. </li>
                     <li><span style="color: black; text-decoration: underline;">black </span>: This is for a notification to user. User do not need to act on this message. </li>
@@ -277,18 +299,18 @@
 
 class _pyspark2teradatamlspk:
     """ Analyses the user script. """
 
     def __init__(self, file_path):
 
         if (not os.path.exists(file_path)):
-            raise FileNotFoundError("File '{}' not found.".format(pyspark_script_path))
+            raise FileNotFoundError("File '{}' not found.".format(file_path))
 
         if not os.path.isfile(file_path):
-            raise FileNotFoundError("Specified path '{}' is not a file.".format(pyspark_script_path))
+            raise FileNotFoundError("Specified path '{}' is not a file.".format(file_path))
 
         self.__file_path = file_path
         user_notes = self.__get_user_notes()
         self.__unimplemented_imports = user_notes.get("not_supported")
         self.__not_supported = user_notes.get("not_supported")
         self.__partially_supported = user_notes.get("partially_supported")
         self.__notification = user_notes.get("notification")
@@ -510,14 +532,178 @@
         elif isinstance(node, ast.Name):
             return node.id, node.lineno, node.end_lineno
         # Extract read.csv() or read.json() or write.csv() kind of notations.
         elif isinstance(node, ast.Call) and isinstance(node.func, ast.Attribute) and isinstance(node.func.value, ast.Attribute):
             return "{}.{}".format(node.func.value.attr, node.func.attr), node.lineno, node.end_lineno
         return None, None, None
 
+    def __recurse_node(self, node, start_line_number, end_line_number, store_line_number):
+        """
+        DESCRIPTION:
+            Function recursively traverse the node, find the line number of this arguments
+            ['inputCol', 'inputCols', 'featuresCol', 'setInputCol', 'setInputCols', 'setFeaturesCol']
+            which should be present in method.
+            Example:
+                * it will include this line 'assembler1(inputCols="column")'
+                   and ignore this line 'inputCols = "name"'
+
+        PARAMETERS:
+            node:
+                Required Argument.
+                Specifies node present in ast.
+                Types: ast object
+
+            start_line_number:
+                Required Argument.
+                Stores start line of method.
+                Types: int
+
+            end_line_number:
+                Required Argument.
+                Stores end line of method.
+                Types: int
+
+            store_line_number:
+                Required Argument.
+                Stores line number of methods which contain
+                ['inputCol', 'inputCols', 'featuresCol', 'setInputCol',
+                 'setInputCols', 'setFeaturesCol']
+                Types: list
+
+        RETURNS:
+            None
+
+        Example:
+            >>> input_file = 'assembler1(inputCols="column")'
+            >>> node = ast.parse(input_file)
+            >>> __recurse_node(node, 0, 0, store_line_number)
+            >>> print(store_line_number)
+            >>>[(1, 1, 'inputCols')]
+
+        """
+        if hasattr(node, "__dict__"):
+            for key, value in node.__dict__.items():
+                if node.__dict__.get('arg', None) in ['inputCol', 'inputCols', 'featuresCol']:
+                    store_line_number.append((start_line_number, end_line_number, node.__dict__.get('arg')))
+                elif isinstance(value, list):
+                    for x in value:
+                        self.__recurse_node(x, node.__dict__.get('lineno', -1), node.__dict__.get('end_lineno', -1), store_line_number)
+                else:
+                    self.__recurse_node(value, node.__dict__.get('lineno', -1), node.__dict__.get('end_lineno', -1), store_line_number)
+        else:
+            if node in ['setInputCol', 'setInputCols', 'setFeaturesCol']:
+                store_line_number.append((start_line_number, end_line_number, node))
+
+    def __change_input_cols(self, store_line_number, user_guide):
+        """
+        DESCRIPTION:
+            Function change the value of arguments ['inputCol', 'inputCols', 'featuresCol',
+            'setInputCol', 'setInputCols', 'setFeaturesCol'] to "<Specify list of column names>"
+            based on store_line_number and also updates html.
+            Example:
+                * assembler1(inputCols="column") --> assembler1(inputCols="<Specify list of column names>")
+
+        PARAMETERS:
+            store_line_number:
+                Required Argument.
+                Stores line number of methods which contain
+                ['inputCol', 'inputCols', 'featuresCol', 'setInputCol',
+                 'setInputCols', 'setFeaturesCol']
+                Types: list
+            user_guide:
+                Required Argument.
+                stores line number and keywords to generate html.
+                Types: Class
+
+        RETURNS:
+            None
+        """
+        # Read the new file
+        with open(self.__get_new_file_name()) as fp:
+            actual_lines = fp.readlines()
+        # Look for featuresCol, InputCols, InputCol, setInputCol, setInputCols, setFeaturesCol
+        for start_line_number, end_line_number, value in store_line_number:
+
+            # Replace values for 'setInputCol', 'setInputCols', 'setFeaturesCol' stored in "store_line_number".
+            if value in ['setInputCol', 'setInputCols', 'setFeaturesCol']:
+                for idx in range(start_line_number, end_line_number+1):
+                    line = actual_lines[idx-1]
+                    # To replace value we first find a ['setInputCol', 'setInputCols', 'setFeaturesCol'] as a index,
+                    # Using that index we find a start index which should start from '(' and end_index should end with
+                    # either ')' or '\n' then we replace the value.
+                    pattern = re.compile(value+r"\s*?\(([^[]*?)[\)|\n]")
+                    indexes = []
+                    for match in re.finditer(pattern, line):
+                        indexes.append(match.span(1))
+                    for start_index, end_index in reversed(indexes):
+                        line = "".join([line[:start_index], "\"<Specify list of column names>\"", line[end_index:]])
+                    actual_lines[idx - 1] = line
+
+                # Replace values for 'inputCol', 'inputCols', 'featuresCol' stored in "store_line_number".
+                # To replace value we first find a ['inputCol', 'inputCols', 'featuresCol'] as a index,
+                # Using that index we find a start index which should start from '=' and end_index should end with
+                # either ')' or '\n' or ',' then we replace the value.
+                # For VectorAssembler we are ignoring this.
+            elif value in ['inputCol', 'inputCols', 'featuresCol']:
+                for idx in range(start_line_number, end_line_number+1):
+                    # ast line number starts with 1 while reading file using 'open' starts with 0
+                    line = actual_lines[idx-1]
+                    pattern = re.compile(value+r"\s*?=([^[]*?)[,|\n|\)]")
+                    indexes = []
+                    exclude_string, st_idx = "VectorAssembler", 0
+                    for match in re.finditer(pattern, line):
+                        if line.find(exclude_string, st_idx, match.span(1)[0]) == -1:
+                            indexes.append(match.span(1))
+                        st_idx = match.span(1)[1]
+                    for start_index, end_index in reversed(indexes):
+                        line = "".join([line[:start_index], "\"<Specify list of column names>\"", line[end_index:]])
+                    actual_lines[idx - 1] = line
+
+        # When neither of this values present
+        # Example:  dt = DecisionTreeRegressor(maxDepth=2)
+        # dt.setVarianceCol("variance")
+        # model = dt.fit(df)
+        # then we dont make change in script but in html.
+        function_names = []
+        for idx, line in enumerate(actual_lines):
+            for key, values in functions_with_default_values.items():
+                start_index = 0
+                end_index = -1
+                if ''.join([key, '(']) in line and values[0] not in line and values[1] not in line:
+                    start_index = idx
+                    i = idx
+                    while i in range(idx, len(actual_lines)):
+                        if values[0] in actual_lines[i] or values[1] in actual_lines[i]:
+                            break
+                        if 'fit' in actual_lines[i]:
+                            end_index = i
+                            break
+                        i+=1
+                if end_index != -1:
+                    function_names.append((key, start_index))
+
+        # Write the new file
+        with open(self.__get_new_file_name(), 'w') as fw:
+            fw.writelines(actual_lines)
+
+        # Html updates when this arguments ['setInputCol', 'setInputCols', 'setFeaturesCol',
+        # 'inputCol', 'inputCols', 'featuresCol'] are present.
+        for start_line_number, end_line_number, keyword in store_line_number:
+            user_guide.add_notes(UserNotes(start_line_number, end_line_number, keyword,
+                                           "Replace following string `Specify list of column names` with list of column names manually",
+                                            UserNoteType.PARTIALLY_SUPPORTED))
+        # Html updates when this arguments ['setInputCol', 'setInputCols', 'setFeaturesCol',
+        # 'inputCol', 'inputCols', 'featuresCol'] are not present.
+        for keyword, start_line in function_names:
+            # read the files using 'open' starts with '0' but in files it should be shown from '1'.
+            user_guide.add_notes(UserNotes(start_line+1, start_line+1, keyword,
+                                           f"Both `featuresCol` and `setfeaturesCol` not used hence default value to `features` will not work. Either use `featuresCol` or `setfeaturesCol` and provide list of columns",
+                                           UserNoteType.PARTIALLY_SUPPORTED))
+
+
     def convert(self):
 
         # First convert the file.
         new_file_path = self.__replace_pyspark_with_tdmlspk()
 
         with open(new_file_path) as fp:
             root_node = ast.parse(fp.read())
@@ -525,17 +711,30 @@
         invalid_imports = self.__get_invalid_imports(root_node)
 
         # Segregate invalid imports according to line no's.
         invalid_imports = self.__segregate_invalid_imports_based_on_line_no(invalid_imports)
 
         new_lines = self.__remove_invalid_imports(invalid_imports)
 
-        print("Python script '{}' converted to '{}' successfully.".format(self.__file_path, new_file_path))
+        with open(new_file_path) as fp:
+            root_node = ast.parse(fp.read())
+        # Store line number of script where this arguments are present
+        # ['setInputCol', 'setInputCols', 'setFeaturesCol', 'inputCol', 'inputCols', 'featuresCol']
+        store_line_number = []
+        # Recurse every line of script
+        for x in root_node.body:
+            self.__recurse_node(x, 0, 0, store_line_number)
+
         # Prepare user guide.
         user_guide = UserGuide(self.__file_path)
+        # Change the lines based on 'store_line_number'
+        self.__change_input_cols(store_line_number, user_guide)
+
+        print("Python script '{}' converted to '{}' successfully.".format(self.__file_path, new_file_path))
+
 
         # Store the invalid import statement along with line numbers.
         # This will be helpfull for preparing the user notes for the
         # lines which use these invalid imports.
         invalid_import_stmts = {}
 
         # First prepare the notes for invalid imports.
```

## teradatamlspk/converter/user_guide.json

### Pretty-printed

 * *Similarity: 0.9222578048651915%*

 * *Differences: {"'not_supported'": "{'range': 'SparkSession.range() and SparkContext.range() is not applicable "*

 * *                    'for Teradata Vantage. Load the data in to Vantage table and then create '*

 * *                    'DataFrame on it.\', \'union\': "Not applicable for Teradata Vantage, if it\'s '*

 * *                    'a TeradataContext API or union is performed on RDD.", \'array\': \'Array '*

 * *                    "columns are not supported in teradatamlspk. ', 'slice': 'teradatamlspk "*

 * *                    'columns […]*

```diff
@@ -11,15 +11,14 @@
         "BinaryType": "Not applicable for Teradata Vantage.",
         "BisectingKMeans": "Not yet available. Use 'BisectingKMeans' from sklearn along with teradataml 'Script' or 'Apply'.",
         "Broadcast": "Not available for Teradata Vantage.",
         "BucketedRandomProjectionLSH": "Not yet available. User's can use open source module along with teradataml Script or Apply to achieve the functionality.",
         "ChiSqSelector": "Not yet available.",
         "CrossValidator": "Not yet available. Use 'GridSearchCV' from sklearn along with teradataml 'Script' or 'Apply'.",
         "DCT": "Not yet available. User's can use 'dct' in scipy module along with teradataml 'Script' or 'Apply'.",
-        "DataFrameWriter": "Not yet available in Teradata Vantage.",
         "DataStreamReader": "Not available for Teradata Vantage.",
         "DataStreamWriter": "Not available for Teradata Vantage.",
         "DeepspeedTorchDistributor": "Not yet available. ",
         "DenseVector": "Not applicable for Teradata. All ML functions accepts multiple columns so pass multiple columns to ML function instead of converting multiple columns to a Vector.",
         "ExecutorResourceRequest": "Not applicable for Teradata Vantage.",
         "ExecutorResourceRequests": "Not applicable for Teradata Vantage.",
         "FMClassifier": "Not yet available. Use 'pylibfm' from pyfm along with teradataml 'Script' or 'Apply'.",
@@ -72,24 +71,25 @@
         "UserDefinedTableFunction": "Not yet available Teradata Vantage.",
         "Vector": "Not applicable for Teradata. All ML functions accepts multiple columns so pass multiple columns to ML function instead of converting multiple columns to a Vector.",
         "VectorUDT": "Not applicable for Teradata. All ML functions accepts multiple columns so pass multiple columns to ML function instead of converting multiple columns to a Vector.",
         "Vectors": "Not applicable for Teradata. All ML functions accepts multiple columns so pass multiple columns to ML function instead of converting multiple columns to a Vectors.",
         "VersionUtils": "Not applicable for Teradata Vantage.",
         "Word2Vec": "Not yet available. Use 'Word2Vec' from gensim.models along with teradataml 'Script' or 'Apply'.",
         "_create_row": "Not applicable for Teradata Vantage. If the data is existed outside of Vantage, load the data in to Vantage using Data Transfer functions available in teradataml.",
+        "_jsparkSession": "API is not applicable to Vantage. Use API's on TeradataSession as most of API's on _jsparkSession are supported on TeradataSession object",
         "accumulator": "Not applicable for Teradata Vantage.",
         "addArtifact": "Not applicable for Teradata Vantage. Look at teradataml Script or Apply to use open source python modules for processing the data.",
         "addArtifacts": "Not applicable for Teradata Vantage. Look at teradataml Script or Apply to use open source python modules for processing the data.",
         "addFile": "Not applicable for Teradata Vantage. ",
         "addPyFile": "Not applicable for Teradata Vantage. ",
         "addTag": "Not applicable for Teradata Vantage. Users should identify the individual SQL queries and ask DBA to cancel/stop them. ",
         "aes_decrypt": "Not available for Teradata Vantage.",
         "aes_encrypt": "Not available for Teradata Vantage.",
         "approxQuantile": "Not yet available. User should write the functionality for this API using teradataml Script or teradataml Apply.",
-        "array": "Not applicable for Teradata Vantage. ",
+        "array": "Array columns are not supported in teradatamlspk. ",
         "array_append": "Not applicable for Teradata Vantage. ",
         "array_compact": "Not applicable for Teradata Vantage. ",
         "array_contains": "Not applicable for Teradata Vantage. ",
         "array_distinct": "Not applicable for Teradata Vantage. ",
         "array_except": "Not applicable for Teradata Vantage. ",
         "array_insert": "Not applicable for Teradata Vantage. ",
         "array_intersect": "Not applicable for Teradata Vantage. ",
@@ -105,15 +105,14 @@
         "array_union": "Not applicable for Teradata Vantage. ",
         "arrays_overlap": "Not applicable for Teradata Vantage. ",
         "arrays_zip": "Not applicable for Teradata Vantage. ",
         "binaryFiles": "Not applicable for Teradata Vantage. ",
         "binaryRecords": "Not applicable for Teradata Vantage. ",
         "bit_count": "Not applicable for Teradata Vantage. ",
         "bit_get": "Not applicable for Teradata Vantage. ",
-        "broadcast": "Not applicable for Teradata Vantage. ",
         "call_function": "Not applicable for Teradata Vantage. ",
         "call_udf": "Not applicable for Teradata Vantage. Convert teradatamlspk DataFrame to teradataml DataFrame using teradatamlspk_df.toTeradataml() and then use map_row or map_partition.",
         "cancelAllJobs": "Not applicable for Teradata Vantage. ",
         "cancelJobGroup": "Not applicable for Teradata Vantage. ",
         "cardinality": "Not applicable for Teradata Vantage. ",
         "clearTags": "Not applicable for Teradata Vantage. Users should identify the individual SQL queries and ask DBA to cancel/stop them. ",
         "client": "Not applicable for Teradata Vantage.",
@@ -126,28 +125,28 @@
         "datepart": "Not yet available. Look at teradataml DataFrame Column API 'to_char'. ",
         "dropDuplicatesWithinWatermark": "Not available for Teradata Vantage. ",
         "dropFields": "Not applicable for Teradata Vantage.",
         "dump_profiles": "Not applicable for Teradata Vantage. ",
         "element_at": "Not applicable for Teradata Vantage. ",
         "emptyRDD": "Not applicable for Teradata Vantage. ",
         "eqNullSafe": "Not yet available. User can use isNull() along with 'when' API instead.",
-        "exists": "Not applicable for Teradata Vantage. ",
+        "exists": "exists function on Column or column name is not applicable for Teradata Vantage. ",
         "explode": "Not applicable for Teradata Vantage. ",
         "explode_outer": "Not applicable for Teradata Vantage. ",
         "extract": "Not yet available. Look at teradataml DataFrame Column API 'to_char'. ",
         "factorial": "Not yet available. User can use teradataml DataFrame.map_row. ",
         "find_in_set": "Not applicable for Teradata Vantage. ",
-        "flatten": "Not applicable for Teradata Vantage. ",
+        "flatten": "teradatamlspk columns does not store a vector/list of values. Hence, flatten on Column or column name is not applicable.",
         "forall": "Not applicable for Teradata Vantage. ",
         "format_string": "Not applicable for Teradata Vantage. ",
         "freqItems": "Not yet available. User can look create a query using function 'XMLAGG' and create a DataFrame on the query. ",
         "from_json": "Not applicable for Teradata Vantage. ",
         "from_unixtime": "Not yet available. ",
         "from_utc_timestamp": "Not yet available. ",
-        "get": "Not applicable for Teradata Vantage. ",
+        "get": "teradatamlspk columns does not store a vector/list of values. Hence, get on Column or column name is not applicable.",
         "getCheckpointDir": "Not applicable for Teradata Vantage. ",
         "getField": "Not applicable for Teradata Vantage.",
         "getItem": "Not applicable for Teradata Vantage.",
         "getLocalProperty": "Not applicable for Teradata Vantage. ",
         "getTags": "Not applicable for Teradata Vantage. Users should identify the individual SQL queries and ask DBA to cancel/stop them. ",
         "get_json_object": "Not yet available. ",
         "getbit": "Not applicable for Teradata Vantage. ",
@@ -187,18 +186,18 @@
         "offset": "Not yet available. User should use sample function as an alternative. ",
         "pandas_api": "Not yet available. Teradata recommends the user to use regular DataFrame API's instead of pandas style API's.",
         "pandas_udf": "Not applicable for Teradata Vantage. ",
         "parallelize": "Not applicable for Teradata Vantage. ",
         "pickleFile": "Not applicable for Teradata Vantage. ",
         "posexplode": "Not applicable for Teradata Vantage. ",
         "posexplode_outer": "Not applicable for Teradata Vantage. ",
-        "range": "Not applicable for Teradata Vantage. Load the data in to Vantage table and then create DataFrame on it.",
+        "range": "SparkSession.range() and SparkContext.range() is not applicable for Teradata Vantage. Load the data in to Vantage table and then create DataFrame on it.",
         "readStream": "Not applicable for Teradata Vantage.",
         "removeTag": "Not applicable for Teradata Vantage. Users should identify the individual SQL queries and ask DBA to cancel/stop them. ",
-        "reverse": "Not applicable for Teradata Vantage. ",
+        "reverse": "Not yet available. ",
         "rlike": "Not available. Teradata recommends user to convert the functionality to use 'like' API.",
         "runJob": "Not applicable for Teradata Vantage. ",
         "schema_of_csv": "Not applicable for Teradata Vantage. ",
         "schema_of_json": "Not applicable for Teradata Vantage. ",
         "sentences": "Not applicable for Teradata Vantage. ",
         "sequence": "Not applicable for Teradata Vantage. ",
         "sequenceFile": "Not applicable for Teradata Vantage. ",
@@ -208,17 +207,17 @@
         "setJobGroup": "Not applicable for Teradata Vantage. ",
         "setLogLevel": "Not applicable for Teradata Vantage. ",
         "setSystemProperty": "Not applicable for Teradata Vantage. ",
         "shiftrightunsigned": "Not yet available. User can use teradataml DataFrame.map_row. ",
         "show_profiles": "Not applicable for Teradata Vantage. ",
         "shuffle": "Not applicable for Teradata Vantage. ",
         "size": "Not applicable for Teradata Vantage. ",
-        "slice": "Not applicable for Teradata Vantage. ",
+        "slice": "teradatamlspk columns does not store a vector/list of values. Hence, slice on Column or column name is not applicable.",
         "sort_array": "Not applicable for Teradata Vantage. ",
-        "split": "Not applicable for Teradata Vantage. ",
+        "split": "teradatamlspk columns does not store a vector/list of values. Hence, split on Column or column name is not applicable.",
         "split_part": "Not applicable for Teradata Vantage. ",
         "statusTracker": "Not applicable for Teradata Vantage. ",
         "streams": "Not applicable for Teradata Vantage.",
         "substring_index": "Not applicable for Teradata Vantage. ",
         "textFile": "Not applicable for Teradata Vantage. ",
         "timestamp_micros": "Not yet available. ",
         "timestamp_millis": "Not yet available. ",
@@ -233,45 +232,53 @@
         "to_utc_timestamp": "Not yet available. ",
         "transform_keys": "Not applicable for Teradata Vantage. ",
         "transform_values": "Not applicable for Teradata Vantage. ",
         "try_element_at": "Not applicable for Teradata Vantage. ",
         "try_to_timestamp": "Not yet available. ",
         "udf": "Not yet available. Convert teradatamlspk DataFrame to teradataml DataFrame using teradatamlspk_df.toTeradataml() and then use map_row or map_partition.",
         "udtf": "Not yet available. Convert teradatamlspk DataFrame to teradataml DataFrame using teradatamlspk_df.toTeradataml() and then use map_row or map_partition.",
-        "union": "Not applicable for Teradata Vantage. ",
+        "union": "Not applicable for Teradata Vantage, if it's a TeradataContext API or union is performed on RDD.",
         "unix_micros": "Not yet available. ",
         "unix_millis": "Not yet available. ",
         "unix_timestamp": "Not yet available. ",
         "wholeTextFiles": "Not applicable for Teradata Vantage. ",
         "window_time": "Not yet available. ",
         "withField": "Not applicable for Teradata Vantage.",
         "withMetadata": "Not yet available. ",
         "withWatermark": "Not applicable to Teradata Vantage.",
+        "write.json": "Not yet Supported",
         "writeStream": "Not applicable to Teradata Vantage.",
-        "writeTo": "Not yet available. User can use DataFrame.to_sql(<table_name>) to write data to a table. to_sql internally calls teradataml DataFrame.to_sql(). ",
+        "writeTo.option": "Not yet Supported",
+        "writeTo.options": "Not yet Supported",
+        "writeTo.overwrite": "Not yet Supported",
+        "writeTo.overwritePartitions": "Not yet Supported",
+        "writeTo.partitionedBy": "Not yet Supported",
+        "writeTo.using": "Not yet Supported",
         "xpath": "Not applicable for Teradata Vantage. ",
         "xpath_boolean": "Not applicable for Teradata Vantage. ",
         "xpath_double": "Not applicable for Teradata Vantage. ",
         "xpath_float": "Not applicable for Teradata Vantage. ",
         "xpath_long": "Not applicable for Teradata Vantage. ",
         "xpath_number": "Not applicable for Teradata Vantage. ",
         "xpath_short": "Not applicable for Teradata Vantage. ",
         "xpath_string": "Not applicable for Teradata Vantage. ",
         "zip_with": "Not applicable for Teradata Vantage. "
     },
     "notification": {
         "SQLContext": "SQLContext methods applicable for Vantage will work. Not applicable methods will throw error. Remove such methods.",
         "SparkConf": "Not applicable for Teradata Vantage. ",
         "SparkContext": "SparkContext changed to TeradataContext. SparkContext methods applicable for Vantage will work with TeradataContext. Not applicable methods will throw error. Remove such methods.",
-        "VectorAssembler": "Vectors are not applicable for Teradata Vantage. All of the ML functions accepts list of columns for features. Pass list of columns to Feature Columns.",
+        "alias": "Does not support BooleanType, BinaryType, TimestampNTZType, ArrayType, StructType and MapType. Remove these types or their corresponding string literal.",
         "applicationId": "Not applicable for Teradata Vantage. Returns always 0 only to maintain parity. ",
         "atanh": "Throws error if data inside the column is 1. ",
         "bin": "Supports only integer values. ",
+        "broadcast": "Same DataFrame is returned and call is not effective. ",
         "cache": "Not applicable for Teradata. Hence API returns the same DataFrame.",
-        "cbrt": "Applicable only for positive values. ",
+        "cast": "Does not support BooleanType, BinaryType, TimestampNTZType, ArrayType, StructType and MapType. Remove these types or their corresponding string literal.",
+        "cbrt": "Supports only for positive values. ",
         "checkpoint": "Not applicable for Teradata. Hence API returns the same DataFrame.",
         "coalesce": "Not applicable for Teradata. Hence API returns the same DataFrame.",
         "cot": "Throws error if data inside the column is 0. ",
         "csc": "Throws error if data inside the column is 0. ",
         "cube": "Pyspark performs aggregation on columns used for grouping whereas teradatamlspk ignores the aggregation of grouping columns.",
         "defaultMinPartitions": "Not applicable for Teradata Vantage. Returns always 1 only to maintain parity. ",
         "defaultParallelism": "User cannot control parallelism in Teradata Vantage. Returns always 1 only to maintain parity. ",
@@ -288,15 +295,15 @@
         "log1p": "Throws error if data inside the column is 0. ",
         "persist": "Not applicable for Teradata. Hence API returns the same DataFrame.",
         "randomSplit": "Argument 'seed' is ignored and not considered while processing.",
         "repartition": "Not applicable for Teradata. Hence API returns the same DataFrame.",
         "repartitionByRange": "Not applicable for Teradata. Hence API returns the same DataFrame.",
         "rollup": "Pyspark performs aggregation on columns used for grouping whereas teradatamlspk ignores the aggregation of grouping columns.",
         "sameSemantics": "Not applicable for Teradata. Hence API returns False always.",
-        "sample": "Argument 'seed' is ignored and not considered while processing.",
+        "sample": "Argument 'seed' in DataFrame.sample() is ignored and not considered while processing.",
         "semanticHash": "Not applicable for Teradata. Hence API returns False always.",
         "sortWithinPartitions": "Not applicable for Teradata. Hence API returns the same DataFrame.",
         "substr": "Column type is not support for arguments 'pos' and 'len'. ",
         "to_char": "Argument 'pattern' should be a string value. ",
         "to_number": "Argument 'pattern' should be a string value. ",
         "uiWebUrl": "Not applicable for Teradata Vantage.",
         "unpersist": "Not applicable for Teradata. Hence API returns the same DataFrame."
@@ -309,56 +316,66 @@
         "StandardScaler": [
             "The DataFrame accepted by StandardScaler.fit() method should have a mandatory column 'id' which should have distinct values. Create a column from function monotonically_increasing_id. Visit user guide for example.",
             "Argument 'outputCol' is not significant. 'transform()' returns all the columns which includes scaled columns. Look at the user guide for more details. "
         ],
         "TeradataContext": "TeradataContext accepts Vantage connection parameters. Refer create_context function parameters in teradataml.",
         "UnivariateFeatureSelector": "Argument 'outputCol' is not significant. Look at the user guide for more details. ",
         "VarianceThresholdSelector": "Argument 'outputCol' is not significant. Look at the user guide for more details. ",
-        "agg": "PySpark accepts global functions also as input however teradatamlspk won't accept those. Incase if user is passing a function to agg function, make sure to convert it to dictionary. ",
+        "agg": [
+            "API does not support the aggregate functions generated using arthimetic operators. For example, df.agg(F.avg(df.col1)+F.sum(df.col2)) is not supported but df.agg(F.avg(df.col1)) is supported. Look at user guide for more details.",
+            "Functions count_distinct and countDistinct are accepted only if it has one column as input."
+        ],
         "colRegex": "Pyspark returns result based on Scala or Java regex, where as teradatamlspk will return based on python regex.",
         "contains": "API is allowed only for filtering the data. Neither select nor withColumn does not support this API. User should use 'when' incase if want to use it in select or withColumn API's. Example - df.select(when(df.gpa.contains('b'), 1).otherwise(0).alias('col5')).show()",
         "createOrReplaceTempView": "teradatamlspk creates a View and users should drop it at end of session.",
         "createTempView": "teradatamlspk creates a View and users should drop it at end of session.",
         "crossJoin": "If both the input DataFrame's share same column names, then column names for those columns are prefixed with as 'l' and 'r' in the output DataFrame. Also the order of the columns varies.",
         "crosstab": "Based on the data, Column names may differ from PySpark. Incase if these column names are being used in next subsequent lines, make sure to change those lines accordingly. ",
-        "endswith": "API is allowed only for filtering the data. Neither select nor withColumn does not support this API. User should use 'when' incase if want to use it in select or withColumn API's. Example - df.select(when(df.gpa.endswith('b'), 1).otherwise(0).alias('col5')).show()",
+        "endswith": "endswith function on Column is allowed only for filtering the data. Neither select nor withColumn does not support this API. User should use 'when' incase if want to use it in select or withColumn API's. Example - df.select(when(df.gpa.endswith('b'), 1).otherwise(0).alias('col5')).show()",
+        "fillna": "All input arguments must be of the same data type or their types must be compatible. For example, If value is a string, and subset contains a non-string column, then teradatamlspk raises an error. User must drop incompatible column(s) or cast them to the compatible ones.",
         "getOrCreate": "TeradataSession.getOrCreate accepts Vantage connection parameters. Refer create_context function parameters in teradataml.",
         "ilike": [
             "API is allowed only for filtering the data. Neither select nor withColumn does not support this API. User should use 'when' incase if want to use it in select or withColumn API's. Example - df.select(when(df.gpa.ilike('test'), 1).otherwise(0).alias('col5')).show()",
             "Argument 'pattern' accepts only strings. ",
             "Argument 'escapeChar' is not supported. "
         ],
         "insertInto": "Not yet available. You can still store the data using df.toTeradataml().to_sql(<table_name>).",
         "isNotNull": "API is allowed only for filtering the data. Neither select nor withColumn does not support this API. User should use 'when' incase if want to use it in select or withColumn API's. Example - df.select(when(df.gpa.isNotNull(), 1).otherwise(0).alias('col5')).show()",
         "isNull": "API is allowed only for filtering the data. Neither select nor withColumn does not support this API. User should use 'when' incase if want to use it in select or withColumn API's. Example - df.select(when(df.gpa.isNull(), 1).otherwise(0).alias('col5')).show()",
-        "join": "If the join is of type inner, cross, outer, full, fullouter, full_outer, left, leftouter, left_outer, right, rightouter, right_outer and if both the input DataFrame's share same column names, then column names for those columns are prefixed with as 'l' and 'r' in the output DataFrame. Also the order of the columns varies.",
+        "join": "If the DataFrame.join() is of type inner, cross, outer, full, fullouter, full_outer, left, leftouter, left_outer, right, rightouter, right_outer and if both the input DataFrame's share same column names, then column names for those columns are prefixed with as 'l' and 'r' in the output DataFrame. Also the order of the columns varies.",
         "like": [
             "API is allowed only for filtering the data. Neither select nor withColumn does not support this API. User should use 'when' incase if want to use it in select or withColumn API's. Example - df.select(when(df.gpa.like('test'), 1).otherwise(0).alias('col5')).show()",
             "Argument 'pattern' accepts only strings. ",
             "Argument 'escapeChar' is not supported. "
         ],
         "melt": "Output DataFrame column names are different when compared with PySpark.",
+        "na.fill": "All input arguments must be of the same data type or their types must be compatible. For example, If value is a string, and subset contains a non-string column, then teradatamlspk raises an error. User must drop incompatible column(s) or cast them to the compatible ones.",
         "newSession": "pyspark2teradataml does not support multiple connections at same time. Hence the API returns existing session instead of creating a new session.",
         "orderBy": [
             "API changes will not propagate to next API\u2019s. To get top n elements or bottom n elements, use ranking with window aggregates and filter it.",
             "ColumnExpressions are not supported. Only Column names are supported."
         ],
+        "pivot": "Based on the data, Column names may differ from Pyspark. User can use \"alias\" property to change column name",
         "read.csv": [
             "File available in local file system, function uses teradataml read_csv. Options are not same as PySpark. teradataml read_csv arguments are accepted as options. Header is mandatory. pyspark2teradataml will not infer the schema automatically and so schema is mandatory.",
             "File available in cloud storage, function reads CSV from NoS. Options are not same as PySpark. teradataml ReadNoS Arguments are accepted as options. "
         ],
+        "read.format": "Not same as PySpark. Function uses teradataml ReadNoS internally if the file is available in cloud storage. Use 'options' or 'option' to pass teradataml ReadNos arguments. Refer to user guide for example.",
         "read.json": "Not same as PySpark. teradataml ReadNoS Arguments are accepted as options. If the file is in local file system, load it to pandas DataFrame",
         "read.parquet": "Not same as PySpark. teradataml ReadNoS Arguments are accepted as options.",
-        "replace": "Not possible instead of raising error. Say, if you are trying to replace an numeric column with a string type, PySpark ignores the replacement but teradatamlspk raises error. Users should mention the value to replace appropriately based on column type.",
+        "replace": "Input arguments for DataFrame.replace() must be of same data type or else the types must be compatible. For example, if value is a string, and 'subset' contains a non-string column, then the non-string column are simply ignored in pyspark, but teradatamlspk raises error. Users should specify the value in 'subset' to replace appropriately based on column type.",
         "resources": "Not yet available.",
         "shiftleft": "Supports only integer values.",
         "shiftright": "Supports only integer values.",
         "sort": [
-            "API changes will not propagate to next API\u2019s. To get top n elements or bottom n elements, use ranking with window aggregates and filter it.",
+            "DataFrame.sort() operation does not propogate the changes to next API\u2019s. To get top n elements or bottom n elements, use ranking with window aggregates and filter it.",
             "ColumnExpressions are not supported. Only Column names are supported."
         ],
-        "startswith": "API is allowed only for filtering the data. Neither select nor withColumn does not support this API. User should use 'when' incase if want to use it in select or withColumn API's. Example - df.select(when(df.gpa.startswith('a'), 1).otherwise(0).alias('col5')).show()",
-        "union": "The behavior is same as unionByName.",
-        "unionAll": "The behavior is same as unionByName.",
-        "unpivot": "Output DataFrame column names are different when compared with PySpark."
+        "startswith": "startswith function on Column is allowed only for filtering the data. Neither select nor withColumn does not support this API. User should use 'when' incase if want to use it in select or withColumn API's. Example - df.select(when(df.gpa.startswith('a'), 1).otherwise(0).alias('col5')).show()",
+        "unpivot": "Output DataFrame column names are different when compared with PySpark.",
+        "write.csv": "mode parameter is not supported. Users can save DataFrame to csv format on cloud platform only",
+        "write.format": "Not same as PySpark. Function uses teradataml WriteNoS internally if the file is available in cloud storage. Use 'options' or 'option' to pass teradataml ReadNos arguments. Refer to user guide for example.",
+        "write.option": "Users can use option of teradataml WriteNOS function",
+        "write.options": "Users can use options of teradataml WriteNOS function",
+        "write.parquet": "mode parameter is not supported. Users can save DataFrame to parquet format on cloud platform only"
     }
 }
```

## teradatamlspk/ml/constants.py

```diff
@@ -580,15 +580,15 @@
             GBTRegressor: {"model": GBTRegressionModel},
             UnivariateFeatureSelector: {"model": UnivariateFeatureSelectorModel}
             }
     return _map.get(ref_class)[ref_class_type]
 
 SPARK_ATTRIBUTES_TO_OSML_ATTRIBUTES = {
     "LinearSVC": {"coefficients": "coef_", "numClasses": "classes_", "numFeatures": "n_features_in_", "intercept": "intercept_"},
-    "DecisionTreeClassifier": {"toDebugString": "tree_", "featureImportances": "feature_importances_", "numClasses": "classes_", "numFeatures": "n_features_in_","thresholds": None, "numNodes": None},
+    "DecisionTreeClassifier": {"toDebugString": "tree_", "featureImportances": "feature_importances_", "numClasses": "n_classes_", "numFeatures": "n_features_in_","thresholds": None, "numNodes": None},
     "MultilayerPerceptronClassifier": {"numClasses": "classes_", "numFeatures": "n_features_in_"},
     "DecisionTreeRegressor": {"featureImportances": "feature_importances_", "numFeatures": "n_features_in_"},
     "LinearRegression": {"coefficients": "coef_", "numFeatures": "n_features_in_", "intercept": "intercept_"},
     "LogisticRegression": {"coefficients": "coef_","numClasses": "classes_", "numFeatures": "n_features_in_", "intercept": "intercept_"},
     "StandardScaler": {"mean": "mean_"},
     "RobustScaler": {"median": "center_", "range": "scale_"},
     "MinMaxScaler": {"originalMax": "data_max_", "originalMin": "data_min_"},
```

## teradatamlspk/ml/util.py

```diff
@@ -91,18 +91,23 @@
         model_obj._spark_model_obj = self
 
         from teradatamlspk.ml.constants import SPARK_ATTRIBUTES_TO_OSML_ATTRIBUTES
         model_attributes = SPARK_ATTRIBUTES_TO_OSML_ATTRIBUTES.get(self.__class__.__name__, {})
 
         for spark_model_attribute, osml_model_attribute in model_attributes.items():
             setattr(model_obj, spark_model_attribute, getattr(self.osml_fitted_model, osml_model_attribute) if osml_model_attribute else None)
-        # model_obj.coefficients = self.osml_fitted_model.coef_
-        # model_obj.numClasses  = self.osml_fitted_model.classes_
-        # model_obj.numFeatures = self.osml_fitted_model.n_features_in_
-        # model_obj.intercept = self.osml_fitted_model.intercept_
+            if spark_model_attribute == "numClasses":
+                numClasses = getattr(self.osml_fitted_model, osml_model_attribute)
+                if isinstance(numClasses, np.ndarray):
+                    setattr(model_obj, spark_model_attribute, len(numClasses))
+            if spark_model_attribute == "intercept":
+                intercept = getattr(self.osml_fitted_model, osml_model_attribute)
+                if isinstance(intercept, np.ndarray):
+                    setattr(model_obj, spark_model_attribute, intercept[0])
+
         return model_obj
 
     def transform(self, dataset):
         from teradatamlspk.sql.dataframe import DataFrame
         X =  dataset._data.select(self.getFeaturesCol())
         Y =  dataset._data.select(self.getlabelCol())
         tdml_df = self._model.predict(X=X, y=Y)
@@ -260,14 +265,18 @@
             "bernoulli" : {"numFeatures": "n_features_", "pi" : "class_log_prior_", "numClasses": "class_count_"},
             "gaussian" : {"sigma": "sigma_", "numClasses": "class_count_"}
         }
 
         model_attributes = SPARK_ATTR_TO_OSML_ATTR.get(self.getModelType(), {})
         for spark_model_attribute, osml_model_attribute in model_attributes.items():
             setattr(model_obj, spark_model_attribute, getattr(self.osml_fitted_model, osml_model_attribute) if osml_model_attribute else None)
+            if spark_model_attribute == "numClasses":
+                numClasses = getattr(self.osml_fitted_model, osml_model_attribute)
+                if isinstance(numClasses, np.ndarray):
+                    setattr(model_obj, spark_model_attribute, len(numClasses))
         return model_obj
 
     def transform(self, dataset):
         from teradatamlspk.sql.dataframe import DataFrame
         X =  dataset._data.select(self.getFeaturesCol())
         Y =  dataset._data.select(self.getlabelCol())
         tdml_df = self._model.predict(X=X, y=Y)
@@ -794,41 +803,37 @@
     def get_label_count_df(self):
         label = self._spark_model_obj.getlabelCol()
         return self._data.select([label]).groupBy(label).count()
     
     def get_ytrue_ypred(self):
         return self._predicted_output.select(self._spark_model_obj.getlabelCol()), self._predicted_output.select(self._spark_model_obj.getpredictionCol())
 
+    def get_weighted_average(self, metric):
+        labels_list = self.get_labels()
+        res_df = self.get_label_count_df()
+        res_df_dict = res_df.to_pandas().to_dict()
+        result = 0
+        count_sum = 0
+        for label, value in zip(labels_list, metric):
+            if res_df_dict['count'][label] > 0:
+                count = res_df_dict['count'][label]
+                result += count * value
+                count_sum += count
+        return result / count_sum
+    
     def fMeasureByLabel(self, beta=1.0):
         """Returns f-measure for each label (category)."""
         y_true_df , y_pred_df = self.get_ytrue_ypred()
         opt = osml.f1_score(y_true=y_true_df._data, y_pred=y_pred_df._data, average=None, labels=self.get_labels())
         return opt.tolist()
 
     def weightedFMeasure(self, beta=1.0):
         """ Returns weighted averaged f-measure. """
-        labels_list = self.get_labels()
-        metrics_list = self.fMeasureByLabel()
-        res_df = self.get_label_count_df()
-        # +-----+-----+
-        # |label|count|
-        # +-----+-----+
-        # |    0|   46|
-        # |    1|   44|
-        # +-----+-----+
-        res_df_dict = res_df.to_pandas().to_dict()
-        result = 0
-        count_sum = 0
-        # labels_list: [0.0, 1.0]
-        # metrics_list: [0.923076923076923, 0.9213483146067415]
-        for label, value in zip(labels_list, metrics_list):
-            count = res_df_dict['count'][label]
-            result += count * value
-            count_sum += count
-        return result/count_sum
+        fMeasure = self.fMeasureByLabel()
+        return self.get_weighted_average(fMeasure)
 
     @property
     def accuracy(self):
         """ Returns accuracy."""
         return (self.predictions[self.predictions[self._spark_model_obj.getlabelCol()] == self.predictions[self._spark_model_obj.getpredictionCol()]].count())/self.predictions.count()
 
     @property
@@ -924,61 +929,42 @@
         confusion_matrix = osml.confusion_matrix(y_true=y_true_df._data, y_pred=y_pred_df._data)
         FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)
         TP = np.diag(confusion_matrix)
         return (TP/(TP+FN)).tolist()
 
     @property
     def weightedFalsePositiveRate(self):
-        # TODO: To be implemented with https://teradata-pe.atlassian.net/browse/ELE-6514.
-        raise NotImplementedError
+        """ Returns weighted false positive rate. """
+        FPR = self.falsePositiveRateByLabel
+        return self.get_weighted_average(FPR)
 
     @property
     def weightedPrecision(self):
         """ Returns weighted averaged precision. """
         y_true_df , y_pred_df = self.get_ytrue_ypred()
         confusion_matrix = osml.confusion_matrix(y_true=y_true_df._data, y_pred=y_pred_df._data)
         FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)
         TP = np.diag(confusion_matrix)
         precision = TP / (TP + FP)
-        labels_list = self.labels
-        res_df = self._data.select(['label']).groupBy('label').count()
-        res_df_dict = res_df.to_pandas().to_dict()
-        result = 0
-        count_sum = 0
-        for label, value in zip(labels_list, precision):
-            if res_df_dict['count'][label] > 0:
-                count = res_df_dict['count'][label]
-                result += count * value
-                count_sum += count
-        return result / count_sum
+        return self.get_weighted_average(precision)
 
     @property
     def weightedRecall(self):
         """ Returns weighted averaged recall. """
         y_true_df , y_pred_df = self.get_ytrue_ypred()
         confusion_matrix = osml.confusion_matrix(y_true=y_true_df._data, y_pred=y_pred_df._data)
         TP = np.diag(confusion_matrix)
         FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)
         recall = TP / (TP + FN)
-        labels_list = self.labels
-        res_df = self._data.select(['label']).groupBy('label').count()
-        res_df_dict = res_df.to_pandas().to_dict()
-        result = 0
-        count_sum = 0
-        for label, value in zip(labels_list, recall):
-            if res_df_dict['count'][label] > 0:
-                count = res_df_dict['count'][label]
-                result += count * value
-                count_sum += count
-        return result / count_sum
+        return self.get_weighted_average(recall)
 
     @property
     def weightedTruePositiveRate(self):
-        # TODO: To be implemented with https://teradata-pe.atlassian.net/browse/ELE-6514.
-        raise NotImplementedError
+        """ Returns weighted true positive rate. """
+        return self.weightedRecall
 
 class _RegressionMetrics(_ModelSummaryMethods):
 
     @property
     def coefficientStandardErrors(self):
         # TODO: To be implemented with https://teradata-pe.atlassian.net/browse/ELE-6518.
         raise NotImplementedError
```

## teradatamlspk/ml/evaluation/__init__.py

```diff
@@ -53,14 +53,18 @@
             res =self.truePositiveRateByLabel
         elif metric == "falsePositiveRateByLabel":
             res = self.falsePositiveRateByLabel
         elif metric == "precisionByLabel":
             res = self.precisionByLabel
         elif metric == "recallByLabel":
             res = self.recallByLabel
+        elif metric == "weightedTruePositiveRate":
+            res = self.weightedTruePositiveRate
+        elif metric == "weightedFalsePositiveRate":
+            res = self.weightedFalsePositiveRate
         elif metric == "fMeasureByLabel":
             res = self.fMeasureByLabel(beta)
         elif metric == "weightedFMeasure":
             res =self.weightedFMeasure(beta)
         elif metric in ["f1","accuracy","weightedPrecision","weightedRecall"]:
             num_labels = self.get_label_count_df()
             ce = ClassificationEvaluator(data=self.data._data,
```

## teradatamlspk/sql/column.py

```diff
@@ -12,28 +12,33 @@
 # ##################################################################
 
 from teradatamlspk.sql.utils import _get_tdml_type
 
 _get_other = lambda other: other._tdml_column if isinstance(other, Column) else other
 from teradataml.dataframe.sql import _SQLColumnExpression
 from sqlalchemy import bindparam
+from teradatamlspk.sql.constants import SQL_NAME_TO_SPARK_TYPES
 
 
 class Column:
     """ teradatamlspk DataFrame Column. """
     __or__ = lambda self, other: Column(tdml_column=self._tdml_column | _get_other(other))
     __and__ = lambda self, other: Column(tdml_column=self._tdml_column & _get_other(other))
     __ne__ = lambda self, other: Column(tdml_column=self._tdml_column != _get_other(other))
     __truediv__ = lambda self, other: Column(tdml_column=(self._tdml_column / _get_other(other)))
     __floordiv__ = lambda self, other: Column(tdml_column=(self._tdml_column // _get_other(other)))
     __mod__ = lambda self, other: Column(tdml_column=(self._tdml_column % _get_other(other)))
     desc = lambda self: Column(tdml_column=self._tdml_column.desc())
     asc = lambda self: Column(tdml_column=self._tdml_column.asc())
     nulls_first = lambda self: Column(tdml_column=self._tdml_column.nulls_first())
     nulls_last = lambda self: Column(tdml_column=self._tdml_column.nulls_last())
+    desc_nulls_first = lambda self: Column(tdml_column=self._tdml_column.desc().nulls_first())
+    desc_nulls_last = lambda self: Column(tdml_column=self._tdml_column.desc().nulls_last())
+    asc_nulls_first = lambda self: Column(tdml_column=self._tdml_column.asc().nulls_first())
+    asc_nulls_last = lambda self: Column(tdml_column=self._tdml_column.asc().nulls_last())
     contains = lambda self, other: Column(tdml_column=self._tdml_column.str.contains(
         other._tdml_column if isinstance(other, Column) else other)==True)
     like = lambda self, other: Column(tdml_column=self._tdml_column.like(other))
     ilike = lambda self, other: Column(tdml_column=self._tdml_column.ilike(other))
     substr = lambda self, startPos, length: Column(tdml_column=self._tdml_column.substr(startPos, length))
     startswith = lambda self, other: Column(tdml_column=self._tdml_column.startswith(
         other._tdml_column if isinstance(other, Column) else other))
@@ -44,25 +49,28 @@
     bitwiseAND = lambda self, other: Column(tdml_column= self._tdml_column.bitand(
         other._tdml_column if isinstance(other, Column) else other))
     bitwiseOR = lambda self, other: Column(tdml_column= self._tdml_column.bitor(
         other._tdml_column if isinstance(other, Column) else other))
     bitwiseXOR = lambda self, other: Column(tdml_column= self._tdml_column.bitxor(
         other._tdml_column if isinstance(other, Column) else other))
     isin = lambda self, *cols: Column(tdml_column=self._tdml_column.isin(cols[0] if len(cols) == 1 and isinstance(cols[0], list) else list(cols)))
-    cast = lambda self, dataType: Column(tdml_column=self._tdml_column.cast(_get_tdml_type(dataType)))
     between = lambda self, lowerBound, upperBound: Column(tdml_column= (self._tdml_column >= _get_other(lowerBound)) & (self._tdml_column <= _get_other(upperBound)))
     when = lambda condition, value: Column(
         tdml_column=_SQLColumnExpression(case((_get_tdml_col(condition).expression, value))))
 
     def __init__(self, jc=None, **kwargs):
         self.__map = {}
         self._tdml_column = _SQLColumnExpression(jc) if jc else kwargs.get("tdml_column")
         self.astype = self.cast
         self.alias_name = None
 
+        # Store _SQLColumnExpression.
+        self.expr_ = kwargs.get("expr_")
+        self.agg_func_params = kwargs.get("agg_func_params")
+
     def alias(self, *alias, **kwargs):
         self.alias_name = alias[0]
         return self
 
     @property
     def name(self):
         """ Property to return the name of the column. """
@@ -159,7 +167,38 @@
                                                   )
         return self
 
     def otherwise(self, value):
         # It will be triggered on top of case.
         self._tdml_column.expression.else_ = value._tdml_column.expression if isinstance(value, Column) else bindparam('value', value)
         return self
+
+    def cast(self, dataType):
+        """ Casts the column into type dataType. """
+        if isinstance(dataType, str):
+            dataType = SQL_NAME_TO_SPARK_TYPES.get(dataType.upper())
+        return Column(tdml_column=self._tdml_column.cast(_get_tdml_type(dataType)))
+
+    def over(self, window):
+        """ Function to get window aggregates. """
+
+        # Extract window aggregate params.
+        window_params = window.get_params()
+
+        # Convert tdmlspk Column to tdml columns.
+        if window_params["partition_columns"]:
+            window_params["partition_columns"] = [col._tdml_column if isinstance(col, Column) else col
+                                                  for col in window_params["partition_columns"]]
+
+        if window_params["order_columns"]:
+            window_params["order_columns"] = [col._tdml_column if isinstance(col, Column) else col
+                                              for col in window_params["order_columns"]]
+
+        # Prepare tdml Column with Window aggregate.
+        tdml_window = self.expr_.window(**window_params)
+
+        # Trigger regular window aggregate.
+        agg_func_params = {**self.agg_func_params}
+        func_name = agg_func_params.pop("name")
+        tdml_col_expr = getattr(tdml_window, func_name)(**agg_func_params)
+
+        return Column(tdml_column=tdml_col_expr)
```

## teradatamlspk/sql/constants.py

```diff
@@ -50,8 +50,42 @@
                      CHAR: CharType,
                      VARCHAR: VarcharType,
                      TDUDT: UserDefinedType}
 
 SPARK_TO_TD_TYPES = {ByteType: BYTE,
                      FractionalType: DecimalType,
                      DoubleType: DECIMAL,
+                     StringType: VARCHAR,
                      **{v:k for k,v in TD_TO_SPARK_TYPES.items() if v != ByteType}}
+
+SQL_NAME_TO_SPARK_TYPES = { "STRING": StringType(),
+                            "INTEGER" : IntegerType(),
+                            "INT" : IntegerType(),
+                            "REAL": FloatType(),
+                            "FLOAT": FloatType(),
+                            "BYTE": ByteType(),
+                            "TINYINT": ByteType(),
+                            "SHORT": ShortType(),
+                            "SMALLINT": ShortType(),
+                            "LONG": LongType(),
+                            "BIGINT": LongType(),
+                            "DOUBLE": DoubleType(),
+                            "DATE": DateType(),
+                            "TIMESTAMP": TimestampType(),
+                            "TIMESTAMP_LTZ": TimestampType(),
+                            "DECIMAL": DecimalType(),
+                            "DEC": DecimalType(),
+                            "NUMERIC": DecimalType(),
+                            "INTERVAL YEAR": IntervalYearType(),
+                            "INTERVAL YEAR TO MONTH": IntervalYearToMonthType(),
+                            "INTERVAL MONTH": IntervalMonthType(),
+                            "INTERVAL DAY": IntervalDayType(),
+                            "INTERVAL DAY TO HOUR": IntervalDayToHourType(),
+                            "INTERVAL DAY TO MINUTE": IntervalDayToMinuteType(),
+                            "INTERVAL DAY TO SECOND": IntervalDayToSecondType(),
+                            "INTERVAL HOUR": IntervalHourType(),
+                            "INTERVAL HOUR TO MINUTE": IntervalHourToMinuteType(),
+                            "INTERVAL HOUR TO SECOND": IntervalHourToSecondType(),
+                            "INTERVAL MINUTE": IntervalMinuteType(),
+                            "INTERVAL MINUTE TO SECOND": IntervalMinuteToSecondType(),
+                            "INTERVAL SECOND": IntervalSecondType()
+}
```

## teradatamlspk/sql/dataframe.py

```diff
@@ -15,22 +15,22 @@
 from teradataml.dataframe.sql import _SQLColumnExpression
 from teradataml.dataframe.setop import td_minus, td_intersect
 from teradataml.dataframe.sql_functions import case
 from teradataml.common.utils import UtilFuncs
 from teradataml.dbutils.dbutils import db_drop_view
 from teradataml.dataframe.setop import concat
 from prettytable import PrettyTable
-from sqlalchemy.sql import literal_column
+from sqlalchemy.sql import literal_column, literal
 from sqlalchemy import func
 from teradatamlspk.sql.utils import AnalysisException
 from teradatamlspk.sql.column import Column
 from teradatamlspk.sql.dataframe_utils import DataFrameUtils as df_utils
 from teradatamlspk.storagelevel import StorageLevel
 from teradatamlspk.common.constants import _SPARK_TO_TDML_FN_MAPPER as FN_MAPPER, _DataFrameReturnDF
-from teradatamlspk.sql.functions import _TeradatamlspkFunction
+from teradatamlspk.sql.readwriter import DataFrameWriter
 from teradatamlspk.sql.types import Row, StructField, StructType
 from teradatamlspk.sql.utils import _get_spark_type
 import math
 from functools import reduce
 
 
 
@@ -48,27 +48,27 @@
         self.cov = lambda col1, col2: next(
             self._data.assign(
                 n=self._data[col1].covar_samp(self._data[col2]), drop_columns=True).itertuples(name=None))[0]
 
         # where and filter are aliases.
         self.where = self.filter
 
-        # default value for head in teradataml is 10 where as in PySpark it is 1.
-        # Hence declaring the method instead of wrapper.
-        self.head = lambda n=1: _DataFrameReturnDF.head_func(self._data.head(n))
-        self.take = lambda num: self.head(num) if num > 1 else [self.head(num)]
+        self.take = lambda num: self.head(num)
         self.first = lambda: self.head()
 
         # sort, sortWithinPartitions and orderBy are aliases.
         self.orderBy = self.sort
         self.sortWithinPartitions = self.sort
 
         # unpivot and melt are aliases.
         self.melt = self.unpivot
 
+        #unionAll and union are aliases
+        self.unionAll = self.union
+
         # createTempView, createOrReplaceTempView, createGlobalTempView, createOrReplaceGlobalTempView and registerTempTable will be alias.
         # we don't support multiple sessions in teraspark
         self.createTempView = self.createOrReplaceTempView
         self.createGlobalTempView = self.createOrReplaceTempView
         self.createOrReplaceGlobalTempView = self.createOrReplaceTempView
         self.registerTempTable = self.createOrReplaceTempView
         self._ml_params = {}
@@ -89,14 +89,26 @@
     sameSemantics = lambda self, other: False
     semanticHash = lambda self: 0
     inputFiles = lambda self: []
     isLocal = lambda self: False    
     foreachPartition = lambda self, f: f(self.toLocalIterator())
     transform = lambda self, func, *args, **kwargs: func(self, *args, **kwargs)
 
+    def head(self, n = None):
+        """ Returns the first n rows."""
+        num = 1 if n is None else n
+        df = self._data.head(num)
+        recs = [Row(**(rec._asdict())) for rec in df.itertuples()]
+        if len(recs) == 1 and n is None:
+            return recs[0]
+        elif len(recs) == 0:
+            return None
+        else:
+            return recs
+
     def show(self, n = 20, truncate = True, vertical = False):
         """ Function to show the DataFrame. """
         tdml_df = self._data
         # If it has ml params, then it should be show in different way.
         if self._ml_params:
             from sqlalchemy import func
             first_col, other_cols = self._ml_params["inputCols"][0], self._ml_params["inputCols"][1:]
@@ -108,35 +120,45 @@
         # If truncate set to True then truncate strings longer than 20 chars.
         # If set to a number greater than zero then truncate strings to length truncate.
         if isinstance(truncate, bool):
             trunc = 20 if truncate else None
         else:
             trunc= truncate if truncate>0 else None
         data = []
-        for rec in tdml_df.itertuples(name=None, num_rows=n):
+
+        # Instead of retrieving "n" records, retrieve "n+1" records.
+        # If variable data has "n+1" records, then the DataFrame has more
+        # than "n" records. So, print a message at end of table.
+        for rec in tdml_df.itertuples(name=None, num_rows=n+1):
             data.append(Row(**{k: str(val)[:trunc] for k,val in zip(tdml_df.columns, rec)}))
 
         # Print dataframe vertically or in tabular format.
         if vertical:
             max_len = max(len(col) for col in self.columns)
             #Print rows vertically as one line per column value.
             for i,row in enumerate(data):
+                # index starts from 0.
+                if i ==n:
+                    break
                 print(f"-RECORD {i}"+"-" * (max_len + 5))
                 for col in self.columns:
                     # left aligned the columns wrt col having max length .
                     print(f"{str(col).ljust(max_len)} | {row[col]}")
         else:
             _table = PrettyTable(align='r', padding_width=0)
-            for rec in data:
-                 _table.add_row(rec)
+            for _index, rec in enumerate(data):
+                # index starts from 0.
+                if _index == n:
+                    break
+                _table.add_row(rec)
             _table.field_names = tdml_df.columns
             print(_table)
 
-        if(len(data) < self.count()):
-            print(f"only showing top {len(data)} rows")
+        if(len(data) > n):
+            print(f"only showing top {n} rows")
 
     def __repr__(self):
         """ String representation of teraspark DataFrame. """
         _iter = map(lambda c: "{}: {}".format(c[0], c[1].lower()), self._data._column_names_and_types)
         return "{}[{}]".format(self.__class__.__name__, ", ".join(_iter))
 
     def __str__(self):
@@ -273,16 +295,14 @@
 
         # Call the corresponding function.
         call_value = getattr(self._data, _tdml_func_name)  # This will hold value for tdml DataFrame properties.
         if callable(call_value):
             call_value = call_value(**tdml_arguments_values)
         # Return result.
         if _return_func:
-            if function_name in ['groupby', 'groupBy']:
-                return _return_func(call_value, self._data)
             return _return_func(call_value)
 
         return call_value
 
     def __getitem__(self, item):
         """ Return a column from the DataFrame or filter the DataFrame using an expression. """
         if item in self.columns and isinstance(item, str):
@@ -291,27 +311,20 @@
         if isinstance(item, Column):
             item = item._tdml_column
 
         return DataFrame(self._data.__getitem__(item))
 
     def withColumn(self, colName, col):
         """ Creates new column with name as `colName` and corresponding value as `col`. """
-        if isinstance(col, (_TeradatamlspkFunction)):
-
-            analytic_function = col
-            if col.is_window_aggregate():
-
-                new_df = self._data.assign(**{colName: analytic_function._get_tdml_column_expression(self._data)})
-
-            else:
-
-                new_df = self._data.assign(**{colName: analytic_function.get_func_expression(self._data)})
-        else:
-            new_df = self._data.assign(**{colName: self.__get_tdml_column(col)})
+        new_df = self._data.assign(**{colName: self.__get_tdml_column(col)})
+        return DataFrame(data=new_df)
 
+    def withColumns(self, colMap):
+        """ Creates new columns with ColMap. Key represents alias name and value represents ColumnExpression. """
+        new_df = self._data.assign(**{colName: self.__get_tdml_column(col) for colName, col in colMap.items()})
         return DataFrame(data=new_df)
 
     def crossJoin(self, other):
         """ Function to perform cartesian product with other DataFrame. """
         join_params_ = {"other": other._data, "how": "cross"}
 
         # Check if dataframe has common column names.
@@ -419,34 +432,39 @@
         if isinstance(cols[0], list):
             cols = cols[0]
 
         for column in cols:
             if isinstance(column, str):
                 _assign_expr[column] = self[column]._tdml_column
 
-            elif isinstance(column, (_TeradatamlspkFunction)):
-
-                # Passed expression is a Function.
-                analy_func = column
-                expression = analy_func.get_func_expression(self._data)
-                column_name = analy_func.alias_name if analy_func.alias_name else str(expression.compile()).replace('"', "")
-                _assign_expr[column_name] = expression
-                _is_column_expression_noticed = True
-
             else:
                 column_name = column.alias_name if column.alias_name else column._tdml_column.compile().replace('"', "")
                 _assign_expr[column_name] = column._tdml_column
                 _is_column_expression_noticed = True
 
         if _is_column_expression_noticed:
             tdml_df = self._data.assign(drop_columns = True, **_assign_expr)
             return DataFrame(tdml_df.select(list(_assign_expr.keys())))
 
         return DataFrame(self._data.select(list(cols)))
 
+    def fillna(self, value, subset=None):
+        """ Replace the null values. """
+        df = self
+        if isinstance(value, dict):
+            for col, val in value.items():
+                df = df.withColumn(col, Column(tdml_column=_SQLColumnExpression(func.nvl(literal_column(col), literal(val)))))
+        else:
+            if subset is None:
+                subset = self.columns
+            subset = list(subset) if isinstance(subset, tuple) else UtilFuncs._as_list(subset)
+            for column in subset:
+                df = df.withColumn(column, Column(tdml_column=_SQLColumnExpression(func.nvl(literal_column(column), literal(value)))))
+        return df
+
     def sort(self, *cols, **kwargs):
         """ Sort the data according to cols. """
         # Extract the column names from cols.
         cols = cols[0] if isinstance(cols, list) else cols
         columns = []
 
         # PySpark prefers ColumnExpression over argument "ascending". However,
@@ -506,14 +524,20 @@
     def __get_list_element(self, l, index, default_value):
         """Internal function to get a value from a list. If specified index does not exist, returns a default value. """
         try:
             return l[index]
         except IndexError:
             return default_value
 
+    def union(self, other):
+        """ Union the data considering column by position."""
+        col_dict = dict(zip(other.columns, self.columns))
+        df = other.withColumnsRenamed(col_dict)
+        return self.concat(df)
+
     def unionByName(self, other, allowMissingColumns=False):
         """ Union the data considering column names."""
         if not allowMissingColumns and not set(self.columns).issubset(set(other.columns)):
             unique_column = next((col2 for col1, col2 in zip(other.columns, self.columns) if col1 != col2), None)
             raise AnalysisException("Cannot resolve column name \"{0}\" among ({1}).".format(str(unique_column),
                                                                                              ", ".join(other.columns)),1)
         return self._tdml_concat(other)
@@ -836,34 +860,67 @@
         return DataFrameNaFunctions(_spark_df=self)
 
     @property
     def stat(self):
         """ Statistic functions for DataFrames. """
         return DataFrameStatFunctions(_spark_df=self)
 
+    def groupBy(self, *expr):
+        is_list = isinstance(expr[0], list)
+        if is_list:
+            return  _DataFrameReturnDF.dataframe_grouped_func(self._data.groupby(expr[0]), self._data)
+        return  _DataFrameReturnDF.dataframe_grouped_func(self._data.groupby(list(expr)), self._data)
+
+    def groupby(self, *expr):
+        return self.groupBy(*expr)
+
     def agg(self, *expr):
         """Perform aggregates using one or more operations."""
-        column_expression = isinstance(expr[0], _TeradatamlspkFunction)
+        column_expression = isinstance(expr[0], Column)
         if column_expression:
-            return DataFrame(self._data.agg(list(expr)))
-        return DataFrame(self._data.agg(*expr))
+            expr_dict, alias_dict = df_utils._get_agg_expr_alias_dict(expr)
+            df = DataFrame(self._data.agg(expr_dict))
+            return df.withColumnsRenamed(alias_dict)
+
+        # Creating a new column with any value only when user passes {'*': 'count'}
+        # We can use that column for count in agg function.
+        _new_data = self._data
+        if expr[0].get('*', None):
+            _new_data = self._data.assign(all_rows_ = 1)
+            expr[0].pop('*')
+            expr[0]['all_rows_'] = 'count'
+        return DataFrame(_new_data.agg(*expr))
 
     def toTeradataml(self):
         """Converts teradatamlspk DataFrame to teradataml DataFrame. """
         return self._data
 
+    def writeTo(self, table, schema_name=None):
+        """Function to write DataFrame to table. """
+        from teradatamlspk.sql.readwriter import DataFrameWriterV2
+        return DataFrameWriterV2(self, table, schema_name)
+
+    @property
+    def write(self):
+        return DataFrameWriter(data=self._data)
+    
+    @property
+    def rdd(self):
+        """Returns the same DataFrame."""
+        return self
+
 class DataFrameNaFunctions:
     """ Work with missing data in DataFrame. """
     def __init__(self, _spark_df):
         """ Constructor. """
         self._spark_df = _spark_df
 
     def fill(self, value, subset=None):
         """ Replace null values. """
-        return self._spark_df.fill(value, subset)
+        return self._spark_df.fillna(value, subset)
 
     def drop(self, how='any', thresh=None, subset=None):
         """ Omit rows with null values """
         return self._spark_df.dropna(how, thresh, subset)
 
     def replace(self, to_replace, value, subset=None):
         """ Replace a value with another value. """
```

## teradatamlspk/sql/dataframe_utils.py

```diff
@@ -7,14 +7,16 @@
 # Secondary Owner: Pradeep Garre(pradeep.garre@teradata.com)
 #
 #
 # Version: 1.0
 #
 # ##################################################################
 
+from teradatamlspk.sql.column import Column
+
 class DataFrameUtils():
 
     @staticmethod
     def _tuple_to_list(args, arg_name):
         """
         Converts a tuple of string into list of string and multiple list of strings in a tuple
         to list of strings.
@@ -36,15 +38,16 @@
             res_list = list(args)
         elif len(args) == 1 and isinstance(args[0], list):
             # Accept one list of strings in tuple.
             res_list = args[0]
         else:
             raise ValueError("'{}' argument accepts only strings or one list of strings".format(arg_name))
         return res_list
-    
+
+    @staticmethod
     def _get_columns_from_tuple_args(args, df_columns):
         """
         Converts a tuple of string, column expression or a list of strings/ column expression in a tuple
         to list of strings.
 
         PARAMETERS:
             args: tuple having list of strings/ column expression, strings or column expression.
@@ -61,7 +64,44 @@
         for arg in args:
             if arg not in df_columns:
                 pass
             else:
                 arg = arg if isinstance(arg, str) else arg._tdml_column.name
                 columns.append(arg)
         return columns
+
+    @staticmethod
+    def _get_agg_expr_alias_dict(expr):
+        """ 
+        Converts a list of Column Expressions to a dict, mapping of column name to aggregate functions.
+        Also, a dict mapping of output column name to alias name (if given else output column name).
+
+        PARAMETERS:
+            expr: list of Column Expressions
+
+        RETURNS:
+            two-tuple of dictionary
+        """
+        expr_dict = {}
+        alias_list = []
+        output_col = []
+        for func in expr:
+
+            # Extract the name of column.
+            col = func.expr_.compile().replace('"', "")
+
+            # Extract function name.
+            if func.agg_func_params["name"] == "count" and func.agg_func_params.get("distinct"):
+                agg_func = "unique"
+            else:
+                agg_func = func.agg_func_params["name"]
+
+            if col in expr_dict:
+                expr_dict[col].append(agg_func)
+            else:
+                expr_dict[col] = [agg_func]
+            col_name = agg_func + "_" + col
+            output_col.append(col_name)
+            alias_list.append(func.alias_name) if func.alias_name is not None else alias_list.append(col_name)
+            alias_dict = dict(zip(output_col, alias_list))
+        return expr_dict, alias_dict
+
```

## teradatamlspk/sql/functions.py

```diff
@@ -9,15 +9,15 @@
 #
 # Version: 1.0
 #
 # ##################################################################
 from sqlalchemy import func, literal_column, literal
 from teradatamlspk.sql.column import Column
 from teradataml.dataframe.sql import _SQLColumnExpression
-from teradatasqlalchemy.types import INTEGER, DATE
+from teradatasqlalchemy.types import INTEGER, DATE, FLOAT, NUMBER
 
 # Suppressing the Validation in teradataml.
 # Suppress the validation. PySpark accepts the columns in the form of a
 # standalone Columns, i.e.,
 # >>> from PySpark.sql.functions import col, sum
 # >>> column_1 = col('x')
 # >>> column_2 = col('y')
@@ -32,112 +32,14 @@
 _Validators.skip_all = True
 from sqlalchemy import literal_column, func, case, literal
 
 _get_sqlalchemy_expr = lambda col: literal_column(col) if isinstance(col, str) else literal(col)
 _get_tdml_col = lambda col: _SQLColumnExpression(_get_sqlalchemy_expr(col)) if isinstance(col, (str, int, float)) else col._tdml_column
 
 
-class _TeradatamlspkFunction:
-    """ Internal Base class for teradatamlspk Functions. """
-    def __init__(self, *cols, **kwargs):
-        self._params = {}
-        self._params["cols"] = cols
-        self._params["window_"] = None
-        self._spark_vantage_function_mapper = {
-            "pow": "power",
-            "variance": "var",
-            "var_pop": "var",
-            "var_samp": "var",
-            "stddev": "stddev_samp",
-            "stddev_pop": "std",
-            "first": "first_value",
-            "last": "last_value",
-            "any_value": "first_value",
-            "count_distinct": "count",
-            "sum_distinct": "sum",
-            "sumDistinct": "sum"
-        }
-        self.alias_name = None
-
-    def alias(self, *alias, **kwargs):
-        self.alias_name = alias[0]
-        return self
-
-    def over(self, window_spec):
-        self._params["window_"] = window_spec
-        return self
-
-    def get_window_params(self):
-        return self._params["window_"].get_params()
-
-    def is_window_aggregate(self):
-        return self._params["window_"] is not None
-
-    def get_columns(self):
-        return self._params["cols"]
-
-    def get_function_name(self):
-        return self._spark_vantage_function_mapper.get(self.__class__.__name__, self.__class__.__name__)
-
-    def get_func_expression(self, tdml_df):
-
-        if self.is_window_aggregate():
-            return self._get_tdml_column_expression(tdml_df)
-
-        cols = []
-        func_name = self.get_function_name()
-
-        if isinstance(self._params["cols"][0], str):
-            tdml_dataframe_column = tdml_df[self._params["cols"][0]]
-        else:
-            tdml_dataframe_column = self._params["cols"][0]._tdml_column
-
-        if getattr(tdml_dataframe_column, func_name, None):
-            return getattr(tdml_dataframe_column, func_name)(*(c._tdml_column if isinstance(c, Column) else c for c in self._params["cols"][1:]))
-
-        for val in self._params["cols"]:
-            if isinstance(val, Column):
-                cols.append(literal_column(val.name))
-            elif isinstance(val, (int, float)):
-                cols.append(literal(val))
-            else:
-                cols.append(val)
-
-        return getattr(func, func_name)(*cols)
-
-    def _get_tdml_column_expression(self, tdml_df):
-        """
-        Function to get the teradataml DataFrame ColumnExpression for
-        the corresponding window aggregates.
-        """
-
-        window_params = self.get_window_params()
-
-        # Convert teradatamlspk ColumnExpression to teradataml ColumnExpression.
-        if window_params["partition_columns"]:
-            window_params["partition_columns"] = [col._tdml_column if isinstance(col, Column) else col
-                                                  for col in window_params["partition_columns"]]
-
-        if window_params["order_columns"]:
-            window_params["order_columns"] = [col._tdml_column if isinstance(col, Column) else col
-                                              for col in window_params["order_columns"]]
-
-        columns_ = self.get_columns()
-
-        # Get the column.
-        # some functions do not have any column to act. In such cases, choose any column.
-        col_s_ = columns_[0] if columns_ else tdml_df.columns[0]
-
-        # Convert column to Column Expression
-        col = tdml_df[col_s_] if isinstance(col_s_, str) else (col_s_._tdml_column if isinstance(col_s_, Column) else col_s_)
-
-        # Window Aggregates Call.
-        _window_obj = getattr(col, "window")(**window_params)
-        return getattr(_window_obj, self.get_function_name())(*(c._tdml_column if isinstance(c, Column) else c for c in columns_[1:]))
-
 col = lambda col: Column(tdml_column=_SQLColumnExpression(col))
 
 column = lambda col: Column(tdml_column=_SQLColumnExpression(col))
 lit = lambda col: Column(tdml_column=_SQLColumnExpression(literal(col)))
 broadcast = lambda df: df
 def coalesce(*cols):
 
@@ -153,15 +55,15 @@
 monotonically_increasing_id = lambda : Column(tdml_column=_SQLColumnExpression('sum(1) over( rows unbounded preceding )'))
 def named_struct(*cols):
     raise NotImplementedError
 nanvl = lambda col1, col2: Column(tdml_column=_SQLColumnExpression(func.nvl(_get_tdml_col(col1).expression, _get_tdml_col(col2).expression)))
 rand = lambda seed=0: Column(tdml_column=_SQLColumnExpression("cast(random(0,999999999) as float)/1000000000 (format '9.999999999')"))
 randn = lambda seed=0: Column(tdml_column=_SQLColumnExpression("cast(random(0,999999999) as float)/1000000000 (format '9.999999999')"))
 spark_partition_id = lambda: Column(tdml_column=_SQLColumnExpression("0"))
-when = lambda condition, value: Column(tdml_column=_SQLColumnExpression(case((_get_tdml_col(condition).expression, value))))
+when = lambda condition, value: Column(tdml_column=_SQLColumnExpression(case((_get_tdml_col(condition).expression, value._tdml_column.expression if isinstance(value, Column) else value))))
 bitwise_not = lambda col: Column(tdml_column=col._tdml_column.bitwise_not() if isinstance(col, Column) else _SQLColumnExpression(col).bitwise_not())
 bitwiseNOT = lambda col: Column(tdml_column=col._tdml_column.bitwise_not() if isinstance(col, Column) else _SQLColumnExpression(col).bitwise_not())
 expr = lambda str: Column(tdml_column=_SQLColumnExpression(str))
 greatest = lambda *cols: Column(tdml_column=_get_tdml_col(cols[0]).greatest(*[_get_tdml_col(col) for col in cols[1:]]))
 least = lambda *cols: Column(tdml_column=_get_tdml_col(cols[0]).least(*[_get_tdml_col(col) for col in cols[1:]]))
 sqrt = lambda col: Column(tdml_column=_get_tdml_col(col).sqrt())
 abs = lambda col: Column(tdml_column=_get_tdml_col(col).abs())
@@ -403,133 +305,65 @@
     # col can be a string or Column Object.
     if isinstance(col, str):
         return Column(tdml_column=_SQLColumnExpression(col).desc().nulls_first())
     return Column(tdml_column=col._tdml_column.desc().nulls_first())
 
 desc_nulls_last = lambda col: desc(col)
 
-avg = type("avg", (_TeradatamlspkFunction, ), {})
-any_value = type("any_value", (_TeradatamlspkFunction, ), {})
-row_number = type("row_number", (_TeradatamlspkFunction, ), {})
-count = type("count", (_TeradatamlspkFunction, ), {})
-rank = type("rank", (_TeradatamlspkFunction, ), {})
-cume_dist = type("cume_dist", (_TeradatamlspkFunction, ), {})
-dense_rank = type("dense_rank", (_TeradatamlspkFunction, ), {})
-percent_rank = type("percent_rank", (_TeradatamlspkFunction, ), {})
-max = type("max", (_TeradatamlspkFunction, ), {})
-mean = type("mean", (_TeradatamlspkFunction, ), {})
-min = type("min", (_TeradatamlspkFunction, ), {})
-sum = type("sum", (_TeradatamlspkFunction, ), {})
-
-class std(_TeradatamlspkFunction):
-    def __init__(self, col):
-        super().__init__(col, False, False)
-
+def _get_agg_expr(col, func_name, **params):
+    """Helper function to get aggregate function expression. """
+    # Params can have teradatamlspk Column. Convert it to teradataml Column.
+    params = {pname: pcol._tdml_column if isinstance(pcol, Column) else pcol for pname,pcol in params.items()}
+    tdml_column = getattr(_SQLColumnExpression(col), func_name)(**params) if isinstance(col, str)\
+        else getattr(col._tdml_column, func_name)(**params)
+    expr_ = _SQLColumnExpression(col) if isinstance(col, str) else col._tdml_column
+    agg_func_params = {"name": func_name, **params}
+    return {"tdml_column": tdml_column, "expr_": expr_, "agg_func_params": agg_func_params}
+
+avg = lambda col: Column(**_get_agg_expr(col, "mean", distinct=False))
+any_value = lambda col, ignoreNulls=None: Column(**_get_agg_expr(col, "first_value"))
+row_number = lambda: Column(**_get_agg_expr('col_', "row_number"))
+count = lambda col: Column(**_get_agg_expr(col, "count", distinct=False))
+rank = lambda: Column(**_get_agg_expr('col_', "rank"))
+cume_dist = lambda: Column(**_get_agg_expr('col_', "cume_dist"))
+dense_rank = lambda: Column(**_get_agg_expr('col_', "dense_rank"))
+percent_rank = lambda: Column(**_get_agg_expr('col_', "percent_rank"))
+max = lambda col: Column(**_get_agg_expr(col, "max", distinct=False))
+mean = lambda col: Column(**_get_agg_expr(col, "mean", distinct=False))
+min = lambda col: Column(**_get_agg_expr(col, "min", distinct=False))
+sum = lambda col: Column(**_get_agg_expr(col, "sum", distinct=False))
+std = lambda col: Column(**_get_agg_expr(col, "std", distinct=False, population=False))
 stddev = std
 stddev_samp = std
-
-class stddev_pop(_TeradatamlspkFunction):
-    def __init__(self, col):
-        super().__init__(col, False, True)
-
-class var_pop(_TeradatamlspkFunction):
-    def __init__(self, col):
-        super().__init__(col, False, True)
-
-class var_samp(_TeradatamlspkFunction):
-    def __init__(self, col):
-        super().__init__(col, False, False)
-
+stddev_pop = lambda col: Column(**_get_agg_expr(col, "std", distinct=False, population=True))
+var_pop = lambda col: Column(**_get_agg_expr(col, "var", distinct=False, population=True))
+var_samp = lambda col: Column(**_get_agg_expr(col, "var", distinct=False, population=False))
 variance = var_samp
-
-class lag(_TeradatamlspkFunction):
-    def __init__(self, col, offset=1, default=None):
-        super().__init__(col, offset, default)
-
-class lead(_TeradatamlspkFunction):
-    def __init__(self, col, offset=1, default=None):
-        super().__init__(col, offset, default)
-
-class count_distinct(_TeradatamlspkFunction):
-    def __init__(self, col, *cols):
-        super().__init__(col, True)
-
+lag = lambda col, offset=1, default=None: Column(**_get_agg_expr(col, "lag", offset_value=offset, default_expression=default))
+lead = lambda col, offset=1, default=None: Column(**_get_agg_expr(col, "lead", offset_value=offset, default_expression=default))
+count_distinct = lambda col: Column(**_get_agg_expr(col, "count", distinct=True))
 countDistinct = count_distinct
-
-class corr(_TeradatamlspkFunction):
-    def __init__(self, col1, col2):
-        super().__init__(col1, col2)
-
-class covar_pop(_TeradatamlspkFunction):
-    def __init__(self, col1, col2):
-        super().__init__(col1, col2)
-
-class covar_samp(_TeradatamlspkFunction):
-    def __init__(self, col1, col2):
-        super().__init__(col1, col2)
-
-class first(_TeradatamlspkFunction):
-    def __init__(self, col, ignorenulls=False):
-        super().__init__(col)
-
-class first_value(_TeradatamlspkFunction):
-    def __init__(self, col, ignorenulls=False):
-        super().__init__(col)
-
-class last(_TeradatamlspkFunction):
-    def __init__(self, col, ignorenulls=False):
-        super().__init__(col)
-
-class last_value(_TeradatamlspkFunction):
-    def __init__(self, col, ignorenulls=False):
-        super().__init__(col)
-
-class regr_avgx(_TeradatamlspkFunction):
-    def __init__(self, y, x):
-        super().__init__(y, x)
-
-class regr_avgy(_TeradatamlspkFunction):
-    def __init__(self, y, x):
-        super().__init__(y, x)
-
-class regr_count(_TeradatamlspkFunction):
-    def __init__(self, y, x):
-        super().__init__(y, x)
-
-class regr_intercept(_TeradatamlspkFunction):
-    def __init__(self, y, x):
-        super().__init__(y, x)
-
-class regr_r2(_TeradatamlspkFunction):
-    def __init__(self, y, x):
-        super().__init__(y, x)
-
-class regr_slope(_TeradatamlspkFunction):
-    def __init__(self, y, x):
-        super().__init__(y, x)
-
-class regr_sxx(_TeradatamlspkFunction):
-    def __init__(self, y, x):
-        super().__init__(y, x)
-
-class regr_sxy(_TeradatamlspkFunction):
-    def __init__(self, y, x):
-        super().__init__(y, x)
-
-class regr_syy(_TeradatamlspkFunction):
-    def __init__(self, y, x):
-        super().__init__(y, x)
-
-class sum_distinct(_TeradatamlspkFunction):
-    def __init__(self, col):
-        super().__init__(col, True)
-
-class sumDistinct(_TeradatamlspkFunction):
-    def __init__(self, col):
-        super().__init__(col, True)
+corr = lambda col1, col2: Column(**_get_agg_expr(col1, "corr", expression=col2))
+covar_pop = lambda col1, col2: Column(**_get_agg_expr(col1, "covar_pop", expression=col2))
+covar_samp = lambda col1, col2: Column(**_get_agg_expr(col1, "covar_samp", expression=col2))
+first = lambda col, ignorenulls=False: Column(**_get_agg_expr(col, "first_value"))
+first_value = lambda col, ignorenulls=False: Column(**_get_agg_expr(col, "first_value"))
+last = lambda col, ignorenulls=False: Column(**_get_agg_expr(col, "last_value"))
+last_value = lambda col, ignorenulls=False: Column(**_get_agg_expr(col, "last_value"))
+regr_avgx = lambda y, x: Column(**_get_agg_expr(y, "regr_avgx", expression=x))
+regr_avgy = lambda y, x: Column(**_get_agg_expr(y, "regr_avgy", expression=x))
+regr_count = lambda y, x: Column(**_get_agg_expr(y, "regr_count", expression=x))
+regr_intercept = lambda y, x: Column(**_get_agg_expr(y, "regr_intercept", expression=x))
+regr_r2 = lambda y, x: Column(**_get_agg_expr(y, "regr_r2", expression=x))
+regr_slope = lambda y, x: Column(**_get_agg_expr(y, "regr_slope", expression=x))
+regr_sxx = lambda y, x: Column(**_get_agg_expr(y, "regr_sxx", expression=x))
+regr_sxy = lambda y, x: Column(**_get_agg_expr(y, "regr_sxy", expression=x))
+regr_syy = lambda y, x: Column(**_get_agg_expr(y, "regr_syy", expression=x))
+sum_distinct = lambda col: Column(**_get_agg_expr(col, "sum", distinct=True))
+sumDistinct = sum_distinct
 
 ascii = lambda col: Column(tdml_column=_get_tdml_col(col).substr(1,1).ascii())
 base64 = unknown
 btrim = lambda str, trim=lit(" "): Column(tdml_column=_get_tdml_col(str).trim(_get_tdml_col(trim)))
 char = lambda col: Column(tdml_column=_get_tdml_col(col).char())
 character_length = lambda str: Column(tdml_column=_get_tdml_col(str).character_length())
 char_length = character_length
@@ -644,8 +478,20 @@
 xpath_boolean = not_implemented
 xpath_double = not_implemented
 xpath_float = not_implemented
 xpath_int = not_implemented
 xpath_long = not_implemented
 xpath_number = not_implemented
 xpath_short = not_implemented
-xpath_string = not_implemented
+xpath_string = not_implemented
+
+def time_difference(col1, col2):
+    """Returns the difference between two timestamps in seconds. """
+    col1 = col1 if isinstance(col1, str) else col1._tdml_column.compile()
+    col2 = col2 if isinstance(col2, str) else col2._tdml_column.compile()
+    s = """
+    (EXTRACT(DAY FROM ({0} - {1} DAY(4) TO SECOND)) * 86400) +
+    (EXTRACT(HOUR FROM ({0} - {1} DAY(4) TO SECOND)) * 3600) +
+    (EXTRACT(MINUTE FROM ({0} - {1} DAY(4) TO SECOND)) * 60) +
+    (EXTRACT(SECOND FROM ({0} - {1} DAY(4)TO SECOND)))
+    """.format(col1, col2)
+    return Column(tdml_column=_SQLColumnExpression(literal_column(s, type_=FLOAT())))
```

## teradatamlspk/sql/group.py

```diff
@@ -1,12 +1,12 @@
 from sqlalchemy import literal_column
 from teradatamlspk.sql import dataframe
-from teradatamlspk.sql.functions import _TeradatamlspkFunction
 from teradatamlspk.sql.column import Column
 from teradataml.dataframe.sql import _SQLColumnExpression
+from teradatamlspk.sql.dataframe_utils import DataFrameUtils as df_utils
 
 
 
 class GroupedData:
     def __init__(self, data=None, non_grouped_data=None):
         self._data = data
         self.avg = self.mean
@@ -16,48 +16,48 @@
     def count(self):
         if self.__pivot_data:
             return self.__new_pivot_df((), "count")
 
         return dataframe.DataFrame(self._data.assign(count = literal_column("count(*)")))
 
     def agg(self, *expr):
-        column_expression = isinstance(expr[0], _TeradatamlspkFunction)
+        column_expression = isinstance(expr[0], Column)
         if self.__pivot_data:
             if column_expression:
-                # Storing the column in __pivot_columns extracting from _TeradatamlspkFunction
+                # Storing the column in __pivot_columns extracting from Column
                 for fun in expr:
-                    # Checking parameter is of instnace Column or string
-                    if isinstance(fun._params['cols'][0], Column):
-                        self.__pivot_columns.update({fun._params['cols'][0]._tdml_column.compile():
-                                                         fun._params['cols'][0]._tdml_column})
-                    else:
-                        self.__pivot_columns.update({fun._params['cols'][0]:
-                                                         _SQLColumnExpression(literal_column(fun._params['cols'][0]))})
+                    self.__pivot_columns.update({fun.alias_name if fun.alias_name else fun.expr_.compile(): fun.expr_})
             else:
                 # If dictionary is given directly storing the columns
                 self.__pivot_columns.update({col: self._data[col] for col in expr[0]})
 
             df = self._non_grouped_data.assign(drop_columns=True, **self.__pivot_columns)
             cols=list()
             if column_expression:
                 for fun in expr:
-                    # Checking parameter is of instance Column or string
-                    if isinstance(fun._params['cols'][0], Column):
-                        cols.append(getattr(df[fun._params['cols'][0]._tdml_column.compile()],
-                                            fun.__class__.__name__)())
-                    else:
-                        cols.append(getattr(df[fun._params['cols'][0]],
-                                            fun.__class__.__name__)())
+                    cols.append(getattr(df[fun.alias_name if fun.alias_name else fun.expr_.compile()],
+                                        fun.agg_func_params['name'])())
 
             else:
                 cols = [getattr(df[key], value)() for key, value in expr[0].items()]
             return dataframe.DataFrame(df.pivot(columns=self.__pivot_data, aggfuncs=cols))
+
         if column_expression:
-            return dataframe.DataFrame(self._data.agg(list(expr)))
-        return dataframe.DataFrame(self._data.agg(*expr))
+            expr_dict, alias_dict = df_utils._get_agg_expr_alias_dict(expr)
+            df = dataframe.DataFrame(self._data.agg(expr_dict))
+            return df.withColumnsRenamed(alias_dict)
+        
+        # Creating a new column with any value only when user passes {'*': 'count'}
+        # We can use that column for count in agg function.
+        _new_data = self._data
+        if expr[0].get('*', None):
+            _new_data = self._data.assign(all_rows_ = 1)
+            expr[0].pop('*')
+            expr[0]['all_rows_'] = 'count'
+        return dataframe.DataFrame(_new_data.agg(*expr))
 
     def max(self, *cols):
         if self.__pivot_data:
             return self.__new_pivot_df(cols, "max")
 
         if cols:
             df = self._data.select(list(set(self._data.groupby_column_list).union(set(cols))))
```

## teradatamlspk/sql/readwriter.py

```diff
@@ -8,17 +8,19 @@
 #
 #
 # Version: 1.0
 #
 # ##################################################################
 from collections import OrderedDict
 import os
-from teradataml import ReadNOS, WriteNOS, read_csv, DataFrame as tdml_DataFrame
+from teradataml import ReadNOS, WriteNOS, read_csv, DataFrame as tdml_DataFrame, copy_to_sql
+from teradataml import get_connection, execute_sql, db_list_tables, db_drop_table
 from teradataml.common.utils import UtilFuncs
 
+
 class DataFrameReader:
     def __init__(self, **kwargs):
 
         # Always
         #   'format' holds the format for file. - parquet, csv, orc etc.
         #   'mode' holds the format for file. - parquet, csv, orc etc.
         self.__params = {**kwargs}
@@ -43,14 +45,17 @@
         return tdmlspk_DataFrame(ReadNOS(**self.__params).result)
 
     def options(self, **options):
         self.__params.update(**options)
         return DataFrameReader(**self.__params)
 
     def load(self, path, format = 'parquet', schema = None, **options):
+        if os.path.exists(path):
+            self.__params.pop('stored_as', None)
+            return self.csv(path = path, schema = schema)
         if not self.__params.get("stored_as"):
             self.__params.update({"stored_as": "PARQUET" if format == 'parquet' else 'TEXTFILE'})
         self.__params.update({"location": path})
         from teradatamlspk.sql.dataframe import DataFrame as tdmlspk_DataFrame
         return tdmlspk_DataFrame(ReadNOS(**self.__params).result)
 
     def csv(self, path, **kwargs):
@@ -103,7 +108,145 @@
             # Then return teradatamlspk DataFrame.
             return tdmlspk_DataFrame(res[0])
 
         self.__params.update({"location": path, "stored_as": "TEXTFILE"})
 
         return tdmlspk_DataFrame(ReadNOS(**self.__params).result)
 
+class DataFrameWriterV2:
+    def __init__(self, df, table, schema_name=None):
+        self._df = df
+        self._table_name = table
+        self._schema_name = schema_name
+
+    def append(self):
+        """ Appends data to table. If table does not exist, function creates the table. """
+        if self.__is_table_exists():
+            sql = """
+            INSERT INTO {}
+            {}
+            """.format(self._get_table_name(), self._df.show_query())
+        else:
+            sql = """
+            CREATE MULTISET TABLE {} AS
+            ({})
+            WITH DATA
+            NO PRIMARY INDEX
+            """.format(self._get_table_name(), self._df.show_query())
+
+        execute_sql(sql)
+
+    def create(self):
+        """Function to create a table based from DataFrame."""
+        sql = """
+        CREATE MULTISET TABLE {} AS
+        ({})
+        WITH DATA
+        NO PRIMARY INDEX
+        """.format(self._get_table_name(), self._df.show_query())
+        execute_sql(sql)
+
+    def createOrReplace(self):
+        """
+        Function replaces the table with DataFrame if table alreay exists.
+        Else, function creates the table with DataFrame.
+        """
+        try:
+            db_drop_table(self._table_name, schema_name=self._schema_name)
+        except Exception:
+            pass
+
+        sql = """
+            CREATE MULTISET TABLE {} AS
+            ({})
+            WITH DATA
+            NO PRIMARY INDEX
+            """.format(self._get_table_name(), self._df.show_query())
+        execute_sql(sql)
+
+    def replace(self):
+        """Function replaces the table with DataFrame. """
+        db_drop_table(self._table_name, schema_name=self._schema_name)
+        sql = """
+        CREATE MULTISET TABLE {} AS
+        ({})
+        WITH DATA
+        NO PRIMARY INDEX
+        """.format(self._get_table_name(), self._df.show_query())
+        execute_sql(sql)
+
+    def __is_table_exists(self):
+        """Internal function to check whether table exists or not. """
+        connection = get_connection()
+        return connection.dialect.has_table(connection, table_name=self._table_name, schema=self._schema_name)
+
+    def _get_table_name(self):
+        return '"{}"."{}"'.format(self._schema_name, self._table_name) if self._schema_name else self._table_name
+
+
+class DataFrameWriter:
+    """
+    teradatamlspk writer class enables users to write DataFrame on aws, azure, google cloud etc.
+    using WriteNOS capability in csv or parquet format.
+    """
+    def __init__(self, **kwargs):
+        """Constructor for writer class."""
+        self.__params = {**kwargs}
+
+    def format(self, source):
+        """Specifies the underlying output data source."""
+        self.__params.update({"stored_as": source.upper()})
+        return DataFrameWriter(**self.__params)
+
+    def json(self, path, **kwargs):
+        """Saves the content of the DataFrame in JSON format (JSON Lines text format or newline-delimited JSON) at the specified path."""
+        self.__params.update({"location": path, "stored_as": "JSON"})
+        WriteNOS(**self.__params)
+
+    def option(self, key, value):
+        """Adds an output option for the underlying data source."option" are same as WriteNOS."""
+        self.__params[key] = value
+        return DataFrameWriter(**self.__params)
+
+    def parquet(self, path, **kwargs):
+        """Saves the content of the DataFrame in Parquet format at the specified path."""
+        self.__params.update({"location": path, "stored_as": "PARQUET"})
+        WriteNOS(**self.__params)
+
+    def options(self, **options):
+        """Adds output options for the underlying data source."options" are same as WriteNOS parameters."""
+        self.__params.update(**options)
+        return DataFrameWriter(**self.__params)
+
+    def save(self, path, format = None, mode = None, partitionBy = None,**options):
+        """
+        Saves the contents of the DataFrame to a data source.
+        The data source is specified by the format and a set of options.
+        """
+        if not self.__params.get("stored_as"):
+            self.__params.update({"stored_as": format.upper()})
+        self.__params.update({"location": path})
+        WriteNOS(**self.__params)
+
+    def csv(self, path, **kwargs):
+        """Saves the content of the DataFrame in CSV format at the specified path."""
+        self.__params.update({"location": path, "stored_as": "CSV"})
+        WriteNOS(**self.__params)
+
+    def saveAsTable(self, name, format = None, mode = 'ignore', partitionBy = None, **options):
+        mode_dict ={"ignore": "fail", "overwrite": "replace", "append": "append"}
+        _args = {"df": self.__params["data"],
+                 "table_name": name,
+                 "if_exists": mode_dict[self.__params["mode"]] if self.__params.get("mode", None) else mode_dict[mode]}
+        if self.__params.get("types", None):
+            _args.update({"types": self.__params["types"]})
+        copy_to_sql(**_args)
+
+    def insertInto(self, tableName, overwrite = False):
+        _args = {"df": self.__params["data"],
+                 "table_name": tableName,
+                 "if_exists": ("replace" if self.__params["overwrite"] else "append")
+                    if self.__params.get("overwrite", None)
+                    else ("replace" if overwrite else "append")}
+        if self.__params.get("types", None):
+            _args.update({"types": self.__params["types"]})
+        copy_to_sql(**_args)
```

## teradatamlspk/sql/session.py

```diff
@@ -6,16 +6,16 @@
 # Primary Owner: Pradeep Garre(pradeep.garre@teradata.com)
 # Secondary Owner: Adithya Avvaru(adithya.avvaru@teradata.com)
 #
 #
 # Version: 1.0
 #
 # ##################################################################
-import teradataml
-from teradataml import create_context, get_context, remove_context, configure, display
+import teradataml, re
+from teradataml import create_context, get_context, remove_context, configure, display, execute_sql
 from teradataml.dataframe.dataframe import DataFrame as tdml_DataFrame
 from teradatamlspk.sql.dataframe import DataFrame
 from teradatamlspk.sql.catalog import Catalog
 from teradatamlspk.sql.readwriter import DataFrameReader
 from teradatamlspk.conf import RuntimeConfig
 
 
@@ -87,15 +87,19 @@
     @property
     def readStream(self):
         raise NotImplemented("The API is not supported in Teradata Vantage.")
 
     def sql(self, sqlQuery, args=None, kwargs=None):
         if args:
             sqlQuery = sqlQuery.format(**args)
-        return DataFrame(tdml_DataFrame.from_query(sqlQuery))
+        if re.search(r'^\s*select', sqlQuery, re.IGNORECASE):
+            return DataFrame(tdml_DataFrame.from_query(sqlQuery))
+        else:
+            return execute_sql(sqlQuery)
+
 
     @property
     def read(self):
         return DataFrameReader()
 
     @staticmethod
     def stop():
```

## teradatamlspk/sql/utils.py

```diff
@@ -42,9 +42,9 @@
 
     # td_type is an instance of some Data Type.
     if isinstance(spark_type, (VarcharType)):
         return SPARK_TO_TD_TYPES.get(spark_type.__class__)(spark_type.length)
 
     if isinstance(spark_type, DecimalType):
         return SPARK_TO_TD_TYPES.get(spark_type.__class__)(spark_type.precision, spark_type.scale)
-
+    
     return SPARK_TO_TD_TYPES.get(spark_type.__class__)()
```

## Comparing `teradatamlspk-20.0.0.0.dist-info/RECORD` & `teradatamlspk-20.0.0.1.dist-info/RECORD`

 * *Files 15% similar despite different names*

```diff
@@ -1,42 +1,42 @@
 teradatamlspk/LICENSE-3RD-PARTY.pdf,sha256=qMbYTwByYPtgSYmWMAtaS3Jdp4ZUoOz22XRE8FCyNFI,99421
-teradatamlspk/LICENSE.pdf,sha256=LSCdTEUR3IabhBHY5B0hkxNclcJ9gjZf-5OueK6N3Ak,66696
-teradatamlspk/README.md,sha256=f3e7Wpx6EhHCCQJgOotUh91ro8X58nTMabPxN8Lh304,5098
+teradatamlspk/LICENSE.pdf,sha256=l00SYExU8OP-eegB09iQm1zmYbhpwh5o2_woddMWsUQ,66850
+teradatamlspk/README.md,sha256=Uh3ZQAHJhzRAANKJSw42HJbKIyrEHx3t10woXpXCTEw,7261
 teradatamlspk/__init__.py,sha256=DHaSKp3QKwH02g3QcHzdXg867_ObsBDWS5rKjKYLVW0,554
-teradatamlspk/_version.py,sha256=MuJT4oqICBEy-E-G4u8dzwxdF3ihakmvDq2RK4-95Z4,20
+teradatamlspk/_version.py,sha256=iLKtkwcd6eYDb2dkebl0quMvucadTD_hZMD-hRZJRc8,20
 teradatamlspk/conf.py,sha256=pSliglYVtMKZZxXf5gBR3uXQx9yFl0qf8FP_TphhzQc,1150
 teradatamlspk/context.py,sha256=tGfXJB7hRNgVdBjXT4ObzqN9h3yjLv_JnqxqqUeMJSE,4527
 teradatamlspk/exceptions.py,sha256=EnpXewdz6hU4Xy5NhX--1jfrvpIoc4x77Y_tRKizOIk,511
 teradatamlspk/storagelevel.py,sha256=g2t1mpomkBVUzkCgAU58HK87ZS7VT4TFNfh-IxbZM_4,1385
 teradatamlspk/common/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-teradatamlspk/common/constants.py,sha256=dquJPUIP-9AEW72cNkWGfgnGNZABxAPQ-ftiFNJ7twI,7672
-teradatamlspk/converter/__init__.py,sha256=FPRGnwNx6-eMgO-3sRQVixF-bcdcmT-wuKiRrJ2ExHE,24347
-teradatamlspk/converter/user_guide.json,sha256=T5RtueJGpnXEKPBBMsoBF3qqimJWS1CVRKCNTUqKr8M,32122
+teradatamlspk/common/constants.py,sha256=zgtjaduc6ZcQfdgZBEymrJcWOXuDuyoAxMXWLFTN21g,6080
+teradatamlspk/converter/__init__.py,sha256=hzEZ0WG-ZJ5teAlwN8uCymCXAIE28QphUjZAOmZ8RVs,35057
+teradatamlspk/converter/user_guide.json,sha256=Xg4axt4wMnzUK10QyBvNTa7uwSmyEQ1zXAkzBxhLfJg,34165
 teradatamlspk/ml/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-teradatamlspk/ml/constants.py,sha256=27TM5YjO74H5PMRi7XZ5PJc9pxfx4ngs8eDVM0oz1vc,33646
-teradatamlspk/ml/util.py,sha256=jvqSMaa2EcvrhJ1z-STS56rfpCqvF3z_CL8sSSdlFEQ,48208
+teradatamlspk/ml/constants.py,sha256=fYTIbwW6dIEPwuGY74hRZvB5qzXNQnk-qZ43lZGocSg,33648
+teradatamlspk/ml/util.py,sha256=C5iQvpdcCeN6iymx9tkI-viS5IC9wZY59skDWhvCizE,47820
 teradatamlspk/ml/classification/__init__.py,sha256=CzOBFBFvlPOkdaE7in_7g2igilZr_JHvOso5_ML8bGc,3397
 teradatamlspk/ml/clustering/__init__.py,sha256=eOT0Y8PVqKtYp7ytiQpc5UobI3rVQhLLmWPuhyZCZ4A,1397
-teradatamlspk/ml/evaluation/__init__.py,sha256=I0_Vx3nkLpN9Yb_WyEEQcoeQ_49lgXeVNcMYG50V4gc,4272
+teradatamlspk/ml/evaluation/__init__.py,sha256=6XddVKHZK1SdzXEa25xMb-Zqp78avnvoWajyZ_UyC14,4472
 teradatamlspk/ml/feature/__init__.py,sha256=_Akd3SF0zM-fEG9IHVMKlOM_K57Arv9bnN8oMU-7btA,4624
 teradatamlspk/ml/param/__init__.py,sha256=aE_Q_u_aCDj1aWWhHdEZ8cBqM90HXtosU9mJ82d1QSw,15432
 teradatamlspk/ml/regression/__init__.py,sha256=DCi4-FyISKERZ2bu_QNqsDM7ikMweh6Ff_2ezHNdXAc,1961
 teradatamlspk/ml/stat/__init__.py,sha256=LQEcI_CcmSuPEpMeWu7XsxNaAwdOlVO2YuYcRSLxCtg,128
 teradatamlspk/sql/__init__.py,sha256=okTd75PHPDlHMvUUFE5c3d8X5WxjSmxsN9yijg4jzss,695
 teradatamlspk/sql/catalog.py,sha256=EtfVma2S6AzgwSii3g24sPizQwGggCVKt40SS_M0pbY,2082
-teradatamlspk/sql/column.py,sha256=xXXE4Vc_8VTID9BubSEldFsk61UEmJoMhrHpT2dmb_0,8021
-teradatamlspk/sql/constants.py,sha256=sYD19PYNnd74DcikAfIXO2DxDKnq82vzGHPs6Munetw,2525
+teradatamlspk/sql/column.py,sha256=mpvUi8ZOZtSlYRHUJpsqermHkOcgbdPgfcK9iQyHj_M,9847
+teradatamlspk/sql/constants.py,sha256=gY9CEH4EHvCgPczMaRinIlIJiLzxcBZt0b5K-yZWKr4,4475
 teradatamlspk/sql/context.py,sha256=UkZw14WzkoX6RNZYC4FrHyvEeCKkOR1of_MUH03Yr0E,2757
-teradatamlspk/sql/dataframe.py,sha256=d5ZH2KHRMtTdJ6xFjObIP2GBOeG0WiPXfI065T8v_Uw,41070
-teradatamlspk/sql/dataframe_utils.py,sha256=_aYmcJ9VQtt4o2YHLym75V4YsieRTF1IGExw2mm8f70,2088
-teradatamlspk/sql/functions.py,sha256=qUt9Jo33RXPAZQsUufytEYc-dE1LmaZJjdRJdLb6m-k,29264
-teradatamlspk/sql/group.py,sha256=I1Y3Lp4avnzU-VOWNJSIjDdTD5qksHxnl0TADeAaICs,5824
-teradatamlspk/sql/readwriter.py,sha256=Bj2q_1sW34MPQyfpEQtLKzEa9F-vXnwTM6j2WjtdTSs,4207
-teradatamlspk/sql/session.py,sha256=pACOgy5KOObtsvOuDOz-Qi-UbVYw4Tjj--gcIBjn_1I,4432
+teradatamlspk/sql/dataframe.py,sha256=IiAxCETqvNK6I-G0CxBxMbDaQ2bC2E1PpgAAcFPnS2I,43124
+teradatamlspk/sql/dataframe_utils.py,sha256=Jc9PEoII4FISleVFatV8TGy275ML2JWQ0zR6a5dFv4g,3441
+teradatamlspk/sql/functions.py,sha256=w7N-1kUgpXW1xgznrLivj5OurOJ70xyBUCKA82XmdXg,26448
+teradatamlspk/sql/group.py,sha256=DbPmiyKeUafq0T_aOsLwoeZdWqXgjMhFr_PslHpwAqA,5538
+teradatamlspk/sql/readwriter.py,sha256=ORBUZZ0bNmgFmwcdwEFClcNtlAjTEjlC91nWQZ5ww1k,9861
+teradatamlspk/sql/session.py,sha256=BnrCnU8Ec_GuUaYfbWurdVDEILNnp_ycEQkZ0OutIYY,4571
 teradatamlspk/sql/types.py,sha256=YNlVX8G-dCQ7NKbu9zS8zPWgLvsMJgNUZF8Pqtx52tM,41272
-teradatamlspk/sql/utils.py,sha256=Bu75O4p_PHlYNZcl9Dgwd4HkbNWsFVdbJJRFyU5lvPM,1679
+teradatamlspk/sql/utils.py,sha256=yrm6bfsA-NxETD7NiAq68DtzVFvZ_Xka6MNk9npjasc,1683
 teradatamlspk/sql/window.py,sha256=j-t3sXbEwVbEpazSVMOrJXCNTYgvONgHH1j6szgF_8Q,2772
-teradatamlspk-20.0.0.0.dist-info/METADATA,sha256=R7tkRLxam9nl8OmEA2F024OgCDR3ZUf2bp4V9Y_5lcU,5860
-teradatamlspk-20.0.0.0.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
-teradatamlspk-20.0.0.0.dist-info/top_level.txt,sha256=p4937rmAD8Aj7K3bhmYT_mIgqwuq5kUYJ4rjHbGUzzM,14
-teradatamlspk-20.0.0.0.dist-info/zip-safe,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
-teradatamlspk-20.0.0.0.dist-info/RECORD,,
+teradatamlspk-20.0.0.1.dist-info/METADATA,sha256=wWqYA1L9s0Xs62eNJYljEXgezc8fUKqxbubnuMcy9x4,8023
+teradatamlspk-20.0.0.1.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
+teradatamlspk-20.0.0.1.dist-info/top_level.txt,sha256=p4937rmAD8Aj7K3bhmYT_mIgqwuq5kUYJ4rjHbGUzzM,14
+teradatamlspk-20.0.0.1.dist-info/zip-safe,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
+teradatamlspk-20.0.0.1.dist-info/RECORD,,
```

