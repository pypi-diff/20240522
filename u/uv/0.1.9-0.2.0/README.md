# Comparing `tmp/uv-0.1.9.tar.gz` & `tmp/uv-0.2.0.tar.gz`

## Comparing `uv-0.1.9.tar` & `uv-0.2.0.tar`

### file list

```diff
@@ -1,280 +1,391 @@
--rw-r--r--   0     1001      127     1178 2024-02-23 02:30:21.000000 uv-0.1.9/crates/gourgeist/Cargo.toml
--rw-r--r--   0     1001      127      740 2024-02-23 02:30:21.000000 uv-0.1.9/crates/gourgeist/README.md
--rw-r--r--   0     1001      127      350 2024-02-23 02:30:21.000000 uv-0.1.9/crates/gourgeist/benchmark.sh
--rw-r--r--   0     1001      127      629 2024-02-23 02:30:21.000000 uv-0.1.9/crates/gourgeist/compare_in_git.sh
--rw-r--r--   0     1001      127      163 2024-02-23 02:30:21.000000 uv-0.1.9/crates/gourgeist/imasnake.py
--rw-r--r--   0     1001      127       96 2024-02-23 02:30:21.000000 uv-0.1.9/crates/gourgeist/oranda.json
--rw-r--r--   0     1001      127     4375 2024-02-23 02:30:21.000000 uv-0.1.9/crates/gourgeist/src/_virtualenv.py
--rw-r--r--   0     1001      127     2237 2024-02-23 02:30:21.000000 uv-0.1.9/crates/gourgeist/src/activate
--rw-r--r--   0     1001      127       20 2024-02-23 02:30:21.000000 uv-0.1.9/crates/gourgeist/src/activator/.gitattributes
--rw-r--r--   0     1001      127     3388 2024-02-23 02:30:21.000000 uv-0.1.9/crates/gourgeist/src/activator/activate
--rw-r--r--   0     1001      127     2259 2024-02-23 02:30:21.000000 uv-0.1.9/crates/gourgeist/src/activator/activate.bat
--rw-r--r--   0     1001      127     2655 2024-02-23 02:30:21.000000 uv-0.1.9/crates/gourgeist/src/activator/activate.csh
--rw-r--r--   0     1001      127     4218 2024-02-23 02:30:21.000000 uv-0.1.9/crates/gourgeist/src/activator/activate.fish
--rw-r--r--   0     1001      127     3903 2024-02-23 02:30:21.000000 uv-0.1.9/crates/gourgeist/src/activator/activate.nu
--rw-r--r--   0     1001      127     2813 2024-02-23 02:30:21.000000 uv-0.1.9/crates/gourgeist/src/activator/activate.ps1
--rw-r--r--   0     1001      127     2457 2024-02-23 02:30:21.000000 uv-0.1.9/crates/gourgeist/src/activator/activate_this.py
--rw-r--r--   0     1001      127     1728 2024-02-23 02:30:21.000000 uv-0.1.9/crates/gourgeist/src/activator/deactivate.bat
--rw-r--r--   0     1001      127     1215 2024-02-23 02:30:21.000000 uv-0.1.9/crates/gourgeist/src/activator/pydoc.bat
--rw-r--r--   0     1001      127    10863 2024-02-23 02:30:21.000000 uv-0.1.9/crates/gourgeist/src/bare.rs
--rw-r--r--   0     1001      127     1766 2024-02-23 02:30:21.000000 uv-0.1.9/crates/gourgeist/src/interpreter.rs
--rw-r--r--   0     1001      127     1911 2024-02-23 02:30:21.000000 uv-0.1.9/crates/gourgeist/src/lib.rs
--rw-r--r--   0     1001      127     1891 2024-02-23 02:30:21.000000 uv-0.1.9/crates/gourgeist/src/main.rs
--rw-r--r--   0     1001      127  1072094 2024-02-23 02:30:21.000000 uv-0.1.9/crates/gourgeist/static/gourgeist.png
--rw-r--r--   0     1001      127      994 2024-02-23 02:30:21.000000 uv-0.1.9/crates/gourgeist/venv_checker.py
--rw-r--r--   0     1001      127     1189 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-interpreter/Cargo.toml
--rw-r--r--   0     1001      127     1779 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-interpreter/src/cfg.rs
--rw-r--r--   0     1001      127     3168 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-interpreter/src/get_interpreter_info.py
--rw-r--r--   0     1001      127    19106 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-interpreter/src/interpreter.rs
--rw-r--r--   0     1001      127     2444 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-interpreter/src/lib.rs
--rw-r--r--   0     1001      127     2158 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-interpreter/src/python_platform.rs
--rw-r--r--   0     1001      127    19212 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-interpreter/src/python_query.rs
--rw-r--r--   0     1001      127     3476 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-interpreter/src/python_version.rs
--rw-r--r--   0     1001      127     4792 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-interpreter/src/virtual_env.rs
--rw-r--r--   0     1001      127     2453 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/Cargo.toml
--rw-r--r--   0     1001      127    12186 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/candidate_selector.rs
--rw-r--r--   0     1001      127     1000 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/constraints.rs
--rw-r--r--   0     1001      127      610 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/dependency_mode.rs
--rw-r--r--   0     1001      127     1149 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/editables.rs
--rw-r--r--   0     1001      127    10250 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/error.rs
--rw-r--r--   0     1001      127     9510 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/finder.rs
--rw-r--r--   0     1001      127      948 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/lib.rs
--rw-r--r--   0     1001      127     1278 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/manifest.rs
--rw-r--r--   0     1001      127     1815 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/options.rs
--rw-r--r--   0     1001      127     1475 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/overrides.rs
--rw-r--r--   0     1001      127     1062 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/pins.rs
--rw-r--r--   0     1001      127     3855 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/prerelease_mode.rs
--rw-r--r--   0     1001      127     6979 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/pubgrub/dependencies.rs
--rw-r--r--   0     1001      127     1063 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/pubgrub/distribution.rs
--rw-r--r--   0     1001      127      498 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/pubgrub/mod.rs
--rw-r--r--   0     1001      127     4763 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/pubgrub/package.rs
--rw-r--r--   0     1001      127     1007 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/pubgrub/priority.rs
--rw-r--r--   0     1001      127    29263 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/pubgrub/report.rs
--rw-r--r--   0     1001      127     4189 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/pubgrub/specifier.rs
--rw-r--r--   0     1001      127     2780 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/python_requirement.rs
--rw-r--r--   0     1001      127    20084 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/resolution.rs
--rw-r--r--   0     1001      127     1613 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/resolution_mode.rs
--rw-r--r--   0     1001      127      881 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/resolver/index.rs
--rw-r--r--   0     1001      127    48590 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/resolver/mod.rs
--rw-r--r--   0     1001      127     7127 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/resolver/provider.rs
--rw-r--r--   0     1001      127     1629 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/resolver/reporter.rs
--rw-r--r--   0     1001      127     3601 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/resolver/urls.rs
--rw-r--r--   0     1001      127    17789 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/version_map.rs
--rw-r--r--   0     1001      127     1656 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/src/yanks.rs
--rw-r--r--   0     1001      127    20177 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-resolver/tests/resolver.rs
--rw-r--r--   0     1001      127     1041 2024-02-23 02:30:21.000000 uv-0.1.9/crates/pep440-rs/Cargo.toml
--rw-r--r--   0     1001      127    10173 2024-02-23 02:30:21.000000 uv-0.1.9/crates/pep440-rs/License-Apache
--rw-r--r--   0     1001      127     1293 2024-02-23 02:30:21.000000 uv-0.1.9/crates/pep440-rs/License-BSD
--rw-r--r--   0     1001      127     2947 2024-02-23 02:30:21.000000 uv-0.1.9/crates/pep440-rs/Readme.md
--rw-r--r--   0     1001      127     2105 2024-02-23 02:30:21.000000 uv-0.1.9/crates/pep440-rs/python/Readme.md
--rw-r--r--   0     1001      127     2843 2024-02-23 02:30:21.000000 uv-0.1.9/crates/pep440-rs/src/lib.rs
--rw-r--r--   0     1001      127   123697 2024-02-23 02:30:21.000000 uv-0.1.9/crates/pep440-rs/src/version.rs
--rw-r--r--   0     1001      127    57538 2024-02-23 02:30:21.000000 uv-0.1.9/crates/pep440-rs/src/version_specifier.rs
--rw-r--r--   0     1001      127     2220 2024-02-23 02:30:21.000000 uv-0.1.9/crates/pep508-rs/Cargo.toml
--rw-r--r--   0     1001      127    10173 2024-02-23 02:30:21.000000 uv-0.1.9/crates/pep508-rs/License-Apache
--rw-r--r--   0     1001      127     1293 2024-02-23 02:30:21.000000 uv-0.1.9/crates/pep508-rs/License-BSD
--rw-r--r--   0     1001      127     3039 2024-02-23 02:30:21.000000 uv-0.1.9/crates/pep508-rs/Readme.md
--rw-r--r--   0     1001      127    54264 2024-02-23 02:30:21.000000 uv-0.1.9/crates/pep508-rs/src/lib.rs
--rw-r--r--   0     1001      127    65624 2024-02-23 02:30:21.000000 uv-0.1.9/crates/pep508-rs/src/marker.rs
--rw-r--r--   0     1001      127    12345 2024-02-23 02:30:21.000000 uv-0.1.9/crates/pep508-rs/src/verbatim_url.rs
--rw-r--r--   0     1001      127      507 2024-02-23 02:30:21.000000 uv-0.1.9/crates/cache-key/Cargo.toml
--rw-r--r--   0     1001      127     8335 2024-02-23 02:30:21.000000 uv-0.1.9/crates/cache-key/src/cache_key.rs
--rw-r--r--   0     1001      127     9335 2024-02-23 02:30:21.000000 uv-0.1.9/crates/cache-key/src/canonical_url.rs
--rw-r--r--   0     1001      127      620 2024-02-23 02:30:21.000000 uv-0.1.9/crates/cache-key/src/digest.rs
--rw-r--r--   0     1001      127      191 2024-02-23 02:30:21.000000 uv-0.1.9/crates/cache-key/src/lib.rs
--rw-r--r--   0     1001      127     1986 2024-02-23 02:30:21.000000 uv-0.1.9/crates/cache-key/src/stable_hash.rs
--rw-r--r--   0     1001      127      136 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-auth/Cargo.toml
--rw-r--r--   0     1001      127     5870 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-auth/src/lib.rs
--rw-r--r--   0     1001      127     1290 2024-02-23 02:30:21.000000 uv-0.1.9/crates/distribution-types/Cargo.toml
--rw-r--r--   0     1001      127     1048 2024-02-23 02:30:21.000000 uv-0.1.9/crates/distribution-types/src/any.rs
--rw-r--r--   0     1001      127     5648 2024-02-23 02:30:21.000000 uv-0.1.9/crates/distribution-types/src/cached.rs
--rw-r--r--   0     1001      127     8047 2024-02-23 02:30:21.000000 uv-0.1.9/crates/distribution-types/src/direct_url.rs
--rw-r--r--   0     1001      127      983 2024-02-23 02:30:21.000000 uv-0.1.9/crates/distribution-types/src/editable.rs
--rw-r--r--   0     1001      127      551 2024-02-23 02:30:21.000000 uv-0.1.9/crates/distribution-types/src/error.rs
--rw-r--r--   0     1001      127     3398 2024-02-23 02:30:21.000000 uv-0.1.9/crates/distribution-types/src/file.rs
--rw-r--r--   0     1001      127     2054 2024-02-23 02:30:21.000000 uv-0.1.9/crates/distribution-types/src/id.rs
--rw-r--r--   0     1001      127     9984 2024-02-23 02:30:21.000000 uv-0.1.9/crates/distribution-types/src/index_url.rs
--rw-r--r--   0     1001      127     5890 2024-02-23 02:30:21.000000 uv-0.1.9/crates/distribution-types/src/installed.rs
--rw-r--r--   0     1001      127    27984 2024-02-23 02:30:21.000000 uv-0.1.9/crates/distribution-types/src/lib.rs
--rw-r--r--   0     1001      127    12193 2024-02-23 02:30:21.000000 uv-0.1.9/crates/distribution-types/src/prioritized_distribution.rs
--rw-r--r--   0     1001      127     4342 2024-02-23 02:30:21.000000 uv-0.1.9/crates/distribution-types/src/resolution.rs
--rw-r--r--   0     1001      127     6640 2024-02-23 02:30:21.000000 uv-0.1.9/crates/distribution-types/src/traits.rs
--rw-r--r--   0     1001      127     1466 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-installer/Cargo.toml
--rw-r--r--   0     1001      127     9791 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-installer/src/downloader.rs
--rw-r--r--   0     1001      127     1827 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-installer/src/editable.rs
--rw-r--r--   0     1001      127     2600 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-installer/src/installer.rs
--rw-r--r--   0     1001      127      464 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-installer/src/lib.rs
--rw-r--r--   0     1001      127    22276 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-installer/src/plan.rs
--rw-r--r--   0     1001      127    15927 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-installer/src/site_packages.rs
--rw-r--r--   0     1001      127      402 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-installer/src/uninstall.rs
--rw-r--r--   0     1001      127      751 2024-02-23 02:30:21.000000 uv-0.1.9/crates/distribution-filename/Cargo.toml
--rw-r--r--   0     1001      127     2339 2024-02-23 02:30:21.000000 uv-0.1.9/crates/distribution-filename/src/lib.rs
--rw-r--r--   0     1001      127      420 2024-02-23 02:30:21.000000 uv-0.1.9/crates/distribution-filename/src/snapshots/distribution_filename__wheel__tests__ok_build_tag.snap
--rw-r--r--   0     1001      127      539 2024-02-23 02:30:21.000000 uv-0.1.9/crates/distribution-filename/src/snapshots/distribution_filename__wheel__tests__ok_multiple_tags.snap
--rw-r--r--   0     1001      127      398 2024-02-23 02:30:21.000000 uv-0.1.9/crates/distribution-filename/src/snapshots/distribution_filename__wheel__tests__ok_single_tags.snap
--rw-r--r--   0     1001      127     7557 2024-02-23 02:30:21.000000 uv-0.1.9/crates/distribution-filename/src/source_dist.rs
--rw-r--r--   0     1001      127    11224 2024-02-23 02:30:21.000000 uv-0.1.9/crates/distribution-filename/src/wheel.rs
--rw-r--r--   0     1001      127      667 2024-02-23 02:30:21.000000 uv-0.1.9/crates/platform-host/Cargo.toml
--rw-r--r--   0     1001      127     6522 2024-02-23 02:30:21.000000 uv-0.1.9/crates/platform-host/src/lib.rs
--rw-r--r--   0     1001      127    11062 2024-02-23 02:30:21.000000 uv-0.1.9/crates/platform-host/src/linux.rs
--rw-r--r--   0     1001      127     1311 2024-02-23 02:30:21.000000 uv-0.1.9/crates/platform-host/src/mac_os.rs
--rw-r--r--   0     1001      127      995 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-cache/Cargo.toml
--rw-r--r--   0     1001      127      195 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-cache/src/by_timestamp.rs
--rw-r--r--   0     1001      127     1376 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-cache/src/cli.rs
--rw-r--r--   0     1001      127    27484 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-cache/src/lib.rs
--rw-r--r--   0     1001      127     3967 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-cache/src/removal.rs
--rw-r--r--   0     1001      127     1663 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-cache/src/timestamp.rs
--rw-r--r--   0     1001      127     2973 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-cache/src/wheel.rs
--rw-r--r--   0     1001      127      439 2024-02-23 02:30:21.000000 uv-0.1.9/crates/platform-tags/Cargo.toml
--rw-r--r--   0     1001      127    18693 2024-02-23 02:30:21.000000 uv-0.1.9/crates/platform-tags/src/lib.rs
--rw-r--r--   0     1001      127      456 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-warnings/Cargo.toml
--rw-r--r--   0     1001      127     1701 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-warnings/src/lib.rs
--rw-r--r--   0     1001      127     1948 2024-02-23 02:30:21.000000 uv-0.1.9/crates/install-wheel-rs/Cargo.toml
--rw-r--r--   0     1001      127      515 2024-02-23 02:30:21.000000 uv-0.1.9/crates/install-wheel-rs/Readme.md
--rw-r--r--   0     1001      127     3753 2024-02-23 02:30:21.000000 uv-0.1.9/crates/install-wheel-rs/src/install_location.rs
--rw-r--r--   0     1001      127     7743 2024-02-23 02:30:21.000000 uv-0.1.9/crates/install-wheel-rs/src/lib.rs
--rw-r--r--   0     1001      127    18729 2024-02-23 02:30:21.000000 uv-0.1.9/crates/install-wheel-rs/src/linker.rs
--rw-r--r--   0     1001      127     2166 2024-02-23 02:30:21.000000 uv-0.1.9/crates/install-wheel-rs/src/main.rs
--rw-r--r--   0     1001      127     1288 2024-02-23 02:30:21.000000 uv-0.1.9/crates/install-wheel-rs/src/pip_compileall.py
--rw-r--r--   0     1001      127     2827 2024-02-23 02:30:21.000000 uv-0.1.9/crates/install-wheel-rs/src/python_bindings.rs
--rw-r--r--   0     1001      127      446 2024-02-23 02:30:21.000000 uv-0.1.9/crates/install-wheel-rs/src/record.rs
--rw-r--r--   0     1001      127     4118 2024-02-23 02:30:21.000000 uv-0.1.9/crates/install-wheel-rs/src/script.rs
--rw-r--r--   0     1001      127     5436 2024-02-23 02:30:21.000000 uv-0.1.9/crates/install-wheel-rs/src/uninstall.rs
--rw-r--r--   0     1001      127    49021 2024-02-23 02:30:21.000000 uv-0.1.9/crates/install-wheel-rs/src/wheel.rs
--rw-r--r--   0     1001      127     1293 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-build/Cargo.toml
--rw-r--r--   0     1001      127       17 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-build/.gitignore
--rw-r--r--   0     1001      127    35515 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-build/src/lib.rs
--rw-r--r--   0     1001      127      626 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-fs/Cargo.toml
--rw-r--r--   0     1001      127     7970 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-fs/src/lib.rs
--rw-r--r--   0     1001      127     2629 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-fs/src/path.rs
--rw-r--r--   0     1001      127      867 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-traits/Cargo.toml
--rw-r--r--   0     1001      127    17668 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-traits/src/lib.rs
--rw-r--r--   0     1001      127      818 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-extract/Cargo.toml
--rw-r--r--   0     1001      127      661 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-extract/src/error.rs
--rw-r--r--   0     1001      127       98 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-extract/src/lib.rs
--rw-r--r--   0     1001      127     6885 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-extract/src/stream.rs
--rw-r--r--   0     1001      127     3562 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-extract/src/sync.rs
--rw-r--r--   0     1001      127     1489 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-extract/src/tar.rs
--rw-r--r--   0     1001      127     5650 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-extract/src/vendor/cloneable_seekable_reader.rs
--rw-r--r--   0     1001      127      112 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-extract/src/vendor/mod.rs
--rw-r--r--   0     1001      127     1020 2024-02-23 02:30:21.000000 uv-0.1.9/crates/pypi-types/Cargo.toml
--rw-r--r--   0     1001      127     2947 2024-02-23 02:30:21.000000 uv-0.1.9/crates/pypi-types/src/base_url.rs
--rw-r--r--   0     1001      127     4361 2024-02-23 02:30:21.000000 uv-0.1.9/crates/pypi-types/src/direct_url.rs
--rw-r--r--   0     1001      127    13755 2024-02-23 02:30:21.000000 uv-0.1.9/crates/pypi-types/src/lenient_requirement.rs
--rw-r--r--   0     1001      127      208 2024-02-23 02:30:21.000000 uv-0.1.9/crates/pypi-types/src/lib.rs
--rw-r--r--   0     1001      127     6749 2024-02-23 02:30:21.000000 uv-0.1.9/crates/pypi-types/src/metadata.rs
--rw-r--r--   0     1001      127     4814 2024-02-23 02:30:21.000000 uv-0.1.9/crates/pypi-types/src/simple_json.rs
--rw-r--r--   0     1001      127      380 2024-02-23 02:30:21.000000 uv-0.1.9/crates/once-map/Cargo.toml
--rw-r--r--   0     1001      127     2863 2024-02-23 02:30:21.000000 uv-0.1.9/crates/once-map/src/lib.rs
--rw-r--r--   0     1001      127     1785 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-client/Cargo.toml
--rw-r--r--   0     1001      127      112 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-client/README.md
--rw-r--r--   0     1001      127    29382 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-client/src/cached_client.rs
--rw-r--r--   0     1001      127     7003 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-client/src/error.rs
--rw-r--r--   0     1001      127    14031 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-client/src/flat_index.rs
--rw-r--r--   0     1001      127    32917 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-client/src/html.rs
--rw-r--r--   0     1001      127    29342 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-client/src/httpcache/control.rs
--rw-r--r--   0     1001      127    60040 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-client/src/httpcache/mod.rs
--rw-r--r--   0     1001      127      527 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-client/src/lib.rs
--rw-r--r--   0     1001      127     1245 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-client/src/middleware.rs
--rw-r--r--   0     1001      127    33190 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-client/src/registry_client.rs
--rw-r--r--   0     1001      127     4495 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-client/src/remote_metadata.rs
--rw-r--r--   0     1001      127    12175 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-client/src/rkyvutil.rs
--rw-r--r--   0     1001      127     1076 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-client/tests/remote_metadata.rs
--rw-r--r--   0     1001      127     1545 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-distribution/Cargo.toml
--rw-r--r--   0     1001      127    19122 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-distribution/src/distribution_database.rs
--rw-r--r--   0     1001      127     4096 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-distribution/src/download.rs
--rw-r--r--   0     1001      127     2304 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-distribution/src/error.rs
--rw-r--r--   0     1001      127     5195 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-distribution/src/index/built_wheel_index.rs
--rw-r--r--   0     1001      127     1393 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-distribution/src/index/cached_wheel.rs
--rw-r--r--   0     1001      127      162 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-distribution/src/index/mod.rs
--rw-r--r--   0     1001      127     5568 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-distribution/src/index/registry_wheel_index.rs
--rw-r--r--   0     1001      127      387 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-distribution/src/lib.rs
--rw-r--r--   0     1001      127      606 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-distribution/src/locks.rs
--rw-r--r--   0     1001      127     1206 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-distribution/src/reporter.rs
--rw-r--r--   0     1001      127     1541 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-distribution/src/source/built_wheel_metadata.rs
--rw-r--r--   0     1001      127      473 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-distribution/src/source/manifest.rs
--rw-r--r--   0     1001      127    39757 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-distribution/src/source/mod.rs
--rw-r--r--   0     1001      127      859 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-distribution/src/unzip.rs
--rw-r--r--   0     1001      127      306 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-normalize/Cargo.toml
--rw-r--r--   0     1001      127     1652 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-normalize/src/extra_name.rs
--rw-r--r--   0     1001      127     5673 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-normalize/src/lib.rs
--rw-r--r--   0     1001      127     2899 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-normalize/src/package_name.rs
--rw-r--r--   0     1001      127      983 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-git/Cargo.toml
--rw-r--r--   0     1001      127    60543 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-git/src/git.rs
--rw-r--r--   0     1001      127    37540 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-git/src/known_hosts.rs
--rw-r--r--   0     1001      127     3707 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-git/src/lib.rs
--rw-r--r--   0     1001      127      902 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-git/src/sha.rs
--rw-r--r--   0     1001      127     4946 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-git/src/source.rs
--rw-r--r--   0     1001      127     1362 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-git/src/util/errors.rs
--rw-r--r--   0     1001      127      733 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-git/src/util/mod.rs
--rw-r--r--   0     1001      127     7297 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-git/src/util/retry.rs
--rw-r--r--   0     1001      127     1160 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/Cargo.toml
--rw-r--r--   0     1001      127    47300 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/src/lib.rs
--rw-r--r--   0     1001      127     4626 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-basic.txt.snap
--rw-r--r--   0     1001      127     2092 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-constraints-a.txt.snap
--rw-r--r--   0     1001      127     1703 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-constraints-b.txt.snap
--rw-r--r--   0     1001      127     1960 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-editable.txt.snap
--rw-r--r--   0     1001      127      240 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-empty.txt.snap
--rw-r--r--   0     1001      127     3068 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-for-poetry.txt.snap
--rw-r--r--   0     1001      127     1298 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-include-a.txt.snap
--rw-r--r--   0     1001      127      569 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-include-b.txt.snap
--rw-r--r--   0     1001      127    11076 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-poetry-with-hashes.txt.snap
--rw-r--r--   0     1001      127     1703 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-small.txt.snap
--rw-r--r--   0     1001      127     1960 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-whitespace.txt.snap
--rw-r--r--   0     1001      127     4626 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-basic.txt.snap
--rw-r--r--   0     1001      127     2092 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-constraints-a.txt.snap
--rw-r--r--   0     1001      127     1703 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-constraints-b.txt.snap
--rw-r--r--   0     1001      127      240 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-empty.txt.snap
--rw-r--r--   0     1001      127     3068 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-for-poetry.txt.snap
--rw-r--r--   0     1001      127     1298 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-include-a.txt.snap
--rw-r--r--   0     1001      127      569 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-include-b.txt.snap
--rw-r--r--   0     1001      127    11076 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-poetry-with-hashes.txt.snap
--rw-r--r--   0     1001      127     1703 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-small.txt.snap
--rw-r--r--   0     1001      127     1960 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-whitespace.txt.snap
--rw-r--r--   0     1001      127       90 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/test-data/requirements-txt/basic.txt
--rw-r--r--   0     1001      127       45 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/test-data/requirements-txt/constraints-a.txt
--rw-r--r--   0     1001      127       27 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/test-data/requirements-txt/constraints-b.txt
--rw-r--r--   0     1001      127      183 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/test-data/requirements-txt/editable.txt
--rw-r--r--   0     1001      127        0 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/test-data/requirements-txt/empty.txt
--rw-r--r--   0     1001      127      101 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/test-data/requirements-txt/for-poetry.txt
--rw-r--r--   0     1001      127       43 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/test-data/requirements-txt/include-a.txt
--rw-r--r--   0     1001      127        5 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/test-data/requirements-txt/include-b.txt
--rw-r--r--   0     1001      127     1183 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/test-data/requirements-txt/poetry-with-hashes.txt
--rw-r--r--   0     1001      127       66 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/test-data/requirements-txt/small.txt
--rw-r--r--   0     1001      127      183 2024-02-23 02:30:21.000000 uv-0.1.9/crates/requirements-txt/test-data/requirements-txt/whitespace.txt
--rw-r--r--   0     1001      127     1224 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-dispatch/Cargo.toml
--rw-r--r--   0     1001      127     9722 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv-dispatch/src/lib.rs
--rw-r--r--   0        0        0     3385 1970-01-01 00:00:00.000000 uv-0.1.9/crates/uv/Cargo.toml
--rw-r--r--   0     1001      127     4359 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv/src/commands/cache_clean.rs
--rw-r--r--   0     1001      127      219 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv/src/commands/cache_dir.rs
--rw-r--r--   0     1001      127     1903 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv/src/commands/mod.rs
--rw-r--r--   0     1001      127    16188 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv/src/commands/pip_compile.rs
--rw-r--r--   0     1001      127     1471 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv/src/commands/pip_freeze.rs
--rw-r--r--   0     1001      127    22091 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv/src/commands/pip_install.rs
--rw-r--r--   0     1001      127    15288 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv/src/commands/pip_sync.rs
--rw-r--r--   0     1001      127     5027 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv/src/commands/pip_uninstall.rs
--rw-r--r--   0     1001      127     9445 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv/src/commands/reporters.rs
--rw-r--r--   0     1001      127     6957 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv/src/commands/venv.rs
--rw-r--r--   0     1001      127    11209 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv/src/compat/mod.rs
--rw-r--r--   0     1001      127     1859 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv/src/confirm.rs
--rw-r--r--   0     1001      127     3879 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv/src/logging.rs
--rw-r--r--   0     1001      127    41927 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv/src/main.rs
--rw-r--r--   0     1001      127     1223 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv/src/printer.rs
--rw-r--r--   0     1001      127    14759 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv/src/requirements.rs
--rw-r--r--   0     1001      127    14363 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv/tests/common/mod.rs
--rw-r--r--   0     1001      127   139730 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv/tests/pip_compile.rs
--rw-r--r--   0     1001      127    19181 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv/tests/pip_compile_scenarios.rs
--rw-r--r--   0     1001      127    50014 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv/tests/pip_install.rs
--rw-r--r--   0     1001      127   109909 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv/tests/pip_install_scenarios.rs
--rw-r--r--   0     1001      127    83331 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv/tests/pip_sync.rs
--rw-r--r--   0     1001      127    14422 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv/tests/pip_uninstall.rs
--rw-r--r--   0     1001      127    20780 2024-02-23 02:30:21.000000 uv-0.1.9/crates/uv/tests/venv.rs
--rw-r--r--   0     1001      127   119809 2024-02-23 02:30:21.000000 uv-0.1.9/Cargo.lock
--rw-r--r--   0        0        0     6700 1970-01-01 00:00:00.000000 uv-0.1.9/Cargo.toml
--rw-r--r--   0     1001      127     2197 2024-02-23 02:30:21.000000 uv-0.1.9/pyproject.toml
--rw-r--r--   0     1001      127      927 2024-02-23 02:30:21.000000 uv-0.1.9/python/uv/__main__.py
--rw-r--r--   0     1001      127      806 2024-02-23 02:30:21.000000 uv-0.1.9/python/uv/__init__.py
--rw-r--r--   0     1001      127        0 2024-02-23 02:30:21.000000 uv-0.1.9/python/uv/py.typed
--rw-r--r--   0     1001      127    17069 2024-02-23 02:30:21.000000 uv-0.1.9/README.md
--rw-r--r--   0     1001      127       29 2024-02-23 02:30:21.000000 uv-0.1.9/rust-toolchain.toml
--rw-r--r--   0        0        0    18436 1970-01-01 00:00:00.000000 uv-0.1.9/PKG-INFO
+-rw-r--r--   0     1001      127      284 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-normalize/Cargo.toml
+-rw-r--r--   0     1001      127     1620 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-normalize/src/extra_name.rs
+-rw-r--r--   0     1001      127     5633 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-normalize/src/lib.rs
+-rw-r--r--   0     1001      127     2847 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-normalize/src/package_name.rs
+-rw-r--r--   0     1001      127     2046 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/Cargo.toml
+-rw-r--r--   0     1001      127        1 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/bare.rs
+-rw-r--r--   0     1001      127    17958 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/candidate_selector.rs
+-rw-r--r--   0     1001      127      610 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/dependency_mode.rs
+-rw-r--r--   0     1001      127     1140 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/dependency_provider.rs
+-rw-r--r--   0     1001      127     1378 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/editables.rs
+-rw-r--r--   0     1001      127    13709 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/error.rs
+-rw-r--r--   0     1001      127     2743 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/exclude_newer.rs
+-rw-r--r--   0     1001      127     1551 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/exclusions.rs
+-rw-r--r--   0     1001      127     8000 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/flat_index.rs
+-rw-r--r--   0     1001      127     1260 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/lib.rs
+-rw-r--r--   0     1001      127    54297 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/lock.rs
+-rw-r--r--   0     1001      127     8769 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/manifest.rs
+-rw-r--r--   0     1001      127    12444 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/marker.rs
+-rw-r--r--   0     1001      127     2149 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/options.rs
+-rw-r--r--   0     1001      127     1122 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/pins.rs
+-rw-r--r--   0     1001      127     6368 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/preferences.rs
+-rw-r--r--   0     1001      127     4046 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/prerelease_mode.rs
+-rw-r--r--   0     1001      127    12211 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/pubgrub/dependencies.rs
+-rw-r--r--   0     1001      127     1088 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/pubgrub/distribution.rs
+-rw-r--r--   0     1001      127      541 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/pubgrub/mod.rs
+-rw-r--r--   0     1001      127     7814 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/pubgrub/package.rs
+-rw-r--r--   0     1001      127     5707 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/pubgrub/priority.rs
+-rw-r--r--   0     1001      127    40255 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/pubgrub/report.rs
+-rw-r--r--   0     1001      127     4890 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/pubgrub/specifier.rs
+-rw-r--r--   0     1001      127     1163 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/python_requirement.rs
+-rw-r--r--   0     1001      127     4971 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/redirect.rs
+-rw-r--r--   0     1001      127    11323 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/resolution/display.rs
+-rw-r--r--   0     1001      127    24688 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/resolution/graph.rs
+-rw-r--r--   0     1001      127     4444 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/resolution/mod.rs
+-rw-r--r--   0     1001      127     1878 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/resolution_mode.rs
+-rw-r--r--   0     1001      127     8210 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/resolver/batch_prefetch.rs
+-rw-r--r--   0     1001      127     1172 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/resolver/index.rs
+-rw-r--r--   0     1001      127    14010 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/resolver/locals.rs
+-rw-r--r--   0     1001      127    66870 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/resolver/mod.rs
+-rw-r--r--   0     1001      127     8756 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/resolver/provider.rs
+-rw-r--r--   0     1001      127     1673 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/resolver/reporter.rs
+-rw-r--r--   0     1001      127     8952 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/resolver/urls.rs
+-rw-r--r--   0     1001      127     3280 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/snapshots/uv_resolver__lock__tests__hash_optional_missing.snap
+-rw-r--r--   0     1001      127      437 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/snapshots/uv_resolver__lock__tests__hash_required_present.snap
+-rw-r--r--   0     1001      127    23582 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/version_map.rs
+-rw-r--r--   0     1001      127     1752 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/src/yanks.rs
+-rw-r--r--   0     1001      127    22764 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-resolver/tests/resolver.rs
+-rw-r--r--   0     1001      127      507 2024-05-22 18:48:21.000000 uv-0.2.0/crates/cache-key/Cargo.toml
+-rw-r--r--   0     1001      127     8335 2024-05-22 18:48:21.000000 uv-0.2.0/crates/cache-key/src/cache_key.rs
+-rw-r--r--   0     1001      127    12317 2024-05-22 18:48:21.000000 uv-0.2.0/crates/cache-key/src/canonical_url.rs
+-rw-r--r--   0     1001      127      620 2024-05-22 18:48:21.000000 uv-0.2.0/crates/cache-key/src/digest.rs
+-rw-r--r--   0     1001      127      191 2024-05-22 18:48:21.000000 uv-0.2.0/crates/cache-key/src/lib.rs
+-rw-r--r--   0     1001      127     1986 2024-05-22 18:48:21.000000 uv-0.2.0/crates/cache-key/src/stable_hash.rs
+-rw-r--r--   0     1001      127     2085 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pep508-rs/Cargo.toml
+-rw-r--r--   0     1001      127    10173 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pep508-rs/License-Apache
+-rw-r--r--   0     1001      127     1293 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pep508-rs/License-BSD
+-rw-r--r--   0     1001      127     3039 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pep508-rs/Readme.md
+-rw-r--r--   0     1001      127     4301 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pep508-rs/src/cursor.rs
+-rw-r--r--   0     1001      127    62005 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pep508-rs/src/lib.rs
+-rw-r--r--   0     1001      127    94653 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pep508-rs/src/marker.rs
+-rw-r--r--   0     1001      127      743 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pep508-rs/src/origin.rs
+-rw-r--r--   0     1001      127    12553 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pep508-rs/src/unnamed.rs
+-rw-r--r--   0     1001      127    18647 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pep508-rs/src/verbatim_url.rs
+-rw-r--r--   0     1001      127      752 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-auth/Cargo.toml
+-rw-r--r--   0     1001      127     9542 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-auth/src/cache.rs
+-rw-r--r--   0     1001      127    11219 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-auth/src/credentials.rs
+-rw-r--r--   0     1001      127     9340 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-auth/src/keyring.rs
+-rw-r--r--   0     1001      127      992 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-auth/src/lib.rs
+-rw-r--r--   0     1001      127    45581 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-auth/src/middleware.rs
+-rw-r--r--   0     1001      127     4661 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-auth/src/realm.rs
+-rw-r--r--   0     1001      127      456 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-warnings/Cargo.toml
+-rw-r--r--   0     1001      127     1701 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-warnings/src/lib.rs
+-rw-r--r--   0     1001      127     1025 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-cache/Cargo.toml
+-rw-r--r--   0     1001      127      509 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-cache/src/archive.rs
+-rw-r--r--   0     1001      127      195 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-cache/src/by_timestamp.rs
+-rw-r--r--   0     1001      127     1768 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-cache/src/cli.rs
+-rw-r--r--   0     1001      127    35224 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-cache/src/lib.rs
+-rw-r--r--   0     1001      127     6138 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-cache/src/removal.rs
+-rw-r--r--   0     1001      127     1663 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-cache/src/timestamp.rs
+-rw-r--r--   0     1001      127     2538 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-cache/src/wheel.rs
+-rw-r--r--   0     1001      127      411 2024-05-22 18:48:21.000000 uv-0.2.0/crates/once-map/Cargo.toml
+-rw-r--r--   0     1001      127     3940 2024-05-22 18:48:21.000000 uv-0.2.0/crates/once-map/src/lib.rs
+-rw-r--r--   0     1001      127     1654 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-requirements/Cargo.toml
+-rw-r--r--   0     1001      127     2206 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-requirements/src/confirm.rs
+-rw-r--r--   0     1001      127      318 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-requirements/src/lib.rs
+-rw-r--r--   0     1001      127    10108 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-requirements/src/lookahead.rs
+-rw-r--r--   0     1001      127    31011 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-requirements/src/pyproject.rs
+-rw-r--r--   0     1001      127     6877 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-requirements/src/source_tree.rs
+-rw-r--r--   0     1001      127     7083 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-requirements/src/sources.rs
+-rw-r--r--   0     1001      127    14908 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-requirements/src/specification.rs
+-rw-r--r--   0     1001      127    13090 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-requirements/src/unnamed.rs
+-rw-r--r--   0     1001      127     1638 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-requirements/src/upgrade.rs
+-rw-r--r--   0     1001      127    25085 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-requirements/src/workspace.rs
+-rw-r--r--   0     1001      127      608 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-filename/Cargo.toml
+-rw-r--r--   0     1001      127     2291 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-filename/src/lib.rs
+-rw-r--r--   0     1001      127      420 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-filename/src/snapshots/distribution_filename__wheel__tests__ok_build_tag.snap
+-rw-r--r--   0     1001      127      539 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-filename/src/snapshots/distribution_filename__wheel__tests__ok_multiple_tags.snap
+-rw-r--r--   0     1001      127      398 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-filename/src/snapshots/distribution_filename__wheel__tests__ok_single_tags.snap
+-rw-r--r--   0     1001      127     7565 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-filename/src/source_dist.rs
+-rw-r--r--   0     1001      127    10961 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-filename/src/wheel.rs
+-rw-r--r--   0     1001      127      322 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-version/Cargo.toml
+-rw-r--r--   0     1001      127      355 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-version/src/lib.rs
+-rw-r--r--   0     1001      127     1004 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pep440-rs/Cargo.toml
+-rw-r--r--   0     1001      127    10173 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pep440-rs/License-Apache
+-rw-r--r--   0     1001      127     1293 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pep440-rs/License-BSD
+-rw-r--r--   0     1001      127     2947 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pep440-rs/Readme.md
+-rw-r--r--   0     1001      127     2105 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pep440-rs/python/Readme.md
+-rw-r--r--   0     1001      127     2922 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pep440-rs/src/lib.rs
+-rw-r--r--   0     1001      127   131052 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pep440-rs/src/version.rs
+-rw-r--r--   0     1001      127    56892 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pep440-rs/src/version_specifier.rs
+-rw-r--r--   0     1001      127     1504 2024-05-22 18:48:21.000000 uv-0.2.0/crates/install-wheel-rs/Cargo.toml
+-rw-r--r--   0     1001      127      515 2024-05-22 18:48:21.000000 uv-0.2.0/crates/install-wheel-rs/Readme.md
+-rw-r--r--   0     1001      127     4332 2024-05-22 18:48:21.000000 uv-0.2.0/crates/install-wheel-rs/src/lib.rs
+-rw-r--r--   0     1001      127    20181 2024-05-22 18:48:21.000000 uv-0.2.0/crates/install-wheel-rs/src/linker.rs
+-rw-r--r--   0     1001      127     7104 2024-05-22 18:48:21.000000 uv-0.2.0/crates/install-wheel-rs/src/metadata.rs
+-rw-r--r--   0     1001      127      474 2024-05-22 18:48:21.000000 uv-0.2.0/crates/install-wheel-rs/src/record.rs
+-rw-r--r--   0     1001      127     5550 2024-05-22 18:48:21.000000 uv-0.2.0/crates/install-wheel-rs/src/script.rs
+-rw-r--r--   0     1001      127    11211 2024-05-22 18:48:21.000000 uv-0.2.0/crates/install-wheel-rs/src/uninstall.rs
+-rw-r--r--   0     1001      127    34841 2024-05-22 18:48:21.000000 uv-0.2.0/crates/install-wheel-rs/src/wheel.rs
+-rw-r--r--   0     1001      127      847 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-fs/Cargo.toml
+-rw-r--r--   0     1001      127     1397 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-fs/src/cachedir.rs
+-rw-r--r--   0     1001      127    11067 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-fs/src/lib.rs
+-rw-r--r--   0     1001      127    10859 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-fs/src/path.rs
+-rw-r--r--   0     1001      127     1807 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-client/Cargo.toml
+-rw-r--r--   0     1001      127      112 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-client/README.md
+-rw-r--r--   0     1001      127     7109 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-client/src/base_client.rs
+-rw-r--r--   0     1001      127    30512 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-client/src/cached_client.rs
+-rw-r--r--   0     1001      127    10417 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-client/src/error.rs
+-rw-r--r--   0     1001      127     9211 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-client/src/flat_index.rs
+-rw-r--r--   0     1001      127    46032 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-client/src/html.rs
+-rw-r--r--   0     1001      127    29250 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-client/src/httpcache/control.rs
+-rw-r--r--   0     1001      127    60607 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-client/src/httpcache/mod.rs
+-rw-r--r--   0     1001      127      648 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-client/src/lib.rs
+-rw-r--r--   0     1001      127     5110 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-client/src/linehaul.rs
+-rw-r--r--   0     1001      127     1228 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-client/src/middleware.rs
+-rw-r--r--   0     1001      127    35668 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-client/src/registry_client.rs
+-rw-r--r--   0     1001      127     4579 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-client/src/remote_metadata.rs
+-rw-r--r--   0     1001      127    12019 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-client/src/rkyvutil.rs
+-rw-r--r--   0     1001      127     1146 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-client/tests/remote_metadata.rs
+-rw-r--r--   0     1001      127     8183 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-client/tests/user_agent_version.rs
+-rw-r--r--   0     1001      127     1183 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-types/Cargo.toml
+-rw-r--r--   0     1001      127     2511 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-types/src/annotation.rs
+-rw-r--r--   0     1001      127     1048 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-types/src/any.rs
+-rw-r--r--   0     1001      127     4711 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-types/src/buildable.rs
+-rw-r--r--   0     1001      127     6718 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-types/src/cached.rs
+-rw-r--r--   0     1001      127     2474 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-types/src/editable.rs
+-rw-r--r--   0     1001      127      985 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-types/src/error.rs
+-rw-r--r--   0     1001      127     7205 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-types/src/file.rs
+-rw-r--r--   0     1001      127     2831 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-types/src/hash.rs
+-rw-r--r--   0     1001      127     3845 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-types/src/id.rs
+-rw-r--r--   0     1001      127    14472 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-types/src/index_url.rs
+-rw-r--r--   0     1001      127    12536 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-types/src/installed.rs
+-rw-r--r--   0     1001      127    35260 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-types/src/lib.rs
+-rw-r--r--   0     1001      127    10788 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-types/src/parsed_url.rs
+-rw-r--r--   0     1001      127    20821 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-types/src/prioritized_distribution.rs
+-rw-r--r--   0     1001      127     8302 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-types/src/requirement.rs
+-rw-r--r--   0     1001      127     1228 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-types/src/requirements.rs
+-rw-r--r--   0     1001      127     6611 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-types/src/resolution.rs
+-rw-r--r--   0     1001      127     6410 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-types/src/resolved.rs
+-rw-r--r--   0     1001      127     3233 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-types/src/specified_requirement.rs
+-rw-r--r--   0     1001      127     8193 2024-05-22 18:48:21.000000 uv-0.2.0/crates/distribution-types/src/traits.rs
+-rw-r--r--   0     1001      127      897 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-extract/Cargo.toml
+-rw-r--r--   0     1001      127     1078 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-extract/src/error.rs
+-rw-r--r--   0     1001      127     4113 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-extract/src/hash.rs
+-rw-r--r--   0     1001      127      112 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-extract/src/lib.rs
+-rw-r--r--   0     1001      127     9419 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-extract/src/stream.rs
+-rw-r--r--   0     1001      127     3565 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-extract/src/sync.rs
+-rw-r--r--   0     1001      127     1489 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-extract/src/tar.rs
+-rw-r--r--   0     1001      127     5685 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-extract/src/vendor/cloneable_seekable_reader.rs
+-rw-r--r--   0     1001      127      112 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-extract/src/vendor/mod.rs
+-rw-r--r--   0     1001      127      841 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-configuration/Cargo.toml
+-rw-r--r--   0     1001      127     1014 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-configuration/src/authentication.rs
+-rw-r--r--   0     1001      127    10722 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-configuration/src/build_options.rs
+-rw-r--r--   0     1001      127     1054 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-configuration/src/concurrency.rs
+-rw-r--r--   0     1001      127     9254 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-configuration/src/config_settings.rs
+-rw-r--r--   0     1001      127     1698 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-configuration/src/constraints.rs
+-rw-r--r--   0     1001      127      437 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-configuration/src/lib.rs
+-rw-r--r--   0     1001      127     4059 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-configuration/src/name_specifiers.rs
+-rw-r--r--   0     1001      127     1706 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-configuration/src/overrides.rs
+-rw-r--r--   0     1001      127     2339 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-configuration/src/package_options.rs
+-rw-r--r--   0     1001      127      782 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-configuration/src/preview.rs
+-rw-r--r--   0     1001      127    11893 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-configuration/src/target_triple.rs
+-rw-r--r--   0     1001      127      970 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-dispatch/Cargo.toml
+-rw-r--r--   0     1001      127    10815 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-dispatch/src/lib.rs
+-rw-r--r--   0     1001      127      774 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-virtualenv/Cargo.toml
+-rw-r--r--   0     1001      127      109 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-virtualenv/README.md
+-rw-r--r--   0     1001      127     4375 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-virtualenv/src/_virtualenv.py
+-rw-r--r--   0     1001      127       20 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-virtualenv/src/activator/.gitattributes
+-rw-r--r--   0     1001      127     3388 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-virtualenv/src/activator/activate
+-rw-r--r--   0     1001      127     2259 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-virtualenv/src/activator/activate.bat
+-rw-r--r--   0     1001      127     2655 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-virtualenv/src/activator/activate.csh
+-rw-r--r--   0     1001      127     4219 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-virtualenv/src/activator/activate.fish
+-rw-r--r--   0     1001      127     3903 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-virtualenv/src/activator/activate.nu
+-rw-r--r--   0     1001      127     2826 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-virtualenv/src/activator/activate.ps1
+-rw-r--r--   0     1001      127     2411 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-virtualenv/src/activator/activate_this.py
+-rw-r--r--   0     1001      127     1728 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-virtualenv/src/activator/deactivate.bat
+-rw-r--r--   0     1001      127     1215 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-virtualenv/src/activator/pydoc.bat
+-rw-r--r--   0     1001      127    16565 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-virtualenv/src/bare.rs
+-rw-r--r--   0     1001      127     2090 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-virtualenv/src/lib.rs
+-rw-r--r--   0     1001      127     1464 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-installer/Cargo.toml
+-rw-r--r--   0     1001      127    12444 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-installer/src/compile.rs
+-rw-r--r--   0     1001      127     9352 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-installer/src/downloader.rs
+-rw-r--r--   0     1001      127     4349 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-installer/src/editable.rs
+-rw-r--r--   0     1001      127     2833 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-installer/src/installer.rs
+-rw-r--r--   0     1001      127      521 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-installer/src/lib.rs
+-rw-r--r--   0     1001      127     2755 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-installer/src/pip_compileall.py
+-rw-r--r--   0     1001      127    22428 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-installer/src/plan.rs
+-rw-r--r--   0     1001      127     7565 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-installer/src/satisfies.rs
+-rw-r--r--   0     1001      127    22636 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-installer/src/site_packages.rs
+-rw-r--r--   0     1001      127      978 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-installer/src/uninstall.rs
+-rw-r--r--   0     1001      127     1145 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-build/Cargo.toml
+-rw-r--r--   0     1001      127       17 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-build/.gitignore
+-rw-r--r--   0     1001      127    48655 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-build/src/lib.rs
+-rw-r--r--   0     1001      127      472 2024-05-22 18:48:21.000000 uv-0.2.0/crates/platform-tags/Cargo.toml
+-rw-r--r--   0     1001      127      161 2024-05-22 18:48:21.000000 uv-0.2.0/crates/platform-tags/src/lib.rs
+-rw-r--r--   0     1001      127     3507 2024-05-22 18:48:21.000000 uv-0.2.0/crates/platform-tags/src/platform.rs
+-rw-r--r--   0     1001      127    76059 2024-05-22 18:48:21.000000 uv-0.2.0/crates/platform-tags/src/tags.rs
+-rw-r--r--   0     1001      127      822 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pypi-types/Cargo.toml
+-rw-r--r--   0     1001      127     1609 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pypi-types/src/base_url.rs
+-rw-r--r--   0     1001      127     4643 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pypi-types/src/direct_url.rs
+-rw-r--r--   0     1001      127    15738 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pypi-types/src/lenient_requirement.rs
+-rw-r--r--   0     1001      127      239 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pypi-types/src/lib.rs
+-rw-r--r--   0     1001      127    19433 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pypi-types/src/metadata.rs
+-rw-r--r--   0     1001      127      575 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pypi-types/src/scheme.rs
+-rw-r--r--   0     1001      127    12178 2024-05-22 18:48:21.000000 uv-0.2.0/crates/pypi-types/src/simple_json.rs
+-rw-r--r--   0     1001      127     1808 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/Cargo.toml
+-rwxr-xr-x   0     1001      127     7922 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/fetch-version-metadata.py
+-rw-r--r--   0     1001      127        0 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/python/__init__.py
+-rw-r--r--   0     1001      127    21931 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/python/get_interpreter_info.py
+-rw-r--r--   0     1001      127    10174 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/python/packaging/LICENSE.APACHE
+-rw-r--r--   0     1001      127     1344 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/python/packaging/LICENSE.BSD
+-rw-r--r--   0     1001      127      316 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/python/packaging/README.md
+-rw-r--r--   0     1001      127      501 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/python/packaging/__init__.py
+-rw-r--r--   0     1001      127     3282 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/python/packaging/_elffile.py
+-rw-r--r--   0     1001      127     9588 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/python/packaging/_manylinux.py
+-rw-r--r--   0     1001      127     2696 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/python/packaging/_musllinux.py
+-rw-r--r--   0     1001      127   182247 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/python-version-metadata.json
+-rw-r--r--   0     1001      127    53798 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/src/discovery.rs
+-rw-r--r--   0     1001      127     8765 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/src/environment.rs
+-rw-r--r--   0     1001      127     1314 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/src/implementation.rs
+-rw-r--r--   0     1001      127    27969 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/src/interpreter.rs
+-rw-r--r--   0     1001      127    74107 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/src/lib.rs
+-rw-r--r--   0     1001      127     8104 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/src/managed/downloads.rs
+-rw-r--r--   0     1001      127     5960 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/src/managed/find.rs
+-rw-r--r--   0     1001      127      253 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/src/managed/mod.rs
+-rw-r--r--   0     1001      127   224411 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/src/managed/python_versions.inc
+-rw-r--r--   0     1001      127      691 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/src/managed/python_versions.inc.mustache
+-rw-r--r--   0     1001      127     4389 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/src/platform.rs
+-rw-r--r--   0     1001      127      430 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/src/pointer_size.rs
+-rw-r--r--   0     1001      127     3234 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/src/py_launcher.rs
+-rw-r--r--   0     1001      127     7736 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/src/python_version.rs
+-rw-r--r--   0     1001      127      950 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/src/target.rs
+-rw-r--r--   0     1001      127     5511 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/src/virtualenv.rs
+-rw-r--r--   0     1001      127     2708 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-interpreter/template-version-metadata.py
+-rw-r--r--   0     1001      127     1002 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-workspace/Cargo.toml
+-rw-r--r--   0     1001      127     7241 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-workspace/src/combine.rs
+-rw-r--r--   0     1001      127      127 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-workspace/src/lib.rs
+-rw-r--r--   0     1001      127     3296 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-workspace/src/settings.rs
+-rw-r--r--   0     1001      127     5558 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-workspace/src/workspace.rs
+-rw-r--r--   0     1001      127     1528 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-distribution/Cargo.toml
+-rw-r--r--   0     1001      127      684 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-distribution/src/archive.rs
+-rw-r--r--   0     1001      127    34434 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-distribution/src/distribution_database.rs
+-rw-r--r--   0     1001      127     2185 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-distribution/src/download.rs
+-rw-r--r--   0     1001      127     7053 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-distribution/src/error.rs
+-rw-r--r--   0     1001      127    10371 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-distribution/src/git.rs
+-rw-r--r--   0     1001      127     7145 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-distribution/src/index/built_wheel_index.rs
+-rw-r--r--   0     1001      127     3435 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-distribution/src/index/cached_wheel.rs
+-rw-r--r--   0     1001      127      162 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-distribution/src/index/mod.rs
+-rw-r--r--   0     1001      127     8729 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-distribution/src/index/registry_wheel_index.rs
+-rw-r--r--   0     1001      127      965 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-distribution/src/lib.rs
+-rw-r--r--   0     1001      127      600 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-distribution/src/locks.rs
+-rw-r--r--   0     1001      127     1225 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-distribution/src/reporter.rs
+-rw-r--r--   0     1001      127     2029 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-distribution/src/source/built_wheel_metadata.rs
+-rw-r--r--   0     1001      127    60426 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-distribution/src/source/mod.rs
+-rw-r--r--   0     1001      127     1998 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-distribution/src/source/revision.rs
+-rw-r--r--   0     1001      127     1223 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/Cargo.toml
+-rw-r--r--   0     1001      127    88063 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/lib.rs
+-rw-r--r--   0     1001      127     2237 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/requirement.rs
+-rw-r--r--   0     1001      127     6200 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-basic.txt.snap
+-rw-r--r--   0     1001      127     2683 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-constraints-a.txt.snap
+-rw-r--r--   0     1001      127     2273 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-constraints-b.txt.snap
+-rw-r--r--   0     1001      127      284 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-empty.txt.snap
+-rw-r--r--   0     1001      127     4140 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-for-poetry.txt.snap
+-rw-r--r--   0     1001      127     1816 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-include-a.txt.snap
+-rw-r--r--   0     1001      127      828 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-include-b.txt.snap
+-rw-r--r--   0     1001      127    11863 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-poetry-with-hashes.txt.snap
+-rw-r--r--   0     1001      127     2257 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-small.txt.snap
+-rw-r--r--   0     1001      127     2544 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-whitespace.txt.snap
+-rw-r--r--   0     1001      127     6200 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-basic.txt.snap
+-rw-r--r--   0     1001      127     2683 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-constraints-a.txt.snap
+-rw-r--r--   0     1001      127     2273 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-constraints-b.txt.snap
+-rw-r--r--   0     1001      127      284 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-empty.txt.snap
+-rw-r--r--   0     1001      127     4140 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-for-poetry.txt.snap
+-rw-r--r--   0     1001      127     1816 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-include-a.txt.snap
+-rw-r--r--   0     1001      127      828 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-include-b.txt.snap
+-rw-r--r--   0     1001      127    11863 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-poetry-with-hashes.txt.snap
+-rw-r--r--   0     1001      127     2257 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-small.txt.snap
+-rw-r--r--   0     1001      127     3692 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-unix-bare-url.txt.snap
+-rw-r--r--   0     1001      127     8155 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-unix-editable.txt.snap
+-rw-r--r--   0     1001      127     2544 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-whitespace.txt.snap
+-rw-r--r--   0     1001      127     3713 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-windows-bare-url.txt.snap
+-rw-r--r--   0     1001      127     8161 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-windows-editable.txt.snap
+-rw-r--r--   0     1001      127      113 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/test-data/requirements-txt/bare-url.txt
+-rw-r--r--   0     1001      127       90 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/test-data/requirements-txt/basic.txt
+-rw-r--r--   0     1001      127       45 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/test-data/requirements-txt/constraints-a.txt
+-rw-r--r--   0     1001      127       27 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/test-data/requirements-txt/constraints-b.txt
+-rw-r--r--   0     1001      127      491 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/test-data/requirements-txt/editable.txt
+-rw-r--r--   0     1001      127        0 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/test-data/requirements-txt/empty.txt
+-rw-r--r--   0     1001      127      101 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/test-data/requirements-txt/for-poetry.txt
+-rw-r--r--   0     1001      127       43 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/test-data/requirements-txt/include-a.txt
+-rw-r--r--   0     1001      127        5 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/test-data/requirements-txt/include-b.txt
+-rw-r--r--   0     1001      127     1183 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/test-data/requirements-txt/poetry-with-hashes.txt
+-rw-r--r--   0     1001      127       66 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/test-data/requirements-txt/small.txt
+-rw-r--r--   0     1001      127      183 2024-05-22 18:48:21.000000 uv-0.2.0/crates/requirements-txt/test-data/requirements-txt/whitespace.txt
+-rw-r--r--   0     1001      127      918 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-git/Cargo.toml
+-rw-r--r--   0     1001      127    63247 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-git/src/git.rs
+-rw-r--r--   0     1001      127    37494 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-git/src/known_hosts.rs
+-rw-r--r--   0     1001      127     3843 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-git/src/lib.rs
+-rw-r--r--   0     1001      127      940 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-git/src/sha.rs
+-rw-r--r--   0     1001      127     5058 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-git/src/source.rs
+-rw-r--r--   0     1001      127     1362 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-git/src/util/errors.rs
+-rw-r--r--   0     1001      127      733 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-git/src/util/mod.rs
+-rw-r--r--   0     1001      127     7295 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-git/src/util/retry.rs
+-rw-r--r--   0     1001      127      790 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-types/Cargo.toml
+-rw-r--r--   0     1001      127      423 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-types/src/builds.rs
+-rw-r--r--   0     1001      127      239 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-types/src/downloads.rs
+-rw-r--r--   0     1001      127     6228 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-types/src/hash.rs
+-rw-r--r--   0     1001      127      219 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-types/src/lib.rs
+-rw-r--r--   0     1001      127     1478 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-types/src/requirements.rs
+-rw-r--r--   0     1001      127     6912 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv-types/src/traits.rs
+-rw-r--r--   0        0        0     3723 1970-01-01 00:00:00.000000 uv-0.2.0/crates/uv/Cargo.toml
+-rw-r--r--   0     1001      127     3065 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/build.rs
+-rw-r--r--   0     1001      127    79305 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/cli.rs
+-rw-r--r--   0     1001      127     4158 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/commands/cache_clean.rs
+-rw-r--r--   0     1001      127      213 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/commands/cache_dir.rs
+-rw-r--r--   0     1001      127     1955 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/commands/cache_prune.rs
+-rw-r--r--   0     1001      127     4868 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/commands/mod.rs
+-rw-r--r--   0     1001      127     2316 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/commands/pip/check.rs
+-rw-r--r--   0     1001      127    28556 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/commands/pip/compile.rs
+-rw-r--r--   0     1001      127     2462 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/commands/pip/freeze.rs
+-rw-r--r--   0     1001      127    16730 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/commands/pip/install.rs
+-rw-r--r--   0     1001      127     6301 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/commands/pip/list.rs
+-rw-r--r--   0     1001      127      198 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/commands/pip/mod.rs
+-rw-r--r--   0     1001      127    24504 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/commands/pip/operations.rs
+-rw-r--r--   0     1001      127     6601 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/commands/pip/show.rs
+-rw-r--r--   0     1001      127    14121 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/commands/pip/sync.rs
+-rw-r--r--   0     1001      127     7279 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/commands/pip/uninstall.rs
+-rw-r--r--   0     1001      127     5135 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/commands/project/lock.rs
+-rw-r--r--   0     1001      127    19397 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/commands/project/mod.rs
+-rw-r--r--   0     1001      127     5914 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/commands/project/run.rs
+-rw-r--r--   0     1001      127     3663 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/commands/project/sync.rs
+-rw-r--r--   0     1001      127    10251 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/commands/reporters.rs
+-rw-r--r--   0     1001      127     3965 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/commands/self_update.rs
+-rw-r--r--   0     1001      127       20 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/commands/tool/mod.rs
+-rw-r--r--   0     1001      127     4227 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/commands/tool/run.rs
+-rw-r--r--   0     1001      127    10652 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/commands/venv.rs
+-rw-r--r--   0     1001      127      570 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/commands/version.rs
+-rw-r--r--   0     1001      127    11263 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/compat/mod.rs
+-rw-r--r--   0     1001      127     7723 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/editables.rs
+-rw-r--r--   0     1001      127     7604 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/logging.rs
+-rw-r--r--   0     1001      127    24207 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/main.rs
+-rw-r--r--   0     1001      127     2326 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/printer.rs
+-rw-r--r--   0     1001      127    38621 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/settings.rs
+-rw-r--r--   0     1001      127     2844 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/shell.rs
+-rw-r--r--   0     1001      127     4816 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/src/version.rs
+-rw-r--r--   0     1001      127     4437 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/tests/cache_prune.rs
+-rw-r--r--   0     1001      127    21602 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/tests/common/mod.rs
+-rw-r--r--   0     1001      127    27008 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/tests/lock.rs
+-rw-r--r--   0     1001      127     6060 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/tests/pip_check.rs
+-rw-r--r--   0     1001      127   305128 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/tests/pip_compile.rs
+-rw-r--r--   0     1001      127    19016 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/tests/pip_compile_scenarios.rs
+-rw-r--r--   0     1001      127     8341 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/tests/pip_freeze.rs
+-rw-r--r--   0     1001      127   151838 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/tests/pip_install.rs
+-rw-r--r--   0     1001      127   159295 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/tests/pip_install_scenarios.rs
+-rw-r--r--   0     1001      127    18430 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/tests/pip_list.rs
+-rw-r--r--   0     1001      127    13190 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/tests/pip_show.rs
+-rw-r--r--   0     1001      127   162257 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/tests/pip_sync.rs
+-rw-r--r--   0     1001      127    14234 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/tests/pip_uninstall.rs
+-rw-r--r--   0     1001      127     1254 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/tests/self_update.rs
+-rw-r--r--   0     1001      127    18144 2024-05-22 18:48:21.000000 uv-0.2.0/crates/uv/tests/venv.rs
+-rw-r--r--   0     1001      127   134744 2024-05-22 18:48:21.000000 uv-0.2.0/Cargo.lock
+-rw-r--r--   0        0        0     8871 1970-01-01 00:00:00.000000 uv-0.2.0/Cargo.toml
+-rw-r--r--   0     1001      127     2366 2024-05-22 18:48:21.000000 uv-0.2.0/pyproject.toml
+-rw-r--r--   0     1001      127     1055 2024-05-22 18:48:21.000000 uv-0.2.0/python/uv/__main__.py
+-rw-r--r--   0     1001      127      806 2024-05-22 18:48:21.000000 uv-0.2.0/python/uv/__init__.py
+-rw-r--r--   0     1001      127        0 2024-05-22 18:48:21.000000 uv-0.2.0/python/uv/py.typed
+-rw-r--r--   0     1001      127    31568 2024-05-22 18:48:21.000000 uv-0.2.0/README.md
+-rw-r--r--   0     1001      127       29 2024-05-22 18:48:21.000000 uv-0.2.0/rust-toolchain.toml
+-rw-r--r--   0     1001      127    11356 2024-05-22 18:48:21.000000 uv-0.2.0/LICENSE-APACHE
+-rw-r--r--   0     1001      127     1077 2024-05-22 18:48:21.000000 uv-0.2.0/LICENSE-MIT
+-rw-r--r--   0        0        0    32935 1970-01-01 00:00:00.000000 uv-0.2.0/PKG-INFO
```

### Comparing `uv-0.1.9/crates/gourgeist/Cargo.toml` & `uv-0.2.0/crates/uv-types/Cargo.toml`

 * *Files 20% similar despite different names*

```diff
@@ -1,44 +1,32 @@
 [package]
-name = "gourgeist"
-version = "0.0.4"
-publish = false
-description = "virtualenv creation implemented in rust"
-keywords = ["virtualenv", "venv", "python"]
-
+name = "uv-types"
+version = "0.0.1"
 edition = { workspace = true }
 rust-version = { workspace = true }
 homepage = { workspace = true }
 documentation = { workspace = true }
 repository = { workspace = true }
 authors = { workspace = true }
 license = { workspace = true }
 
-[[bin]]
-name = "gourgeist"
-required-features = ["cli"]
-
 [lints]
 workspace = true
 
 [dependencies]
-platform-host = { path = "../platform-host" }
-uv-cache = { path = "../uv-cache" }
-uv-fs = { path = "../uv-fs" }
-uv-interpreter = { path = "../uv-interpreter" }
+distribution-types = { workspace = true }
+once-map = { workspace = true }
+pep440_rs = { workspace = true }
+pep508_rs = { workspace = true }
+pypi-types = { workspace = true }
+uv-cache = { workspace = true }
+uv-interpreter = { workspace = true }
+uv-normalize = { workspace = true }
+uv-configuration = { workspace = true }
 
-anstream = { workspace = true }
-cachedir = { workspace = true }
-camino = { workspace = true }
-clap = { workspace = true, features = ["derive"], optional = true }
-directories = { workspace = true }
-fs-err = { workspace = true }
-serde = { workspace = true }
-serde_json = { workspace = true }
-tempfile = { workspace = true }
+anyhow = { workspace = true }
+rustc-hash = { workspace = true }
 thiserror = { workspace = true }
-tracing = { workspace = true }
-tracing-subscriber = { workspace = true, optional = true }
-which = { workspace = true }
+url = { workspace = true }
 
 [features]
-cli = ["clap", "tracing-subscriber"]
+default = []
```

### Comparing `uv-0.1.9/crates/gourgeist/src/_virtualenv.py` & `uv-0.2.0/crates/uv-virtualenv/src/_virtualenv.py`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/gourgeist/src/activator/activate` & `uv-0.2.0/crates/uv-virtualenv/src/activator/activate`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/gourgeist/src/activator/activate.bat` & `uv-0.2.0/crates/uv-virtualenv/src/activator/activate.bat`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/gourgeist/src/activator/activate.csh` & `uv-0.2.0/crates/uv-virtualenv/src/activator/activate.csh`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/gourgeist/src/activator/activate.fish` & `uv-0.2.0/crates/uv-virtualenv/src/activator/activate.fish`

 * *Files 0% similar despite different names*

```diff
@@ -79,15 +79,15 @@
 # Unset irrelevant variables.
 deactivate nondestructive
 
 set -gx VIRTUAL_ENV '{{ VIRTUAL_ENV_DIR }}'
 
 # https://github.com/fish-shell/fish-shell/issues/436 altered PATH handling
 if test (echo $FISH_VERSION | head -c 1) -lt 3
-   set -gx _OLD_VIRTUAL_PATH (_bashify_path $PATH)
+    set -gx _OLD_VIRTUAL_PATH (_bashify_path $PATH)
 else
     set -gx _OLD_VIRTUAL_PATH $PATH
 end
 set -gx PATH "$VIRTUAL_ENV"'/{{ BIN_NAME }}' $PATH
 
 # Prompt override provided?
 # If not, just use the environment name.
```

### Comparing `uv-0.1.9/crates/gourgeist/src/activator/activate.nu` & `uv-0.2.0/crates/uv-virtualenv/src/activator/activate.nu`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/gourgeist/src/activator/activate.ps1` & `uv-0.2.0/crates/uv-virtualenv/src/activator/activate.ps1`

 * *Files 2% similar despite different names*

```diff
@@ -63,15 +63,15 @@
 }
 else {
     $env:VIRTUAL_ENV_PROMPT = $( Split-Path $env:VIRTUAL_ENV -Leaf )
 }
 
 New-Variable -Scope global -Name _OLD_VIRTUAL_PATH -Value $env:PATH
 
-$env:PATH = "$env:VIRTUAL_ENV/{{ BIN_NAME }};" + $env:PATH
+$env:PATH = "$env:VIRTUAL_ENV/{{ BIN_NAME }}{{ PATH_SEP }}" + $env:PATH
 if (!$env:VIRTUAL_ENV_DISABLE_PROMPT) {
     function global:_old_virtual_prompt {
         ""
     }
     $function:_old_virtual_prompt = $function:prompt
 
     function global:prompt {
```

### Comparing `uv-0.1.9/crates/gourgeist/src/activator/activate_this.py` & `uv-0.2.0/crates/uv-virtualenv/src/activator/activate_this.py`

 * *Files 14% similar despite different names*

```diff
@@ -18,40 +18,42 @@
 # LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
 # OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
 # WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 
 """
 Activate virtualenv for current interpreter:
 
-Use exec(open(this_file).read(), {'__file__': this_file}).
+import runpy
+runpy.run_path(this_file)
 
 This can be used when you must use an existing Python interpreter, not the virtualenv bin/python.
 """  # noqa: D415
+
 from __future__ import annotations
 
 import os
 import site
 import sys
 
 try:
     abs_file = os.path.abspath(__file__)
 except NameError as exc:
-    msg = "You must use exec(open(this_file).read(), {'__file__': this_file}))"
+    msg = "You must use import runpy; runpy.run_path(this_file)"
     raise AssertionError(msg) from exc
 
 bin_dir = os.path.dirname(abs_file)
 base = bin_dir[: -len("{{ BIN_NAME }}") - 1]  # strip away the bin part from the __file__, plus the path separator
 
 # prepend bin to PATH (this file is inside the bin directory)
 os.environ["PATH"] = os.pathsep.join([bin_dir, *os.environ.get("PATH", "").split(os.pathsep)])
 os.environ["VIRTUAL_ENV"] = base  # virtual env is right above bin directory
-os.environ["VIRTUAL_ENV_PROMPT"] = "" or os.path.basename(base)  # noqa: SIM222
+os.environ["VIRTUAL_ENV_PROMPT"] = "{{ VIRTUAL_PROMPT }}" or os.path.basename(base)  # noqa: SIM222
 
 # add the virtual environments libraries to the host python import mechanism
 prev_length = len(sys.path)
 for lib in "{{ RELATIVE_SITE_PACKAGES }}".split(os.pathsep):
     path = os.path.realpath(os.path.join(bin_dir, lib))
-    site.addsitedir(path.decode("utf-8") if "" else path)
+    site.addsitedir(path)
 sys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]
 
 sys.real_prefix = sys.prefix
 sys.prefix = base
```

### Comparing `uv-0.1.9/crates/gourgeist/src/activator/deactivate.bat` & `uv-0.2.0/crates/uv-virtualenv/src/activator/deactivate.bat`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/gourgeist/src/activator/pydoc.bat` & `uv-0.2.0/crates/uv-virtualenv/src/activator/pydoc.bat`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/gourgeist/src/lib.rs` & `uv-0.2.0/crates/uv-virtualenv/src/lib.rs`

 * *Files 12% similar despite different names*

```diff
@@ -1,32 +1,31 @@
 use std::io;
 use std::path::Path;
 
-use camino::{FromPathError, Utf8Path};
 use thiserror::Error;
 
-pub use interpreter::parse_python_cli;
-use platform_host::PlatformError;
-use uv_interpreter::{Interpreter, Virtualenv};
+use platform_tags::PlatformError;
+use uv_interpreter::{Interpreter, PythonEnvironment};
 
 pub use crate::bare::create_bare_venv;
 
 mod bare;
-mod interpreter;
 
 #[derive(Debug, Error)]
 pub enum Error {
     #[error(transparent)]
     IO(#[from] io::Error),
-    #[error("Failed to determine python interpreter to use")]
-    InvalidPythonInterpreter(#[source] Box<dyn std::error::Error + Sync + Send>),
+    #[error("Failed to determine Python interpreter to use")]
+    Discovery(#[from] uv_interpreter::DiscoveryError),
+    #[error("Failed to determine Python interpreter to use")]
+    InterpreterNotFound(#[from] uv_interpreter::InterpreterNotFound),
     #[error(transparent)]
     Platform(#[from] PlatformError),
-    #[error("Reserved key used for pyvenv.cfg: {0}")]
-    ReservedConfigKey(String),
+    #[error("Could not find a suitable Python executable for the virtual environment based on the interpreter: {0}")]
+    NotFound(String),
 }
 
 /// The value to use for the shell prompt when inside a virtual environment.
 #[derive(Debug)]
 pub enum Prompt {
     /// Use the current directory name as the prompt.
     CurrentDirectoryName,
@@ -37,30 +36,35 @@
     None,
 }
 
 impl Prompt {
     /// Determine the prompt value to be used from the command line arguments.
     pub fn from_args(prompt: Option<String>) -> Self {
         match prompt {
-            Some(prompt) if prompt == "." => Prompt::CurrentDirectoryName,
-            Some(prompt) => Prompt::Static(prompt),
-            None => Prompt::None,
+            Some(prompt) if prompt == "." => Self::CurrentDirectoryName,
+            Some(prompt) => Self::Static(prompt),
+            None => Self::None,
         }
     }
 }
 
 /// Create a virtualenv.
 pub fn create_venv(
     location: &Path,
     interpreter: Interpreter,
     prompt: Prompt,
-    extra_cfg: Vec<(String, String)>,
-) -> Result<Virtualenv, Error> {
-    let location: &Utf8Path = location
-        .try_into()
-        .map_err(|err: FromPathError| err.into_io_error())?;
-    let paths = create_bare_venv(location, &interpreter, prompt, extra_cfg)?;
-    Ok(Virtualenv::from_interpreter(
-        interpreter,
-        paths.root.as_std_path(),
-    ))
+    system_site_packages: bool,
+    allow_existing: bool,
+) -> Result<PythonEnvironment, Error> {
+    // Create the virtualenv at the given location.
+    let virtualenv = create_bare_venv(
+        location,
+        &interpreter,
+        prompt,
+        system_site_packages,
+        allow_existing,
+    )?;
+
+    // Create the corresponding `PythonEnvironment`.
+    let interpreter = interpreter.with_virtualenv(virtualenv);
+    Ok(PythonEnvironment::from_interpreter(interpreter))
 }
```

### Comparing `uv-0.1.9/crates/uv-interpreter/Cargo.toml` & `uv-0.2.0/crates/uv-installer/Cargo.toml`

 * *Files 16% similar despite different names*

```diff
@@ -1,41 +1,49 @@
 [package]
-name = "uv-interpreter"
+name = "uv-installer"
 version = "0.0.1"
 edition = { workspace = true }
 rust-version = { workspace = true }
 homepage = { workspace = true }
 documentation = { workspace = true }
 repository = { workspace = true }
 authors = { workspace = true }
 license = { workspace = true }
 
 [lints]
 workspace = true
 
 [dependencies]
-cache-key = { path = "../cache-key" }
-pep440_rs = { path = "../pep440-rs" }
-pep508_rs = { path = "../pep508-rs", features = ["serde"] }
-platform-host = { path = "../platform-host" }
-platform-tags = { path = "../platform-tags" }
-uv-cache = { path = "../uv-cache" }
-uv-fs = { path = "../uv-fs" }
+cache-key = { workspace = true }
+distribution-filename = { workspace = true }
+distribution-types = { workspace = true }
+install-wheel-rs = { workspace = true, default-features = false }
+pep440_rs = { workspace = true }
+pep508_rs = { workspace = true }
+platform-tags = { workspace = true }
+pypi-types = { workspace = true }
+requirements-txt = { workspace = true }
+uv-cache = { workspace = true }
+uv-configuration = { workspace = true }
+uv-distribution = { workspace = true }
+uv-extract = { workspace = true }
+uv-fs = { workspace = true }
+uv-git = { workspace = true }
+uv-interpreter = { workspace = true }
+uv-normalize = { workspace = true }
+uv-types = { workspace = true }
+uv-warnings = { workspace = true }
 
-fs-err = { workspace = true, features = ["tokio"] }
-once_cell = { workspace = true }
-regex = { workspace = true }
-rmp-serde = { workspace = true }
-same-file = { workspace = true }
-serde = { workspace = true, features = ["derive"] }
-serde_json = { workspace = true }
+anyhow = { workspace = true }
+async-channel = { workspace = true }
+fs-err = { workspace = true }
+futures = { workspace = true }
+rayon = { workspace = true }
+rustc-hash = { workspace = true }
+serde = { workspace = true }
+tempfile = { workspace = true }
 thiserror = { workspace = true }
 tokio = { workspace = true }
+toml = { workspace = true }
 tracing = { workspace = true }
-which = { workspace = true}
-
-[dev-dependencies]
-anyhow = { version = "1.0.79" }
-indoc = { version = "2.0.4" }
-insta = { version = "1.34.0" }
-itertools = { version = "0.12.0" }
-tempfile = { version = "3.9.0" }
+url = { workspace = true }
+walkdir = { workspace = true }
```

### Comparing `uv-0.1.9/crates/uv-interpreter/src/interpreter.rs` & `uv-0.2.0/crates/uv-virtualenv/src/bare.rs`

 * *Files 24% similar despite different names*

```diff
@@ -1,518 +1,453 @@
-use std::ffi::{OsStr, OsString};
-use std::io::Write;
-use std::path::{Path, PathBuf};
-use std::process::Command;
+//! Create a bare virtualenv without any packages installed.
 
-use fs_err as fs;
-use once_cell::sync::OnceCell;
-use serde::{Deserialize, Serialize};
-use tracing::{debug, warn};
-
-use cache_key::digest;
-use pep440_rs::Version;
-use pep508_rs::MarkerEnvironment;
-use platform_host::Platform;
-use platform_tags::{Tags, TagsError};
-use uv_cache::{Cache, CacheBucket, CachedByTimestamp, Freshness, Timestamp};
-use uv_fs::write_atomic_sync;
-
-use crate::python_platform::PythonPlatform;
-use crate::python_query::try_find_default_python;
-use crate::virtual_env::detect_virtual_env;
-use crate::{find_requested_python, Error, PythonVersion};
-
-/// A Python executable and its associated platform markers.
-#[derive(Debug, Clone)]
-pub struct Interpreter {
-    pub(crate) platform: PythonPlatform,
-    pub(crate) markers: Box<MarkerEnvironment>,
-    pub(crate) base_exec_prefix: PathBuf,
-    pub(crate) base_prefix: PathBuf,
-    pub(crate) stdlib: PathBuf,
-    pub(crate) sys_executable: PathBuf,
-    tags: OnceCell<Tags>,
-}
-
-impl Interpreter {
-    /// Detect the interpreter info for the given Python executable.
-    pub fn query(executable: &Path, platform: &Platform, cache: &Cache) -> Result<Self, Error> {
-        let info = InterpreterQueryResult::query_cached(executable, cache)?;
-
-        debug_assert!(
-            info.sys_executable.is_absolute(),
-            "`sys.executable` is not an absolute Python; Python installation is broken: {}",
-            info.sys_executable.display()
-        );
-
-        Ok(Self {
-            platform: PythonPlatform(platform.to_owned()),
-            markers: Box::new(info.markers),
-            base_exec_prefix: info.base_exec_prefix,
-            base_prefix: info.base_prefix,
-            stdlib: info.stdlib,
-            sys_executable: info.sys_executable,
-            tags: OnceCell::new(),
-        })
-    }
+use std::env;
+use std::env::consts::EXE_SUFFIX;
+use std::io;
+use std::io::{BufWriter, Write};
+use std::path::Path;
 
-    // TODO(konstin): Find a better way mocking the fields
-    pub fn artificial(
-        platform: Platform,
-        markers: MarkerEnvironment,
-        base_exec_prefix: PathBuf,
-        base_prefix: PathBuf,
-        sys_executable: PathBuf,
-        stdlib: PathBuf,
-    ) -> Self {
-        Self {
-            platform: PythonPlatform(platform),
-            markers: Box::new(markers),
-            base_exec_prefix,
-            base_prefix,
-            stdlib,
-            sys_executable,
-            tags: OnceCell::new(),
-        }
+use fs_err as fs;
+use fs_err::File;
+use itertools::Itertools;
+use tracing::info;
+
+use pypi_types::Scheme;
+use uv_fs::{cachedir, Simplified};
+use uv_interpreter::{Interpreter, VirtualEnvironment};
+use uv_version::version;
+
+use crate::{Error, Prompt};
+
+/// The bash activate scripts with the venv dependent paths patches out
+const ACTIVATE_TEMPLATES: &[(&str, &str)] = &[
+    ("activate", include_str!("activator/activate")),
+    ("activate.csh", include_str!("activator/activate.csh")),
+    ("activate.fish", include_str!("activator/activate.fish")),
+    ("activate.nu", include_str!("activator/activate.nu")),
+    ("activate.ps1", include_str!("activator/activate.ps1")),
+    ("activate.bat", include_str!("activator/activate.bat")),
+    ("deactivate.bat", include_str!("activator/deactivate.bat")),
+    ("pydoc.bat", include_str!("activator/pydoc.bat")),
+    (
+        "activate_this.py",
+        include_str!("activator/activate_this.py"),
+    ),
+];
+const VIRTUALENV_PATCH: &str = include_str!("_virtualenv.py");
+
+/// Very basic `.cfg` file format writer.
+fn write_cfg(f: &mut impl Write, data: &[(String, String)]) -> io::Result<()> {
+    for (key, value) in data {
+        writeln!(f, "{key} = {value}")?;
     }
+    Ok(())
+}
 
-    /// Return a new [`Interpreter`] with the given base prefix.
-    #[must_use]
-    pub fn with_base_prefix(self, base_prefix: PathBuf) -> Self {
-        Self {
-            base_prefix,
-            ..self
+/// Write all the files that belong to a venv without any packages installed.
+pub fn create_bare_venv(
+    location: &Path,
+    interpreter: &Interpreter,
+    prompt: Prompt,
+    system_site_packages: bool,
+    allow_existing: bool,
+) -> Result<VirtualEnvironment, Error> {
+    // Determine the base Python executable; that is, the Python executable that should be
+    // considered the "base" for the virtual environment. This is typically the Python executable
+    // from the [`Interpreter`]; however, if the interpreter is a virtual environment itself, then
+    // the base Python executable is the Python executable of the interpreter's base interpreter.
+    let base_python = if cfg!(unix) {
+        // On Unix, follow symlinks to resolve the base interpreter, since the Python executable in
+        // a virtual environment is a symlink to the base interpreter.
+        uv_fs::canonicalize_executable(interpreter.sys_executable())?
+    } else if cfg!(windows) {
+        // On Windows, follow `virtualenv`. If we're in a virtual environment, use
+        // `sys._base_executable` if it exists; if not, use `sys.base_prefix`. For example, with
+        // Python installed from the Windows Store, `sys.base_prefix` is slightly "incorrect".
+        //
+        // If we're _not_ in a virtual environment, use the interpreter's executable, since it's
+        // already a "system Python". We canonicalize the path to ensure that it's real and
+        // consistent, though we don't expect any symlinks on Windows.
+        if interpreter.is_virtualenv() {
+            if let Some(base_executable) = interpreter.base_executable() {
+                base_executable.to_path_buf()
+            } else {
+                // Assume `python.exe`, though the exact executable name is never used (below) on
+                // Windows, only its parent directory.
+                interpreter.base_prefix().join("python.exe")
+            }
+        } else {
+            uv_fs::canonicalize_executable(interpreter.sys_executable())?
         }
-    }
-
-    /// Find the best available Python interpreter to use.
-    ///
-    /// If no Python version is provided, we will use the first available interpreter.
-    ///
-    /// If a Python version is provided, we will first try to find an exact match. If
-    /// that cannot be found and a patch version was requested, we will look for a match
-    /// without comparing the patch version number. If that cannot be found, we fall back to
-    /// the first available version.
-    ///
-    /// See [`Self::find_version`] for details on the precedence of Python lookup locations.
-    pub fn find_best(
-        python_version: Option<&PythonVersion>,
-        platform: &Platform,
-        cache: &Cache,
-    ) -> Result<Self, Error> {
-        // First, check for an exact match (or the first available version if no Python version was provided)
-        if let Some(interpreter) = Self::find_version(python_version, platform, cache)? {
-            return Ok(interpreter);
-        }
-
-        if let Some(python_version) = python_version {
-            // If that fails, and a specific patch version was requested try again allowing a
-            // different patch version
-            if python_version.patch().is_some() {
-                if let Some(interpreter) =
-                    Self::find_version(Some(&python_version.without_patch()), platform, cache)?
+    } else {
+        unimplemented!("Only Windows and Unix are supported")
+    };
+
+    // Validate the existing location.
+    match location.metadata() {
+        Ok(metadata) => {
+            if metadata.is_file() {
+                return Err(Error::IO(io::Error::new(
+                    io::ErrorKind::AlreadyExists,
+                    format!("File exists at `{}`", location.user_display()),
+                )));
+            } else if metadata.is_dir() {
+                if allow_existing {
+                    info!("Allowing existing directory");
+                } else if location.join("pyvenv.cfg").is_file() {
+                    info!("Removing existing directory");
+                    fs::remove_dir_all(location)?;
+                    fs::create_dir_all(location)?;
+                } else if location
+                    .read_dir()
+                    .is_ok_and(|mut dir| dir.next().is_none())
                 {
-                    return Ok(interpreter);
+                    info!("Ignoring empty directory");
+                } else {
+                    return Err(Error::IO(io::Error::new(
+                        io::ErrorKind::AlreadyExists,
+                        format!(
+                            "The directory `{}` exists, but it's not a virtualenv",
+                            location.user_display()
+                        ),
+                    )));
                 }
             }
-
-            // If a Python version was requested but cannot be fulfilled, just take any version
-            if let Some(interpreter) = Self::find_version(None, platform, cache)? {
-                return Ok(interpreter);
-            }
-        }
-
-        Err(Error::PythonNotFound)
-    }
-
-    /// Find a Python interpreter.
-    ///
-    /// We check, in order, the following locations:
-    ///
-    /// - `VIRTUAL_ENV` and `CONDA_PREFIX`
-    /// - A `.venv` folder
-    /// - If a python version is given: `pythonx.y`
-    /// - `python3` (unix) or `python.exe` (windows)
-    ///
-    /// If `UV_TEST_PYTHON_PATH` is set, we will not check for Python versions in the
-    /// global PATH, instead we will search using the provided path. Virtual environments
-    /// will still be respected.
-    ///
-    /// If a version is provided and an interpreter cannot be found with the given version,
-    /// we will return [`None`].
-    pub fn find_version(
-        python_version: Option<&PythonVersion>,
-        platform: &Platform,
-        cache: &Cache,
-    ) -> Result<Option<Self>, Error> {
-        let version_matches = |interpreter: &Self| -> bool {
-            if let Some(python_version) = python_version {
-                // If a patch version was provided, check for an exact match
-                python_version.is_satisfied_by(interpreter)
-            } else {
-                // The version always matches if one was not provided
-                true
-            }
-        };
-
-        // Check if the venv Python matches.
-        let python_platform = PythonPlatform::from(platform.to_owned());
-        if let Some(venv) = detect_virtual_env(&python_platform)? {
-            let executable = python_platform.venv_python(venv);
-            let interpreter = Self::query(&executable, &python_platform.0, cache)?;
-
-            if version_matches(&interpreter) {
-                return Ok(Some(interpreter));
-            }
-        };
-
-        // Look for the requested version with by search for `python{major}.{minor}` in `PATH` on
-        // Unix and `py --list-paths` on Windows.
-        let interpreter = if let Some(python_version) = python_version {
-            find_requested_python(&python_version.string, platform, cache)?
-        } else {
-            try_find_default_python(platform, cache)?
-        };
-
-        if let Some(interpreter) = interpreter {
-            debug_assert!(version_matches(&interpreter));
-            Ok(Some(interpreter))
-        } else {
-            Ok(None)
         }
-    }
-
-    /// Find the Python interpreter in `PATH`, respecting `UV_PYTHON_PATH`.
-    ///
-    /// Returns `Ok(None)` if not found.
-    pub fn find_executable<R: AsRef<OsStr> + Into<OsString> + Copy>(
-        requested: R,
-    ) -> Result<Option<PathBuf>, Error> {
-        let result = if let Some(isolated) = std::env::var_os("UV_TEST_PYTHON_PATH") {
-            which::which_in(requested, Some(isolated), std::env::current_dir()?)
-        } else {
-            which::which(requested)
-        };
-
-        match result {
-            Err(which::Error::CannotFindBinaryPath) => Ok(None),
-            Err(err) => Err(Error::WhichError(requested.into(), err)),
-            Ok(path) => Ok(Some(path)),
+        Err(err) if err.kind() == io::ErrorKind::NotFound => {
+            fs::create_dir_all(location)?;
         }
+        Err(err) => return Err(Error::IO(err)),
     }
 
-    /// Returns the path to the Python virtual environment.
-    #[inline]
-    pub fn platform(&self) -> &Platform {
-        &self.platform
-    }
+    let location = location.canonicalize()?;
 
-    /// Returns the [`MarkerEnvironment`] for this Python executable.
-    #[inline]
-    pub const fn markers(&self) -> &MarkerEnvironment {
-        &self.markers
-    }
+    let bin_name = if cfg!(unix) {
+        "bin"
+    } else if cfg!(windows) {
+        "Scripts"
+    } else {
+        unimplemented!("Only Windows and Unix are supported")
+    };
+    let scripts = location.join(&interpreter.virtualenv().scripts);
+    let prompt = match prompt {
+        Prompt::CurrentDirectoryName => env::current_dir()?
+            .file_name()
+            .map(|name| name.to_string_lossy().to_string()),
+        Prompt::Static(value) => Some(value),
+        Prompt::None => None,
+    };
+
+    // Add the CACHEDIR.TAG.
+    cachedir::ensure_tag(&location)?;
+
+    // Create a `.gitignore` file to ignore all files in the venv.
+    fs::write(location.join(".gitignore"), "*")?;
+
+    // Per PEP 405, the Python `home` is the parent directory of the interpreter.
+    let python_home = base_python.parent().ok_or_else(|| {
+        io::Error::new(
+            io::ErrorKind::NotFound,
+            "The Python interpreter needs to have a parent directory",
+        )
+    })?;
 
-    /// Returns the [`Tags`] for this Python executable.
-    pub fn tags(&self) -> Result<&Tags, TagsError> {
-        self.tags.get_or_try_init(|| {
-            Tags::from_env(
-                self.platform(),
-                self.python_tuple(),
-                self.implementation_name(),
-                self.implementation_tuple(),
-            )
+    // Different names for the python interpreter
+    fs::create_dir_all(&scripts)?;
+    let executable = scripts.join(format!("python{EXE_SUFFIX}"));
+
+    #[cfg(unix)]
+    {
+        uv_fs::replace_symlink(&base_python, &executable)?;
+        uv_fs::replace_symlink(
+            "python",
+            scripts.join(format!("python{}", interpreter.python_major())),
+        )?;
+        uv_fs::replace_symlink(
+            "python",
+            scripts.join(format!(
+                "python{}.{}",
+                interpreter.python_major(),
+                interpreter.python_minor(),
+            )),
+        )?;
+    }
+
+    // No symlinking on Windows, at least not on a regular non-dev non-admin Windows install.
+    if cfg!(windows) {
+        copy_launcher_windows(
+            WindowsExecutable::Python,
+            interpreter,
+            &base_python,
+            &scripts,
+            python_home,
+        )?;
+        copy_launcher_windows(
+            WindowsExecutable::Pythonw,
+            interpreter,
+            &base_python,
+            &scripts,
+            python_home,
+        )?;
+    }
+
+    #[cfg(not(any(unix, windows)))]
+    {
+        compile_error!("Only Windows and Unix are supported")
+    }
+
+    // Add all the activate scripts for different shells
+    for (name, template) in ACTIVATE_TEMPLATES {
+        let path_sep = if cfg!(windows) { ";" } else { ":" };
+
+        let relative_site_packages = [
+            interpreter.virtualenv().purelib.as_path(),
+            interpreter.virtualenv().platlib.as_path(),
+        ]
+        .iter()
+        .dedup()
+        .map(|path| {
+            pathdiff::diff_paths(path, &interpreter.virtualenv().scripts)
+                .expect("Failed to calculate relative path to site-packages")
         })
-    }
-
-    /// Returns the Python version.
-    #[inline]
-    pub const fn python_version(&self) -> &Version {
-        &self.markers.python_full_version.version
-    }
+        .map(|path| path.simplified().to_str().unwrap().replace('\\', "\\\\"))
+        .join(path_sep);
 
-    /// Return the major version of this Python version.
-    pub fn python_major(&self) -> u8 {
-        let major = self.markers.python_full_version.version.release()[0];
-        u8::try_from(major).expect("invalid major version")
-    }
-
-    /// Return the minor version of this Python version.
-    pub fn python_minor(&self) -> u8 {
-        let minor = self.markers.python_full_version.version.release()[1];
-        u8::try_from(minor).expect("invalid minor version")
-    }
-
-    /// Return the patch version of this Python version.
-    pub fn python_patch(&self) -> u8 {
-        let minor = self.markers.python_full_version.version.release()[2];
-        u8::try_from(minor).expect("invalid patch version")
+        let activator = template
+            .replace(
+                "{{ VIRTUAL_ENV_DIR }}",
+                // SAFETY: `unwrap` is guaranteed to succeed because `location` is an `Utf8PathBuf`.
+                location.simplified().to_str().unwrap(),
+            )
+            .replace("{{ BIN_NAME }}", bin_name)
+            .replace(
+                "{{ VIRTUAL_PROMPT }}",
+                prompt.as_deref().unwrap_or_default(),
+            )
+            .replace("{{ PATH_SEP }}", path_sep)
+            .replace("{{ RELATIVE_SITE_PACKAGES }}", &relative_site_packages);
+        fs::write(scripts.join(name), activator)?;
+    }
+
+    let mut pyvenv_cfg_data: Vec<(String, String)> = vec![
+        (
+            "home".to_string(),
+            python_home.simplified_display().to_string(),
+        ),
+        (
+            "implementation".to_string(),
+            interpreter
+                .markers()
+                .platform_python_implementation()
+                .to_string(),
+        ),
+        ("uv".to_string(), version().to_string()),
+        (
+            "version_info".to_string(),
+            interpreter.markers().python_full_version().string.clone(),
+        ),
+        (
+            "include-system-site-packages".to_string(),
+            if system_site_packages {
+                "true".to_string()
+            } else {
+                "false".to_string()
+            },
+        ),
+    ];
+
+    if let Some(prompt) = prompt {
+        pyvenv_cfg_data.push(("prompt".to_string(), prompt));
+    }
+
+    let mut pyvenv_cfg = BufWriter::new(File::create(location.join("pyvenv.cfg"))?);
+    write_cfg(&mut pyvenv_cfg, &pyvenv_cfg_data)?;
+    drop(pyvenv_cfg);
+
+    // Construct the path to the `site-packages` directory.
+    let site_packages = location.join(&interpreter.virtualenv().purelib);
+    fs::create_dir_all(&site_packages)?;
+
+    // If necessary, create a symlink from `lib64` to `lib`.
+    // See: https://github.com/python/cpython/blob/b228655c227b2ca298a8ffac44d14ce3d22f6faa/Lib/venv/__init__.py#L135C11-L135C16
+    #[cfg(unix)]
+    if interpreter.pointer_size().is_64()
+        && interpreter.markers().os_name() == "posix"
+        && interpreter.markers().sys_platform() != "darwin"
+    {
+        let lib64 = location.join("lib64");
+        let lib = location.join("lib");
+        match std::os::unix::fs::symlink(lib, lib64) {
+            Ok(()) => {}
+            Err(err) if err.kind() == io::ErrorKind::AlreadyExists => {}
+            Err(err) => {
+                return Err(err.into());
+            }
+        }
     }
 
-    /// Returns the Python version as a simple tuple.
-    pub fn python_tuple(&self) -> (u8, u8) {
-        (self.python_major(), self.python_minor())
-    }
+    // Populate `site-packages` with a `_virtualenv.py` file.
+    fs::write(site_packages.join("_virtualenv.py"), VIRTUALENV_PATCH)?;
+    fs::write(site_packages.join("_virtualenv.pth"), "import _virtualenv")?;
+
+    Ok(VirtualEnvironment {
+        scheme: Scheme {
+            purelib: location.join(&interpreter.virtualenv().purelib),
+            platlib: location.join(&interpreter.virtualenv().platlib),
+            scripts: location.join(&interpreter.virtualenv().scripts),
+            data: location.join(&interpreter.virtualenv().data),
+            include: location.join(&interpreter.virtualenv().include),
+        },
+        root: location,
+        executable,
+    })
+}
 
-    /// Return the major version of the implementation (e.g., `CPython` or `PyPy`).
-    pub fn implementation_major(&self) -> u8 {
-        let major = self.markers.implementation_version.version.release()[0];
-        u8::try_from(major).expect("invalid major version")
-    }
+#[derive(Debug, Copy, Clone)]
+enum WindowsExecutable {
+    /// The `python.exe` executable (or `venvlauncher.exe` launcher shim).
+    Python,
+    /// The `pythonw.exe` executable (or `venvwlauncher.exe` launcher shim).
+    Pythonw,
+}
 
-    /// Return the minor version of the implementation (e.g., `CPython` or `PyPy`).
-    pub fn implementation_minor(&self) -> u8 {
-        let minor = self.markers.implementation_version.version.release()[1];
-        u8::try_from(minor).expect("invalid minor version")
+impl WindowsExecutable {
+    /// The name of the Python executable.
+    fn exe(self) -> &'static str {
+        match self {
+            WindowsExecutable::Python => "python.exe",
+            WindowsExecutable::Pythonw => "pythonw.exe",
+        }
     }
 
-    /// Returns the implementation version as a simple tuple.
-    pub fn implementation_tuple(&self) -> (u8, u8) {
-        (self.implementation_major(), self.implementation_minor())
+    /// The name of the launcher shim.
+    fn launcher(self) -> &'static str {
+        match self {
+            WindowsExecutable::Python => "venvlauncher.exe",
+            WindowsExecutable::Pythonw => "venvwlauncher.exe",
+        }
     }
+}
 
-    pub fn implementation_name(&self) -> &str {
-        &self.markers.implementation_name
-    }
-    pub fn base_exec_prefix(&self) -> &Path {
-        &self.base_exec_prefix
-    }
-    pub fn base_prefix(&self) -> &Path {
-        &self.base_prefix
+/// <https://github.com/python/cpython/blob/d457345bbc6414db0443819290b04a9a4333313d/Lib/venv/__init__.py#L261-L267>
+/// <https://github.com/pypa/virtualenv/blob/d9fdf48d69f0d0ca56140cf0381edbb5d6fe09f5/src/virtualenv/create/via_global_ref/builtin/cpython/cpython3.py#L78-L83>
+///
+/// There's two kinds of applications on windows: Those that allocate a console (python.exe)
+/// and those that don't because they use window(s) (pythonw.exe).
+fn copy_launcher_windows(
+    executable: WindowsExecutable,
+    interpreter: &Interpreter,
+    base_python: &Path,
+    scripts: &Path,
+    python_home: &Path,
+) -> Result<(), Error> {
+    // First priority: the `python.exe` and `pythonw.exe` shims.
+    let shim = interpreter
+        .stdlib()
+        .join("venv")
+        .join("scripts")
+        .join("nt")
+        .join(executable.exe());
+    match fs_err::copy(shim, scripts.join(executable.exe())) {
+        Ok(_) => return Ok(()),
+        Err(err) if err.kind() == io::ErrorKind::NotFound => {}
+        Err(err) => {
+            return Err(err.into());
+        }
     }
 
-    /// `sysconfig.get_path("stdlib")`
-    pub fn stdlib(&self) -> &Path {
-        &self.stdlib
-    }
-    pub fn sys_executable(&self) -> &Path {
-        &self.sys_executable
+    // Second priority: the `venvlauncher.exe` and `venvwlauncher.exe` shims.
+    // These are equivalent to the `python.exe` and `pythonw.exe` shims, which were
+    // renamed in Python 3.13.
+    let shim = interpreter
+        .stdlib()
+        .join("venv")
+        .join("scripts")
+        .join("nt")
+        .join(executable.launcher());
+    match fs_err::copy(shim, scripts.join(executable.exe())) {
+        Ok(_) => return Ok(()),
+        Err(err) if err.kind() == io::ErrorKind::NotFound => {}
+        Err(err) => {
+            return Err(err.into());
+        }
     }
-}
 
-#[derive(Debug, Deserialize, Serialize, Clone)]
-pub(crate) struct InterpreterQueryResult {
-    pub(crate) markers: MarkerEnvironment,
-    pub(crate) base_exec_prefix: PathBuf,
-    pub(crate) base_prefix: PathBuf,
-    pub(crate) stdlib: PathBuf,
-    pub(crate) sys_executable: PathBuf,
-}
-
-impl InterpreterQueryResult {
-    /// Return the resolved [`InterpreterQueryResult`] for the given Python executable.
-    pub(crate) fn query(interpreter: &Path) -> Result<Self, Error> {
-        let script = include_str!("get_interpreter_info.py");
-        let output = if cfg!(windows)
-            && interpreter
-                .extension()
-                .is_some_and(|extension| extension == "bat")
-        {
-            // Multiline arguments aren't well-supported in batch files and `pyenv-win`, for example, trips over it.
-            // We work around this batch limitation by passing the script via stdin instead.
-            // This is somewhat more expensive because we have to spawn a new thread to write the
-            // stdin to avoid deadlocks in case the child process waits for the parent to read stdout.
-            // The performance overhead is the reason why we only applies this to batch files.
-            // https://github.com/pyenv-win/pyenv-win/issues/589
-            let mut child = Command::new(interpreter)
-                .arg("-")
-                .stdin(std::process::Stdio::piped())
-                .stdout(std::process::Stdio::piped())
-                .spawn()
-                .map_err(|err| Error::PythonSubcommandLaunch {
-                    interpreter: interpreter.to_path_buf(),
-                    err,
-                })?;
-
-            let mut stdin = child.stdin.take().unwrap();
-
-            // From the Rust documentation:
-            // If the child process fills its stdout buffer, it may end up
-            // waiting until the parent reads the stdout, and not be able to
-            // read stdin in the meantime, causing a deadlock.
-            // Writing from another thread ensures that stdout is being read
-            // at the same time, avoiding the problem.
-            std::thread::spawn(move || {
-                stdin
-                    .write_all(script.as_bytes())
-                    .expect("failed to write to stdin");
-            });
-
-            child.wait_with_output()
-        } else {
-            Command::new(interpreter).arg("-c").arg(script).output()
+    // Third priority: on Conda at least, we can look for the launcher shim next to
+    // the Python executable itself.
+    let shim = base_python.with_file_name(executable.launcher());
+    match fs_err::copy(shim, scripts.join(executable.exe())) {
+        Ok(_) => return Ok(()),
+        Err(err) if err.kind() == io::ErrorKind::NotFound => {}
+        Err(err) => {
+            return Err(err.into());
         }
-        .map_err(|err| Error::PythonSubcommandLaunch {
-            interpreter: interpreter.to_path_buf(),
-            err,
-        })?;
-
-        // stderr isn't technically a criterion for success, but i don't know of any cases where there
-        // should be stderr output and if there is, we want to know
-        if !output.status.success() || !output.stderr.is_empty() {
-            return Err(Error::PythonSubcommandOutput {
-                message: format!(
-                    "Querying Python at `{}` failed with status {}",
-                    interpreter.display(),
-                    output.status,
-                ),
-                stdout: String::from_utf8_lossy(&output.stdout).trim().to_string(),
-                stderr: String::from_utf8_lossy(&output.stderr).trim().to_string(),
-            });
-        }
-        let data = serde_json::from_slice::<Self>(&output.stdout).map_err(|err| {
-            Error::PythonSubcommandOutput {
-                message: format!(
-                    "Querying Python at `{}` did not return the expected data: {err}",
-                    interpreter.display(),
-                ),
-                stdout: String::from_utf8_lossy(&output.stdout).trim().to_string(),
-                stderr: String::from_utf8_lossy(&output.stderr).trim().to_string(),
-            }
-        })?;
-
-        Ok(data)
     }
 
-    /// A wrapper around [`markers::query_interpreter_info`] to cache the computed markers.
-    ///
-    /// Running a Python script is (relatively) expensive, and the markers won't change
-    /// unless the Python executable changes, so we use the executable's last modified
-    /// time as a cache key.
-    pub(crate) fn query_cached(executable: &Path, cache: &Cache) -> Result<Self, Error> {
-        let executable_bytes = executable.as_os_str().as_encoded_bytes();
-
-        let cache_entry = cache.entry(
-            CacheBucket::Interpreter,
-            "",
-            format!("{}.msgpack", digest(&executable_bytes)),
-        );
-
-        let modified = Timestamp::from_path(fs_err::canonicalize(executable)?)?;
-
-        // Read from the cache.
-        if cache
-            .freshness(&cache_entry, None)
-            .is_ok_and(Freshness::is_fresh)
-        {
-            if let Ok(data) = fs::read(cache_entry.path()) {
-                match rmp_serde::from_slice::<CachedByTimestamp<Self>>(&data) {
-                    Ok(cached) => {
-                        if cached.timestamp == modified {
-                            debug!("Using cached markers for: {}", executable.display());
-                            return Ok(cached.data);
-                        }
-
-                        debug!(
-                            "Ignoring stale cached markers for: {}",
-                            executable.display()
-                        );
+    // Fourth priority: if the launcher shim doesn't exist, assume this is
+    // an embedded Python. Copy the Python executable itself, along with
+    // the DLLs, `.pyd` files, and `.zip` files in the same directory.
+    match fs_err::copy(
+        base_python.with_file_name(executable.exe()),
+        scripts.join(executable.exe()),
+    ) {
+        Ok(_) => {
+            // Copy `.dll` and `.pyd` files from the top-level, and from the
+            // `DLLs` subdirectory (if it exists).
+            for directory in [
+                python_home,
+                interpreter.base_prefix().join("DLLs").as_path(),
+            ] {
+                let entries = match fs_err::read_dir(directory) {
+                    Ok(read_dir) => read_dir,
+                    Err(err) if err.kind() == io::ErrorKind::NotFound => {
+                        continue;
                     }
                     Err(err) => {
-                        warn!(
-                            "Broken cache entry at {}, removing: {err}",
-                            cache_entry.path().display()
-                        );
-                        let _ = fs_err::remove_file(cache_entry.path());
+                        return Err(err.into());
+                    }
+                };
+                for entry in entries {
+                    let entry = entry?;
+                    let path = entry.path();
+                    if path.extension().is_some_and(|ext| {
+                        ext.eq_ignore_ascii_case("dll") || ext.eq_ignore_ascii_case("pyd")
+                    }) {
+                        if let Some(file_name) = path.file_name() {
+                            fs_err::copy(&path, scripts.join(file_name))?;
+                        }
                     }
                 }
             }
-        }
 
-        // Otherwise, run the Python script.
-        debug!("Detecting markers for: {}", executable.display());
-        let info = Self::query(executable)?;
-
-        // If `executable` is a pyenv shim, a bash script that redirects to the activated
-        // python executable at another path, we're not allowed to cache the interpreter info.
-        if same_file::is_same_file(executable, &info.sys_executable).unwrap_or(false) {
-            fs::create_dir_all(cache_entry.dir())?;
-            write_atomic_sync(
-                cache_entry.path(),
-                rmp_serde::to_vec(&CachedByTimestamp {
-                    timestamp: modified,
-                    data: info.clone(),
-                })?,
-            )?;
-        }
+            // Copy `.zip` files from the top-level.
+            match fs_err::read_dir(python_home) {
+                Ok(entries) => {
+                    for entry in entries {
+                        let entry = entry?;
+                        let path = entry.path();
+                        if path
+                            .extension()
+                            .is_some_and(|ext| ext.eq_ignore_ascii_case("zip"))
+                        {
+                            if let Some(file_name) = path.file_name() {
+                                fs_err::copy(&path, scripts.join(file_name))?;
+                            }
+                        }
+                    }
+                }
+                Err(err) if err.kind() == io::ErrorKind::NotFound => {}
+                Err(err) => {
+                    return Err(err.into());
+                }
+            };
 
-        Ok(info)
+            return Ok(());
+        }
+        Err(err) if err.kind() == io::ErrorKind::NotFound => {}
+        Err(err) => {
+            return Err(err.into());
+        }
     }
-}
-
-#[cfg(unix)]
-#[cfg(test)]
-mod tests {
-    use std::str::FromStr;
-
-    use fs_err as fs;
-    use indoc::{formatdoc, indoc};
-    use tempfile::tempdir;
-
-    use pep440_rs::Version;
-    use platform_host::Platform;
-    use uv_cache::Cache;
-
-    use crate::Interpreter;
-
-    #[test]
-    fn test_cache_invalidation() {
-        let mock_dir = tempdir().unwrap();
-        let mocked_interpreter = mock_dir.path().join("python");
-        let json = indoc! {r##"
-            {
-                "markers": {
-                    "implementation_name": "cpython",
-                    "implementation_version": "3.12.0",
-                    "os_name": "posix",
-                    "platform_machine": "x86_64",
-                    "platform_python_implementation": "CPython",
-                    "platform_release": "6.5.0-13-generic",
-                    "platform_system": "Linux",
-                    "platform_version": "#13-Ubuntu SMP PREEMPT_DYNAMIC Fri Nov  3 12:16:05 UTC 2023",
-                    "python_full_version": "3.12.0",
-                    "python_version": "3.12",
-                    "sys_platform": "linux"
-                },
-                "base_exec_prefix": "/home/ferris/.pyenv/versions/3.12.0",
-                "base_prefix": "/home/ferris/.pyenv/versions/3.12.0",
-                "stdlib": "/usr/lib/python3.12",
-                "sys_executable": "/home/ferris/projects/uv/.venv/bin/python"
-            }
-        "##};
 
-        let cache = Cache::temp().unwrap();
-        let platform = Platform::current().unwrap();
-
-        fs::write(
-            &mocked_interpreter,
-            formatdoc! {r##"
-            #!/bin/bash
-            echo '{json}'
-            "##},
-        )
-        .unwrap();
-        fs::set_permissions(
-            &mocked_interpreter,
-            std::os::unix::fs::PermissionsExt::from_mode(0o770),
-        )
-        .unwrap();
-        let interpreter = Interpreter::query(&mocked_interpreter, &platform, &cache).unwrap();
-        assert_eq!(
-            interpreter.markers.python_version.version,
-            Version::from_str("3.12").unwrap()
-        );
-        fs::write(
-            &mocked_interpreter,
-            formatdoc! {r##"
-            #!/bin/bash
-            echo '{}'
-            "##, json.replace("3.12", "3.13")},
-        )
-        .unwrap();
-        let interpreter = Interpreter::query(&mocked_interpreter, &platform, &cache).unwrap();
-        assert_eq!(
-            interpreter.markers.python_version.version,
-            Version::from_str("3.13").unwrap()
-        );
-    }
+    Err(Error::NotFound(base_python.user_display().to_string()))
 }
```

### Comparing `uv-0.1.9/crates/uv-interpreter/src/virtual_env.rs` & `uv-0.2.0/crates/uv-interpreter/src/virtualenv.rs`

 * *Files 24% similar despite different names*

```diff
@@ -1,144 +1,165 @@
-use std::env;
-use std::env::consts::EXE_SUFFIX;
-use std::path::{Path, PathBuf};
+use std::{
+    env, io,
+    path::{Path, PathBuf},
+};
+
+use fs_err as fs;
+use pypi_types::Scheme;
+use thiserror::Error;
+use tracing::{debug, info};
+
+/// The layout of a virtual environment.
+#[derive(Debug)]
+pub struct VirtualEnvironment {
+    /// The absolute path to the root of the virtualenv, e.g., `/path/to/.venv`.
+    pub root: PathBuf,
+
+    /// The path to the Python interpreter inside the virtualenv, e.g., `.venv/bin/python`
+    /// (Unix, Python 3.11).
+    pub executable: PathBuf,
 
-use tracing::debug;
-
-use platform_host::Platform;
-use uv_cache::Cache;
-use uv_fs::{LockedFile, Normalized};
-
-use crate::cfg::Configuration;
-use crate::python_platform::PythonPlatform;
-use crate::{Error, Interpreter};
+    /// The [`Scheme`] paths for the virtualenv, as returned by (e.g.) `sysconfig.get_paths()`.
+    pub scheme: Scheme,
+}
 
-/// A Python executable and its associated platform markers.
+/// A parsed `pyvenv.cfg`
 #[derive(Debug, Clone)]
-pub struct Virtualenv {
-    root: PathBuf,
-    interpreter: Interpreter,
-}
-
-impl Virtualenv {
-    /// Venv the current Python executable from the host environment.
-    pub fn from_env(platform: Platform, cache: &Cache) -> Result<Self, Error> {
-        let platform = PythonPlatform::from(platform);
-        let Some(venv) = detect_virtual_env(&platform)? else {
-            return Err(Error::NotFound);
-        };
-        let venv = fs_err::canonicalize(venv)?;
-        let executable = platform.venv_python(&venv);
-        let interpreter = Interpreter::query(&executable, &platform.0, cache)?;
-
-        debug_assert!(
-            interpreter.base_prefix == interpreter.base_exec_prefix,
-            "Not a virtualenv (Python: {}, prefix: {})",
-            executable.display(),
-            interpreter.base_prefix.display()
-        );
-
-        Ok(Self {
-            root: venv,
-            interpreter,
-        })
-    }
-
-    /// Creating a new venv from a Python interpreter changes this.
-    pub fn from_interpreter(interpreter: Interpreter, venv: &Path) -> Self {
-        Self {
-            interpreter: interpreter.with_base_prefix(venv.to_path_buf()),
-            root: venv.to_path_buf(),
-        }
-    }
-
-    /// Returns the location of the python interpreter
-    pub fn python_executable(&self) -> PathBuf {
-        self.bin_dir().join(format!("python{EXE_SUFFIX}"))
-    }
-
-    pub fn root(&self) -> &Path {
-        &self.root
-    }
-
-    /// Return the [`Interpreter`] for this virtual environment.
-    pub fn interpreter(&self) -> &Interpreter {
-        &self.interpreter
-    }
+pub struct PyVenvConfiguration {
+    /// If the `virtualenv` package was used to create the virtual environment.
+    pub(crate) virtualenv: bool,
+    /// If the `uv` package was used to create the virtual environment.
+    pub(crate) uv: bool,
+}
 
-    /// Return the [`Configuration`] for this virtual environment, as extracted from the
-    /// `pyvenv.cfg` file.
-    pub fn cfg(&self) -> Result<Configuration, Error> {
-        Ok(Configuration::parse(self.root.join("pyvenv.cfg"))?)
-    }
+#[derive(Debug, Error)]
+pub enum Error {
+    #[error("Broken virtualenv `{0}`: `pyvenv.cfg` is missing")]
+    MissingPyVenvCfg(PathBuf),
+    #[error("Broken virtualenv `{0}`: `pyvenv.cfg` could not be parsed")]
+    ParsePyVenvCfg(PathBuf, #[source] io::Error),
+    #[error(transparent)]
+    IO(#[from] io::Error),
+}
 
-    /// Returns the path to the `site-packages` directory inside a virtual environment.
-    pub fn site_packages(&self) -> PathBuf {
-        self.interpreter
-            .platform
-            .venv_site_packages(&self.root, self.interpreter().python_tuple())
+/// Locate an active virtual environment by inspecting environment variables.
+///
+/// Supports `VIRTUAL_ENV` and `CONDA_PREFIX`.
+pub(crate) fn virtualenv_from_env() -> Option<PathBuf> {
+    if let Some(dir) = env::var_os("VIRTUAL_ENV").filter(|value| !value.is_empty()) {
+        info!(
+            "Found active virtual environment (via VIRTUAL_ENV) at: {}",
+            Path::new(&dir).display()
+        );
+        return Some(PathBuf::from(dir));
     }
 
-    pub fn bin_dir(&self) -> PathBuf {
-        if cfg!(unix) {
-            self.root().join("bin")
-        } else if cfg!(windows) {
-            self.root().join("Scripts")
-        } else {
-            unimplemented!("Only Windows and Unix are supported")
-        }
+    if let Some(dir) = env::var_os("CONDA_PREFIX").filter(|value| !value.is_empty()) {
+        info!(
+            "Found active virtual environment (via CONDA_PREFIX) at: {}",
+            Path::new(&dir).display()
+        );
+        return Some(PathBuf::from(dir));
     }
 
-    /// Lock the virtual environment to prevent concurrent writes.
-    pub fn lock(&self) -> Result<LockedFile, std::io::Error> {
-        LockedFile::acquire(self.root.join(".lock"), self.root.normalized_display())
-    }
+    None
 }
 
-/// Locate the current virtual environment.
-pub(crate) fn detect_virtual_env(target: &PythonPlatform) -> Result<Option<PathBuf>, Error> {
-    match (
-        env::var_os("VIRTUAL_ENV").filter(|value| !value.is_empty()),
-        env::var_os("CONDA_PREFIX").filter(|value| !value.is_empty()),
-    ) {
-        (Some(dir), None) => {
-            debug!(
-                "Found a virtualenv through VIRTUAL_ENV at: {}",
-                Path::new(&dir).display()
-            );
-            return Ok(Some(PathBuf::from(dir)));
-        }
-        (None, Some(dir)) => {
-            debug!(
-                "Found a virtualenv through CONDA_PREFIX at: {}",
-                Path::new(&dir).display()
-            );
-            return Ok(Some(PathBuf::from(dir)));
-        }
-        (Some(venv), Some(conda)) if venv == conda => return Ok(Some(PathBuf::from(venv))),
-        (Some(_), Some(_)) => {
-            return Err(Error::Conflict);
-        }
-        (None, None) => {
-            // No environment variables set. Try to find a virtualenv in the current directory.
-        }
-    };
+/// Locate a virtual environment by searching the file system.
+///
+/// Searches for a `.venv` directory in the current or any parent directory. If the current
+/// directory is itself a virtual environment (or a subdirectory of a virtual environment), the
+/// containing virtual environment is returned.
+pub(crate) fn virtualenv_from_working_dir() -> Result<Option<PathBuf>, Error> {
+    let current_dir = crate::current_dir()?;
 
-    // Search for a `.venv` directory in the current or any parent directory.
-    let current_dir = env::current_dir().expect("Failed to detect current directory");
     for dir in current_dir.ancestors() {
+        // If we're _within_ a virtualenv, return it.
+        if dir.join("pyvenv.cfg").is_file() {
+            debug!("Found a virtual environment at: {}", dir.display());
+            return Ok(Some(dir.to_path_buf()));
+        }
+
+        // Otherwise, search for a `.venv` directory.
         let dot_venv = dir.join(".venv");
         if dot_venv.is_dir() {
             if !dot_venv.join("pyvenv.cfg").is_file() {
                 return Err(Error::MissingPyVenvCfg(dot_venv));
             }
-            let python = target.venv_python(&dot_venv);
-            if !python.is_file() {
-                return Err(Error::BrokenVenv(dot_venv, python));
-            }
-            debug!("Found a virtualenv named .venv at: {}", dot_venv.display());
+            debug!("Found a virtual environment at: {}", dot_venv.display());
             return Ok(Some(dot_venv));
         }
     }
 
     Ok(None)
 }
+
+/// Returns the path to the `python` executable inside a virtual environment.
+pub(crate) fn virtualenv_python_executable(venv: impl AsRef<Path>) -> PathBuf {
+    let venv = venv.as_ref();
+    if cfg!(windows) {
+        // Search for `python.exe` in the `Scripts` directory.
+        let default_executable = venv.join("Scripts").join("python.exe");
+        if default_executable.exists() {
+            return default_executable;
+        }
+
+        // Apparently, Python installed via msys2 on Windows _might_ produce a POSIX-like layout.
+        // See: https://github.com/PyO3/maturin/issues/1108
+        let executable = venv.join("bin").join("python.exe");
+        if executable.exists() {
+            return executable;
+        }
+
+        // Fallback for Conda environments.
+        let executable = venv.join("python.exe");
+        if executable.exists() {
+            return executable;
+        }
+
+        // If none of these exist, return the standard location
+        default_executable
+    } else {
+        // Search for `python` in the `bin` directory.
+        venv.join("bin").join("python")
+    }
+}
+
+impl PyVenvConfiguration {
+    /// Parse a `pyvenv.cfg` file into a [`PyVenvConfiguration`].
+    pub fn parse(cfg: impl AsRef<Path>) -> Result<Self, Error> {
+        let mut virtualenv = false;
+        let mut uv = false;
+
+        // Per https://snarky.ca/how-virtual-environments-work/, the `pyvenv.cfg` file is not a
+        // valid INI file, and is instead expected to be parsed by partitioning each line on the
+        // first equals sign.
+        let content = fs::read_to_string(&cfg)
+            .map_err(|err| Error::ParsePyVenvCfg(cfg.as_ref().to_path_buf(), err))?;
+        for line in content.lines() {
+            let Some((key, _value)) = line.split_once('=') else {
+                continue;
+            };
+            match key.trim() {
+                "virtualenv" => {
+                    virtualenv = true;
+                }
+                "uv" => {
+                    uv = true;
+                }
+                _ => {}
+            }
+        }
+
+        Ok(Self { virtualenv, uv })
+    }
+
+    /// Returns true if the virtual environment was created with the `virtualenv` package.
+    pub fn is_virtualenv(&self) -> bool {
+        self.virtualenv
+    }
+
+    /// Returns true if the virtual environment was created with the `uv` package.
+    pub fn is_uv(&self) -> bool {
+        self.uv
+    }
+}
```

### Comparing `uv-0.1.9/crates/uv-resolver/Cargo.toml` & `uv-0.2.0/crates/uv-resolver/Cargo.toml`

 * *Files 12% similar despite different names*

```diff
@@ -9,66 +9,63 @@
 authors = { workspace = true }
 license = { workspace = true }
 
 [lints]
 workspace = true
 
 [dependencies]
-cache-key = { path = "../cache-key" }
-distribution-filename = { path = "../distribution-filename", features = ["serde"] }
-distribution-types = { path = "../distribution-types" }
-install-wheel-rs = { path = "../install-wheel-rs" }
-once-map = { path = "../once-map" }
-pep440_rs = { path = "../pep440-rs", features = ["pubgrub"] }
-pep508_rs = { path = "../pep508-rs" }
-platform-host = { path = "../platform-host" }
-platform-tags = { path = "../platform-tags" }
-uv-cache = { path = "../uv-cache" }
-uv-client = { path = "../uv-client" }
-uv-distribution = { path = "../uv-distribution" }
-uv-git = { path = "../uv-git", features = ["vendored-openssl"] }
-uv-interpreter = { path = "../uv-interpreter" }
-uv-normalize = { path = "../uv-normalize" }
-uv-traits = { path = "../uv-traits" }
-uv-warnings = { path = "../uv-warnings" }
-pypi-types = { path = "../pypi-types" }
+cache-key = { workspace = true }
+distribution-filename = { workspace = true }
+distribution-types = { workspace = true }
+install-wheel-rs = { workspace = true }
+once-map = { workspace = true }
+pep440_rs = { workspace = true }
+pep508_rs = { workspace = true }
+platform-tags = { workspace = true }
+pypi-types = { workspace = true }
+requirements-txt = { workspace = true }
+uv-cache = { workspace = true }
+uv-client = { workspace = true }
+uv-configuration = { workspace = true }
+uv-distribution = { workspace = true }
+uv-git = { workspace = true }
+uv-interpreter = { workspace = true }
+uv-normalize = { workspace = true }
+uv-types = { workspace = true }
+uv-warnings = { workspace = true }
 
 anstream = { workspace = true }
 anyhow = { workspace = true }
 chrono = { workspace = true }
 clap = { workspace = true, features = ["derive"], optional = true }
 dashmap = { workspace = true }
 derivative = { workspace = true }
 either = { workspace = true }
-fs-err = { workspace = true, features = ["tokio"] }
 futures = { workspace = true }
 indexmap = { workspace = true }
 itertools = { workspace = true }
 once_cell = { workspace = true }
 owo-colors = { workspace = true }
 petgraph = { workspace = true }
 pubgrub = { workspace = true }
-reqwest = { workspace = true }
-rkyv = { workspace = true, features = ["strict", "validation"] }
+rkyv = { workspace = true }
 rustc-hash = { workspace = true }
-serde_json = { workspace = true }
-sha2 = { workspace = true }
-tempfile = { workspace = true }
+schemars = { workspace = true, optional = true }
+serde = { workspace = true }
+textwrap = { workspace = true }
 thiserror = { workspace = true }
-tokio = { workspace = true, features = ["macros"] }
-tokio-stream  = { workspace = true }
-tokio-util = { workspace = true, features = ["compat"] }
+tokio = { workspace = true }
+tokio-stream = { workspace = true }
 tracing = { workspace = true }
 url = { workspace = true }
-zip = { workspace = true }
 
 [dev-dependencies]
-gourgeist = { path = "../gourgeist" }
-uv-interpreter = { path = "../uv-interpreter" }
+uv-interpreter = { workspace = true }
 
 once_cell = { version = "1.19.0" }
-insta = { version = "1.34.0" }
+insta = { version = "1.36.1" }
+toml = { workspace = true }
 
 [features]
 default = ["pypi"]
 # Introduces a dependency on PyPI.
 pypi = []
```

### Comparing `uv-0.1.9/crates/uv-resolver/src/candidate_selector.rs` & `uv-0.2.0/crates/uv-resolver/src/candidate_selector.rs`

 * *Files 24% similar despite different names*

```diff
@@ -1,120 +1,193 @@
+use itertools::Itertools;
 use pubgrub::range::Range;
+use tracing::debug;
 
-use rustc_hash::FxHashMap;
-
-use distribution_types::CompatibleDist;
+use distribution_types::{CompatibleDist, IncompatibleDist, IncompatibleSource};
 use distribution_types::{DistributionMetadata, IncompatibleWheel, Name, PrioritizedDist};
 use pep440_rs::Version;
-use pep508_rs::{Requirement, VersionOrUrl};
+use pep508_rs::MarkerEnvironment;
+use uv_configuration::IndexStrategy;
 use uv_normalize::PackageName;
+use uv_types::InstalledPackagesProvider;
 
+use crate::preferences::Preferences;
 use crate::prerelease_mode::PreReleaseStrategy;
-
 use crate::resolution_mode::ResolutionStrategy;
 use crate::version_map::{VersionMap, VersionMapDistHandle};
-use crate::{Manifest, Options};
+use crate::{Exclusions, Manifest, Options};
 
 #[derive(Debug, Clone)]
+#[allow(clippy::struct_field_names)]
 pub(crate) struct CandidateSelector {
     resolution_strategy: ResolutionStrategy,
     prerelease_strategy: PreReleaseStrategy,
-    preferences: Preferences,
+    index_strategy: IndexStrategy,
 }
 
 impl CandidateSelector {
     /// Return a [`CandidateSelector`] for the given [`Manifest`].
-    pub(crate) fn for_resolution(manifest: &Manifest, options: Options) -> Self {
+    pub(crate) fn for_resolution(
+        options: Options,
+        manifest: &Manifest,
+        markers: Option<&MarkerEnvironment>,
+    ) -> Self {
         Self {
             resolution_strategy: ResolutionStrategy::from_mode(
                 options.resolution_mode,
-                manifest.requirements.as_slice(),
+                manifest,
+                markers,
+                options.dependency_mode,
             ),
             prerelease_strategy: PreReleaseStrategy::from_mode(
                 options.prerelease_mode,
-                manifest.requirements.as_slice(),
+                manifest,
+                markers,
+                options.dependency_mode,
             ),
-            preferences: Preferences::from(manifest.preferences.as_slice()),
+            index_strategy: options.index_strategy,
         }
     }
 
     #[inline]
     #[allow(dead_code)]
     pub(crate) fn resolution_strategy(&self) -> &ResolutionStrategy {
         &self.resolution_strategy
     }
 
     #[inline]
     #[allow(dead_code)]
     pub(crate) fn prerelease_strategy(&self) -> &PreReleaseStrategy {
         &self.prerelease_strategy
     }
-}
-
-/// A set of pinned packages that should be preserved during resolution, if possible.
-#[derive(Debug, Clone)]
-struct Preferences(FxHashMap<PackageName, Version>);
-
-impl Preferences {
-    fn get(&self, package_name: &PackageName) -> Option<&Version> {
-        self.0.get(package_name)
-    }
-}
 
-impl From<&[Requirement]> for Preferences {
-    fn from(requirements: &[Requirement]) -> Self {
-        Self(
-            requirements
-                .iter()
-                .filter_map(|requirement| {
-                    let Some(VersionOrUrl::VersionSpecifier(version_specifiers)) =
-                        requirement.version_or_url.as_ref()
-                    else {
-                        return None;
-                    };
-                    let [version_specifier] = version_specifiers.as_ref() else {
-                        return None;
-                    };
-                    Some((
-                        requirement.name.clone(),
-                        version_specifier.version().clone(),
-                    ))
-                })
-                .collect(),
-        )
+    #[inline]
+    #[allow(dead_code)]
+    pub(crate) fn index_strategy(&self) -> &IndexStrategy {
+        &self.index_strategy
     }
 }
 
 #[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]
 enum AllowPreRelease {
     Yes,
     No,
     IfNecessary,
 }
 
 impl CandidateSelector {
     /// Select a [`Candidate`] from a set of candidate versions and files.
-    pub(crate) fn select<'a>(
+    ///
+    /// Unless present in the provided [`Exclusions`], local distributions from the
+    /// [`InstalledPackagesProvider`] are preferred over remote distributions in
+    /// the [`VersionMap`].
+    pub(crate) fn select<'a, InstalledPackages: InstalledPackagesProvider>(
         &'a self,
         package_name: &'a PackageName,
-        range: &'a Range<Version>,
-        version_map: &'a VersionMap,
+        range: &Range<Version>,
+        version_maps: &'a [VersionMap],
+        preferences: &'a Preferences,
+        installed_packages: &'a InstalledPackages,
+        exclusions: &'a Exclusions,
+    ) -> Option<Candidate<'a>> {
+        if let Some(preferred) = Self::get_preferred(
+            package_name,
+            range,
+            version_maps,
+            preferences,
+            installed_packages,
+            exclusions,
+        ) {
+            return Some(preferred);
+        }
+
+        self.select_no_preference(package_name, range, version_maps)
+    }
+
+    /// Get a preferred version if one exists. This is the preference from a lockfile or a locally
+    /// installed version.
+    fn get_preferred<'a, InstalledPackages: InstalledPackagesProvider>(
+        package_name: &'a PackageName,
+        range: &Range<Version>,
+        version_maps: &'a [VersionMap],
+        preferences: &'a Preferences,
+        installed_packages: &'a InstalledPackages,
+        exclusions: &'a Exclusions,
     ) -> Option<Candidate<'a>> {
         // If the package has a preference (e.g., an existing version from an existing lockfile),
         // and the preference satisfies the current range, use that.
-        if let Some(version) = self.preferences.get(package_name) {
+        if let Some(version) = preferences.version(package_name) {
             if range.contains(version) {
-                if let Some(file) = version_map.get(version) {
+                // Check for a locally installed distribution that matches the preferred version
+                if !exclusions.contains(package_name) {
+                    let installed_dists = installed_packages.get_packages(package_name);
+                    match installed_dists.as_slice() {
+                        [] => {}
+                        [dist] => {
+                            if dist.version() == version {
+                                debug!("Found installed version of {dist} that satisfies preference in {range}");
+
+                                return Some(Candidate {
+                                    name: package_name,
+                                    version,
+                                    dist: CandidateDist::Compatible(CompatibleDist::InstalledDist(
+                                        dist,
+                                    )),
+                                });
+                            }
+                        }
+                        // We do not consider installed distributions with multiple versions because
+                        // during installation these must be reinstalled from the remote
+                        _ => {
+                            debug!("Ignoring installed versions of {package_name}: multiple distributions found");
+                        }
+                    }
+                }
+
+                // Check for a remote distribution that matches the preferred version
+                if let Some(file) = version_maps
+                    .iter()
+                    .find_map(|version_map| version_map.get(version))
+                {
                     return Some(Candidate::new(package_name, version, file));
                 }
             }
         }
 
-        // Determine the appropriate prerelease strategy for the current package.
-        let allow_prerelease = match &self.prerelease_strategy {
+        // Check for a locally installed distribution that satisfies the range
+        if !exclusions.contains(package_name) {
+            let installed_dists = installed_packages.get_packages(package_name);
+            match installed_dists.as_slice() {
+                [] => {}
+                [dist] => {
+                    let version = dist.version();
+                    if range.contains(version) {
+                        debug!("Found installed version of {dist} that satisfies {range}");
+
+                        return Some(Candidate {
+                            name: package_name,
+                            version,
+                            dist: CandidateDist::Compatible(CompatibleDist::InstalledDist(dist)),
+                        });
+                    }
+                }
+                // We do not consider installed distributions with multiple versions because
+                // during installation these must be reinstalled from the remote
+                _ => {
+                    debug!("Ignoring installed versions of {package_name}: multiple distributions found");
+                }
+            }
+        }
+
+        None
+    }
+
+    /// Determine the appropriate prerelease strategy for the current package.
+    fn allow_prereleases(&self, package_name: &PackageName) -> AllowPreRelease {
+        match &self.prerelease_strategy {
             PreReleaseStrategy::Disallow => AllowPreRelease::No,
             PreReleaseStrategy::Allow => AllowPreRelease::Yes,
             PreReleaseStrategy::IfNecessary => AllowPreRelease::IfNecessary,
             PreReleaseStrategy::Explicit(packages) => {
                 if packages.contains(package_name) {
                     AllowPreRelease::Yes
                 } else {
@@ -124,48 +197,85 @@
             PreReleaseStrategy::IfNecessaryOrExplicit(packages) => {
                 if packages.contains(package_name) {
                     AllowPreRelease::Yes
                 } else {
                     AllowPreRelease::IfNecessary
                 }
             }
-        };
+        }
+    }
 
+    /// Select a [`Candidate`] without checking for version preference such as an existing
+    /// lockfile.
+    pub(crate) fn select_no_preference<'a>(
+        &'a self,
+        package_name: &'a PackageName,
+        range: &Range<Version>,
+        version_maps: &'a [VersionMap],
+    ) -> Option<Candidate> {
         tracing::trace!(
-            "selecting candidate for package {:?} with range {:?} with {} versions",
-            package_name,
-            range,
-            version_map.len()
+            "selecting candidate for package {package_name} with range {range:?} with {} remote versions",
+            version_maps.iter().map(VersionMap::len).sum::<usize>(),
         );
-        match &self.resolution_strategy {
-            ResolutionStrategy::Highest => Self::select_candidate(
-                version_map.iter().rev(),
-                package_name,
-                range,
-                allow_prerelease,
-            ),
-            ResolutionStrategy::Lowest => {
-                Self::select_candidate(version_map.iter(), package_name, range, allow_prerelease)
+        let highest = self.use_highest_version(package_name);
+        let allow_prerelease = self.allow_prereleases(package_name);
+
+        if self.index_strategy == IndexStrategy::UnsafeBestMatch {
+            if highest {
+                Self::select_candidate(
+                    version_maps
+                        .iter()
+                        .map(|version_map| version_map.iter().rev())
+                        .kmerge_by(|(version1, _), (version2, _)| version1 > version2),
+                    package_name,
+                    range,
+                    allow_prerelease,
+                )
+            } else {
+                Self::select_candidate(
+                    version_maps
+                        .iter()
+                        .map(VersionMap::iter)
+                        .kmerge_by(|(version1, _), (version2, _)| version1 < version2),
+                    package_name,
+                    range,
+                    allow_prerelease,
+                )
             }
-            ResolutionStrategy::LowestDirect(direct_dependencies) => {
-                if direct_dependencies.contains(package_name) {
+        } else {
+            if highest {
+                version_maps.iter().find_map(|version_map| {
                     Self::select_candidate(
-                        version_map.iter(),
+                        version_map.iter().rev(),
                         package_name,
                         range,
                         allow_prerelease,
                     )
-                } else {
+                })
+            } else {
+                version_maps.iter().find_map(|version_map| {
                     Self::select_candidate(
-                        version_map.iter().rev(),
+                        version_map.iter(),
                         package_name,
                         range,
                         allow_prerelease,
                     )
-                }
+                })
+            }
+        }
+    }
+
+    /// By default, we select the latest version, but we also allow using the lowest version instead
+    /// to check the lower bounds.
+    pub(crate) fn use_highest_version(&self, package_name: &PackageName) -> bool {
+        match &self.resolution_strategy {
+            ResolutionStrategy::Highest => true,
+            ResolutionStrategy::Lowest => false,
+            ResolutionStrategy::LowestDirect(direct_dependencies) => {
+                !direct_dependencies.contains(package_name)
             }
         }
     }
 
     /// Select the first-matching [`Candidate`] from a set of candidate versions and files,
     /// preferring wheels over source distributions.
     fn select_candidate<'a>(
@@ -177,19 +287,18 @@
         #[derive(Debug)]
         enum PreReleaseCandidate<'a> {
             NotNecessary,
             IfNecessary(&'a Version, &'a PrioritizedDist),
         }
 
         let mut prerelease = None;
-        let mut steps = 0;
+        let mut steps = 0usize;
         for (version, maybe_dist) in versions {
             steps += 1;
-
-            let dist = if version.any_prerelease() {
+            let candidate = if version.any_prerelease() {
                 if range.contains(version) {
                     match allow_prerelease {
                         AllowPreRelease::Yes => {
                             let Some(dist) = maybe_dist.prioritized_dist() else {
                                 continue;
                             };
                             tracing::trace!(
@@ -198,15 +307,15 @@
                                 package_name,
                                 range,
                                 steps,
                                 version,
                             );
                             // If pre-releases are allowed, treat them equivalently
                             // to stable distributions.
-                            dist
+                            Candidate::new(package_name, version, dist)
                         }
                         AllowPreRelease::IfNecessary => {
                             let Some(dist) = maybe_dist.prioritized_dist() else {
                                 continue;
                             };
                             // If pre-releases are allowed as a fallback, store the
                             // first-matching prerelease.
@@ -224,39 +333,50 @@
                 }
             } else {
                 // If we have at least one stable release, we shouldn't allow the "if-necessary"
                 // pre-release strategy, regardless of whether that stable release satisfies the
                 // current range.
                 prerelease = Some(PreReleaseCandidate::NotNecessary);
 
-                // Always return the first-matching stable distribution.
+                // Return the first-matching stable distribution.
                 if range.contains(version) {
                     let Some(dist) = maybe_dist.prioritized_dist() else {
                         continue;
                     };
                     tracing::trace!(
                         "found candidate for package {:?} with range {:?} \
                          after {} steps: {:?} version",
                         package_name,
                         range,
                         steps,
                         version,
                     );
-                    dist
+                    Candidate::new(package_name, version, dist)
                 } else {
                     continue;
                 }
             };
 
-            // Skip empty candidates due to exclude newer
-            if dist.exclude_newer() && dist.incompatible_wheel().is_none() && dist.get().is_none() {
+            // If candidate is not compatible due to exclude newer, continue searching.
+            // This is a special case — we pretend versions with exclude newer incompatibilities
+            // do not exist so that they are not present in error messages in our test suite.
+            // TODO(zanieb): Now that `--exclude-newer` is user facing we may want to consider
+            // flagging this behavior such that we _will_ report filtered distributions due to
+            // exclude-newer in our error messages.
+            if matches!(
+                candidate.dist(),
+                CandidateDist::Incompatible(
+                    IncompatibleDist::Source(IncompatibleSource::ExcludeNewer(_))
+                        | IncompatibleDist::Wheel(IncompatibleWheel::ExcludeNewer(_))
+                )
+            ) {
                 continue;
             }
 
-            return Some(Candidate::new(package_name, version, dist));
+            return Some(candidate);
         }
         tracing::trace!(
             "exhausted all candidates for package {:?} with range {:?} \
              after {} steps",
             package_name,
             range,
             steps,
@@ -270,33 +390,34 @@
         }
     }
 }
 
 #[derive(Debug, Clone)]
 pub(crate) enum CandidateDist<'a> {
     Compatible(CompatibleDist<'a>),
-    Incompatible(Option<&'a IncompatibleWheel>),
-    ExcludeNewer,
+    Incompatible(IncompatibleDist),
 }
 
 impl<'a> From<&'a PrioritizedDist> for CandidateDist<'a> {
     fn from(value: &'a PrioritizedDist) -> Self {
         if let Some(dist) = value.get() {
             CandidateDist::Compatible(dist)
         } else {
-            if value.exclude_newer() && value.incompatible_wheel().is_none() {
-                // If empty because of exclude-newer, mark as a special case
-                CandidateDist::ExcludeNewer
+            // TODO(zanieb)
+            // We always return the source distribution (if one exists) instead of the wheel
+            // but in the future we may want to return both so the resolver can explain
+            // why neither distribution kind can be used.
+            let dist = if let Some(incompatibility) = value.incompatible_source() {
+                IncompatibleDist::Source(incompatibility.clone())
+            } else if let Some(incompatibility) = value.incompatible_wheel() {
+                IncompatibleDist::Wheel(incompatibility.clone())
             } else {
-                CandidateDist::Incompatible(
-                    value
-                        .incompatible_wheel()
-                        .map(|(_, incompatibility)| incompatibility),
-                )
-            }
+                IncompatibleDist::Unavailable
+            };
+            CandidateDist::Incompatible(dist)
         }
     }
 }
 
 #[derive(Debug, Clone)]
 pub(crate) struct Candidate<'a> {
     /// The name of the package.
@@ -344,11 +465,11 @@
 impl Name for Candidate<'_> {
     fn name(&self) -> &PackageName {
         self.name
     }
 }
 
 impl DistributionMetadata for Candidate<'_> {
-    fn version_or_url(&self) -> distribution_types::VersionOrUrl {
-        distribution_types::VersionOrUrl::Version(self.version)
+    fn version_or_url(&self) -> distribution_types::VersionOrUrlRef {
+        distribution_types::VersionOrUrlRef::Version(self.version)
     }
 }
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `uv-0.1.9/crates/uv-resolver/src/dependency_mode.rs` & `uv-0.2.0/crates/uv-resolver/src/dependency_mode.rs`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/uv-resolver/src/error.rs` & `uv-0.2.0/crates/uv-resolver/src/error.rs`

 * *Files 19% similar despite different names*

```diff
@@ -1,152 +1,201 @@
-use std::collections::BTreeSet;
-use std::convert::Infallible;
+use std::collections::{BTreeMap, BTreeSet};
 use std::fmt::Formatter;
+use std::ops::Deref;
+use std::sync::Arc;
 
-use dashmap::{DashMap, DashSet};
 use indexmap::IndexMap;
 use pubgrub::range::Range;
-use pubgrub::report::{DefaultStringReporter, DerivationTree, Reporter};
-use rustc_hash::FxHashMap;
+use pubgrub::report::{DefaultStringReporter, DerivationTree, External, Reporter};
+use rustc_hash::{FxHashMap, FxHashSet};
 
-use distribution_types::{BuiltDist, IndexLocations, PathBuiltDist, PathSourceDist, SourceDist};
-use once_map::OnceMap;
+use dashmap::DashMap;
+use distribution_types::{BuiltDist, IndexLocations, InstalledDist, ParsedUrlError, SourceDist};
 use pep440_rs::Version;
 use pep508_rs::Requirement;
 use uv_normalize::PackageName;
 
 use crate::candidate_selector::CandidateSelector;
-use crate::pubgrub::{PubGrubPackage, PubGrubPython, PubGrubReportFormatter};
+use crate::dependency_provider::UvDependencyProvider;
+use crate::pubgrub::{PubGrubPackage, PubGrubPackageInner, PubGrubPython, PubGrubReportFormatter};
 use crate::python_requirement::PythonRequirement;
-use crate::resolver::{UnavailablePackage, VersionsResponse};
+use crate::resolver::{
+    FxOnceMap, IncompletePackage, UnavailablePackage, UnavailableReason, VersionsResponse,
+};
 
 #[derive(Debug, thiserror::Error)]
 pub enum ResolveError {
-    #[error("Failed to find a version of {0} that satisfies the requirement")]
+    #[error("Failed to find a version of `{0}` that satisfies the requirement")]
     NotFound(Requirement),
 
     #[error(transparent)]
     Client(#[from] uv_client::Error),
 
-    #[error("The channel is closed, was there a panic?")]
+    #[error("The channel closed unexpectedly")]
     ChannelClosed,
 
     #[error(transparent)]
     Join(#[from] tokio::task::JoinError),
 
     #[error("Attempted to wait on an unregistered task")]
     Unregistered,
 
     #[error("Package metadata name `{metadata}` does not match given name `{given}`")]
     NameMismatch {
         given: PackageName,
         metadata: PackageName,
     },
 
-    #[error("~= operator requires at least two release segments: {0}")]
+    #[error("~= operator requires at least two release segments: `{0}`")]
     InvalidTildeEquals(pep440_rs::VersionSpecifier),
 
     #[error("Requirements contain conflicting URLs for package `{0}`:\n- {1}\n- {2}")]
     ConflictingUrlsDirect(PackageName, String, String),
 
     #[error("There are conflicting URLs for package `{0}`:\n- {1}\n- {2}")]
     ConflictingUrlsTransitive(PackageName, String, String),
 
-    #[error("There are conflicting versions for `{0}`: {1}")]
-    ConflictingVersions(String, String),
-
     #[error("Package `{0}` attempted to resolve via URL: {1}. URL dependencies must be expressed as direct requirements or constraints. Consider adding `{0} @ {1}` to your dependencies or constraints file.")]
     DisallowedUrl(PackageName, String),
 
     #[error("There are conflicting editable requirements for package `{0}`:\n- {1}\n- {2}")]
     ConflictingEditables(PackageName, String, String),
 
     #[error(transparent)]
     DistributionType(#[from] distribution_types::Error),
 
-    #[error("Failed to download: {0}")]
+    #[error("Failed to download `{0}`")]
     Fetch(Box<BuiltDist>, #[source] uv_distribution::Error),
 
-    #[error("Failed to download and build: {0}")]
+    #[error("Failed to download and build `{0}`")]
     FetchAndBuild(Box<SourceDist>, #[source] uv_distribution::Error),
 
-    #[error("Failed to read: {0}")]
-    Read(Box<PathBuiltDist>, #[source] uv_distribution::Error),
+    #[error("Failed to read `{0}`")]
+    Read(Box<BuiltDist>, #[source] uv_distribution::Error),
+
+    // TODO(zanieb): Use `thiserror` in `InstalledDist` so we can avoid chaining `anyhow`
+    #[error("Failed to read metadata from installed package `{0}`")]
+    ReadInstalled(Box<InstalledDist>, #[source] anyhow::Error),
 
-    #[error("Failed to build: {0}")]
-    Build(Box<PathSourceDist>, #[source] uv_distribution::Error),
+    #[error("Failed to build `{0}`")]
+    Build(Box<SourceDist>, #[source] uv_distribution::Error),
 
     #[error(transparent)]
     NoSolution(#[from] NoSolutionError),
 
     #[error("{package} {version} depends on itself")]
     SelfDependency {
         /// Package whose dependencies we want.
         package: Box<PubGrubPackage>,
         /// Version of the package for which we want the dependencies.
         version: Box<Version>,
     },
 
+    #[error("Attempted to construct an invalid version specifier")]
+    InvalidVersion(#[from] pep440_rs::VersionSpecifierBuildError),
+
+    #[error("In `--require-hashes` mode, all requirements must be pinned upfront with `==`, but found: `{0}`")]
+    UnhashedPackage(PackageName),
+
+    // TODO(konsti): Attach the distribution that contained the invalid requirement as error source.
+    #[error("Failed to parse requirements")]
+    DirectUrl(#[from] Box<ParsedUrlError>),
+
     /// Something unexpected happened.
     #[error("{0}")]
     Failure(String),
 }
 
 impl<T> From<tokio::sync::mpsc::error::SendError<T>> for ResolveError {
     /// Drop the value we want to send to not leak the private type we're sending.
     /// The tokio error only says "channel closed", so we don't lose information.
     fn from(_value: tokio::sync::mpsc::error::SendError<T>) -> Self {
         Self::ChannelClosed
     }
 }
 
-impl From<pubgrub::error::PubGrubError<PubGrubPackage, Range<Version>, Infallible>>
-    for ResolveError
-{
-    fn from(
-        value: pubgrub::error::PubGrubError<PubGrubPackage, Range<Version>, Infallible>,
-    ) -> Self {
+/// Given a [`DerivationTree`], collapse any [`External::FromDependencyOf`] incompatibilities
+/// wrap an [`PubGrubPackageInner::Extra`] package.
+fn collapse_extra_proxies(
+    derivation_tree: &mut DerivationTree<PubGrubPackage, Range<Version>, UnavailableReason>,
+) {
+    match derivation_tree {
+        DerivationTree::External(_) => {}
+        DerivationTree::Derived(derived) => {
+            match (
+                Arc::make_mut(&mut derived.cause1),
+                Arc::make_mut(&mut derived.cause2),
+            ) {
+                (
+                    DerivationTree::External(External::FromDependencyOf(package, ..)),
+                    ref mut cause,
+                ) if matches!(&**package, PubGrubPackageInner::Extra { .. }) => {
+                    collapse_extra_proxies(cause);
+                    *derivation_tree = cause.clone();
+                }
+                (
+                    ref mut cause,
+                    DerivationTree::External(External::FromDependencyOf(package, ..)),
+                ) if matches!(&**package, PubGrubPackageInner::Extra { .. }) => {
+                    collapse_extra_proxies(cause);
+                    *derivation_tree = cause.clone();
+                }
+                _ => {
+                    collapse_extra_proxies(Arc::make_mut(&mut derived.cause1));
+                    collapse_extra_proxies(Arc::make_mut(&mut derived.cause2));
+                }
+            }
+        }
+    }
+}
+
+impl From<pubgrub::error::PubGrubError<UvDependencyProvider>> for ResolveError {
+    fn from(value: pubgrub::error::PubGrubError<UvDependencyProvider>) -> Self {
         match value {
             // These are all never type variant that can never match, but never is experimental
             pubgrub::error::PubGrubError::ErrorChoosingPackageVersion(_)
             | pubgrub::error::PubGrubError::ErrorInShouldCancel(_)
             | pubgrub::error::PubGrubError::ErrorRetrievingDependencies { .. } => {
                 unreachable!()
             }
-            pubgrub::error::PubGrubError::Failure(inner) => ResolveError::Failure(inner),
-            pubgrub::error::PubGrubError::NoSolution(derivation_tree) => {
-                ResolveError::NoSolution(NoSolutionError {
+            pubgrub::error::PubGrubError::Failure(inner) => Self::Failure(inner),
+            pubgrub::error::PubGrubError::NoSolution(mut derivation_tree) => {
+                collapse_extra_proxies(&mut derivation_tree);
+
+                Self::NoSolution(NoSolutionError {
                     derivation_tree,
                     // The following should be populated before display for the best error messages
                     available_versions: IndexMap::default(),
                     selector: None,
                     python_requirement: None,
                     index_locations: None,
                     unavailable_packages: FxHashMap::default(),
+                    incomplete_packages: FxHashMap::default(),
                 })
             }
             pubgrub::error::PubGrubError::SelfDependency { package, version } => {
-                ResolveError::SelfDependency {
+                Self::SelfDependency {
                     package: Box::new(package),
                     version: Box::new(version),
                 }
             }
         }
     }
 }
 
 /// A wrapper around [`pubgrub::error::PubGrubError::NoSolution`] that displays a resolution failure report.
 #[derive(Debug)]
 pub struct NoSolutionError {
-    derivation_tree: DerivationTree<PubGrubPackage, Range<Version>>,
+    derivation_tree: DerivationTree<PubGrubPackage, Range<Version>, UnavailableReason>,
     available_versions: IndexMap<PubGrubPackage, BTreeSet<Version>>,
     selector: Option<CandidateSelector>,
     python_requirement: Option<PythonRequirement>,
     index_locations: Option<IndexLocations>,
     unavailable_packages: FxHashMap<PackageName, UnavailablePackage>,
+    incomplete_packages: FxHashMap<PackageName, BTreeMap<Version, IncompletePackage>>,
 }
 
 impl std::error::Error for NoSolutionError {}
 
 impl std::fmt::Display for NoSolutionError {
     fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
         // Write the derivation report.
@@ -160,14 +209,15 @@
 
         // Include any additional hints.
         for hint in formatter.hints(
             &self.derivation_tree,
             &self.selector,
             &self.index_locations,
             &self.unavailable_packages,
+            &self.incomplete_packages,
         ) {
             write!(f, "\n\n{hint}")?;
         }
 
         Ok(())
     }
 }
@@ -176,48 +226,50 @@
     /// Update the available versions attached to the error using the given package version index.
     ///
     /// Only packages used in the error's derivation tree will be retrieved.
     #[must_use]
     pub(crate) fn with_available_versions(
         mut self,
         python_requirement: &PythonRequirement,
-        visited: &DashSet<PackageName>,
-        package_versions: &OnceMap<PackageName, VersionsResponse>,
+        visited: &FxHashSet<PackageName>,
+        package_versions: &FxOnceMap<PackageName, Arc<VersionsResponse>>,
     ) -> Self {
         let mut available_versions = IndexMap::default();
         for package in self.derivation_tree.packages() {
-            match package {
-                PubGrubPackage::Root(_) => {}
-                PubGrubPackage::Python(PubGrubPython::Installed) => {
+            match &**package {
+                PubGrubPackageInner::Root(_) => {}
+                PubGrubPackageInner::Python(PubGrubPython::Installed) => {
                     available_versions.insert(
                         package.clone(),
-                        BTreeSet::from([python_requirement.installed().clone()]),
+                        BTreeSet::from([python_requirement.installed().deref().clone()]),
                     );
                 }
-                PubGrubPackage::Python(PubGrubPython::Target) => {
+                PubGrubPackageInner::Python(PubGrubPython::Target) => {
                     available_versions.insert(
                         package.clone(),
-                        BTreeSet::from([python_requirement.target().clone()]),
+                        BTreeSet::from([python_requirement.target().deref().clone()]),
                     );
                 }
-                PubGrubPackage::Package(name, ..) => {
+                PubGrubPackageInner::Extra { .. } => {}
+                PubGrubPackageInner::Package { name, .. } => {
                     // Avoid including available versions for packages that exist in the derivation
                     // tree, but were never visited during resolution. We _may_ have metadata for
                     // these packages, but it's non-deterministic, and omitting them ensures that
                     // we represent the state of the resolver at the time of failure.
                     if visited.contains(name) {
                         if let Some(response) = package_versions.get(name) {
-                            if let VersionsResponse::Found(ref version_map) = *response {
-                                available_versions.insert(
-                                    package.clone(),
-                                    version_map
-                                        .iter()
-                                        .map(|(version, _)| version.clone())
-                                        .collect(),
-                                );
+                            if let VersionsResponse::Found(ref version_maps) = *response {
+                                for version_map in version_maps {
+                                    available_versions
+                                        .entry(package.clone())
+                                        .or_insert_with(BTreeSet::new)
+                                        .extend(
+                                            version_map.iter().map(|(version, _)| version.clone()),
+                                        );
+                                }
                             }
                         }
                     }
                 }
             }
         }
         self.available_versions = available_versions;
@@ -242,25 +294,47 @@
     #[must_use]
     pub(crate) fn with_unavailable_packages(
         mut self,
         unavailable_packages: &DashMap<PackageName, UnavailablePackage>,
     ) -> Self {
         let mut new = FxHashMap::default();
         for package in self.derivation_tree.packages() {
-            if let PubGrubPackage::Package(name, ..) = package {
-                if let Some(entry) = unavailable_packages.get(name) {
-                    let reason = entry.value();
+            if let PubGrubPackageInner::Package { name, .. } = &**package {
+                if let Some(reason) = unavailable_packages.get(name) {
                     new.insert(name.clone(), reason.clone());
                 }
             }
         }
         self.unavailable_packages = new;
         self
     }
 
+    /// Update the incomplete packages attached to the error.
+    #[must_use]
+    pub(crate) fn with_incomplete_packages(
+        mut self,
+        incomplete_packages: &DashMap<PackageName, DashMap<Version, IncompletePackage>>,
+    ) -> Self {
+        let mut new = FxHashMap::default();
+        for package in self.derivation_tree.packages() {
+            if let PubGrubPackageInner::Package { name, .. } = &**package {
+                if let Some(versions) = incomplete_packages.get(name) {
+                    for entry in versions.iter() {
+                        let (version, reason) = entry.pair();
+                        new.entry(name.clone())
+                            .or_insert_with(BTreeMap::default)
+                            .insert(version.clone(), reason.clone());
+                    }
+                }
+            }
+        }
+        self.incomplete_packages = new;
+        self
+    }
+
     /// Update the Python requirements attached to the error.
     #[must_use]
     pub(crate) fn with_python_requirement(
         mut self,
         python_requirement: &PythonRequirement,
     ) -> Self {
         self.python_requirement = Some(python_requirement.clone());
```

### Comparing `uv-0.1.9/crates/uv-resolver/src/finder.rs` & `uv-0.2.0/crates/uv-distribution/src/index/registry_wheel_index.rs`

 * *Files 22% similar despite different names*

```diff
@@ -1,251 +1,220 @@
-//! Given a set of selected packages, find a compatible set of distributions to install.
-//!
-//! This is similar to running `pip install` with the `--no-deps` flag.
+use std::collections::hash_map::Entry;
+use std::collections::BTreeMap;
 
-use anyhow::Result;
-use futures::{stream, Stream, StreamExt, TryStreamExt};
 use rustc_hash::FxHashMap;
-use uv_traits::NoBinary;
 
-use distribution_filename::DistFilename;
-use distribution_types::{Dist, IndexUrl, Resolution};
-use pep508_rs::{Requirement, VersionOrUrl};
-use platform_tags::{TagCompatibility, Tags};
-use uv_client::{
-    FlatDistributions, FlatIndex, OwnedArchive, RegistryClient, SimpleMetadata, SimpleMetadatum,
-};
-use uv_interpreter::Interpreter;
+use distribution_types::{CachedRegistryDist, FlatIndexLocation, Hashed, IndexLocations, IndexUrl};
+use pep440_rs::Version;
+use pep508_rs::VerbatimUrl;
+use platform_tags::Tags;
+use uv_cache::{Cache, CacheBucket, WheelCache};
+use uv_fs::{directories, files, symlinks};
 use uv_normalize::PackageName;
+use uv_types::HashStrategy;
 
-use crate::error::ResolveError;
+use crate::index::cached_wheel::CachedWheel;
+use crate::source::{HttpRevisionPointer, LocalRevisionPointer, HTTP_REVISION, LOCAL_REVISION};
 
-pub struct DistFinder<'a> {
+/// A local index of distributions that originate from a registry, like `PyPI`.
+#[derive(Debug)]
+pub struct RegistryWheelIndex<'a> {
+    cache: &'a Cache,
     tags: &'a Tags,
-    client: &'a RegistryClient,
-    reporter: Option<Box<dyn Reporter>>,
-    interpreter: &'a Interpreter,
-    flat_index: &'a FlatIndex,
-    no_binary: &'a NoBinary,
+    index_locations: &'a IndexLocations,
+    hasher: &'a HashStrategy,
+    index: FxHashMap<&'a PackageName, BTreeMap<Version, CachedRegistryDist>>,
 }
 
-impl<'a> DistFinder<'a> {
-    /// Initialize a new distribution finder.
+impl<'a> RegistryWheelIndex<'a> {
+    /// Initialize an index of registry distributions.
     pub fn new(
+        cache: &'a Cache,
         tags: &'a Tags,
-        client: &'a RegistryClient,
-        interpreter: &'a Interpreter,
-        flat_index: &'a FlatIndex,
-        no_binary: &'a NoBinary,
+        index_locations: &'a IndexLocations,
+        hasher: &'a HashStrategy,
     ) -> Self {
         Self {
+            cache,
             tags,
-            client,
-            reporter: None,
-            interpreter,
-            flat_index,
-            no_binary,
+            index_locations,
+            hasher,
+            index: FxHashMap::default(),
         }
     }
 
-    /// Set the [`Reporter`] to use for this resolution.
-    #[must_use]
-    pub fn with_reporter(self, reporter: impl Reporter + 'static) -> Self {
-        Self {
-            reporter: Some(Box::new(reporter)),
-            ..self
-        }
-    }
-
-    /// Resolve a single pinned package, either as cached network request
-    /// (version or no constraint) or by constructing a URL [`Dist`] from the
-    /// specifier URL.
-    async fn resolve_requirement(
-        &self,
-        requirement: &Requirement,
-        flat_index: Option<&FlatDistributions>,
-    ) -> Result<(PackageName, Dist), ResolveError> {
-        match requirement.version_or_url.as_ref() {
-            None | Some(VersionOrUrl::VersionSpecifier(_)) => {
-                // Query the index(es) (cached) to get the URLs for the available files.
-                let (index, raw_metadata) = self.client.simple(&requirement.name).await?;
-                let metadata = OwnedArchive::deserialize(&raw_metadata);
-
-                // Pick a version that satisfies the requirement.
-                let Some(dist) = self.select(requirement, metadata, &index, flat_index) else {
-                    return Err(ResolveError::NotFound(requirement.clone()));
-                };
-
-                if let Some(reporter) = self.reporter.as_ref() {
-                    reporter.on_progress(&dist);
-                }
-
-                let normalized_name = requirement.name.clone();
-                Ok((normalized_name, dist))
-            }
-            Some(VersionOrUrl::Url(url)) => {
-                // We have a URL; fetch the distribution directly.
-                let package_name = requirement.name.clone();
-                let package = Dist::from_url(package_name.clone(), url.clone())?;
-                Ok((package_name, package))
-            }
-        }
-    }
-
-    /// Resolve the pinned packages in parallel
-    pub fn resolve_stream<'data>(
-        &'data self,
-        requirements: &'data [Requirement],
-    ) -> impl Stream<Item = Result<(PackageName, Dist), ResolveError>> + 'data {
-        stream::iter(requirements)
-            .map(move |requirement| {
-                self.resolve_requirement(requirement, self.flat_index.get(&requirement.name))
-            })
-            .buffer_unordered(32)
-    }
-
-    /// Resolve a set of pinned packages into a set of wheels.
-    pub async fn resolve(&self, requirements: &[Requirement]) -> Result<Resolution, ResolveError> {
-        if requirements.is_empty() {
-            return Ok(Resolution::default());
-        }
-
-        let resolution: FxHashMap<PackageName, Dist> =
-            self.resolve_stream(requirements).try_collect().await?;
-
-        if let Some(reporter) = self.reporter.as_ref() {
-            reporter.on_complete();
-        }
-
-        Ok(Resolution::new(resolution))
+    /// Return an iterator over available wheels for a given package.
+    ///
+    /// If the package is not yet indexed, this will index the package by reading from the cache.
+    pub fn get(
+        &mut self,
+        name: &'a PackageName,
+    ) -> impl Iterator<Item = (&Version, &CachedRegistryDist)> {
+        self.get_impl(name).iter().rev()
     }
 
-    /// Select a version that satisfies the requirement.
+    /// Get the best wheel for the given package name and version.
     ///
-    /// Wheels are preferred to source distributions unless `no_binary` excludes wheels
-    /// for the requirement.
-    fn select(
-        &self,
-        requirement: &Requirement,
-        metadata: SimpleMetadata,
-        index: &IndexUrl,
-        flat_index: Option<&FlatDistributions>,
-    ) -> Option<Dist> {
-        let no_binary = match self.no_binary {
-            NoBinary::None => false,
-            NoBinary::All => true,
-            NoBinary::Packages(packages) => packages.contains(&requirement.name),
+    /// If the package is not yet indexed, this will index the package by reading from the cache.
+    pub fn get_version(
+        &mut self,
+        name: &'a PackageName,
+        version: &Version,
+    ) -> Option<&CachedRegistryDist> {
+        self.get_impl(name).get(version)
+    }
+
+    /// Get an entry in the index.
+    fn get_impl(&mut self, name: &'a PackageName) -> &BTreeMap<Version, CachedRegistryDist> {
+        let versions = match self.index.entry(name) {
+            Entry::Occupied(entry) => entry.into_mut(),
+            Entry::Vacant(entry) => entry.insert(Self::index(
+                name,
+                self.cache,
+                self.tags,
+                self.index_locations,
+                self.hasher,
+            )),
         };
+        versions
+    }
 
-        // Prioritize the flat index by initializing the "best" matches with its entries.
-        let matching_override = if let Some(flat_index) = flat_index {
-            match &requirement.version_or_url {
-                None => flat_index.iter().next(),
-                Some(VersionOrUrl::Url(_)) => None,
-                Some(VersionOrUrl::VersionSpecifier(specifiers)) => flat_index
-                    .iter()
-                    .find(|(version, _)| specifiers.contains(version)),
-            }
-        } else {
-            None
-        };
-        let (mut best_version, mut best_wheel, mut best_sdist) =
-            if let Some((version, resolvable_dist)) = matching_override {
-                (
-                    Some(version.clone()),
-                    resolvable_dist
-                        .compatible_wheel()
-                        .map(|(dist, tag_priority)| (dist.dist.clone(), *tag_priority)),
-                    resolvable_dist.source().map(|dist| dist.dist.clone()),
-                )
-            } else {
-                (None, None, None)
-            };
-
-        for SimpleMetadatum { version, files } in metadata.into_iter().rev() {
-            // If we iterated past the first-compatible version, break.
-            if best_version
-                .as_ref()
-                .is_some_and(|best_version| *best_version != version)
-            {
-                break;
-            }
-
-            // If the version does not satisfy the requirement, continue.
-            if !requirement.is_satisfied_by(&version) {
-                continue;
-            }
+    /// Add a package to the index by reading from the cache.
+    fn index(
+        package: &PackageName,
+        cache: &Cache,
+        tags: &Tags,
+        index_locations: &IndexLocations,
+        hasher: &HashStrategy,
+    ) -> BTreeMap<Version, CachedRegistryDist> {
+        let mut versions = BTreeMap::new();
+
+        // Collect into owned `IndexUrl`.
+        let flat_index_urls: Vec<IndexUrl> = index_locations
+            .flat_index()
+            .filter_map(|flat_index| match flat_index {
+                FlatIndexLocation::Path(path) => {
+                    let path = fs_err::canonicalize(path).ok()?;
+                    Some(IndexUrl::Path(VerbatimUrl::from_path(path).ok()?))
+                }
+                FlatIndexLocation::Url(url) => {
+                    Some(IndexUrl::Url(VerbatimUrl::from_url(url.clone())))
+                }
+            })
+            .collect();
 
-            if !no_binary {
-                // Find the most-compatible wheel
-                for version_wheel in files.wheels {
-                    // Only add dists compatible with the python version.
-                    // This is relevant for source dists which give no other indication of their
-                    // compatibility and wheels which may be tagged `py3-none-any` but
-                    // have `requires-python: ">=3.9"`
-                    if !version_wheel.file.requires_python.as_ref().map_or(
-                        true,
-                        |requires_python| {
-                            requires_python.contains(self.interpreter.python_version())
-                        },
-                    ) {
-                        continue;
+        for index_url in index_locations.indexes().chain(flat_index_urls.iter()) {
+            // Index all the wheels that were downloaded directly from the registry.
+            let wheel_dir = cache.shard(
+                CacheBucket::Wheels,
+                WheelCache::Index(index_url).wheel_dir(package.to_string()),
+            );
+
+            // For registry wheels, the cache structure is: `<index>/<package-name>/<wheel>.http`
+            // or `<index>/<package-name>/<version>/<wheel>.rev`.
+            for file in files(&wheel_dir) {
+                match index_url {
+                    // Add files from remote registries.
+                    IndexUrl::Pypi(_) | IndexUrl::Url(_) => {
+                        if file
+                            .extension()
+                            .is_some_and(|ext| ext.eq_ignore_ascii_case("http"))
+                        {
+                            if let Some(wheel) =
+                                CachedWheel::from_http_pointer(wheel_dir.join(file), cache)
+                            {
+                                // Enforce hash-checking based on the built distribution.
+                                if wheel.satisfies(hasher.get_package(package)) {
+                                    Self::add_wheel(wheel, tags, &mut versions);
+                                }
+                            }
+                        }
                     }
-
-                    best_version = Some(version.clone());
-                    if let TagCompatibility::Compatible(priority) =
-                        version_wheel.name.compatibility(self.tags)
-                    {
-                        if best_wheel
-                            .as_ref()
-                            .map_or(true, |(.., existing)| priority > *existing)
+                    // Add files from local registries (e.g., `--find-links`).
+                    IndexUrl::Path(_) => {
+                        if file
+                            .extension()
+                            .is_some_and(|ext| ext.eq_ignore_ascii_case("rev"))
                         {
-                            best_wheel = Some((
-                                Dist::from_registry(
-                                    DistFilename::WheelFilename(version_wheel.name),
-                                    version_wheel.file,
-                                    index.clone(),
-                                ),
-                                priority,
-                            ));
+                            if let Some(wheel) =
+                                CachedWheel::from_local_pointer(wheel_dir.join(file), cache)
+                            {
+                                // Enforce hash-checking based on the built distribution.
+                                if wheel.satisfies(hasher.get_package(package)) {
+                                    Self::add_wheel(wheel, tags, &mut versions);
+                                }
+                            }
                         }
                     }
                 }
             }
 
-            // Find the most-compatible sdist, if no wheel was found.
-            if best_wheel.is_none() {
-                for version_sdist in files.source_dists {
-                    // Only add dists compatible with the python version.
-                    // This is relevant for source dists which give no other indication of their
-                    // compatibility and wheels which may be tagged `py3-none-any` but
-                    // have `requires-python: ">=3.9"`
-                    if !version_sdist.file.requires_python.as_ref().map_or(
-                        true,
-                        |requires_python| {
-                            requires_python.contains(self.interpreter.python_version())
-                        },
-                    ) {
-                        continue;
+            // Index all the built wheels, created by downloading and building source distributions
+            // from the registry.
+            let cache_shard = cache.shard(
+                CacheBucket::BuiltWheels,
+                WheelCache::Index(index_url).wheel_dir(package.to_string()),
+            );
+
+            // For registry wheels, the cache structure is: `<index>/<package-name>/<version>/`.
+            for shard in directories(&cache_shard) {
+                // Read the existing metadata from the cache, if it exists.
+                let cache_shard = cache_shard.shard(shard);
+
+                // Read the revision from the cache.
+                let revision = match index_url {
+                    // Add files from remote registries.
+                    IndexUrl::Pypi(_) | IndexUrl::Url(_) => {
+                        let revision_entry = cache_shard.entry(HTTP_REVISION);
+                        if let Ok(Some(pointer)) = HttpRevisionPointer::read_from(revision_entry) {
+                            Some(pointer.into_revision())
+                        } else {
+                            None
+                        }
+                    }
+                    // Add files from local registries (e.g., `--find-links`).
+                    IndexUrl::Path(_) => {
+                        let revision_entry = cache_shard.entry(LOCAL_REVISION);
+                        if let Ok(Some(pointer)) = LocalRevisionPointer::read_from(revision_entry) {
+                            Some(pointer.into_revision())
+                        } else {
+                            None
+                        }
                     }
+                };
 
-                    best_version = Some(version_sdist.name.version.clone());
-                    best_sdist = Some(Dist::from_registry(
-                        DistFilename::SourceDistFilename(version_sdist.name),
-                        version_sdist.file,
-                        index.clone(),
-                    ));
+                if let Some(revision) = revision {
+                    // Enforce hash-checking based on the source distribution.
+                    if revision.satisfies(hasher.get_package(package)) {
+                        for wheel_dir in symlinks(cache_shard.join(revision.id())) {
+                            if let Some(wheel) = CachedWheel::from_built_source(wheel_dir) {
+                                Self::add_wheel(wheel, tags, &mut versions);
+                            }
+                        }
+                    }
                 }
             }
         }
 
-        best_wheel.map_or(best_sdist, |(wheel, ..)| Some(wheel))
+        versions
     }
-}
-
-pub trait Reporter: Send + Sync {
-    /// Callback to invoke when a package is resolved to a specific distribution.
-    fn on_progress(&self, dist: &Dist);
 
-    /// Callback to invoke when the resolution is complete.
-    fn on_complete(&self);
+    /// Add the [`CachedWheel`] to the index.
+    fn add_wheel(
+        wheel: CachedWheel,
+        tags: &Tags,
+        versions: &mut BTreeMap<Version, CachedRegistryDist>,
+    ) {
+        let dist_info = wheel.into_registry_dist();
+
+        // Pick the wheel with the highest priority
+        let compatibility = dist_info.filename.compatibility(tags);
+        if let Some(existing) = versions.get_mut(&dist_info.filename.version) {
+            // Override if we have better compatibility
+            if compatibility > existing.filename.compatibility(tags) {
+                *existing = dist_info;
+            }
+        } else if compatibility.is_compatible() {
+            versions.insert(dist_info.filename.version.clone(), dist_info);
+        }
+    }
 }
```

### Comparing `uv-0.1.9/crates/uv-resolver/src/lib.rs` & `uv-0.2.0/crates/uv-resolver/src/lib.rs`

 * *Files 11% similar despite different names*

```diff
@@ -1,34 +1,47 @@
 pub use dependency_mode::DependencyMode;
+pub use editables::BuiltEditableMetadata;
 pub use error::ResolveError;
-pub use finder::{DistFinder, Reporter as FinderReporter};
+pub use exclude_newer::ExcludeNewer;
+pub use exclusions::Exclusions;
+pub use flat_index::FlatIndex;
+pub use lock::{Lock, LockError};
 pub use manifest::Manifest;
 pub use options::{Options, OptionsBuilder};
+pub use preferences::{Preference, PreferenceError};
 pub use prerelease_mode::PreReleaseMode;
 pub use python_requirement::PythonRequirement;
 pub use resolution::{AnnotationStyle, Diagnostic, DisplayResolutionGraph, ResolutionGraph};
 pub use resolution_mode::ResolutionMode;
 pub use resolver::{
-    BuildId, DefaultResolverProvider, InMemoryIndex, PackageVersionsResult,
+    BuildId, DefaultResolverProvider, InMemoryIndex, MetadataResponse, PackageVersionsResult,
     Reporter as ResolverReporter, Resolver, ResolverProvider, VersionsResponse,
     WheelMetadataResult,
 };
 pub use version_map::VersionMap;
+pub use yanks::AllowedYanks;
 
+mod bare;
 mod candidate_selector;
-mod constraints;
+
 mod dependency_mode;
+mod dependency_provider;
 mod editables;
 mod error;
-mod finder;
+mod exclude_newer;
+mod exclusions;
+mod flat_index;
+mod lock;
 mod manifest;
+mod marker;
 mod options;
-mod overrides;
 mod pins;
+mod preferences;
 mod prerelease_mode;
 mod pubgrub;
 mod python_requirement;
+mod redirect;
 mod resolution;
 mod resolution_mode;
 mod resolver;
 mod version_map;
 mod yanks;
```

### Comparing `uv-0.1.9/crates/uv-resolver/src/options.rs` & `uv-0.2.0/crates/uv-resolver/src/options.rs`

 * *Files 21% similar despite different names*

```diff
@@ -1,27 +1,29 @@
-use chrono::{DateTime, Utc};
+use uv_configuration::IndexStrategy;
 
-use crate::{DependencyMode, PreReleaseMode, ResolutionMode};
+use crate::{DependencyMode, ExcludeNewer, PreReleaseMode, ResolutionMode};
 
 /// Options for resolving a manifest.
 #[derive(Debug, Default, Copy, Clone)]
 pub struct Options {
     pub resolution_mode: ResolutionMode,
     pub prerelease_mode: PreReleaseMode,
     pub dependency_mode: DependencyMode,
-    pub exclude_newer: Option<DateTime<Utc>>,
+    pub exclude_newer: Option<ExcludeNewer>,
+    pub index_strategy: IndexStrategy,
 }
 
 /// Builder for [`Options`].
 #[derive(Debug, Default, Clone)]
 pub struct OptionsBuilder {
     resolution_mode: ResolutionMode,
     prerelease_mode: PreReleaseMode,
     dependency_mode: DependencyMode,
-    exclude_newer: Option<DateTime<Utc>>,
+    exclude_newer: Option<ExcludeNewer>,
+    index_strategy: IndexStrategy,
 }
 
 impl OptionsBuilder {
     /// Creates a new builder.
     pub fn new() -> Self {
         Self::default()
     }
@@ -45,22 +47,30 @@
     pub fn dependency_mode(mut self, dependency_mode: DependencyMode) -> Self {
         self.dependency_mode = dependency_mode;
         self
     }
 
     /// Sets the exclusion date.
     #[must_use]
-    pub fn exclude_newer(mut self, exclude_newer: Option<DateTime<Utc>>) -> Self {
+    pub fn exclude_newer(mut self, exclude_newer: Option<ExcludeNewer>) -> Self {
         self.exclude_newer = exclude_newer;
         self
     }
 
+    /// Sets the index strategy.
+    #[must_use]
+    pub fn index_strategy(mut self, index_strategy: IndexStrategy) -> Self {
+        self.index_strategy = index_strategy;
+        self
+    }
+
     /// Builds the options.
     pub fn build(self) -> Options {
         Options {
             resolution_mode: self.resolution_mode,
             prerelease_mode: self.prerelease_mode,
             dependency_mode: self.dependency_mode,
             exclude_newer: self.exclude_newer,
+            index_strategy: self.index_strategy,
         }
     }
 }
```

### Comparing `uv-0.1.9/crates/uv-resolver/src/overrides.rs` & `uv-0.2.0/crates/uv-configuration/src/overrides.rs`

 * *Files 10% similar despite different names*

```diff
@@ -1,44 +1,49 @@
 use std::hash::BuildHasherDefault;
 
 use itertools::Either;
 use rustc_hash::FxHashMap;
 
-use pep508_rs::Requirement;
+use distribution_types::Requirement;
 use uv_normalize::PackageName;
 
 /// A set of overrides for a set of requirements.
 #[derive(Debug, Default, Clone)]
-pub(crate) struct Overrides(FxHashMap<PackageName, Vec<Requirement>>);
+pub struct Overrides(FxHashMap<PackageName, Vec<Requirement>>);
 
 impl Overrides {
     /// Create a new set of overrides from a set of requirements.
-    pub(crate) fn from_requirements(requirements: Vec<Requirement>) -> Self {
+    pub fn from_requirements(requirements: Vec<Requirement>) -> Self {
         let mut overrides: FxHashMap<PackageName, Vec<Requirement>> =
             FxHashMap::with_capacity_and_hasher(requirements.len(), BuildHasherDefault::default());
         for requirement in requirements {
             overrides
                 .entry(requirement.name.clone())
                 .or_default()
                 .push(requirement);
         }
         Self(overrides)
     }
 
+    /// Return an iterator over all [`Requirement`]s in the override set.
+    pub fn requirements(&self) -> impl Iterator<Item = &Requirement> {
+        self.0.values().flat_map(|requirements| requirements.iter())
+    }
+
     /// Get the overrides for a package.
-    pub(crate) fn get(&self, name: &PackageName) -> Option<&Vec<Requirement>> {
+    pub fn get(&self, name: &PackageName) -> Option<&Vec<Requirement>> {
         self.0.get(name)
     }
 
     /// Apply the overrides to a set of requirements.
-    pub(crate) fn apply<'a>(
+    pub fn apply<'a>(
         &'a self,
-        requirements: &'a [Requirement],
+        requirements: impl IntoIterator<Item = &'a Requirement>,
     ) -> impl Iterator<Item = &Requirement> {
-        requirements.iter().flat_map(|requirement| {
+        requirements.into_iter().flat_map(|requirement| {
             if let Some(overrides) = self.get(&requirement.name) {
                 Either::Left(overrides.iter())
             } else {
                 Either::Right(std::iter::once(requirement))
             }
         })
     }
```

### Comparing `uv-0.1.9/crates/uv-resolver/src/pins.rs` & `uv-0.2.0/crates/uv-resolver/src/pins.rs`

 * *Files 16% similar despite different names*

```diff
@@ -1,28 +1,32 @@
 use rustc_hash::FxHashMap;
 
-use distribution_types::{CompatibleDist, Dist};
+use distribution_types::{CompatibleDist, ResolvedDist};
 use uv_normalize::PackageName;
 
 use crate::candidate_selector::Candidate;
 
 /// A set of package versions pinned to specific files.
 ///
 /// For example, given `Flask==3.0.0`, the [`FilePins`] would contain a mapping from `Flask` to
 /// `3.0.0` to the specific wheel or source distribution archive that was pinned for that version.
-#[derive(Debug, Default)]
-pub(crate) struct FilePins(FxHashMap<PackageName, FxHashMap<pep440_rs::Version, Dist>>);
+#[derive(Clone, Debug, Default)]
+pub(crate) struct FilePins(FxHashMap<PackageName, FxHashMap<pep440_rs::Version, ResolvedDist>>);
 
 impl FilePins {
     /// Pin a candidate package.
     pub(crate) fn insert(&mut self, candidate: &Candidate, dist: &CompatibleDist) {
         self.0.entry(candidate.name().clone()).or_default().insert(
             candidate.version().clone(),
-            dist.for_installation().dist.clone(),
+            dist.for_installation().to_owned(),
         );
     }
 
     /// Return the pinned file for the given package name and version, if it exists.
-    pub(crate) fn get(&self, name: &PackageName, version: &pep440_rs::Version) -> Option<&Dist> {
+    pub(crate) fn get(
+        &self,
+        name: &PackageName,
+        version: &pep440_rs::Version,
+    ) -> Option<&ResolvedDist> {
         self.0.get(name)?.get(version)
     }
 }
```

### Comparing `uv-0.1.9/crates/uv-resolver/src/prerelease_mode.rs` & `uv-0.2.0/crates/uv-resolver/src/prerelease_mode.rs`

 * *Files 27% similar despite different names*

```diff
@@ -1,14 +1,19 @@
+use distribution_types::RequirementSource;
 use rustc_hash::FxHashSet;
 
-use pep508_rs::{Requirement, VersionOrUrl};
+use pep508_rs::MarkerEnvironment;
 use uv_normalize::PackageName;
 
-#[derive(Debug, Default, Clone, Copy, PartialEq, Eq)]
+use crate::{DependencyMode, Manifest};
+
+#[derive(Debug, Default, Clone, Copy, PartialEq, Eq, serde::Deserialize)]
+#[serde(deny_unknown_fields, rename_all = "kebab-case")]
 #[cfg_attr(feature = "clap", derive(clap::ValueEnum))]
+#[cfg_attr(feature = "schemars", derive(schemars::JsonSchema))]
 pub enum PreReleaseMode {
     /// Disallow all pre-release versions.
     Disallow,
 
     /// Allow all pre-release versions.
     Allow,
 
@@ -44,55 +49,61 @@
 
     /// Allow pre-release versions if all versions of a package are pre-release, or if the package
     /// has an explicit pre-release marker in its version requirements.
     IfNecessaryOrExplicit(FxHashSet<PackageName>),
 }
 
 impl PreReleaseStrategy {
-    pub(crate) fn from_mode(mode: PreReleaseMode, direct_dependencies: &[Requirement]) -> Self {
+    pub(crate) fn from_mode(
+        mode: PreReleaseMode,
+        manifest: &Manifest,
+        markers: Option<&MarkerEnvironment>,
+        dependencies: DependencyMode,
+    ) -> Self {
         match mode {
             PreReleaseMode::Disallow => Self::Disallow,
             PreReleaseMode::Allow => Self::Allow,
             PreReleaseMode::IfNecessary => Self::IfNecessary,
             PreReleaseMode::Explicit => Self::Explicit(
-                direct_dependencies
-                    .iter()
+                manifest
+                    .requirements(markers, dependencies)
                     .filter(|requirement| {
-                        let Some(version_or_url) = &requirement.version_or_url else {
+                        let RequirementSource::Registry { specifier, .. } = &requirement.source
+                        else {
                             return false;
                         };
-                        let version_specifiers = match version_or_url {
-                            VersionOrUrl::VersionSpecifier(version_specifiers) => {
-                                version_specifiers
-                            }
-                            VersionOrUrl::Url(_) => return false,
-                        };
-                        version_specifiers
+                        specifier
                             .iter()
                             .any(pep440_rs::VersionSpecifier::any_prerelease)
                     })
                     .map(|requirement| requirement.name.clone())
                     .collect(),
             ),
             PreReleaseMode::IfNecessaryOrExplicit => Self::IfNecessaryOrExplicit(
-                direct_dependencies
-                    .iter()
+                manifest
+                    .requirements(markers, dependencies)
                     .filter(|requirement| {
-                        let Some(version_or_url) = &requirement.version_or_url else {
+                        let RequirementSource::Registry { specifier, .. } = &requirement.source
+                        else {
                             return false;
                         };
-                        let version_specifiers = match version_or_url {
-                            VersionOrUrl::VersionSpecifier(version_specifiers) => {
-                                version_specifiers
-                            }
-                            VersionOrUrl::Url(_) => return false,
-                        };
-                        version_specifiers
+                        specifier
                             .iter()
                             .any(pep440_rs::VersionSpecifier::any_prerelease)
                     })
                     .map(|requirement| requirement.name.clone())
                     .collect(),
             ),
         }
     }
+
+    /// Returns `true` if a [`PackageName`] is allowed to have pre-release versions.
+    pub(crate) fn allows(&self, package: &PackageName) -> bool {
+        match self {
+            Self::Disallow => false,
+            Self::Allow => true,
+            Self::IfNecessary => false,
+            Self::Explicit(packages) => packages.contains(package),
+            Self::IfNecessaryOrExplicit(packages) => packages.contains(package),
+        }
+    }
 }
```

### Comparing `uv-0.1.9/crates/uv-resolver/src/pubgrub/distribution.rs` & `uv-0.2.0/crates/uv-resolver/src/pubgrub/distribution.rs`

 * *Files 13% similar despite different names*

```diff
@@ -1,38 +1,37 @@
-use distribution_types::{DistributionMetadata, Name, VersionOrUrl};
+use distribution_types::{DistributionMetadata, Name, VerbatimParsedUrl, VersionOrUrlRef};
 use pep440_rs::Version;
-use pep508_rs::VerbatimUrl;
 use uv_normalize::PackageName;
 
 #[derive(Debug)]
 pub(crate) enum PubGrubDistribution<'a> {
     Registry(&'a PackageName, &'a Version),
-    Url(&'a PackageName, &'a VerbatimUrl),
+    Url(&'a PackageName, &'a VerbatimParsedUrl),
 }
 
 impl<'a> PubGrubDistribution<'a> {
     pub(crate) fn from_registry(name: &'a PackageName, version: &'a Version) -> Self {
         Self::Registry(name, version)
     }
 
-    pub(crate) fn from_url(name: &'a PackageName, url: &'a VerbatimUrl) -> Self {
+    pub(crate) fn from_url(name: &'a PackageName, url: &'a VerbatimParsedUrl) -> Self {
         Self::Url(name, url)
     }
 }
 
 impl Name for PubGrubDistribution<'_> {
     fn name(&self) -> &PackageName {
         match self {
             Self::Registry(name, _) => name,
             Self::Url(name, _) => name,
         }
     }
 }
 
 impl DistributionMetadata for PubGrubDistribution<'_> {
-    fn version_or_url(&self) -> VersionOrUrl {
+    fn version_or_url(&self) -> VersionOrUrlRef {
         match self {
-            Self::Registry(_, version) => VersionOrUrl::Version(version),
-            Self::Url(_, url) => VersionOrUrl::Url(url),
+            Self::Registry(_, version) => VersionOrUrlRef::Version(version),
+            Self::Url(_, url) => VersionOrUrlRef::Url(&url.verbatim),
         }
     }
 }
```

### Comparing `uv-0.1.9/crates/uv-resolver/src/pubgrub/report.rs` & `uv-0.2.0/crates/uv-resolver/src/pubgrub/report.rs`

 * *Files 20% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 use std::borrow::Cow;
 use std::cmp::Ordering;
-use std::collections::BTreeSet;
+use std::collections::{BTreeMap, BTreeSet};
 use std::ops::Bound;
 
 use derivative::Derivative;
 use distribution_types::IndexLocations;
 use indexmap::{IndexMap, IndexSet};
 use owo_colors::OwoColorize;
 use pep440_rs::Version;
@@ -12,39 +12,43 @@
 use pubgrub::report::{DerivationTree, Derived, External, ReportFormatter};
 use pubgrub::term::Term;
 use pubgrub::type_aliases::Map;
 use rustc_hash::FxHashMap;
 use uv_normalize::PackageName;
 
 use crate::candidate_selector::CandidateSelector;
-use crate::prerelease_mode::PreReleaseStrategy;
 use crate::python_requirement::PythonRequirement;
-use crate::resolver::UnavailablePackage;
+use crate::resolver::{IncompletePackage, UnavailablePackage, UnavailableReason};
 
-use super::PubGrubPackage;
+use super::{PubGrubPackage, PubGrubPackageInner};
 
 #[derive(Debug)]
 pub(crate) struct PubGrubReportFormatter<'a> {
     /// The versions that were available for each package
     pub(crate) available_versions: &'a IndexMap<PubGrubPackage, BTreeSet<Version>>,
 
     /// The versions that were available for each package
     pub(crate) python_requirement: Option<&'a PythonRequirement>,
 }
 
-impl ReportFormatter<PubGrubPackage, Range<Version>> for PubGrubReportFormatter<'_> {
+impl ReportFormatter<PubGrubPackage, Range<Version>, UnavailableReason>
+    for PubGrubReportFormatter<'_>
+{
     type Output = String;
 
-    fn format_external(&self, external: &External<PubGrubPackage, Range<Version>>) -> Self::Output {
+    fn format_external(
+        &self,
+        external: &External<PubGrubPackage, Range<Version>, UnavailableReason>,
+    ) -> Self::Output {
         match external {
             External::NotRoot(package, version) => {
                 format!("we are solving dependencies of {package} {version}")
             }
-            External::NoVersions(package, set, reason) => {
-                if matches!(package, PubGrubPackage::Python(_)) {
+            External::NoVersions(package, set) => {
+                if matches!(&**package, PubGrubPackageInner::Python(_)) {
                     if let Some(python) = self.python_requirement {
                         if python.target() == python.installed() {
                             // Simple case, the installed version is the same as the target version
                             return format!(
                                 "the current {package} version ({}) does not satisfy {}",
                                 python.target(),
                                 PackageRange::compatibility(package, set)
@@ -76,31 +80,21 @@
                     debug_assert!(
                         false,
                         "Error reporting should always be provided with Python versions"
                     );
                 }
                 let set = self.simplify_set(set, package);
 
-                // Check for a reason
-                if let Some(reason) = reason {
-                    let formatted = if set.as_ref() == &Range::full() {
-                        format!("{package} {reason}")
-                    } else {
-                        format!("{package}{set} {reason}")
-                    };
-                    return formatted;
-                }
-
                 if set.as_ref() == &Range::full() {
                     format!("there are no versions of {package}")
                 } else if set.as_singleton().is_some() {
                     format!("there is no version of {package}{set}")
                 } else {
                     let complement = set.complement();
-                    let segments = complement.iter().collect::<Vec<_>>().len();
+                    let segments = complement.iter().count();
                     // Simple case, there's a single range to report
                     if segments == 1 {
                         format!(
                             "only {} is available",
                             PackageRange::compatibility(package, &complement)
                         )
                     // Complex case, there are multiple ranges
@@ -109,35 +103,44 @@
                             "only the following versions of {} {}",
                             package,
                             PackageRange::available(package, &complement)
                         )
                     }
                 }
             }
-            External::Unavailable(package, set, reason) => match package {
-                PubGrubPackage::Root(Some(name)) => {
+            External::Custom(package, set, reason) => match &**package {
+                PubGrubPackageInner::Root(Some(name)) => {
                     format!("{name} cannot be used because {reason}")
                 }
-                PubGrubPackage::Root(None) => {
+                PubGrubPackageInner::Root(None) => {
                     format!("your requirements cannot be used because {reason}")
                 }
-                _ => format!(
-                    "{}is unusable because {reason}",
-                    Padded::new("", &PackageRange::compatibility(package, set), " ")
-                ),
+                _ => match reason {
+                    UnavailableReason::Package(reason) => {
+                        // While there may be a term attached, this error applies to the entire
+                        // package, so we show it for the entire package
+                        format!("{}{reason}", Padded::new("", &package, " "))
+                    }
+                    UnavailableReason::Version(reason) => {
+                        format!(
+                            "{}{reason}",
+                            Padded::new("", &PackageRange::compatibility(package, set), " ")
+                        )
+                    }
+                },
             },
             External::FromDependencyOf(package, package_set, dependency, dependency_set) => {
                 let package_set = self.simplify_set(package_set, package);
                 let dependency_set = self.simplify_set(dependency_set, dependency);
-                match package {
-                    PubGrubPackage::Root(Some(name)) => format!(
+                match &**package {
+                    PubGrubPackageInner::Root(Some(name)) => format!(
                         "{name} depends on {}",
                         PackageRange::dependency(dependency, &dependency_set)
                     ),
-                    PubGrubPackage::Root(None) => format!(
+                    PubGrubPackageInner::Root(None) => format!(
                         "you require {}",
                         PackageRange::dependency(dependency, &dependency_set)
                     ),
                     _ => format!(
                         "{}",
                         PackageRange::compatibility(package, &package_set)
                             .depends_on(dependency, &dependency_set),
@@ -145,25 +148,35 @@
                 }
             }
         }
     }
 
     /// Try to print terms of an incompatibility in a human-readable way.
     fn format_terms(&self, terms: &Map<PubGrubPackage, Term<Range<Version>>>) -> String {
-        let terms_vec: Vec<_> = terms.iter().collect();
+        let mut terms_vec: Vec<_> = terms.iter().collect();
+        // We avoid relying on hashmap iteration order here by always sorting
+        // by package first.
+        terms_vec.sort_by(|&(pkg1, _), &(pkg2, _)| pkg1.cmp(pkg2));
         match terms_vec.as_slice() {
-            [] | [(PubGrubPackage::Root(_), _)] => "the requirements are unsatisfiable".into(),
-            [(package @ PubGrubPackage::Package(..), Term::Positive(range))] => {
+            [] => "the requirements are unsatisfiable".into(),
+            [(root, _)] if matches!(&**(*root), PubGrubPackageInner::Root(_)) => {
+                "the requirements are unsatisfiable".into()
+            }
+            [(package, Term::Positive(range))]
+                if matches!(&**(*package), PubGrubPackageInner::Package { .. }) =>
+            {
                 let range = self.simplify_set(range, package);
                 format!(
                     "{} cannot be used",
                     PackageRange::compatibility(package, &range)
                 )
             }
-            [(package @ PubGrubPackage::Package(..), Term::Negative(range))] => {
+            [(package, Term::Negative(range))]
+                if matches!(&**(*package), PubGrubPackageInner::Package { .. }) =>
+            {
                 let range = self.simplify_set(range, package);
                 format!(
                     "{} must be used",
                     PackageRange::compatibility(package, &range)
                 )
             }
             [(p1, Term::Positive(r1)), (p2, Term::Negative(r2))] => self.format_external(
@@ -195,37 +208,35 @@
             }
         }
     }
 
     /// Simplest case, we just combine two external incompatibilities.
     fn explain_both_external(
         &self,
-        external1: &External<PubGrubPackage, Range<Version>>,
-        external2: &External<PubGrubPackage, Range<Version>>,
+        external1: &External<PubGrubPackage, Range<Version>, UnavailableReason>,
+        external2: &External<PubGrubPackage, Range<Version>, UnavailableReason>,
         current_terms: &Map<PubGrubPackage, Term<Range<Version>>>,
     ) -> String {
-        let external1 = self.format_external(external1);
-        let external2 = self.format_external(external2);
+        let external = self.format_both_external(external1, external2);
         let terms = self.format_terms(current_terms);
 
         format!(
-            "Because {}and {}we can conclude that {}",
-            Padded::from_string("", &external1, " "),
-            Padded::from_string("", &external2, ", "),
-            Padded::from_string("", &terms, ".")
+            "Because {}we can conclude that {}",
+            Padded::from_string("", &external, ", "),
+            Padded::from_string("", &terms, "."),
         )
     }
 
     /// Both causes have already been explained so we use their refs.
     fn explain_both_ref(
         &self,
         ref_id1: usize,
-        derived1: &Derived<PubGrubPackage, Range<Version>>,
+        derived1: &Derived<PubGrubPackage, Range<Version>, UnavailableReason>,
         ref_id2: usize,
-        derived2: &Derived<PubGrubPackage, Range<Version>>,
+        derived2: &Derived<PubGrubPackage, Range<Version>, UnavailableReason>,
         current_terms: &Map<PubGrubPackage, Term<Range<Version>>>,
     ) -> String {
         // TODO: order should be chosen to make it more logical.
 
         let derived1_terms = self.format_terms(&derived1.terms);
         let derived2_terms = self.format_terms(&derived2.terms);
         let current_terms = self.format_terms(current_terms);
@@ -242,16 +253,16 @@
 
     /// One cause is derived (already explained so one-line),
     /// the other is a one-line external cause,
     /// and finally we conclude with the current incompatibility.
     fn explain_ref_and_external(
         &self,
         ref_id: usize,
-        derived: &Derived<PubGrubPackage, Range<Version>>,
-        external: &External<PubGrubPackage, Range<Version>>,
+        derived: &Derived<PubGrubPackage, Range<Version>, UnavailableReason>,
+        external: &External<PubGrubPackage, Range<Version>, UnavailableReason>,
         current_terms: &Map<PubGrubPackage, Term<Range<Version>>>,
     ) -> String {
         // TODO: order should be chosen to make it more logical.
 
         let derived_terms = self.format_terms(&derived.terms);
         let external = self.format_external(external);
         let current_terms = self.format_terms(current_terms);
@@ -264,15 +275,15 @@
             Padded::new("", &current_terms, "."),
         )
     }
 
     /// Add an external cause to the chain of explanations.
     fn and_explain_external(
         &self,
-        external: &External<PubGrubPackage, Range<Version>>,
+        external: &External<PubGrubPackage, Range<Version>, UnavailableReason>,
         current_terms: &Map<PubGrubPackage, Term<Range<Version>>>,
     ) -> String {
         let external = self.format_external(external);
         let terms = self.format_terms(current_terms);
 
         format!(
             "And because {}we can conclude that {}",
@@ -281,15 +292,15 @@
         )
     }
 
     /// Add an already explained incompat to the chain of explanations.
     fn and_explain_ref(
         &self,
         ref_id: usize,
-        derived: &Derived<PubGrubPackage, Range<Version>>,
+        derived: &Derived<PubGrubPackage, Range<Version>, UnavailableReason>,
         current_terms: &Map<PubGrubPackage, Term<Range<Version>>>,
     ) -> String {
         let derived = self.format_terms(&derived.terms);
         let current = self.format_terms(current_terms);
 
         format!(
             "And because we know from ({}) that {}we can conclude that {}",
@@ -298,32 +309,83 @@
             Padded::from_string("", &current, "."),
         )
     }
 
     /// Add an already explained incompat to the chain of explanations.
     fn and_explain_prior_and_external(
         &self,
-        prior_external: &External<PubGrubPackage, Range<Version>>,
-        external: &External<PubGrubPackage, Range<Version>>,
+        prior_external: &External<PubGrubPackage, Range<Version>, UnavailableReason>,
+        external: &External<PubGrubPackage, Range<Version>, UnavailableReason>,
         current_terms: &Map<PubGrubPackage, Term<Range<Version>>>,
     ) -> String {
-        let prior_external = self.format_external(prior_external);
-        let external = self.format_external(external);
+        let external = self.format_both_external(prior_external, external);
         let terms = self.format_terms(current_terms);
 
         format!(
-            "And because {}and {}we can conclude that {}",
-            Padded::from_string("", &prior_external, " "),
+            "And because {}we can conclude that {}",
             Padded::from_string("", &external, ", "),
             Padded::from_string("", &terms, "."),
         )
     }
 }
 
 impl PubGrubReportFormatter<'_> {
+    /// Format two external incompatibilities, combining them if possible.
+    fn format_both_external(
+        &self,
+        external1: &External<PubGrubPackage, Range<Version>, UnavailableReason>,
+        external2: &External<PubGrubPackage, Range<Version>, UnavailableReason>,
+    ) -> String {
+        match (external1, external2) {
+            (
+                External::FromDependencyOf(package1, package_set1, dependency1, dependency_set1),
+                External::FromDependencyOf(package2, _, dependency2, dependency_set2),
+            ) if package1 == package2 => {
+                let dependency_set1 = self.simplify_set(dependency_set1, dependency1);
+                let dependency1 = PackageRange::dependency(dependency1, &dependency_set1);
+
+                let dependency_set2 = self.simplify_set(dependency_set2, dependency2);
+                let dependency2 = PackageRange::dependency(dependency2, &dependency_set2);
+
+                match &**package1 {
+                    PubGrubPackageInner::Root(Some(name)) => format!(
+                        "{name} depends on {}and {}",
+                        Padded::new("", &dependency1, " "),
+                        dependency2,
+                    ),
+                    PubGrubPackageInner::Root(None) => format!(
+                        "you require {}and {}",
+                        Padded::new("", &dependency1, " "),
+                        dependency2,
+                    ),
+                    _ => {
+                        let package_set = self.simplify_set(package_set1, package1);
+
+                        format!(
+                            "{}",
+                            PackageRange::compatibility(package1, &package_set)
+                                .depends_on(dependency1.package, &dependency_set1)
+                                .and(dependency2.package, &dependency_set2),
+                        )
+                    }
+                }
+            }
+            _ => {
+                let external1 = self.format_external(external1);
+                let external2 = self.format_external(external2);
+
+                format!(
+                    "{}and {}",
+                    Padded::from_string("", &external1, " "),
+                    &external2,
+                )
+            }
+        }
+    }
+
     /// Simplify a [`Range`] of versions using the available versions for a package.
     fn simplify_set<'a>(
         &self,
         set: &'a Range<Version>,
         package: &PubGrubPackage,
     ) -> Cow<'a, Range<Version>> {
         if set == &Range::full() {
@@ -335,49 +397,49 @@
 
     /// Generate the [`PubGrubHints`] for a derivation tree.
     ///
     /// The [`PubGrubHints`] help users resolve errors by providing additional context or modifying
     /// their requirements.
     pub(crate) fn hints(
         &self,
-        derivation_tree: &DerivationTree<PubGrubPackage, Range<Version>>,
+        derivation_tree: &DerivationTree<PubGrubPackage, Range<Version>, UnavailableReason>,
         selector: &Option<CandidateSelector>,
         index_locations: &Option<IndexLocations>,
         unavailable_packages: &FxHashMap<PackageName, UnavailablePackage>,
+        incomplete_packages: &FxHashMap<PackageName, BTreeMap<Version, IncompletePackage>>,
     ) -> IndexSet<PubGrubHint> {
         /// Returns `true` if pre-releases were allowed for a package.
         fn allowed_prerelease(package: &PubGrubPackage, selector: &CandidateSelector) -> bool {
-            match selector.prerelease_strategy() {
-                PreReleaseStrategy::Disallow => false,
-                PreReleaseStrategy::Allow => true,
-                PreReleaseStrategy::IfNecessary => false,
-                PreReleaseStrategy::Explicit(packages) => {
-                    if let PubGrubPackage::Package(package, ..) = package {
-                        packages.contains(package)
-                    } else {
-                        false
-                    }
-                }
-                PreReleaseStrategy::IfNecessaryOrExplicit(packages) => {
-                    if let PubGrubPackage::Package(package, ..) = package {
-                        packages.contains(package)
-                    } else {
-                        false
-                    }
-                }
-            }
+            let PubGrubPackageInner::Package { name, .. } = &**package else {
+                return false;
+            };
+            selector.prerelease_strategy().allows(name)
         }
 
         let mut hints = IndexSet::default();
         match derivation_tree {
             DerivationTree::External(external) => match external {
-                External::NoVersions(package, set, _) => {
+                External::Custom(package, set, _) | External::NoVersions(package, set) => {
                     // Check for no versions due to pre-release options
                     if let Some(selector) = selector {
-                        if set.bounds().any(Version::any_prerelease) {
+                        let any_prerelease = set.iter().any(|(start, end)| {
+                            let is_pre1 = match start {
+                                Bound::Included(version) => version.any_prerelease(),
+                                Bound::Excluded(version) => version.any_prerelease(),
+                                Bound::Unbounded => false,
+                            };
+                            let is_pre2 = match end {
+                                Bound::Included(version) => version.any_prerelease(),
+                                Bound::Excluded(version) => version.any_prerelease(),
+                                Bound::Unbounded => false,
+                            };
+                            is_pre1 || is_pre2
+                        });
+
+                        if any_prerelease {
                             // A pre-release marker appeared in the version requirements.
                             if !allowed_prerelease(package, selector) {
                                 hints.insert(PubGrubHint::PreReleaseRequested {
                                     package: package.clone(),
                                     range: self.simplify_set(set, package).into_owned(),
                                 });
                             }
@@ -401,45 +463,99 @@
                     }
 
                     // Check for no versions due to no `--find-links` flat index
                     if let Some(index_locations) = index_locations {
                         let no_find_links =
                             index_locations.flat_index().peekable().peek().is_none();
 
-                        if let PubGrubPackage::Package(name, ..) = package {
+                        if let PubGrubPackageInner::Package { name, .. } = &**package {
+                            // Add hints due to the package being entirely unavailable.
                             match unavailable_packages.get(name) {
                                 Some(UnavailablePackage::NoIndex) => {
                                     if no_find_links {
                                         hints.insert(PubGrubHint::NoIndex);
                                     }
                                 }
                                 Some(UnavailablePackage::Offline) => {
                                     hints.insert(PubGrubHint::Offline);
                                 }
-                                _ => {}
+                                Some(UnavailablePackage::InvalidMetadata(reason)) => {
+                                    hints.insert(PubGrubHint::InvalidPackageMetadata {
+                                        package: package.clone(),
+                                        reason: reason.clone(),
+                                    });
+                                }
+                                Some(UnavailablePackage::InvalidStructure(reason)) => {
+                                    hints.insert(PubGrubHint::InvalidPackageStructure {
+                                        package: package.clone(),
+                                        reason: reason.clone(),
+                                    });
+                                }
+                                Some(UnavailablePackage::NotFound) => {}
+                                None => {}
+                            }
+
+                            // Add hints due to the package being unavailable at specific versions.
+                            if let Some(versions) = incomplete_packages.get(name) {
+                                for (version, incomplete) in versions.iter().rev() {
+                                    if set.contains(version) {
+                                        match incomplete {
+                                            IncompletePackage::Offline => {
+                                                hints.insert(PubGrubHint::Offline);
+                                            }
+                                            IncompletePackage::InvalidMetadata(reason) => {
+                                                hints.insert(PubGrubHint::InvalidVersionMetadata {
+                                                    package: package.clone(),
+                                                    version: version.clone(),
+                                                    reason: reason.clone(),
+                                                });
+                                            }
+                                            IncompletePackage::InconsistentMetadata(reason) => {
+                                                hints.insert(
+                                                    PubGrubHint::InconsistentVersionMetadata {
+                                                        package: package.clone(),
+                                                        version: version.clone(),
+                                                        reason: reason.clone(),
+                                                    },
+                                                );
+                                            }
+                                            IncompletePackage::InvalidStructure(reason) => {
+                                                hints.insert(
+                                                    PubGrubHint::InvalidVersionStructure {
+                                                        package: package.clone(),
+                                                        version: version.clone(),
+                                                        reason: reason.clone(),
+                                                    },
+                                                );
+                                            }
+                                        }
+                                        break;
+                                    }
+                                }
                             }
                         }
                     }
                 }
                 External::NotRoot(..) => {}
-                External::Unavailable(..) => {}
                 External::FromDependencyOf(..) => {}
             },
             DerivationTree::Derived(derived) => {
                 hints.extend(self.hints(
                     &derived.cause1,
                     selector,
                     index_locations,
                     unavailable_packages,
+                    incomplete_packages,
                 ));
                 hints.extend(self.hints(
                     &derived.cause2,
                     selector,
                     index_locations,
                     unavailable_packages,
+                    incomplete_packages,
                 ));
             }
         }
         hints
     }
 }
 
@@ -460,57 +576,159 @@
         package: PubGrubPackage,
         #[derivative(PartialEq = "ignore", Hash = "ignore")]
         range: Range<Version>,
     },
     /// Requirements were unavailable due to lookups in the index being disabled and no extra
     /// index was provided via `--find-links`
     NoIndex,
-    /// A package was not found in the registry, but
+    /// A package was not found in the registry, but network access was disabled.
     Offline,
+    /// Metadata for a package could not be parsed.
+    InvalidPackageMetadata {
+        package: PubGrubPackage,
+        #[derivative(PartialEq = "ignore", Hash = "ignore")]
+        reason: String,
+    },
+    /// The structure of a package was invalid (e.g., multiple `.dist-info` directories).
+    InvalidPackageStructure {
+        package: PubGrubPackage,
+        #[derivative(PartialEq = "ignore", Hash = "ignore")]
+        reason: String,
+    },
+    /// Metadata for a package version could not be parsed.
+    InvalidVersionMetadata {
+        package: PubGrubPackage,
+        #[derivative(PartialEq = "ignore", Hash = "ignore")]
+        version: Version,
+        #[derivative(PartialEq = "ignore", Hash = "ignore")]
+        reason: String,
+    },
+    /// Metadata for a package version was inconsistent (e.g., the package name did not match that
+    /// of the file).
+    InconsistentVersionMetadata {
+        package: PubGrubPackage,
+        #[derivative(PartialEq = "ignore", Hash = "ignore")]
+        version: Version,
+        #[derivative(PartialEq = "ignore", Hash = "ignore")]
+        reason: String,
+    },
+    /// The structure of a package version was invalid (e.g., multiple `.dist-info` directories).
+    InvalidVersionStructure {
+        package: PubGrubPackage,
+        #[derivative(PartialEq = "ignore", Hash = "ignore")]
+        version: Version,
+        #[derivative(PartialEq = "ignore", Hash = "ignore")]
+        reason: String,
+    },
 }
 
 impl std::fmt::Display for PubGrubHint {
     fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
         match self {
-            PubGrubHint::PreReleaseAvailable { package, version } => {
+            Self::PreReleaseAvailable { package, version } => {
                 write!(
                     f,
                     "{}{} Pre-releases are available for {} in the requested range (e.g., {}), but pre-releases weren't enabled (try: `--prerelease=allow`)",
                     "hint".bold().cyan(),
                     ":".bold(),
                     package.bold(),
                     version.bold()
                 )
             }
-            PubGrubHint::PreReleaseRequested { package, range } => {
+            Self::PreReleaseRequested { package, range } => {
                 write!(
                     f,
                     "{}{} {} was requested with a pre-release marker (e.g., {}), but pre-releases weren't enabled (try: `--prerelease=allow`)",
                     "hint".bold().cyan(),
                     ":".bold(),
                     package.bold(),
                     PackageRange::compatibility(package, range).bold()
                 )
             }
-            PubGrubHint::NoIndex => {
+            Self::NoIndex => {
                 write!(
                     f,
                     "{}{} Packages were unavailable because index lookups were disabled and no additional package locations were provided (try: `--find-links <uri>`)",
                     "hint".bold().cyan(),
                     ":".bold(),
                 )
             }
-            PubGrubHint::Offline => {
+            Self::Offline => {
                 write!(
                     f,
                     "{}{} Packages were unavailable because the network was disabled",
                     "hint".bold().cyan(),
                     ":".bold(),
                 )
             }
+            Self::InvalidPackageMetadata { package, reason } => {
+                write!(
+                    f,
+                    "{}{} Metadata for {} could not be parsed:\n{}",
+                    "hint".bold().cyan(),
+                    ":".bold(),
+                    package.bold(),
+                    textwrap::indent(reason, "  ")
+                )
+            }
+            Self::InvalidPackageStructure { package, reason } => {
+                write!(
+                    f,
+                    "{}{} The structure of {} was invalid:\n{}",
+                    "hint".bold().cyan(),
+                    ":".bold(),
+                    package.bold(),
+                    textwrap::indent(reason, "  ")
+                )
+            }
+            Self::InvalidVersionMetadata {
+                package,
+                version,
+                reason,
+            } => {
+                write!(
+                    f,
+                    "{}{} Metadata for {}=={} could not be parsed:\n{}",
+                    "hint".bold().cyan(),
+                    ":".bold(),
+                    package.bold(),
+                    version.bold(),
+                    textwrap::indent(reason, "  ")
+                )
+            }
+            Self::InvalidVersionStructure {
+                package,
+                version,
+                reason,
+            } => {
+                write!(
+                    f,
+                    "{}{} The structure of {}=={} was invalid:\n{}",
+                    "hint".bold().cyan(),
+                    ":".bold(),
+                    package.bold(),
+                    version.bold(),
+                    textwrap::indent(reason, "  ")
+                )
+            }
+            PubGrubHint::InconsistentVersionMetadata {
+                package,
+                version,
+                reason,
+            } => {
+                write!(
+                    f,
+                    "{}{} Metadata for {}=={} was inconsistent:\n{}",
+                    "hint".bold().cyan(),
+                    ":".bold(),
+                    package.bold(),
+                    version.bold(),
+                    textwrap::indent(reason, "  ")
+                )
+            }
         }
     }
 }
 
 /// A [`Term`] and [`PubGrubPackage`] combination for display.
 struct PackageTerm<'a> {
     package: &'a PubGrubPackage,
@@ -574,55 +792,61 @@
             matches!(segments.as_slice(), [(Bound::Unbounded, Bound::Unbounded)])
         }
     }
 }
 
 impl std::fmt::Display for PackageRange<'_> {
     fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
+        // Exit early for the root package — the range is not meaningful
+        let package = match &**self.package {
+            PubGrubPackageInner::Root(Some(name)) => return write!(f, "{name}"),
+            PubGrubPackageInner::Root(None) => return write!(f, "your requirements"),
+            _ => self.package,
+        };
+
         if self.range.is_empty() {
-            write!(f, "∅")?;
-        } else {
-            let segments: Vec<_> = self.range.iter().collect();
-            if segments.len() > 1 {
-                match self.kind {
-                    PackageRangeKind::Dependency => write!(f, "one of:")?,
-                    PackageRangeKind::Compatibility => write!(f, "any of:")?,
-                    PackageRangeKind::Available => write!(f, "are available:")?,
-                }
-            }
-            let package = fmt_package(self.package);
-            for segment in &segments {
-                if segments.len() > 1 {
-                    write!(f, "\n    ")?;
-                }
-                match segment {
-                    (Bound::Unbounded, Bound::Unbounded) => match self.kind {
-                        PackageRangeKind::Dependency => write!(f, "{package}")?,
-                        PackageRangeKind::Compatibility => write!(f, "all versions of {package}")?,
-                        PackageRangeKind::Available => write!(f, "{package}")?,
-                    },
-                    (Bound::Unbounded, Bound::Included(v)) => write!(f, "{package}<={v}")?,
-                    (Bound::Unbounded, Bound::Excluded(v)) => write!(f, "{package}<{v}")?,
-                    (Bound::Included(v), Bound::Unbounded) => write!(f, "{package}>={v}")?,
-                    (Bound::Included(v), Bound::Included(b)) => {
-                        if v == b {
-                            write!(f, "{package}=={v}")?;
-                        } else {
-                            write!(f, "{package}>={v},<={b}")?;
-                        }
-                    }
-                    (Bound::Included(v), Bound::Excluded(b)) => write!(f, "{package}>={v},<{b}")?,
-                    (Bound::Excluded(v), Bound::Unbounded) => write!(f, "{package}>{v}")?,
-                    (Bound::Excluded(v), Bound::Included(b)) => write!(f, "{package}>{v},<={b}")?,
-                    (Bound::Excluded(v), Bound::Excluded(b)) => write!(f, "{package}>{v},<{b}")?,
-                };
+            return write!(f, "{package} ∅");
+        }
+
+        let segments: Vec<_> = self.range.iter().collect();
+        if segments.len() > 1 {
+            match self.kind {
+                PackageRangeKind::Dependency => write!(f, "one of:")?,
+                PackageRangeKind::Compatibility => write!(f, "any of:")?,
+                PackageRangeKind::Available => write!(f, "are available:")?,
             }
+        }
+        for segment in &segments {
             if segments.len() > 1 {
-                writeln!(f)?;
+                write!(f, "\n    ")?;
             }
+            match segment {
+                (Bound::Unbounded, Bound::Unbounded) => match self.kind {
+                    PackageRangeKind::Dependency => write!(f, "{package}")?,
+                    PackageRangeKind::Compatibility => write!(f, "all versions of {package}")?,
+                    PackageRangeKind::Available => write!(f, "{package}")?,
+                },
+                (Bound::Unbounded, Bound::Included(v)) => write!(f, "{package}<={v}")?,
+                (Bound::Unbounded, Bound::Excluded(v)) => write!(f, "{package}<{v}")?,
+                (Bound::Included(v), Bound::Unbounded) => write!(f, "{package}>={v}")?,
+                (Bound::Included(v), Bound::Included(b)) => {
+                    if v == b {
+                        write!(f, "{package}=={v}")?;
+                    } else {
+                        write!(f, "{package}>={v},<={b}")?;
+                    }
+                }
+                (Bound::Included(v), Bound::Excluded(b)) => write!(f, "{package}>={v},<{b}")?,
+                (Bound::Excluded(v), Bound::Unbounded) => write!(f, "{package}>{v}")?,
+                (Bound::Excluded(v), Bound::Included(b)) => write!(f, "{package}>{v},<={b}")?,
+                (Bound::Excluded(v), Bound::Excluded(b)) => write!(f, "{package}>{v},<{b}")?,
+            };
+        }
+        if segments.len() > 1 {
+            writeln!(f)?;
         }
         Ok(())
     }
 }
 
 impl PackageRange<'_> {
     fn compatibility<'a>(
@@ -647,42 +871,65 @@
     fn available<'a>(package: &'a PubGrubPackage, range: &'a Range<Version>) -> PackageRange<'a> {
         PackageRange {
             package,
             range,
             kind: PackageRangeKind::Available,
         }
     }
+
     fn depends_on<'a>(
         &'a self,
         package: &'a PubGrubPackage,
         range: &'a Range<Version>,
     ) -> DependsOn<'a> {
         DependsOn {
-            first: self,
-            second: PackageRange::dependency(package, range),
+            package: self,
+            dependency1: PackageRange::dependency(package, range),
+            dependency2: None,
         }
     }
 }
 
-/// A representation of A depends on B.
+/// A representation of A depends on B (and C).
 #[derive(Debug)]
 struct DependsOn<'a> {
-    first: &'a PackageRange<'a>,
-    second: PackageRange<'a>,
+    package: &'a PackageRange<'a>,
+    dependency1: PackageRange<'a>,
+    dependency2: Option<PackageRange<'a>>,
+}
+
+impl<'a> DependsOn<'a> {
+    /// Adds an additional dependency.
+    ///
+    /// Note this overwrites previous calls to `DependsOn::and`.
+    fn and(mut self, package: &'a PubGrubPackage, range: &'a Range<Version>) -> DependsOn<'a> {
+        self.dependency2 = Some(PackageRange::dependency(package, range));
+        self
+    }
 }
 
 impl std::fmt::Display for DependsOn<'_> {
     fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-        write!(f, "{}", Padded::new("", self.first, " "))?;
-        if self.first.plural() {
+        write!(f, "{}", Padded::new("", self.package, " "))?;
+        if self.package.plural() {
             write!(f, "depend on ")?;
         } else {
             write!(f, "depends on ")?;
         };
-        write!(f, "{}", self.second)?;
+
+        match self.dependency2 {
+            Some(ref dependency2) => write!(
+                f,
+                "{}and{}",
+                Padded::new("", &self.dependency1, " "),
+                Padded::new(" ", &dependency2, "")
+            )?,
+            None => write!(f, "{}", self.dependency1)?,
+        }
+
         Ok(())
     }
 }
 
 /// Inserts the given padding on the left and right sides of the content if
 /// the content does not start and end with whitespace respectively.
 #[derive(Debug)]
@@ -730,15 +977,7 @@
                 result.push_str(self.right);
             }
         }
 
         write!(f, "{result}")
     }
 }
-
-fn fmt_package(package: &PubGrubPackage) -> String {
-    match package {
-        PubGrubPackage::Root(Some(name)) => name.to_string(),
-        PubGrubPackage::Root(None) => "you require".to_string(),
-        _ => format!("{package}"),
-    }
-}
```

### Comparing `uv-0.1.9/crates/uv-resolver/src/pubgrub/specifier.rs` & `uv-0.2.0/crates/uv-resolver/src/pubgrub/specifier.rs`

 * *Files 10% similar despite different names*

```diff
@@ -6,24 +6,24 @@
 use crate::ResolveError;
 
 /// A range of versions that can be used to satisfy a requirement.
 #[derive(Debug)]
 pub(crate) struct PubGrubSpecifier(Range<Version>);
 
 impl From<PubGrubSpecifier> for Range<Version> {
-    /// Convert a `PubGrub` specifier to a range of versions.
+    /// Convert a PubGrub specifier to a range of versions.
     fn from(specifier: PubGrubSpecifier) -> Self {
         specifier.0
     }
 }
 
 impl TryFrom<&VersionSpecifier> for PubGrubSpecifier {
     type Error = ResolveError;
 
-    /// Convert a PEP 508 specifier to a `PubGrub`-compatible version range.
+    /// Convert a PEP 508 specifier to a PubGrub-compatible version range.
     fn try_from(specifier: &VersionSpecifier) -> Result<Self, ResolveError> {
         let ranges = match specifier.operator() {
             Operator::Equal => {
                 let version = specifier.version().clone();
                 Range::singleton(version)
             }
             Operator::ExactEqual => {
@@ -34,33 +34,46 @@
                 let version = specifier.version().clone();
                 Range::singleton(version).complement()
             }
             Operator::TildeEqual => {
                 let [rest @ .., last, _] = specifier.version().release() else {
                     return Err(ResolveError::InvalidTildeEquals(specifier.clone()));
                 };
-                let upper = pep440_rs::Version::new(rest.iter().chain([&(last + 1)]))
+                let upper = Version::new(rest.iter().chain([&(last + 1)]))
                     .with_epoch(specifier.version().epoch())
                     .with_dev(Some(0));
                 let version = specifier.version().clone();
                 Range::from_range_bounds(version..upper)
             }
             Operator::LessThan => {
                 let version = specifier.version().clone();
-                Range::strictly_lower_than(version)
+                if version.any_prerelease() {
+                    Range::strictly_lower_than(version)
+                } else {
+                    // Per PEP 440: "The exclusive ordered comparison <V MUST NOT allow a
+                    // pre-release of the specified version unless the specified version is itself a
+                    // pre-release.
+                    Range::strictly_lower_than(version.with_min(Some(0)))
+                }
             }
             Operator::LessThanEqual => {
                 let version = specifier.version().clone();
                 Range::lower_than(version)
             }
             Operator::GreaterThan => {
                 // Per PEP 440: "The exclusive ordered comparison >V MUST NOT allow a post-release of
                 // the given version unless V itself is a post release."
                 let version = specifier.version().clone();
-                Range::strictly_higher_than(version)
+                if let Some(dev) = version.dev() {
+                    Range::higher_than(version.with_dev(Some(dev + 1)))
+                } else if let Some(post) = version.post() {
+                    Range::higher_than(version.with_post(Some(post + 1)))
+                } else {
+                    Range::strictly_higher_than(version.with_max(Some(0)))
+                }
             }
             Operator::GreaterThanEqual => {
                 let version = specifier.version().clone();
                 Range::higher_than(version)
             }
             Operator::EqualStar => {
                 let low = specifier.version().clone().with_dev(Some(0));
```

### Comparing `uv-0.1.9/crates/uv-resolver/src/python_requirement.rs` & `uv-0.2.0/crates/uv-installer/src/installer.rs`

 * *Files 25% similar despite different names*

```diff
@@ -1,71 +1,87 @@
-use distribution_types::{CompatibleDist, Dist};
-use pep440_rs::{Version, VersionSpecifiers};
-use pep508_rs::MarkerEnvironment;
-use uv_interpreter::Interpreter;
-
-#[derive(Debug, Clone, Eq, PartialEq, Ord, PartialOrd)]
-pub struct PythonRequirement {
-    /// The installed version of Python.
-    installed: Version,
-    /// The target version of Python; that is, the version of Python for which we are resolving
-    /// dependencies. This is typically the same as the installed version, but may be different
-    /// when specifying an alternate Python version for the resolution.
-    target: Version,
+use anyhow::{Context, Error, Result};
+use rayon::iter::{IntoParallelRefIterator, ParallelIterator};
+use tracing::instrument;
+
+use distribution_types::CachedDist;
+use uv_interpreter::PythonEnvironment;
+
+pub struct Installer<'a> {
+    venv: &'a PythonEnvironment,
+    link_mode: install_wheel_rs::linker::LinkMode,
+    reporter: Option<Box<dyn Reporter>>,
+    installer_name: Option<String>,
 }
 
-impl PythonRequirement {
-    pub fn new(interpreter: &Interpreter, markers: &MarkerEnvironment) -> Self {
+impl<'a> Installer<'a> {
+    /// Initialize a new installer.
+    pub fn new(venv: &'a PythonEnvironment) -> Self {
         Self {
-            installed: interpreter.python_version().clone(),
-            target: markers.python_full_version.version.clone(),
+            venv,
+            link_mode: install_wheel_rs::linker::LinkMode::default(),
+            reporter: None,
+            installer_name: Some("uv".to_string()),
         }
     }
 
-    /// Return the installed version of Python.
-    pub(crate) fn installed(&self) -> &Version {
-        &self.installed
+    /// Set the [`LinkMode`][`install_wheel_rs::linker::LinkMode`] to use for this installer.
+    #[must_use]
+    pub fn with_link_mode(self, link_mode: install_wheel_rs::linker::LinkMode) -> Self {
+        Self { link_mode, ..self }
     }
 
-    /// Return the target version of Python.
-    pub(crate) fn target(&self) -> &Version {
-        &self.target
-    }
-
-    /// If the dist doesn't match the given Python requirement, return the version specifiers.
-    pub(crate) fn validate_dist<'a>(
-        &self,
-        dist: &'a CompatibleDist,
-    ) -> Option<&'a VersionSpecifiers> {
-        // Validate the _installed_ file.
-        let requires_python = dist.for_installation().requires_python.as_ref()?;
-
-        // If the dist doesn't support the target Python version, return the failing version
-        // specifiers.
-        if !requires_python.contains(self.target()) {
-            return Some(requires_python);
+    /// Set the [`Reporter`] to use for this installer.
+    #[must_use]
+    pub fn with_reporter(self, reporter: impl Reporter + 'static) -> Self {
+        Self {
+            reporter: Some(Box::new(reporter)),
+            ..self
         }
+    }
 
-        // If the dist is a source distribution, and doesn't support the installed Python
-        // version, return the failing version specifiers, since we won't be able to build it.
-        if matches!(dist.for_installation().dist, Dist::Source(_)) {
-            if !requires_python.contains(self.installed()) {
-                return Some(requires_python);
-            }
+    /// Set the `installer_name` to something other than `"uv"`.
+    #[must_use]
+    pub fn with_installer_name(self, installer_name: Option<String>) -> Self {
+        Self {
+            installer_name,
+            ..self
         }
+    }
 
-        // Validate the resolved file.
-        let requires_python = dist.for_resolution().requires_python.as_ref()?;
+    /// Install a set of wheels into a Python virtual environment.
+    #[instrument(skip_all, fields(num_wheels = %wheels.len()))]
+    pub fn install(self, wheels: &[CachedDist]) -> Result<()> {
+        let layout = self.venv.interpreter().layout();
+        tokio::task::block_in_place(|| {
+            wheels.par_iter().try_for_each(|wheel| {
+                install_wheel_rs::linker::install_wheel(
+                    &layout,
+                    wheel.path(),
+                    wheel.filename(),
+                    wheel
+                        .parsed_url()?
+                        .as_ref()
+                        .map(pypi_types::DirectUrl::try_from)
+                        .transpose()?
+                        .as_ref(),
+                    self.installer_name.as_deref(),
+                    self.link_mode,
+                )
+                .with_context(|| format!("Failed to install: {} ({wheel})", wheel.filename()))?;
+
+                if let Some(reporter) = self.reporter.as_ref() {
+                    reporter.on_install_progress(wheel);
+                }
+
+                Ok::<(), Error>(())
+            })
+        })
+    }
+}
 
-        // If the dist is a source distribution, and doesn't support the installed Python
-        // version, return the failing version specifiers, since we won't be able to build it.
-        // This isn't strictly necessary, since if `dist.resolve_metadata()` is a source distribution, it
-        // should be the same file as `dist.install_metadata()` (validated above).
-        if matches!(dist.for_resolution().dist, Dist::Source(_)) {
-            if !requires_python.contains(self.installed()) {
-                return Some(requires_python);
-            }
-        }
+pub trait Reporter: Send + Sync {
+    /// Callback to invoke when a dependency is installed.
+    fn on_install_progress(&self, wheel: &CachedDist);
 
-        None
-    }
+    /// Callback to invoke when the resolution is complete.
+    fn on_install_complete(&self);
 }
```

### Comparing `uv-0.1.9/crates/uv-resolver/src/resolution.rs` & `uv-0.2.0/crates/uv-installer/src/site_packages.rs`

 * *Files 23% similar despite different names*

```diff
@@ -1,501 +1,555 @@
-use std::borrow::Cow;
-use std::hash::BuildHasherDefault;
-
-use anyhow::Result;
-use dashmap::DashMap;
-use owo_colors::OwoColorize;
-use petgraph::visit::EdgeRef;
-use petgraph::Direction;
-use pubgrub::range::Range;
-use pubgrub::solver::{Kind, State};
-use pubgrub::type_aliases::SelectedDependencies;
-use rustc_hash::FxHashMap;
+use std::iter::Flatten;
+use std::path::PathBuf;
+use std::{collections::BTreeSet, hash::BuildHasherDefault};
+
+use anyhow::{Context, Result};
+use fs_err as fs;
+use rustc_hash::{FxHashMap, FxHashSet};
 use url::Url;
 
-use distribution_types::{Dist, DistributionMetadata, LocalEditable, Name, PackageId, Verbatim};
-use once_map::OnceMap;
-use pep440_rs::Version;
-use pep508_rs::VerbatimUrl;
-use pypi_types::{Hashes, Metadata21};
-use uv_normalize::{ExtraName, PackageName};
-
-use crate::editables::Editables;
-use crate::pins::FilePins;
-use crate::pubgrub::{PubGrubDistribution, PubGrubPackage, PubGrubPriority};
-use crate::resolver::VersionsResponse;
-use crate::ResolveError;
-
-/// Indicate the style of annotation comments, used to indicate the dependencies that requested each
-/// package.
-#[derive(Debug, Default, Copy, Clone, PartialEq)]
-#[cfg_attr(feature = "clap", derive(clap::ValueEnum))]
-pub enum AnnotationStyle {
-    /// Render the annotations on a single, comma-separated line.
-    Line,
-    /// Render each annotation on its own line.
-    #[default]
-    Split,
-}
-
-/// A complete resolution graph in which every node represents a pinned package and every edge
-/// represents a dependency between two pinned packages.
-#[derive(Debug)]
-pub struct ResolutionGraph {
-    /// The underlying graph.
-    petgraph: petgraph::graph::Graph<Dist, Range<Version>, petgraph::Directed>,
-    /// The metadata for every distribution in this resolution.
-    hashes: FxHashMap<PackageName, Vec<Hashes>>,
-    /// The set of editable requirements in this resolution.
-    editables: Editables,
-    /// Any diagnostics that were encountered while building the graph.
-    diagnostics: Vec<Diagnostic>,
+use distribution_types::{
+    InstalledDist, Name, Requirement, UnresolvedRequirement, UnresolvedRequirementSpecification,
+};
+use pep440_rs::{Version, VersionSpecifiers};
+use requirements_txt::EditableRequirement;
+use uv_cache::{ArchiveTarget, ArchiveTimestamp};
+use uv_interpreter::PythonEnvironment;
+use uv_normalize::PackageName;
+use uv_types::InstalledPackagesProvider;
+
+use crate::is_dynamic;
+use crate::satisfies::RequirementSatisfaction;
+
+/// An index over the packages installed in an environment.
+///
+/// Packages are indexed by both name and (for editable installs) URL.
+#[derive(Debug, Clone)]
+pub struct SitePackages {
+    venv: PythonEnvironment,
+    /// The vector of all installed distributions. The `by_name` and `by_url` indices index into
+    /// this vector. The vector may contain `None` values, which represent distributions that were
+    /// removed from the virtual environment.
+    distributions: Vec<Option<InstalledDist>>,
+    /// The installed distributions, keyed by name. Although the Python runtime does not support it,
+    /// it is possible to have multiple distributions with the same name to be present in the
+    /// virtual environment, which we handle gracefully.
+    by_name: FxHashMap<PackageName, Vec<usize>>,
+    /// The installed editable distributions, keyed by URL.
+    by_url: FxHashMap<Url, Vec<usize>>,
 }
 
-impl ResolutionGraph {
-    /// Create a new graph from the resolved `PubGrub` state.
-    pub(crate) fn from_state(
-        selection: &SelectedDependencies<PubGrubPackage, Version>,
-        pins: &FilePins,
-        packages: &OnceMap<PackageName, VersionsResponse>,
-        distributions: &OnceMap<PackageId, Metadata21>,
-        redirects: &DashMap<Url, Url>,
-        state: &State<PubGrubPackage, Range<Version>, PubGrubPriority>,
-        editables: Editables,
-    ) -> Result<Self, ResolveError> {
-        // TODO(charlie): petgraph is a really heavy and unnecessary dependency here. We should
-        // write our own graph, given that our requirements are so simple.
-        let mut petgraph = petgraph::graph::Graph::with_capacity(selection.len(), selection.len());
-        let mut hashes =
-            FxHashMap::with_capacity_and_hasher(selection.len(), BuildHasherDefault::default());
-        let mut diagnostics = Vec::new();
-
-        // Add every package to the graph.
-        let mut inverse =
-            FxHashMap::with_capacity_and_hasher(selection.len(), BuildHasherDefault::default());
-        for (package, version) in selection {
-            match package {
-                PubGrubPackage::Package(package_name, None, None) => {
-                    // Create the distribution.
-                    let pinned_package = if let Some((editable, _)) = editables.get(package_name) {
-                        Dist::from_editable(package_name.clone(), editable.clone())?
-                    } else {
-                        pins.get(package_name, version)
-                            .expect("Every package should be pinned")
-                            .clone()
-                    };
-
-                    // Add its hashes to the index.
-                    if let Some(versions_response) = packages.get(package_name) {
-                        if let VersionsResponse::Found(ref version_map) = *versions_response {
-                            hashes.insert(package_name.clone(), {
-                                let mut hashes = version_map.hashes(version);
-                                hashes.sort_unstable();
-                                hashes
-                            });
-                        }
-                    }
-
-                    // Add the distribution to the graph.
-                    let index = petgraph.add_node(pinned_package);
-                    inverse.insert(package_name, index);
-                }
-                PubGrubPackage::Package(package_name, None, Some(url)) => {
-                    // Create the distribution.
-                    let pinned_package = if let Some((editable, _)) = editables.get(package_name) {
-                        Dist::from_editable(package_name.clone(), editable.clone())?
-                    } else {
-                        let url = redirects.get(url).map_or_else(
-                            || url.clone(),
-                            |url| VerbatimUrl::unknown(url.value().clone()),
-                        );
-                        Dist::from_url(package_name.clone(), url)?
-                    };
-
-                    // Add its hashes to the index.
-                    if let Some(versions_response) = packages.get(package_name) {
-                        if let VersionsResponse::Found(ref version_map) = *versions_response {
-                            hashes.insert(package_name.clone(), {
-                                let mut hashes = version_map.hashes(version);
-                                hashes.sort_unstable();
-                                hashes
-                            });
-                        }
-                    }
-
-                    // Add the distribution to the graph.
-                    let index = petgraph.add_node(pinned_package);
-                    inverse.insert(package_name, index);
-                }
-                PubGrubPackage::Package(package_name, Some(extra), None) => {
-                    // Validate that the `extra` exists.
-                    let dist = PubGrubDistribution::from_registry(package_name, version);
-
-                    if let Some((editable, metadata)) = editables.get(package_name) {
-                        if !metadata.provides_extras.contains(extra) {
-                            let pinned_package =
-                                Dist::from_editable(package_name.clone(), editable.clone())?;
-
-                            diagnostics.push(Diagnostic::MissingExtra {
-                                dist: pinned_package,
-                                extra: extra.clone(),
-                            });
-                        }
-                    } else {
-                        let metadata = distributions.get(&dist.package_id()).unwrap_or_else(|| {
-                            panic!(
-                                "Every package should have metadata: {:?}",
-                                dist.package_id()
-                            )
-                        });
-
-                        if !metadata.provides_extras.contains(extra) {
-                            let pinned_package = pins
-                                .get(package_name, version)
-                                .unwrap_or_else(|| {
-                                    panic!("Every package should be pinned: {package_name:?}")
-                                })
-                                .clone();
-
-                            diagnostics.push(Diagnostic::MissingExtra {
-                                dist: pinned_package,
-                                extra: extra.clone(),
-                            });
-                        }
-                    }
+impl SitePackages {
+    /// Build an index of installed packages from the given Python executable.
+    pub fn from_executable(venv: &PythonEnvironment) -> Result<SitePackages> {
+        let mut distributions: Vec<Option<InstalledDist>> = Vec::new();
+        let mut by_name = FxHashMap::default();
+        let mut by_url = FxHashMap::default();
+
+        for site_packages in venv.site_packages() {
+            // Read the site-packages directory.
+            let site_packages = match fs::read_dir(site_packages) {
+                Ok(site_packages) => {
+                    // Collect sorted directory paths; `read_dir` is not stable across platforms
+                    let dist_likes: BTreeSet<_> = site_packages
+                        .filter_map(|read_dir| match read_dir {
+                            Ok(entry) => match entry.file_type() {
+                                Ok(file_type) => (file_type.is_dir()
+                                    || entry
+                                        .path()
+                                        .extension()
+                                        .map_or(false, |ext| ext == "egg-link"))
+                                .then_some(Ok(entry.path())),
+                                Err(err) => Some(Err(err)),
+                            },
+                            Err(err) => Some(Err(err)),
+                        })
+                        .collect::<Result<_, std::io::Error>>()?;
+                    dist_likes
                 }
-                PubGrubPackage::Package(package_name, Some(extra), Some(url)) => {
-                    // Validate that the `extra` exists.
-                    let dist = PubGrubDistribution::from_url(package_name, url);
-
-                    if let Some((editable, metadata)) = editables.get(package_name) {
-                        if !metadata.provides_extras.contains(extra) {
-                            let pinned_package =
-                                Dist::from_editable(package_name.clone(), editable.clone())?;
-
-                            diagnostics.push(Diagnostic::MissingExtra {
-                                dist: pinned_package,
-                                extra: extra.clone(),
-                            });
-                        }
-                    } else {
-                        let metadata = distributions.get(&dist.package_id()).unwrap_or_else(|| {
-                            panic!(
-                                "Every package should have metadata: {:?}",
-                                dist.package_id()
-                            )
-                        });
-
-                        if !metadata.provides_extras.contains(extra) {
-                            let url = redirects.get(url).map_or_else(
-                                || url.clone(),
-                                |url| VerbatimUrl::unknown(url.value().clone()),
-                            );
-                            let pinned_package = Dist::from_url(package_name.clone(), url)?;
-
-                            diagnostics.push(Diagnostic::MissingExtra {
-                                dist: pinned_package,
-                                extra: extra.clone(),
-                            });
-                        }
-                    }
+                Err(err) if err.kind() == std::io::ErrorKind::NotFound => {
+                    return Ok(Self {
+                        venv: venv.clone(),
+                        distributions,
+                        by_name,
+                        by_url,
+                    });
                 }
-                _ => {}
+                Err(err) => return Err(err).context("Failed to read site-packages directory"),
             };
-        }
 
-        // Add every edge to the graph.
-        for (package, version) in selection {
-            for id in &state.incompatibilities[package] {
-                if let Kind::FromDependencyOf(
-                    self_package,
-                    self_version,
-                    dependency_package,
-                    dependency_range,
-                ) = &state.incompatibility_store[*id].kind
-                {
-                    let PubGrubPackage::Package(self_package, _, _) = self_package else {
-                        continue;
-                    };
-                    let PubGrubPackage::Package(dependency_package, _, _) = dependency_package
-                    else {
-                        continue;
-                    };
+            // Index all installed packages by name.
+            for path in site_packages {
+                let Some(dist_info) = InstalledDist::try_from_path(&path)
+                    .with_context(|| format!("Failed to read metadata: from {}", path.display()))?
+                else {
+                    continue;
+                };
 
-                    // For extras, we include a dependency between the extra and the base package.
-                    if self_package == dependency_package {
-                        continue;
-                    }
+                let idx = distributions.len();
 
-                    if self_version.contains(version) {
-                        let self_index = &inverse[self_package];
-                        let dependency_index = &inverse[dependency_package];
-                        petgraph.update_edge(
-                            *self_index,
-                            *dependency_index,
-                            dependency_range.clone(),
-                        );
-                    }
+                // Index the distribution by name.
+                by_name
+                    .entry(dist_info.name().clone())
+                    .or_insert_with(Vec::new)
+                    .push(idx);
+
+                // Index the distribution by URL.
+                if let InstalledDist::Url(dist) = &dist_info {
+                    by_url
+                        .entry(dist.url.clone())
+                        .or_insert_with(Vec::new)
+                        .push(idx);
                 }
+
+                // Add the distribution to the database.
+                distributions.push(Some(dist_info));
             }
         }
 
         Ok(Self {
-            petgraph,
-            hashes,
-            editables,
-            diagnostics,
+            venv: venv.clone(),
+            distributions,
+            by_name,
+            by_url,
         })
     }
 
-    /// Return the number of packages in the graph.
-    pub fn len(&self) -> usize {
-        self.petgraph.node_count()
+    /// Returns an iterator over the installed distributions.
+    pub fn iter(&self) -> impl Iterator<Item = &InstalledDist> {
+        self.distributions.iter().flatten()
+    }
+
+    /// Returns the installed distributions for a given package.
+    pub fn get_packages(&self, name: &PackageName) -> Vec<&InstalledDist> {
+        let Some(indexes) = self.by_name.get(name) else {
+            return Vec::new();
+        };
+        indexes
+            .iter()
+            .flat_map(|&index| &self.distributions[index])
+            .collect()
+    }
+
+    /// Remove the given packages from the index, returning all installed versions, if any.
+    pub fn remove_packages(&mut self, name: &PackageName) -> Vec<InstalledDist> {
+        let Some(indexes) = self.by_name.get(name) else {
+            return Vec::new();
+        };
+        indexes
+            .iter()
+            .filter_map(|index| std::mem::take(&mut self.distributions[*index]))
+            .collect()
+    }
+
+    /// Returns the distributions installed from the given URL, if any.
+    pub fn get_urls(&self, url: &Url) -> Vec<&InstalledDist> {
+        let Some(indexes) = self.by_url.get(url) else {
+            return Vec::new();
+        };
+        indexes
+            .iter()
+            .flat_map(|&index| &self.distributions[index])
+            .collect()
+    }
+
+    /// Returns the editable distribution installed from the given URL, if any.
+    pub fn get_editables(&self, url: &Url) -> Vec<&InstalledDist> {
+        let Some(indexes) = self.by_url.get(url) else {
+            return Vec::new();
+        };
+        indexes
+            .iter()
+            .flat_map(|&index| &self.distributions[index])
+            .filter(|dist| dist.is_editable())
+            .collect()
+    }
+
+    /// Remove the editable distribution installed from the given URL, if any.
+    pub fn remove_editables(&mut self, url: &Url) -> Vec<InstalledDist> {
+        let Some(indexes) = self.by_url.get(url) else {
+            return Vec::new();
+        };
+        indexes
+            .iter()
+            .filter_map(|index| {
+                let dist = &mut self.distributions[*index];
+                if dist.as_ref().is_some_and(InstalledDist::is_editable) {
+                    std::mem::take(dist)
+                } else {
+                    None
+                }
+            })
+            .collect()
     }
 
-    /// Return `true` if there are no packages in the graph.
-    pub fn is_empty(&self) -> bool {
-        self.petgraph.node_count() == 0
+    /// Returns `true` if there are any installed packages.
+    pub fn any(&self) -> bool {
+        self.distributions.iter().any(Option::is_some)
     }
 
-    /// Return the [`Diagnostic`]s that were encountered while building the graph.
-    pub fn diagnostics(&self) -> &[Diagnostic] {
-        &self.diagnostics
-    }
+    /// Validate the installed packages in the virtual environment.
+    pub fn diagnostics(&self) -> Result<Vec<Diagnostic>> {
+        let mut diagnostics = Vec::new();
 
-    /// Return the underlying graph.
-    pub fn petgraph(&self) -> &petgraph::graph::Graph<Dist, Range<Version>, petgraph::Directed> {
-        &self.petgraph
-    }
-}
+        for (package, indexes) in &self.by_name {
+            let mut distributions = indexes.iter().flat_map(|index| &self.distributions[*index]);
 
-/// A [`std::fmt::Display`] implementation for the resolution graph.
-#[derive(Debug)]
-pub struct DisplayResolutionGraph<'a> {
-    /// The underlying graph.
-    resolution: &'a ResolutionGraph,
-    /// Whether to include hashes in the output.
-    show_hashes: bool,
-    /// Whether to include annotations in the output, to indicate which dependency or dependencies
-    /// requested each package.
-    include_annotations: bool,
-    /// The style of annotation comments, used to indicate the dependencies that requested each
-    /// package.
-    annotation_style: AnnotationStyle,
-}
+            // Find the installed distribution for the given package.
+            let Some(distribution) = distributions.next() else {
+                continue;
+            };
 
-impl<'a> From<&'a ResolutionGraph> for DisplayResolutionGraph<'a> {
-    fn from(resolution: &'a ResolutionGraph) -> Self {
-        Self::new(resolution, false, true, AnnotationStyle::default())
-    }
-}
+            if let Some(conflict) = distributions.next() {
+                // There are multiple installed distributions for the same package.
+                diagnostics.push(Diagnostic::DuplicatePackage {
+                    package: package.clone(),
+                    paths: std::iter::once(distribution.path().to_owned())
+                        .chain(std::iter::once(conflict.path().to_owned()))
+                        .chain(distributions.map(|dist| dist.path().to_owned()))
+                        .collect(),
+                });
+                continue;
+            }
 
-impl<'a> DisplayResolutionGraph<'a> {
-    /// Create a new [`DisplayResolutionGraph`] for the given graph.
-    pub fn new(
-        underlying: &'a ResolutionGraph,
-        show_hashes: bool,
-        include_annotations: bool,
-        annotation_style: AnnotationStyle,
-    ) -> DisplayResolutionGraph<'a> {
-        Self {
-            resolution: underlying,
-            show_hashes,
-            include_annotations,
-            annotation_style,
-        }
-    }
-}
+            for index in indexes {
+                let Some(distribution) = &self.distributions[*index] else {
+                    continue;
+                };
 
-/// Write the graph in the `{name}=={version}` format of requirements.txt that pip uses.
-impl std::fmt::Display for DisplayResolutionGraph<'_> {
-    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-        #[derive(Debug)]
-        enum Node<'a> {
-            /// A node linked to an editable distribution.
-            Editable(&'a PackageName, &'a LocalEditable),
-            /// A node linked to a non-editable distribution.
-            Distribution(&'a PackageName, &'a Dist),
-        }
+                // Determine the dependencies for the given package.
+                let Ok(metadata) = distribution.metadata() else {
+                    diagnostics.push(Diagnostic::IncompletePackage {
+                        package: package.clone(),
+                        path: distribution.path().to_owned(),
+                    });
+                    continue;
+                };
 
-        #[derive(Debug, PartialEq, Eq, PartialOrd, Ord)]
-        enum NodeKey<'a> {
-            /// A node linked to an editable distribution, sorted by verbatim representation.
-            Editable(Cow<'a, str>),
-            /// A node linked to a non-editable distribution, sorted by package name.
-            Distribution(&'a PackageName),
-        }
+                // Verify that the package is compatible with the current Python version.
+                if let Some(requires_python) = metadata.requires_python.as_ref() {
+                    if !requires_python.contains(self.venv.interpreter().python_version()) {
+                        diagnostics.push(Diagnostic::IncompatiblePythonVersion {
+                            package: package.clone(),
+                            version: self.venv.interpreter().python_version().clone(),
+                            requires_python: requires_python.clone(),
+                        });
+                    }
+                }
+
+                // Verify that the dependencies are installed.
+                for dependency in &metadata.requires_dist {
+                    if !dependency.evaluate_markers(self.venv.interpreter().markers(), &[]) {
+                        continue;
+                    }
 
-        impl<'a> Node<'a> {
-            /// Return the name of the package.
-            fn name(&self) -> &'a PackageName {
-                match self {
-                    Node::Editable(name, _) => name,
-                    Node::Distribution(name, _) => name,
+                    let installed = self.get_packages(&dependency.name);
+                    match installed.as_slice() {
+                        [] => {
+                            // No version installed.
+                            diagnostics.push(Diagnostic::MissingDependency {
+                                package: package.clone(),
+                                requirement: dependency.clone(),
+                            });
+                        }
+                        [installed] => {
+                            match &dependency.version_or_url {
+                                None | Some(pep508_rs::VersionOrUrl::Url(_)) => {
+                                    // Nothing to do (accept any installed version).
+                                }
+                                Some(pep508_rs::VersionOrUrl::VersionSpecifier(
+                                    version_specifier,
+                                )) => {
+                                    // The installed version doesn't satisfy the requirement.
+                                    if !version_specifier.contains(installed.version()) {
+                                        diagnostics.push(Diagnostic::IncompatibleDependency {
+                                            package: package.clone(),
+                                            version: installed.version().clone(),
+                                            requirement: dependency.clone(),
+                                        });
+                                    }
+                                }
+                            }
+                        }
+                        _ => {
+                            // There are multiple installed distributions for the same package.
+                        }
+                    }
                 }
             }
+        }
 
-            /// Return a comparable key for the node.
-            fn key(&self) -> NodeKey<'a> {
-                match self {
-                    Node::Editable(_, editable) => NodeKey::Editable(editable.verbatim()),
-                    Node::Distribution(name, _) => NodeKey::Distribution(name),
+        Ok(diagnostics)
+    }
+
+    /// Returns if the installed packages satisfy the given requirements.
+    pub fn satisfies(
+        &self,
+        requirements: &[UnresolvedRequirementSpecification],
+        editables: &[EditableRequirement],
+        constraints: &[Requirement],
+    ) -> Result<SatisfiesResult> {
+        let mut stack = Vec::with_capacity(requirements.len());
+        let mut seen =
+            FxHashSet::with_capacity_and_hasher(requirements.len(), BuildHasherDefault::default());
+
+        // Add the direct requirements to the queue.
+        for entry in requirements {
+            if entry
+                .requirement
+                .evaluate_markers(Some(self.venv.interpreter().markers()), &[])
+            {
+                if seen.insert(entry.clone()) {
+                    stack.push(entry.clone());
                 }
             }
         }
 
-        // Collect all packages.
-        let mut nodes = self
-            .resolution
-            .petgraph
-            .node_indices()
-            .map(|index| {
-                let dist = &self.resolution.petgraph[index];
-                let name = dist.name();
-                let node = if let Some((editable, _)) = self.resolution.editables.get(name) {
-                    Node::Editable(name, editable)
-                } else {
-                    Node::Distribution(name, dist)
-                };
-                (index, node)
-            })
-            .collect::<Vec<_>>();
-
-        // Sort the nodes by name, but with editable packages first.
-        nodes.sort_unstable_by_key(|(index, node)| (node.key(), *index));
+        // Verify that all editable requirements are met.
+        for requirement in editables {
+            let installed = self.get_editables(requirement.raw());
+            match installed.as_slice() {
+                [] => {
+                    // The package isn't installed.
+                    return Ok(SatisfiesResult::Unsatisfied(requirement.to_string()));
+                }
+                [distribution] => {
+                    // Is the editable out-of-date?
+                    if !ArchiveTimestamp::up_to_date_with(
+                        &requirement.path,
+                        ArchiveTarget::Install(distribution),
+                    )? {
+                        return Ok(SatisfiesResult::Unsatisfied(requirement.to_string()));
+                    }
 
-        // Print out the dependency graph.
-        for (index, node) in nodes {
-            // Display the node itself.
-            let mut line = match node {
-                Node::Distribution(_, dist) => format!("{}", dist.verbatim()),
-                Node::Editable(_, editable) => format!("-e {}", editable.verbatim()),
-            };
+                    // Does the editable have dynamic metadata?
+                    if is_dynamic(&requirement.path) {
+                        return Ok(SatisfiesResult::Unsatisfied(requirement.to_string()));
+                    }
 
-            // Display the distribution hashes, if any.
-            let mut has_hashes = false;
-            if self.show_hashes {
-                if let Some(hashes) = self
-                    .resolution
-                    .hashes
-                    .get(node.name())
-                    .filter(|hashes| !hashes.is_empty())
-                {
-                    for hash in hashes {
-                        if let Some(hash) = hash.to_string() {
-                            has_hashes = true;
-                            line.push_str(" \\\n");
-                            line.push_str("    --hash=");
-                            line.push_str(&hash);
+                    // Recurse into the dependencies.
+                    let metadata = distribution
+                        .metadata()
+                        .with_context(|| format!("Failed to read metadata for: {distribution}"))?;
+
+                    // Add the dependencies to the queue.
+                    for dependency in metadata.requires_dist {
+                        if dependency.evaluate_markers(
+                            self.venv.interpreter().markers(),
+                            &requirement.extras,
+                        ) {
+                            let dependency = UnresolvedRequirementSpecification {
+                                requirement: UnresolvedRequirement::Named(
+                                    Requirement::from_pep508(dependency)?,
+                                ),
+                                hashes: vec![],
+                            };
+                            if seen.insert(dependency.clone()) {
+                                stack.push(dependency);
+                            }
                         }
                     }
                 }
+                _ => {
+                    // There are multiple installed distributions for the same package.
+                    return Ok(SatisfiesResult::Unsatisfied(requirement.to_string()));
+                }
             }
+        }
 
-            // Determine the annotation comment and separator (between comment and requirement).
-            let mut annotation = None;
-
-            if self.include_annotations {
-                // Display all dependencies.
-                let mut edges = self
-                    .resolution
-                    .petgraph
-                    .edges_directed(index, Direction::Incoming)
-                    .map(|edge| &self.resolution.petgraph[edge.source()])
-                    .collect::<Vec<_>>();
-                edges.sort_unstable_by_key(|package| package.name());
-
-                match self.annotation_style {
-                    AnnotationStyle::Line => {
-                        if !edges.is_empty() {
-                            let separator = if has_hashes { "\n    " } else { "  " };
-                            let deps = edges
-                                .into_iter()
-                                .map(|dependency| dependency.name().to_string())
-                                .collect::<Vec<_>>()
-                                .join(", ");
-                            let comment = format!("# via {deps}").green().to_string();
-                            annotation = Some((separator, comment));
+        // Verify that all non-editable requirements are met.
+        while let Some(entry) = stack.pop() {
+            let installed = match &entry.requirement {
+                UnresolvedRequirement::Named(requirement) => self.get_packages(&requirement.name),
+                UnresolvedRequirement::Unnamed(requirement) => self.get_urls(requirement.url.raw()),
+            };
+            match installed.as_slice() {
+                [] => {
+                    // The package isn't installed.
+                    return Ok(SatisfiesResult::Unsatisfied(entry.requirement.to_string()));
+                }
+                [distribution] => {
+                    match RequirementSatisfaction::check(
+                        distribution,
+                        entry.requirement.source()?.as_ref(),
+                    )? {
+                        RequirementSatisfaction::Mismatch | RequirementSatisfaction::OutOfDate => {
+                            return Ok(SatisfiesResult::Unsatisfied(entry.requirement.to_string()))
                         }
+                        RequirementSatisfaction::Satisfied => {}
                     }
-                    AnnotationStyle::Split => match edges.as_slice() {
-                        [] => {}
-                        [edge] => {
-                            let separator = "\n";
-                            let comment = format!("    # via {}", edge.name()).green().to_string();
-                            annotation = Some((separator, comment));
+                    // Validate that the installed version satisfies the constraints.
+                    for constraint in constraints {
+                        match RequirementSatisfaction::check(distribution, &constraint.source)? {
+                            RequirementSatisfaction::Mismatch
+                            | RequirementSatisfaction::OutOfDate => {
+                                return Ok(SatisfiesResult::Unsatisfied(
+                                    entry.requirement.to_string(),
+                                ))
+                            }
+                            RequirementSatisfaction::Satisfied => {}
                         }
-                        edges => {
-                            let separator = "\n";
-                            let deps = edges
-                                .iter()
-                                .map(|dependency| format!("    #   {}", dependency.name()))
-                                .collect::<Vec<_>>()
-                                .join("\n");
-                            let comment = format!("    # via\n{deps}").green().to_string();
-                            annotation = Some((separator, comment));
+                    }
+
+                    // Recurse into the dependencies.
+                    let metadata = distribution
+                        .metadata()
+                        .with_context(|| format!("Failed to read metadata for: {distribution}"))?;
+
+                    // Add the dependencies to the queue.
+                    for dependency in metadata.requires_dist {
+                        if dependency.evaluate_markers(
+                            self.venv.interpreter().markers(),
+                            entry.requirement.extras(),
+                        ) {
+                            let dependency = UnresolvedRequirementSpecification {
+                                requirement: UnresolvedRequirement::Named(
+                                    Requirement::from_pep508(dependency)?,
+                                ),
+                                hashes: vec![],
+                            };
+                            if seen.insert(dependency.clone()) {
+                                stack.push(dependency);
+                            }
                         }
-                    },
+                    }
+                }
+                _ => {
+                    // There are multiple installed distributions for the same package.
+                    return Ok(SatisfiesResult::Unsatisfied(entry.requirement.to_string()));
                 }
-            }
-
-            if let Some((separator, comment)) = annotation {
-                // Assemble the line with the annotations and remove trailing whitespaces.
-                for line in format!("{line:24}{separator}{comment}").lines() {
-                    let line = line.trim_end();
-                    writeln!(f, "{line}")?;
-                }
-            } else {
-                // Write the line as is.
-                writeln!(f, "{line}")?;
             }
         }
 
-        Ok(())
+        Ok(SatisfiesResult::Fresh {
+            recursive_requirements: seen,
+        })
     }
 }
 
-impl From<ResolutionGraph> for distribution_types::Resolution {
-    fn from(graph: ResolutionGraph) -> Self {
-        Self::new(
-            graph
-                .petgraph
-                .node_indices()
-                .map(|node| {
-                    (
-                        graph.petgraph[node].name().clone(),
-                        graph.petgraph[node].clone(),
-                    )
-                })
-                .collect(),
-        )
+/// We check if all requirements are already satisfied, recursing through the requirements tree.
+#[derive(Debug)]
+pub enum SatisfiesResult {
+    /// All requirements are recursively satisfied.
+    Fresh {
+        /// The flattened set (transitive closure) of all requirements checked.
+        recursive_requirements: FxHashSet<UnresolvedRequirementSpecification>,
+    },
+    /// We found an unsatisfied requirement. Since we exit early, we only know about the first
+    /// unsatisfied requirement.
+    Unsatisfied(String),
+}
+
+impl IntoIterator for SitePackages {
+    type Item = InstalledDist;
+    type IntoIter = Flatten<std::vec::IntoIter<Option<InstalledDist>>>;
+
+    fn into_iter(self) -> Self::IntoIter {
+        self.distributions.into_iter().flatten()
     }
 }
 
 #[derive(Debug)]
 pub enum Diagnostic {
-    MissingExtra {
-        /// The distribution that was requested with an non-existent extra. For example,
-        /// `black==23.10.0`.
-        dist: Dist,
-        /// The extra that was requested. For example, `colorama` in `black[colorama]`.
-        extra: ExtraName,
+    IncompletePackage {
+        /// The package that is missing metadata.
+        package: PackageName,
+        /// The path to the package.
+        path: PathBuf,
+    },
+    IncompatiblePythonVersion {
+        /// The package that requires a different version of Python.
+        package: PackageName,
+        /// The version of Python that is installed.
+        version: Version,
+        /// The version of Python that is required.
+        requires_python: VersionSpecifiers,
+    },
+    MissingDependency {
+        /// The package that is missing a dependency.
+        package: PackageName,
+        /// The dependency that is missing.
+        requirement: pep508_rs::Requirement,
+    },
+    IncompatibleDependency {
+        /// The package that has an incompatible dependency.
+        package: PackageName,
+        /// The version of the package that is installed.
+        version: Version,
+        /// The dependency that is incompatible.
+        requirement: pep508_rs::Requirement,
+    },
+    DuplicatePackage {
+        /// The package that has multiple installed distributions.
+        package: PackageName,
+        /// The installed versions of the package.
+        paths: Vec<PathBuf>,
     },
 }
 
 impl Diagnostic {
     /// Convert the diagnostic into a user-facing message.
     pub fn message(&self) -> String {
         match self {
-            Self::MissingExtra { dist, extra } => {
-                format!("The package `{dist}` does not have an extra named `{extra}`.")
+            Self::IncompletePackage { package, path } => format!(
+                "The package `{package}` is broken or incomplete (unable to read `METADATA`). Consider recreating the virtualenv, or removing the package directory at: {}.", path.display(),
+            ),
+            Self::IncompatiblePythonVersion {
+                package,
+                version,
+                requires_python,
+            } => format!(
+                "The package `{package}` requires Python {requires_python}, but `{version}` is installed."
+            ),
+            Self::MissingDependency {
+                package,
+                requirement,
+            } => {
+                format!("The package `{package}` requires `{requirement}`, but it's not installed.")
+            }
+            Self::IncompatibleDependency {
+                package,
+                version,
+                requirement,
+            } => format!(
+                "The package `{package}` requires `{requirement}`, but `{version}` is installed."
+            ),
+            Self::DuplicatePackage { package, paths } => {
+                let mut paths = paths.clone();
+                paths.sort();
+                format!(
+                    "The package `{package}` has multiple installed distributions:{}",
+                    paths.iter().fold(String::new(), |acc, path| acc + &format!("\n  - {}", path.display()))
+                )
             }
         }
     }
 
     /// Returns `true` if the [`PackageName`] is involved in this diagnostic.
     pub fn includes(&self, name: &PackageName) -> bool {
         match self {
-            Self::MissingExtra { dist, .. } => name == dist.name(),
+            Self::IncompletePackage { package, .. } => name == package,
+            Self::IncompatiblePythonVersion { package, .. } => name == package,
+            Self::MissingDependency { package, .. } => name == package,
+            Self::IncompatibleDependency {
+                package,
+                requirement,
+                ..
+            } => name == package || &requirement.name == name,
+            Self::DuplicatePackage { package, .. } => name == package,
         }
     }
 }
+
+impl InstalledPackagesProvider for SitePackages {
+    fn iter(&self) -> impl Iterator<Item = &InstalledDist> {
+        self.iter()
+    }
+
+    fn get_packages(&self, name: &PackageName) -> Vec<&InstalledDist> {
+        self.get_packages(name)
+    }
+
+    fn get_editables(&self, url: &Url) -> Vec<&InstalledDist> {
+        self.get_editables(url)
+    }
+}
```

### Comparing `uv-0.1.9/crates/uv-resolver/src/resolution_mode.rs` & `uv-0.2.0/crates/uv-resolver/src/resolution_mode.rs`

 * *Files 17% similar despite different names*

```diff
@@ -1,14 +1,18 @@
 use rustc_hash::FxHashSet;
 
-use pep508_rs::Requirement;
+use pep508_rs::MarkerEnvironment;
 use uv_normalize::PackageName;
 
-#[derive(Debug, Default, Clone, Copy, PartialEq, Eq)]
+use crate::{DependencyMode, Manifest};
+
+#[derive(Debug, Default, Clone, Copy, PartialEq, Eq, serde::Deserialize)]
+#[serde(deny_unknown_fields, rename_all = "kebab-case")]
 #[cfg_attr(feature = "clap", derive(clap::ValueEnum))]
+#[cfg_attr(feature = "schemars", derive(schemars::JsonSchema))]
 pub enum ResolutionMode {
     /// Resolve the highest compatible version of each package.
     #[default]
     Highest,
     /// Resolve the lowest compatible version of each package.
     Lowest,
     /// Resolve the lowest compatible version of any direct dependencies, and the highest
@@ -26,20 +30,25 @@
     Lowest,
     /// Resolve the lowest compatible version of any direct dependencies, and the highest
     /// compatible version of any transitive dependencies.
     LowestDirect(FxHashSet<PackageName>),
 }
 
 impl ResolutionStrategy {
-    pub(crate) fn from_mode(mode: ResolutionMode, direct_dependencies: &[Requirement]) -> Self {
+    pub(crate) fn from_mode(
+        mode: ResolutionMode,
+        manifest: &Manifest,
+        markers: Option<&MarkerEnvironment>,
+        dependencies: DependencyMode,
+    ) -> Self {
         match mode {
             ResolutionMode::Highest => Self::Highest,
             ResolutionMode::Lowest => Self::Lowest,
             ResolutionMode::LowestDirect => Self::LowestDirect(
-                direct_dependencies
-                    .iter()
-                    .map(|requirement| requirement.name.clone())
+                manifest
+                    .user_requirements(markers, dependencies)
+                    .cloned()
                     .collect(),
             ),
         }
     }
 }
```

### Comparing `uv-0.1.9/crates/uv-resolver/src/resolver/mod.rs` & `uv-0.2.0/crates/uv-resolver/src/resolver/mod.rs`

 * *Files 22% similar despite different names*

```diff
@@ -1,1013 +1,1360 @@
 //! Given a set of requirements, find a set of compatible packages.
 
 use std::borrow::Cow;
 use std::fmt::{Display, Formatter};
+use std::ops::Deref;
 use std::sync::Arc;
+use std::thread;
 
 use anyhow::Result;
-use dashmap::{DashMap, DashSet};
-use futures::{FutureExt, StreamExt};
+use dashmap::DashMap;
+use futures::{FutureExt, StreamExt, TryFutureExt};
 use itertools::Itertools;
 use pubgrub::error::PubGrubError;
 use pubgrub::range::Range;
 use pubgrub::solver::{Incompatibility, State};
 use rustc_hash::{FxHashMap, FxHashSet};
-use tokio::select;
+use tokio::sync::mpsc::{self, Receiver, Sender};
+use tokio::sync::oneshot;
 use tokio_stream::wrappers::ReceiverStream;
-use tracing::{debug, info_span, instrument, trace, warn, Instrument};
-use url::Url;
+use tracing::{debug, enabled, instrument, trace, warn, Level};
 
-use distribution_filename::WheelFilename;
 use distribution_types::{
-    BuiltDist, Dist, DistributionMetadata, IncompatibleWheel, Name, RemoteSource, SourceDist,
-    VersionOrUrl,
+    BuiltDist, Dist, DistributionMetadata, IncompatibleDist, IncompatibleSource, IncompatibleWheel,
+    InstalledDist, RemoteSource, Requirement, ResolvedDist, ResolvedDistRef, SourceDist,
+    VersionOrUrlRef,
 };
-use pep440_rs::{Version, VersionSpecifiers, MIN_VERSION};
-use pep508_rs::{MarkerEnvironment, Requirement};
-use platform_tags::{IncompatibleTag, Tags};
-use pypi_types::{Metadata21, Yanked};
+pub(crate) use locals::Locals;
+use pep440_rs::{Version, MIN_VERSION};
+use pep508_rs::MarkerEnvironment;
+use platform_tags::Tags;
+use pypi_types::Metadata23;
 pub(crate) use urls::Urls;
-use uv_client::{FlatIndex, RegistryClient};
-use uv_distribution::DistributionDatabase;
-use uv_interpreter::Interpreter;
+use uv_configuration::{Constraints, Overrides};
+use uv_distribution::{ArchiveMetadata, DistributionDatabase};
 use uv_normalize::PackageName;
-use uv_traits::BuildContext;
+use uv_types::{BuildContext, HashStrategy, InstalledPackagesProvider};
 
 use crate::candidate_selector::{CandidateDist, CandidateSelector};
-use crate::constraints::Constraints;
+use crate::dependency_provider::UvDependencyProvider;
 use crate::editables::Editables;
 use crate::error::ResolveError;
 use crate::manifest::Manifest;
-use crate::overrides::Overrides;
 use crate::pins::FilePins;
+use crate::preferences::Preferences;
 use crate::pubgrub::{
-    PubGrubDependencies, PubGrubDistribution, PubGrubPackage, PubGrubPriorities, PubGrubPython,
-    PubGrubSpecifier,
+    PubGrubDependencies, PubGrubDistribution, PubGrubPackage, PubGrubPackageInner,
+    PubGrubPriorities, PubGrubPython, PubGrubRequirement, PubGrubSpecifier,
 };
 use crate::python_requirement::PythonRequirement;
 use crate::resolution::ResolutionGraph;
+use crate::resolver::batch_prefetch::BatchPrefetcher;
+pub(crate) use crate::resolver::index::FxOnceMap;
 pub use crate::resolver::index::InMemoryIndex;
 pub use crate::resolver::provider::{
-    DefaultResolverProvider, PackageVersionsResult, ResolverProvider, VersionsResponse,
-    WheelMetadataResult,
+    DefaultResolverProvider, MetadataResponse, PackageVersionsResult, ResolverProvider,
+    VersionsResponse, WheelMetadataResult,
 };
 use crate::resolver::reporter::Facade;
 pub use crate::resolver::reporter::{BuildId, Reporter};
 use crate::yanks::AllowedYanks;
-use crate::{DependencyMode, Options};
+use crate::{DependencyMode, Exclusions, FlatIndex, Options};
 
+mod batch_prefetch;
 mod index;
+mod locals;
 mod provider;
 mod reporter;
 mod urls;
 
-/// The package version is unavailable and cannot be used
-/// Unlike [`PackageUnavailable`] this applies to a single version of the package
-#[derive(Debug, Clone)]
+/// The reason why a package or a version cannot be used.
+#[derive(Debug, Clone, Eq, PartialEq)]
+pub(crate) enum UnavailableReason {
+    /// The entire package cannot be used.
+    Package(UnavailablePackage),
+    /// A single version cannot be used.
+    Version(UnavailableVersion),
+}
+
+impl Display for UnavailableReason {
+    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
+        match self {
+            Self::Version(version) => Display::fmt(version, f),
+            Self::Package(package) => Display::fmt(package, f),
+        }
+    }
+}
+
+/// The package version is unavailable and cannot be used. Unlike [`PackageUnavailable`], this
+/// applies to a single version of the package.
+///
+/// Most variant are from [`MetadataResponse`] without the error source (since we don't format
+/// the source).
+#[derive(Debug, Clone, Eq, PartialEq)]
 pub(crate) enum UnavailableVersion {
-    /// Version is incompatible due to the `Requires-Python` version specifiers for that package.
-    RequiresPython(VersionSpecifiers),
-    /// Version is incompatible because it is yanked
-    Yanked(Yanked),
     /// Version is incompatible because it has no usable distributions
-    NoDistributions(Option<IncompatibleWheel>),
+    IncompatibleDist(IncompatibleDist),
+    /// The wheel metadata was found, but could not be parsed.
+    InvalidMetadata,
+    /// The wheel metadata was found, but the metadata was inconsistent.
+    InconsistentMetadata,
+    /// The wheel has an invalid structure.
+    InvalidStructure,
+    /// The wheel metadata was not found in the cache and the network is not available.
+    Offline,
+    /// Forward any kind of resolver error.
+    ResolverError(String),
 }
 
-/// The package is unavailable and cannot be used
-#[derive(Debug, Clone)]
+impl Display for UnavailableVersion {
+    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
+        match self {
+            UnavailableVersion::IncompatibleDist(invalid_dist) => Display::fmt(invalid_dist, f),
+            UnavailableVersion::InvalidMetadata => f.write_str("has invalid metadata"),
+            UnavailableVersion::InconsistentMetadata => f.write_str("has inconsistent metadata"),
+            UnavailableVersion::InvalidStructure => f.write_str("has an invalid package format"),
+            UnavailableVersion::Offline => f.write_str(
+                "network connectivity is disabled, but the metadata wasn't found in the cache",
+            ),
+            UnavailableVersion::ResolverError(err) => f.write_str(err),
+        }
+    }
+}
+
+/// The package is unavailable and cannot be used.
+#[derive(Debug, Clone, Eq, PartialEq)]
 pub(crate) enum UnavailablePackage {
-    /// Index lookups were disabled (i.e., `--no-index`) and the package was not found in a flat index (i.e. from `--find-links`)
+    /// Index lookups were disabled (i.e., `--no-index`) and the package was not found in a flat index (i.e. from `--find-links`).
     NoIndex,
     /// Network requests were disabled (i.e., `--offline`), and the package was not found in the cache.
     Offline,
-    /// The package was not found in the registry
+    /// The package was not found in the registry.
     NotFound,
+    /// The package metadata was found, but could not be parsed.
+    InvalidMetadata(String),
+    /// The package has an invalid structure.
+    InvalidStructure(String),
+}
+
+impl UnavailablePackage {
+    pub(crate) fn as_str(&self) -> &'static str {
+        match self {
+            UnavailablePackage::NoIndex => "was not found in the provided package locations",
+            UnavailablePackage::Offline => "was not found in the cache",
+            UnavailablePackage::NotFound => "was not found in the package registry",
+            UnavailablePackage::InvalidMetadata(_) => "has invalid metadata",
+            UnavailablePackage::InvalidStructure(_) => "has an invalid package format",
+        }
+    }
+}
+
+impl Display for UnavailablePackage {
+    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
+        f.write_str(self.as_str())
+    }
+}
+
+/// The package is unavailable at specific versions.
+#[derive(Debug, Clone)]
+pub(crate) enum IncompletePackage {
+    /// Network requests were disabled (i.e., `--offline`), and the wheel metadata was not found in the cache.
+    Offline,
+    /// The wheel metadata was found, but could not be parsed.
+    InvalidMetadata(String),
+    /// The wheel metadata was found, but the metadata was inconsistent.
+    InconsistentMetadata(String),
+    /// The wheel has an invalid structure.
+    InvalidStructure(String),
 }
 
 enum ResolverVersion {
     /// A usable version
     Available(Version),
     /// A version that is not usable for some reason
     Unavailable(Version, UnavailableVersion),
 }
 
-pub struct Resolver<'a, Provider: ResolverProvider> {
+pub struct Resolver<Provider: ResolverProvider, InstalledPackages: InstalledPackagesProvider> {
+    state: ResolverState<InstalledPackages>,
+    provider: Provider,
+}
+
+/// State that is shared between the prefetcher and the PubGrub solver during
+/// resolution.
+struct ResolverState<InstalledPackages: InstalledPackagesProvider> {
     project: Option<PackageName>,
     requirements: Vec<Requirement>,
     constraints: Constraints,
     overrides: Overrides,
+    preferences: Preferences,
+    exclusions: Exclusions,
     editables: Editables,
-    allowed_yanks: AllowedYanks,
     urls: Urls,
+    locals: Locals,
     dependency_mode: DependencyMode,
-    markers: &'a MarkerEnvironment,
+    hasher: HashStrategy,
+    /// When not set, the resolver is in "universal" mode.
+    markers: Option<MarkerEnvironment>,
     python_requirement: PythonRequirement,
     selector: CandidateSelector,
-    index: &'a InMemoryIndex,
-    /// Incompatibilities for packages that are entirely unavailable
+    index: InMemoryIndex,
+    installed_packages: InstalledPackages,
+    /// Incompatibilities for packages that are entirely unavailable.
     unavailable_packages: DashMap<PackageName, UnavailablePackage>,
-    /// The set of all registry-based packages visited during resolution.
-    visited: DashSet<PackageName>,
+    /// Incompatibilities for packages that are unavailable at specific versions.
+    incomplete_packages: DashMap<PackageName, DashMap<Version, IncompletePackage>>,
     reporter: Option<Arc<dyn Reporter>>,
-    provider: Provider,
 }
 
-impl<'a, Context: BuildContext + Send + Sync> Resolver<'a, DefaultResolverProvider<'a, Context>> {
+impl<'a, Context: BuildContext, InstalledPackages: InstalledPackagesProvider>
+    Resolver<DefaultResolverProvider<'a, Context>, InstalledPackages>
+{
     /// Initialize a new resolver using the default backend doing real requests.
     ///
     /// Reads the flat index entries.
+    ///
+    /// # Marker environment
+    ///
+    /// The marker environment is optional.
+    ///
+    /// When a marker environment is not provided, the resolver is said to be
+    /// in "universal" mode. When in universal mode, the resolution produced
+    /// may contain multiple versions of the same package. And thus, in order
+    /// to use the resulting resolution, there must be a "universal"-aware
+    /// reader of the resolution that knows to exclude distributions that can't
+    /// be used in the current environment.
+    ///
+    /// When a marker environment is provided, the resolver is in
+    /// "non-universal" mode, which corresponds to standard `pip` behavior that
+    /// works only for a specific marker environment.
     #[allow(clippy::too_many_arguments)]
     pub fn new(
         manifest: Manifest,
         options: Options,
-        markers: &'a MarkerEnvironment,
-        interpreter: &'a Interpreter,
+        python_requirement: &'a PythonRequirement,
+        markers: Option<&'a MarkerEnvironment>,
         tags: &'a Tags,
-        client: &'a RegistryClient,
         flat_index: &'a FlatIndex,
         index: &'a InMemoryIndex,
+        hasher: &'a HashStrategy,
         build_context: &'a Context,
+        installed_packages: InstalledPackages,
+        database: DistributionDatabase<'a, Context>,
     ) -> Result<Self, ResolveError> {
         let provider = DefaultResolverProvider::new(
-            client,
-            DistributionDatabase::new(build_context.cache(), tags, client, build_context),
+            database,
             flat_index,
             tags,
-            PythonRequirement::new(interpreter, markers),
+            python_requirement.clone(),
+            AllowedYanks::from_manifest(&manifest, markers, options.dependency_mode),
+            hasher,
             options.exclude_newer,
             build_context.no_binary(),
+            build_context.no_build(),
         );
+
         Self::new_custom_io(
             manifest,
             options,
+            hasher,
             markers,
-            PythonRequirement::new(interpreter, markers),
+            python_requirement,
             index,
             provider,
+            installed_packages,
         )
     }
 }
 
-impl<'a, Provider: ResolverProvider> Resolver<'a, Provider> {
+impl<Provider: ResolverProvider, InstalledPackages: InstalledPackagesProvider>
+    Resolver<Provider, InstalledPackages>
+{
     /// Initialize a new resolver using a user provided backend.
+    #[allow(clippy::too_many_arguments)]
     pub fn new_custom_io(
         manifest: Manifest,
         options: Options,
-        markers: &'a MarkerEnvironment,
-        python_requirement: PythonRequirement,
-        index: &'a InMemoryIndex,
+        hasher: &HashStrategy,
+        markers: Option<&MarkerEnvironment>,
+        python_requirement: &PythonRequirement,
+        index: &InMemoryIndex,
         provider: Provider,
+        installed_packages: InstalledPackages,
     ) -> Result<Self, ResolveError> {
-        let selector = CandidateSelector::for_resolution(&manifest, options);
-
-        // Determine the allowed yanked package versions
-        let allowed_yanks = manifest
-            .requirements
-            .iter()
-            .chain(manifest.constraints.iter())
-            .collect();
-
-        Ok(Self {
-            index,
+        let state = ResolverState {
+            index: index.clone(),
             unavailable_packages: DashMap::default(),
-            visited: DashSet::default(),
-            selector,
-            allowed_yanks,
+            incomplete_packages: DashMap::default(),
+            selector: CandidateSelector::for_resolution(options, &manifest, markers),
             dependency_mode: options.dependency_mode,
-            urls: Urls::from_manifest(&manifest, markers)?,
+            urls: Urls::from_manifest(&manifest, markers, options.dependency_mode)?,
+            locals: Locals::from_manifest(&manifest, markers, options.dependency_mode),
             project: manifest.project,
             requirements: manifest.requirements,
-            constraints: Constraints::from_requirements(manifest.constraints),
-            overrides: Overrides::from_requirements(manifest.overrides),
+            constraints: manifest.constraints,
+            overrides: manifest.overrides,
+            preferences: Preferences::from_iter(manifest.preferences, markers),
+            exclusions: manifest.exclusions,
             editables: Editables::from_requirements(manifest.editables),
-            markers,
-            python_requirement,
+            hasher: hasher.clone(),
+            markers: markers.cloned(),
+            python_requirement: python_requirement.clone(),
             reporter: None,
-            provider,
-        })
+            installed_packages,
+        };
+        Ok(Self { state, provider })
     }
 
     /// Set the [`Reporter`] to use for this installer.
     #[must_use]
     pub fn with_reporter(self, reporter: impl Reporter + 'static) -> Self {
         let reporter = Arc::new(reporter);
+
         Self {
-            reporter: Some(reporter.clone()),
+            state: ResolverState {
+                reporter: Some(reporter.clone()),
+                ..self.state
+            },
             provider: self.provider.with_reporter(Facade { reporter }),
-            ..self
         }
     }
 
     /// Resolve a set of requirements into a set of pinned versions.
     pub async fn resolve(self) -> Result<ResolutionGraph, ResolveError> {
+        let state = Arc::new(self.state);
+        let provider = Arc::new(self.provider);
+
         // A channel to fetch package metadata (e.g., given `flask`, fetch all versions) and version
         // metadata (e.g., given `flask==1.0.0`, fetch the metadata for that version).
-        // Channel size is set to the same size as the task buffer for simplicity.
-        let (request_sink, request_stream) = tokio::sync::mpsc::channel(50);
+        // Channel size is set large to accommodate batch prefetching.
+        let (request_sink, request_stream) = mpsc::channel(300);
 
         // Run the fetcher.
-        let requests_fut = self.fetch(request_stream).fuse();
-
-        // Run the solver.
-        let resolve_fut = self.solve(&request_sink).fuse();
+        let requests_fut = state
+            .clone()
+            .fetch(provider.clone(), request_stream)
+            .map_err(|err| (err, FxHashSet::default()))
+            .fuse();
+
+        // Spawn the PubGrub solver on a dedicated thread.
+        let solver = state.clone();
+        let (tx, rx) = oneshot::channel();
+        thread::Builder::new()
+            .name("uv-resolver".into())
+            .spawn(move || {
+                let result = solver.solve(request_sink);
+                tx.send(result).unwrap();
+            })
+            .unwrap();
+
+        let resolve_fut = async move {
+            rx.await
+                .map_err(|_| (ResolveError::ChannelClosed, FxHashSet::default()))
+                .and_then(|result| result)
+        };
 
-        let resolution = select! {
-            result = requests_fut => {
-                result?;
-                return Err(ResolveError::ChannelClosed);
-            }
-            resolution = resolve_fut => {
-                resolution.map_err(|err| {
-                    // Add version information to improve unsat error messages.
-                    if let ResolveError::NoSolution(err) = err {
-                        ResolveError::NoSolution(
-                            err
-                            .with_available_versions(&self.python_requirement, &self.visited, &self.index.packages)
-                            .with_selector(self.selector.clone())
-                            .with_python_requirement(&self.python_requirement)
-                            .with_index_locations(self.provider.index_locations())
-                            .with_unavailable_packages(&self.unavailable_packages)
+        // Wait for both to complete.
+        match tokio::try_join!(requests_fut, resolve_fut) {
+            Ok(((), resolution)) => {
+                state.on_complete();
+                Ok(resolution)
+            }
+            Err((err, visited)) => {
+                // Add version information to improve unsat error messages.
+                Err(if let ResolveError::NoSolution(err) = err {
+                    ResolveError::NoSolution(
+                        err.with_available_versions(
+                            &state.python_requirement,
+                            &visited,
+                            state.index.packages(),
                         )
-                    } else {
-                        err
-                    }
-                })?
+                        .with_selector(state.selector.clone())
+                        .with_python_requirement(&state.python_requirement)
+                        .with_index_locations(provider.index_locations())
+                        .with_unavailable_packages(&state.unavailable_packages)
+                        .with_incomplete_packages(&state.incomplete_packages),
+                    )
+                } else {
+                    err
+                })
             }
-        };
-
-        self.on_complete();
+        }
+    }
+}
 
-        Ok(resolution)
+impl<InstalledPackages: InstalledPackagesProvider> ResolverState<InstalledPackages> {
+    #[instrument(skip_all)]
+    fn solve(
+        self: Arc<Self>,
+        request_sink: Sender<Request>,
+    ) -> Result<ResolutionGraph, (ResolveError, FxHashSet<PackageName>)> {
+        let mut visited = FxHashSet::default();
+        self.solve_tracked(&mut visited, request_sink)
+            .map_err(|err| (err, visited))
     }
 
-    /// Run the `PubGrub` solver.
+    /// Run the PubGrub solver, updating the `visited` set for each package visited during
+    /// resolution.
     #[instrument(skip_all)]
-    async fn solve(
-        &self,
-        request_sink: &tokio::sync::mpsc::Sender<Request>,
+    fn solve_tracked(
+        self: Arc<Self>,
+        visited: &mut FxHashSet<PackageName>,
+        request_sink: Sender<Request>,
     ) -> Result<ResolutionGraph, ResolveError> {
-        let root = PubGrubPackage::Root(self.project.clone());
-
-        // Keep track of the packages for which we've requested metadata.
-        let mut pins = FilePins::default();
-        let mut priorities = PubGrubPriorities::default();
-
-        // Start the solve.
-        let mut state = State::init(root.clone(), MIN_VERSION.clone());
-        let mut added_dependencies: FxHashMap<PubGrubPackage, FxHashSet<Version>> =
-            FxHashMap::default();
-        let mut next = root;
+        let root = PubGrubPackage::from(PubGrubPackageInner::Root(self.project.clone()));
+        let mut prefetcher = BatchPrefetcher::default();
+        let mut state = SolveState {
+            pubgrub: State::init(root.clone(), MIN_VERSION.clone()),
+            next: root,
+            pins: FilePins::default(),
+            priorities: PubGrubPriorities::default(),
+            added_dependencies: FxHashMap::default(),
+        };
 
         debug!(
             "Solving with target Python version {}",
             self.python_requirement.target()
         );
 
         loop {
             // Run unit propagation.
-            state.unit_propagation(next)?;
+            state.pubgrub.unit_propagation(state.next)?;
 
-            // Pre-visit all candidate packages, to allow metadata to be fetched in parallel.
-            Self::pre_visit(state.partial_solution.prioritized_packages(), request_sink).await?;
+            // Pre-visit all candidate packages, to allow metadata to be fetched in parallel. If
+            // the dependency mode is direct, we only need to visit the root package.
+            if self.dependency_mode.is_transitive() {
+                Self::pre_visit(
+                    state.pubgrub.partial_solution.prioritized_packages(),
+                    &request_sink,
+                )?;
+            }
 
             // Choose a package version.
-            let Some(highest_priority_pkg) =
-                state
-                    .partial_solution
-                    .pick_highest_priority_pkg(|package, _range| {
-                        priorities.get(package).unwrap_or_default()
-                    })
+            let Some(highest_priority_pkg) = state
+                .pubgrub
+                .partial_solution
+                .pick_highest_priority_pkg(|package, _range| state.priorities.get(package))
             else {
-                let selection = state.partial_solution.extract_solution();
+                if enabled!(Level::DEBUG) {
+                    prefetcher.log_tried_versions();
+                }
+                let selection = state.pubgrub.partial_solution.extract_solution();
                 return ResolutionGraph::from_state(
                     &selection,
-                    &pins,
-                    &self.index.packages,
-                    &self.index.distributions,
-                    &self.index.redirects,
-                    &state,
+                    &state.pins,
+                    self.index.packages(),
+                    self.index.distributions(),
+                    &state.pubgrub,
+                    &self.preferences,
                     self.editables.clone(),
                 );
             };
-            next = highest_priority_pkg;
+            state.next = highest_priority_pkg;
+
+            prefetcher.version_tried(state.next.clone());
 
             let term_intersection = state
+                .pubgrub
                 .partial_solution
-                .term_intersection_for_package(&next)
+                .term_intersection_for_package(&state.next)
                 .ok_or_else(|| {
                     PubGrubError::Failure("a package was chosen but we don't have a term.".into())
                 })?;
-            let decision = self
-                .choose_version(
-                    &next,
-                    term_intersection.unwrap_positive(),
-                    &mut pins,
-                    request_sink,
-                )
-                .await?;
+            let decision = self.choose_version(
+                &state.next,
+                term_intersection.unwrap_positive(),
+                &mut state.pins,
+                visited,
+                &request_sink,
+            )?;
 
             // Pick the next compatible version.
             let version = match decision {
                 None => {
-                    debug!("No compatible version found for: {next}");
+                    debug!("No compatible version found for: {next}", next = state.next);
 
                     let term_intersection = state
+                        .pubgrub
                         .partial_solution
-                        .term_intersection_for_package(&next)
+                        .term_intersection_for_package(&state.next)
                         .expect("a package was chosen but we don't have a term.");
 
-                    let reason = {
-                        if let PubGrubPackage::Package(ref package_name, _, _) = next {
-                            // Check if the decision was due to the package being unavailable
-                            self.unavailable_packages
-                                .get(package_name)
-                                .map(|entry| match *entry {
-                                    UnavailablePackage::NoIndex => {
-                                        "was not found in the provided package locations"
-                                    }
-                                    UnavailablePackage::Offline => "was not found in the cache",
-                                    UnavailablePackage::NotFound => {
-                                        "was not found in the package registry"
-                                    }
-                                })
-                        } else {
-                            None
+                    // Check if the decision was due to the package being unavailable
+                    if let PubGrubPackageInner::Package { ref name, .. } = &*state.next {
+                        if let Some(entry) = self.unavailable_packages.get(name) {
+                            state
+                                .pubgrub
+                                .add_incompatibility(Incompatibility::custom_term(
+                                    state.next.clone(),
+                                    term_intersection.clone(),
+                                    UnavailableReason::Package(entry.clone()),
+                                ));
+                            continue;
                         }
-                    };
-
-                    let inc = Incompatibility::no_versions(
-                        next.clone(),
-                        term_intersection.clone(),
-                        reason.map(ToString::to_string),
-                    );
+                    }
 
-                    state.add_incompatibility(inc);
+                    state
+                        .pubgrub
+                        .add_incompatibility(Incompatibility::no_versions(
+                            state.next.clone(),
+                            term_intersection.clone(),
+                        ));
                     continue;
                 }
                 Some(version) => version,
             };
             let version = match version {
                 ResolverVersion::Available(version) => version,
-                ResolverVersion::Unavailable(version, unavailable) => {
-                    let reason = match unavailable {
-                        UnavailableVersion::RequiresPython(requires_python) => {
-                            // Incompatible requires-python versions are special in that we track
-                            // them as incompatible dependencies instead of marking the package version
-                            // as unavailable directly
-                            let python_version = requires_python
-                                .iter()
-                                .map(PubGrubSpecifier::try_from)
-                                .fold_ok(Range::full(), |range, specifier| {
-                                    range.intersection(&specifier.into())
-                                })?;
-
-                            let package = &next;
-                            for kind in [PubGrubPython::Installed, PubGrubPython::Target] {
-                                state.add_incompatibility(Incompatibility::from_dependency(
+                ResolverVersion::Unavailable(version, reason) => {
+                    // Incompatible requires-python versions are special in that we track
+                    // them as incompatible dependencies instead of marking the package version
+                    // as unavailable directly
+                    if let UnavailableVersion::IncompatibleDist(
+                        IncompatibleDist::Source(IncompatibleSource::RequiresPython(
+                            requires_python,
+                        ))
+                        | IncompatibleDist::Wheel(IncompatibleWheel::RequiresPython(requires_python)),
+                    ) = reason
+                    {
+                        let python_version = requires_python
+                            .iter()
+                            .map(PubGrubSpecifier::try_from)
+                            .fold_ok(Range::full(), |range, specifier| {
+                                range.intersection(&specifier.into())
+                            })?;
+
+                        let package = &state.next;
+                        for kind in [PubGrubPython::Installed, PubGrubPython::Target] {
+                            state
+                                .pubgrub
+                                .add_incompatibility(Incompatibility::from_dependency(
                                     package.clone(),
                                     Range::singleton(version.clone()),
-                                    (PubGrubPackage::Python(kind), python_version.clone()),
+                                    (
+                                        PubGrubPackage::from(PubGrubPackageInner::Python(kind)),
+                                        python_version.clone(),
+                                    ),
                                 ));
-                            }
-                            state.partial_solution.add_decision(next.clone(), version);
-                            continue;
-                        }
-                        UnavailableVersion::Yanked(yanked) => match yanked {
-                            Yanked::Bool(_) => "it was yanked".to_string(),
-                            Yanked::Reason(reason) => format!(
-                                "it was yanked (reason: {})",
-                                reason.trim().trim_end_matches('.')
-                            ),
-                        },
-                        UnavailableVersion::NoDistributions(best_incompatible) => {
-                            if let Some(best_incompatible) = best_incompatible {
-                                match best_incompatible {
-                                    IncompatibleWheel::NoBinary => "no source distribution is available and using wheels is disabled".to_string(),
-                                    IncompatibleWheel::RequiresPython => "no wheels are available that meet your required Python version".to_string(),
-                                    IncompatibleWheel::Tag(tag) => {
-                                        match tag {
-                                            IncompatibleTag::Invalid => "no wheels are available with valid tags".to_string(),
-                                            IncompatibleTag::Python => "no wheels are available with a matching Python implementation".to_string(),
-                                            IncompatibleTag::Abi => "no wheels are available with a matching Python ABI".to_string(),
-                                            IncompatibleTag::Platform => "no wheels are available with a matching platform".to_string(),
-                                        }
-                                    }
-                                }
-                            } else {
-                                // TODO(zanieb): It's unclear why we would encounter this case still
-                                "no wheels are available for your system".to_string()
-                            }
                         }
+                        state
+                            .pubgrub
+                            .partial_solution
+                            .add_decision(state.next.clone(), version);
+                        continue;
                     };
-                    state.add_incompatibility(Incompatibility::unavailable(
-                        next.clone(),
-                        version.clone(),
-                        reason,
-                    ));
+                    state
+                        .pubgrub
+                        .add_incompatibility(Incompatibility::custom_version(
+                            state.next.clone(),
+                            version.clone(),
+                            UnavailableReason::Version(reason),
+                        ));
                     continue;
                 }
             };
 
-            self.on_progress(&next, &version);
-
-            if added_dependencies
-                .entry(next.clone())
+            prefetcher.prefetch_batches(
+                &state.next,
+                &version,
+                term_intersection.unwrap_positive(),
+                &request_sink,
+                &self.index,
+                &self.selector,
+            )?;
+
+            self.on_progress(&state.next, &version);
+
+            if state
+                .added_dependencies
+                .entry(state.next.clone())
                 .or_default()
                 .insert(version.clone())
             {
                 // Retrieve that package dependencies.
-                let package = &next;
-                let dependencies = match self
-                    .get_dependencies(package, &version, &mut priorities, request_sink)
-                    .await?
-                {
+                let package = &state.next;
+                let dependencies = match self.get_dependencies(
+                    package,
+                    &version,
+                    &mut state.priorities,
+                    &request_sink,
+                )? {
                     Dependencies::Unavailable(reason) => {
-                        let message = {
-                            if matches!(package, PubGrubPackage::Root(_)) {
-                                // Including front-matter for the root package is redundant
-                                reason.clone()
-                            } else {
-                                format!("its dependencies are unusable because {reason}")
-                            }
-                        };
-                        state.add_incompatibility(Incompatibility::unavailable(
-                            package.clone(),
-                            version.clone(),
-                            message,
-                        ));
+                        state
+                            .pubgrub
+                            .add_incompatibility(Incompatibility::custom_version(
+                                package.clone(),
+                                version.clone(),
+                                UnavailableReason::Version(reason),
+                            ));
                         continue;
                     }
                     Dependencies::Available(constraints)
                         if constraints
                             .iter()
                             .any(|(dependency, _)| dependency == package) =>
                     {
+                        if enabled!(Level::DEBUG) {
+                            prefetcher.log_tried_versions();
+                        }
                         return Err(PubGrubError::SelfDependency {
                             package: package.clone(),
                             version: version.clone(),
                         }
                         .into());
                     }
                     Dependencies::Available(constraints) => constraints,
                 };
 
                 // Add that package and version if the dependencies are not problematic.
-                let dep_incompats = state.add_incompatibility_from_dependencies(
+                let dep_incompats = state.pubgrub.add_incompatibility_from_dependencies(
                     package.clone(),
                     version.clone(),
                     dependencies,
                 );
 
-                state.partial_solution.add_version(
+                state.pubgrub.partial_solution.add_version(
                     package.clone(),
                     version,
                     dep_incompats,
-                    &state.incompatibility_store,
+                    &state.pubgrub.incompatibility_store,
                 );
             } else {
                 // `dep_incompats` are already in `incompatibilities` so we know there are not satisfied
                 // terms and can add the decision directly.
-                state.partial_solution.add_decision(next.clone(), version);
+                state
+                    .pubgrub
+                    .partial_solution
+                    .add_decision(state.next.clone(), version);
             }
         }
     }
 
     /// Visit a [`PubGrubPackage`] prior to selection. This should be called on a [`PubGrubPackage`]
     /// before it is selected, to allow metadata to be fetched in parallel.
-    async fn visit_package(
+    fn visit_package(
         &self,
         package: &PubGrubPackage,
-        priorities: &mut PubGrubPriorities,
-        request_sink: &tokio::sync::mpsc::Sender<Request>,
+        request_sink: &Sender<Request>,
     ) -> Result<(), ResolveError> {
-        match package {
-            PubGrubPackage::Root(_) => {}
-            PubGrubPackage::Python(_) => {}
-            PubGrubPackage::Package(package_name, _extra, None) => {
+        match &**package {
+            PubGrubPackageInner::Root(_) => {}
+            PubGrubPackageInner::Python(_) => {}
+            PubGrubPackageInner::Extra { .. } => {}
+            PubGrubPackageInner::Package {
+                name, url: None, ..
+            } => {
+                // Verify that the package is allowed under the hash-checking policy.
+                if !self.hasher.allows_package(name) {
+                    return Err(ResolveError::UnhashedPackage(name.clone()));
+                }
+
                 // Emit a request to fetch the metadata for this package.
-                if self.index.packages.register(package_name.clone()) {
-                    priorities.add(package_name.clone());
-                    request_sink
-                        .send(Request::Package(package_name.clone()))
-                        .await?;
+                if self.index.packages().register(name.clone()) {
+                    request_sink.blocking_send(Request::Package(name.clone()))?;
                 }
             }
-            PubGrubPackage::Package(package_name, _extra, Some(url)) => {
+            PubGrubPackageInner::Package {
+                name,
+                url: Some(url),
+                ..
+            } => {
+                // Verify that the package is allowed under the hash-checking policy.
+                if !self.hasher.allows_url(&url.verbatim) {
+                    return Err(ResolveError::UnhashedPackage(name.clone()));
+                }
+
+                // If the package is an editable, we don't need to fetch metadata.
+                if self.editables.contains(name) {
+                    return Ok(());
+                }
+
                 // Emit a request to fetch the metadata for this distribution.
-                let dist = Dist::from_url(package_name.clone(), url.clone())?;
-                if self.index.distributions.register(dist.package_id()) {
-                    priorities.add(dist.name().clone());
-                    request_sink.send(Request::Dist(dist)).await?;
+                let dist = Dist::from_url(name.clone(), url.clone())?;
+                if self.index.distributions().register(dist.version_id()) {
+                    request_sink.blocking_send(Request::Dist(dist))?;
                 }
             }
         }
         Ok(())
     }
 
     /// Visit the set of [`PubGrubPackage`] candidates prior to selection. This allows us to fetch
     /// metadata for all of the packages in parallel.
-    async fn pre_visit<'data>(
+    fn pre_visit<'data>(
         packages: impl Iterator<Item = (&'data PubGrubPackage, &'data Range<Version>)>,
-        request_sink: &tokio::sync::mpsc::Sender<Request>,
+        request_sink: &Sender<Request>,
     ) -> Result<(), ResolveError> {
         // Iterate over the potential packages, and fetch file metadata for any of them. These
         // represent our current best guesses for the versions that we _might_ select.
         for (package, range) in packages {
-            let PubGrubPackage::Package(package_name, _extra, None) = package else {
+            let PubGrubPackageInner::Package {
+                name,
+                extra: None,
+                marker: None,
+                url: None,
+            } = &**package
+            else {
                 continue;
             };
-            request_sink
-                .send(Request::Prefetch(package_name.clone(), range.clone()))
-                .await?;
+            request_sink.blocking_send(Request::Prefetch(name.clone(), range.clone()))?;
         }
         Ok(())
     }
 
     /// Given a set of candidate packages, choose the next package (and version) to add to the
     /// partial solution.
     ///
     /// Returns [None] when there are no versions in the given range.
     #[instrument(skip_all, fields(%package))]
-    async fn choose_version(
+    fn choose_version(
         &self,
         package: &PubGrubPackage,
         range: &Range<Version>,
         pins: &mut FilePins,
-        request_sink: &tokio::sync::mpsc::Sender<Request>,
+        visited: &mut FxHashSet<PackageName>,
+        request_sink: &Sender<Request>,
     ) -> Result<Option<ResolverVersion>, ResolveError> {
-        match package {
-            PubGrubPackage::Root(_) => Ok(Some(ResolverVersion::Available(MIN_VERSION.clone()))),
+        match &**package {
+            PubGrubPackageInner::Root(_) => {
+                Ok(Some(ResolverVersion::Available(MIN_VERSION.clone())))
+            }
 
-            PubGrubPackage::Python(PubGrubPython::Installed) => {
+            PubGrubPackageInner::Python(PubGrubPython::Installed) => {
                 let version = self.python_requirement.installed();
                 if range.contains(version) {
-                    Ok(Some(ResolverVersion::Available(version.clone())))
+                    Ok(Some(ResolverVersion::Available(version.deref().clone())))
                 } else {
                     Ok(None)
                 }
             }
 
-            PubGrubPackage::Python(PubGrubPython::Target) => {
+            PubGrubPackageInner::Python(PubGrubPython::Target) => {
                 let version = self.python_requirement.target();
                 if range.contains(version) {
-                    Ok(Some(ResolverVersion::Available(version.clone())))
+                    Ok(Some(ResolverVersion::Available(version.deref().clone())))
                 } else {
                     Ok(None)
                 }
             }
 
-            PubGrubPackage::Package(package_name, extra, Some(url)) => {
-                if let Some(extra) = extra {
-                    debug!(
-                        "Searching for a compatible version of {package_name}[{extra}] @ {url} ({range})",
-                    );
-                } else {
-                    debug!(
-                        "Searching for a compatible version of {package_name} @ {url} ({range})"
-                    );
-                }
+            PubGrubPackageInner::Extra {
+                name,
+                url: Some(url),
+                ..
+            }
+            | PubGrubPackageInner::Package {
+                name,
+                url: Some(url),
+                ..
+            } => {
+                debug!(
+                    "Searching for a compatible version of {package} @ {} ({range})",
+                    url.verbatim
+                );
 
                 // If the dist is an editable, return the version from the editable metadata.
-                if let Some((_local, metadata)) = self.editables.get(package_name) {
-                    let version = metadata.version.clone();
-                    return if range.contains(&version) {
-                        Ok(Some(ResolverVersion::Available(version)))
-                    } else {
-                        Ok(None)
-                    };
+                if let Some(editable) = self.editables.get(name) {
+                    let version = &editable.metadata.version;
+
+                    // The version is incompatible with the requirement.
+                    if !range.contains(version) {
+                        return Ok(None);
+                    }
+
+                    // The version is incompatible due to its Python requirement.
+                    if let Some(requires_python) = editable.metadata.requires_python.as_ref() {
+                        let target = self.python_requirement.target();
+                        if !requires_python.contains(target) {
+                            return Ok(Some(ResolverVersion::Unavailable(
+                                version.clone(),
+                                UnavailableVersion::IncompatibleDist(IncompatibleDist::Source(
+                                    IncompatibleSource::RequiresPython(requires_python.clone()),
+                                )),
+                            )));
+                        }
+                    }
+
+                    return Ok(Some(ResolverVersion::Available(version.clone())));
                 }
 
-                if let Ok(wheel_filename) = WheelFilename::try_from(url.raw()) {
-                    // If the URL is that of a wheel, extract the version.
-                    let version = wheel_filename.version;
-                    if range.contains(&version) {
-                        Ok(Some(ResolverVersion::Available(version)))
-                    } else {
-                        Ok(None)
+                let dist = PubGrubDistribution::from_url(name, url);
+                let response = self
+                    .index
+                    .distributions()
+                    .wait_blocking(&dist.version_id())
+                    .ok_or(ResolveError::Unregistered)?;
+
+                // If we failed to fetch the metadata for a URL, we can't proceed.
+                let metadata = match &*response {
+                    MetadataResponse::Found(archive) => &archive.metadata,
+                    MetadataResponse::Offline => {
+                        self.unavailable_packages
+                            .insert(name.clone(), UnavailablePackage::Offline);
+                        return Ok(None);
                     }
-                } else {
-                    // Otherwise, assume this is a source distribution.
-                    let dist = PubGrubDistribution::from_url(package_name, url);
-                    let metadata = self
-                        .index
-                        .distributions
-                        .wait(&dist.package_id())
-                        .await
-                        .ok_or(ResolveError::Unregistered)?;
-                    let version = &metadata.version;
-                    if range.contains(version) {
-                        Ok(Some(ResolverVersion::Available(version.clone())))
-                    } else {
-                        Ok(None)
+                    MetadataResponse::InvalidMetadata(err) => {
+                        self.unavailable_packages.insert(
+                            name.clone(),
+                            UnavailablePackage::InvalidMetadata(err.to_string()),
+                        );
+                        return Ok(None);
+                    }
+                    MetadataResponse::InconsistentMetadata(err) => {
+                        self.unavailable_packages.insert(
+                            name.clone(),
+                            UnavailablePackage::InvalidMetadata(err.to_string()),
+                        );
+                        return Ok(None);
+                    }
+                    MetadataResponse::InvalidStructure(err) => {
+                        self.unavailable_packages.insert(
+                            name.clone(),
+                            UnavailablePackage::InvalidStructure(err.to_string()),
+                        );
+                        return Ok(None);
+                    }
+                };
+
+                let version = &metadata.version;
+
+                // The version is incompatible with the requirement.
+                if !range.contains(version) {
+                    return Ok(None);
+                }
+
+                // The version is incompatible due to its Python requirement.
+                if let Some(requires_python) = metadata.requires_python.as_ref() {
+                    let target = self.python_requirement.target();
+                    if !requires_python.contains(target) {
+                        return Ok(Some(ResolverVersion::Unavailable(
+                            version.clone(),
+                            UnavailableVersion::IncompatibleDist(IncompatibleDist::Source(
+                                IncompatibleSource::RequiresPython(requires_python.clone()),
+                            )),
+                        )));
                     }
                 }
+
+                Ok(Some(ResolverVersion::Available(version.clone())))
             }
 
-            PubGrubPackage::Package(package_name, extra, None) => {
+            PubGrubPackageInner::Extra {
+                name, url: None, ..
+            }
+            | PubGrubPackageInner::Package {
+                name, url: None, ..
+            } => {
                 // Wait for the metadata to be available.
                 let versions_response = self
                     .index
-                    .packages
-                    .wait(package_name)
-                    .instrument(info_span!("package_wait", %package_name))
-                    .await
+                    .packages()
+                    .wait_blocking(name)
                     .ok_or(ResolveError::Unregistered)?;
-                self.visited.insert(package_name.clone());
+                visited.insert(name.clone());
 
-                let version_map = match *versions_response {
-                    VersionsResponse::Found(ref version_map) => version_map,
-                    // Short-circuit if we do not find any versions for the package
+                let version_maps = match *versions_response {
+                    VersionsResponse::Found(ref version_maps) => version_maps.as_slice(),
                     VersionsResponse::NoIndex => {
                         self.unavailable_packages
-                            .insert(package_name.clone(), UnavailablePackage::NoIndex);
-
-                        return Ok(None);
+                            .insert(name.clone(), UnavailablePackage::NoIndex);
+                        &[]
                     }
                     VersionsResponse::Offline => {
                         self.unavailable_packages
-                            .insert(package_name.clone(), UnavailablePackage::Offline);
-
-                        return Ok(None);
+                            .insert(name.clone(), UnavailablePackage::Offline);
+                        &[]
                     }
                     VersionsResponse::NotFound => {
                         self.unavailable_packages
-                            .insert(package_name.clone(), UnavailablePackage::NotFound);
-
-                        return Ok(None);
+                            .insert(name.clone(), UnavailablePackage::NotFound);
+                        &[]
                     }
                 };
 
-                if let Some(extra) = extra {
-                    debug!(
-                        "Searching for a compatible version of {package_name}[{extra}] ({range})",
-                    );
-                } else {
-                    debug!("Searching for a compatible version of {package_name} ({range})");
-                }
+                debug!("Searching for a compatible version of {package} ({range})");
 
                 // Find a version.
-                let Some(candidate) = self.selector.select(package_name, range, version_map) else {
+                let Some(candidate) = self.selector.select(
+                    name,
+                    range,
+                    version_maps,
+                    &self.preferences,
+                    &self.installed_packages,
+                    &self.exclusions,
+                ) else {
                     // Short circuit: we couldn't find _any_ versions for a package.
                     return Ok(None);
                 };
 
                 let dist = match candidate.dist() {
                     CandidateDist::Compatible(dist) => dist,
-                    CandidateDist::ExcludeNewer => {
-                        // If the version is incompatible because of `exclude_newer`, pretend the versions do not exist
-                        return Ok(None);
-                    }
                     CandidateDist::Incompatible(incompatibility) => {
-                        // If the version is incompatible because no distributions match, exit early.
+                        // If the version is incompatible because no distributions are compatible, exit early.
                         return Ok(Some(ResolverVersion::Unavailable(
                             candidate.version().clone(),
-                            UnavailableVersion::NoDistributions(incompatibility.cloned()),
+                            UnavailableVersion::IncompatibleDist(incompatibility.clone()),
                         )));
                     }
                 };
 
-                // If the version is incompatible because it was yanked, exit early.
-                if dist.yanked().is_yanked() {
-                    if self
-                        .allowed_yanks
-                        .allowed(package_name, candidate.version())
-                    {
-                        warn!("Allowing yanked version: {}", candidate.package_id());
-                    } else {
-                        return Ok(Some(ResolverVersion::Unavailable(
-                            candidate.version().clone(),
-                            UnavailableVersion::Yanked(dist.yanked().clone()),
-                        )));
-                    }
-                }
-
-                // If the version is incompatible because of its Python requirement
-                if let Some(requires_python) = self.python_requirement.validate_dist(dist) {
-                    return Ok(Some(ResolverVersion::Unavailable(
-                        candidate.version().clone(),
-                        UnavailableVersion::RequiresPython(requires_python.clone()),
-                    )));
-                }
+                let filename = match dist.for_installation() {
+                    ResolvedDistRef::InstallableRegistrySourceDist { sdist, .. } => sdist
+                        .filename()
+                        .unwrap_or(Cow::Borrowed("unknown filename")),
+                    ResolvedDistRef::InstallableRegistryBuiltDist { wheel, .. } => wheel
+                        .filename()
+                        .unwrap_or(Cow::Borrowed("unknown filename")),
+                    ResolvedDistRef::Installed(_) => Cow::Borrowed("installed"),
+                };
 
-                if let Some(extra) = extra {
-                    debug!(
-                        "Selecting: {}[{}]=={} ({})",
-                        candidate.name(),
-                        extra,
-                        candidate.version(),
-                        dist.for_resolution()
-                            .dist
-                            .filename()
-                            .unwrap_or(Cow::Borrowed("unknown filename"))
-                    );
-                } else {
-                    debug!(
-                        "Selecting: {}=={} ({})",
-                        candidate.name(),
-                        candidate.version(),
-                        dist.for_resolution()
-                            .dist
-                            .filename()
-                            .unwrap_or(Cow::Borrowed("unknown filename"))
-                    );
-                }
+                debug!(
+                    "Selecting: {}=={} ({})",
+                    package,
+                    candidate.version(),
+                    filename,
+                );
 
                 // We want to return a package pinned to a specific version; but we _also_ want to
                 // store the exact file that we selected to satisfy that version.
                 pins.insert(&candidate, dist);
 
                 let version = candidate.version().clone();
 
                 // Emit a request to fetch the metadata for this version.
-                if self.index.distributions.register(candidate.package_id()) {
-                    let dist = dist.for_resolution().dist.clone();
-                    request_sink.send(Request::Dist(dist)).await?;
+                if matches!(&**package, PubGrubPackageInner::Package { .. }) {
+                    if self.index.distributions().register(candidate.version_id()) {
+                        let request = Request::from(dist.for_resolution());
+                        request_sink.blocking_send(request)?;
+                    }
                 }
 
                 Ok(Some(ResolverVersion::Available(version)))
             }
         }
     }
 
     /// Given a candidate package and version, return its dependencies.
     #[instrument(skip_all, fields(%package, %version))]
-    async fn get_dependencies(
+    fn get_dependencies(
         &self,
         package: &PubGrubPackage,
         version: &Version,
         priorities: &mut PubGrubPriorities,
-        request_sink: &tokio::sync::mpsc::Sender<Request>,
+        request_sink: &Sender<Request>,
     ) -> Result<Dependencies, ResolveError> {
-        match package {
-            PubGrubPackage::Root(_) => {
+        match &**package {
+            PubGrubPackageInner::Root(_) => {
                 // Add the root requirements.
-                let constraints = PubGrubDependencies::from_requirements(
+                let dependencies = PubGrubDependencies::from_requirements(
                     &self.requirements,
                     &self.constraints,
                     &self.overrides,
                     None,
                     None,
                     &self.urls,
-                    self.markers,
+                    &self.locals,
+                    self.markers.as_ref(),
                 );
 
-                let mut constraints = match constraints {
-                    Ok(constraints) => constraints,
+                let mut dependencies = match dependencies {
+                    Ok(dependencies) => dependencies,
                     Err(err) => {
-                        return Ok(Dependencies::Unavailable(uncapitalize(err.to_string())));
+                        return Ok(Dependencies::Unavailable(
+                            UnavailableVersion::ResolverError(uncapitalize(err.to_string())),
+                        ));
                     }
                 };
 
-                for (package, version) in constraints.iter() {
+                for (package, version) in dependencies.iter() {
                     debug!("Adding direct dependency: {package}{version}");
 
+                    // Update the package priorities.
+                    priorities.insert(package, version);
+
                     // Emit a request to fetch the metadata for this package.
-                    self.visit_package(package, priorities, request_sink)
-                        .await?;
+                    self.visit_package(package, request_sink)?;
                 }
 
                 // Add a dependency on each editable.
-                for (editable, metadata) in self.editables.iter() {
-                    constraints.push(
-                        PubGrubPackage::from_package(metadata.name.clone(), None, &self.urls),
-                        Range::singleton(metadata.version.clone()),
+                for editable in self.editables.iter() {
+                    let package = PubGrubPackage::from_package(
+                        editable.metadata.name.clone(),
+                        None,
+                        None,
+                        &self.urls,
                     );
-                    for extra in &editable.extras {
-                        constraints.push(
+                    let version = Range::singleton(editable.metadata.version.clone());
+
+                    // Update the package priorities.
+                    priorities.insert(&package, &version);
+
+                    // Add the editable as a direct dependency.
+                    dependencies.push(package, version);
+
+                    // Add a dependency on each extra.
+                    for extra in &editable.built.extras {
+                        dependencies.push(
                             PubGrubPackage::from_package(
-                                metadata.name.clone(),
+                                editable.metadata.name.clone(),
                                 Some(extra.clone()),
+                                None,
                                 &self.urls,
                             ),
-                            Range::singleton(metadata.version.clone()),
+                            Range::singleton(editable.metadata.version.clone()),
                         );
                     }
+
+                    // Add any constraints.
+                    for constraint in self
+                        .constraints
+                        .get(&editable.metadata.name)
+                        .into_iter()
+                        .flatten()
+                    {
+                        if constraint.evaluate_markers(self.markers.as_ref(), &[]) {
+                            let PubGrubRequirement { package, version } =
+                                PubGrubRequirement::from_constraint(
+                                    constraint,
+                                    &self.urls,
+                                    &self.locals,
+                                )?;
+                            dependencies.push(package, version);
+                        }
+                    }
                 }
 
-                Ok(Dependencies::Available(constraints.into()))
+                Ok(Dependencies::Available(dependencies.into()))
             }
 
-            PubGrubPackage::Python(_) => Ok(Dependencies::Available(Vec::default())),
+            PubGrubPackageInner::Python(_) => Ok(Dependencies::Available(Vec::default())),
 
-            PubGrubPackage::Package(package_name, extra, url) => {
+            PubGrubPackageInner::Package {
+                name,
+                extra,
+                marker: _marker,
+                url,
+            } => {
                 // If we're excluding transitive dependencies, short-circuit.
                 if self.dependency_mode.is_direct() {
                     // If an extra is provided, wait for the metadata to be available, since it's
-                    // still required for reporting diagnostics.
-                    if extra.is_some() {
+                    // still required for generating the lock file.
+                    if !self.editables.contains(name) {
                         // Determine the distribution to lookup.
                         let dist = match url {
-                            Some(url) => PubGrubDistribution::from_url(package_name, url),
-                            None => PubGrubDistribution::from_registry(package_name, version),
+                            Some(url) => PubGrubDistribution::from_url(name, url),
+                            None => PubGrubDistribution::from_registry(name, version),
                         };
-                        let package_id = dist.package_id();
+                        let version_id = dist.version_id();
 
                         // Wait for the metadata to be available.
                         self.index
-                            .distributions
-                            .wait(&package_id)
-                            .instrument(info_span!("distributions_wait", %package_id))
-                            .await
+                            .distributions()
+                            .wait_blocking(&version_id)
                             .ok_or(ResolveError::Unregistered)?;
                     }
 
                     return Ok(Dependencies::Available(Vec::default()));
                 }
 
                 // Determine if the distribution is editable.
-                if let Some((_local, metadata)) = self.editables.get(package_name) {
-                    let mut constraints = PubGrubDependencies::from_requirements(
-                        &metadata.requires_dist,
+                if let Some(editable) = self.editables.get(name) {
+                    let requirements: Vec<_> = editable
+                        .metadata
+                        .requires_dist
+                        .iter()
+                        .cloned()
+                        .map(Requirement::from_pep508)
+                        .collect::<Result<_, _>>()?;
+                    let dependencies = PubGrubDependencies::from_requirements(
+                        &requirements,
                         &self.constraints,
                         &self.overrides,
-                        Some(package_name),
+                        Some(name),
                         extra.as_ref(),
                         &self.urls,
-                        self.markers,
+                        &self.locals,
+                        self.markers.as_ref(),
                     )?;
 
-                    for (package, version) in constraints.iter() {
-                        debug!("Adding transitive dependency: {package}{version}");
+                    for (dep_package, dep_version) in dependencies.iter() {
+                        debug!("Adding transitive dependency for {package}=={version}: {dep_package}{dep_version}");
 
-                        // Emit a request to fetch the metadata for this package.
-                        self.visit_package(package, priorities, request_sink)
-                            .await?;
-                    }
+                        // Update the package priorities.
+                        priorities.insert(dep_package, dep_version);
 
-                    // If a package has an extra, insert a constraint on the base package.
-                    if extra.is_some() {
-                        constraints.push(
-                            PubGrubPackage::Package(package_name.clone(), None, url.clone()),
-                            Range::singleton(version.clone()),
-                        );
+                        // Emit a request to fetch the metadata for this package.
+                        self.visit_package(dep_package, request_sink)?;
                     }
 
-                    return Ok(Dependencies::Available(constraints.into()));
+                    return Ok(Dependencies::Available(dependencies.into()));
                 }
 
                 // Determine the distribution to lookup.
                 let dist = match url {
-                    Some(url) => PubGrubDistribution::from_url(package_name, url),
-                    None => PubGrubDistribution::from_registry(package_name, version),
+                    Some(url) => PubGrubDistribution::from_url(name, url),
+                    None => PubGrubDistribution::from_registry(name, version),
                 };
-                let package_id = dist.package_id();
+                let version_id = dist.version_id();
 
-                // If the package does not exist in the registry, we cannot fetch its dependencies
-                if self.unavailable_packages.get(package_name).is_some() {
+                // If the package does not exist in the registry or locally, we cannot fetch its dependencies
+                if self.unavailable_packages.get(name).is_some()
+                    && self.installed_packages.get_packages(name).is_empty()
+                {
                     debug_assert!(
                         false,
                         "Dependencies were requested for a package that is not available"
                     );
-                    return Ok(Dependencies::Unavailable(
-                        "The package is unavailable".to_string(),
-                    ));
+                    return Err(ResolveError::Failure(format!(
+                        "The package is unavailable: {name}"
+                    )));
                 }
 
                 // Wait for the metadata to be available.
-                let metadata = self
+                let response = self
                     .index
-                    .distributions
-                    .wait(&package_id)
-                    .instrument(info_span!("distributions_wait", %package_id))
-                    .await
+                    .distributions()
+                    .wait_blocking(&version_id)
                     .ok_or(ResolveError::Unregistered)?;
 
-                let mut constraints = PubGrubDependencies::from_requirements(
-                    &metadata.requires_dist,
+                let metadata = match &*response {
+                    MetadataResponse::Found(archive) => &archive.metadata,
+                    MetadataResponse::Offline => {
+                        self.incomplete_packages
+                            .entry(name.clone())
+                            .or_default()
+                            .insert(version.clone(), IncompletePackage::Offline);
+                        return Ok(Dependencies::Unavailable(UnavailableVersion::Offline));
+                    }
+                    MetadataResponse::InvalidMetadata(err) => {
+                        warn!("Unable to extract metadata for {name}: {err}");
+                        self.incomplete_packages
+                            .entry(name.clone())
+                            .or_default()
+                            .insert(
+                                version.clone(),
+                                IncompletePackage::InvalidMetadata(err.to_string()),
+                            );
+                        return Ok(Dependencies::Unavailable(
+                            UnavailableVersion::InvalidMetadata,
+                        ));
+                    }
+                    MetadataResponse::InconsistentMetadata(err) => {
+                        warn!("Unable to extract metadata for {name}: {err}");
+                        self.incomplete_packages
+                            .entry(name.clone())
+                            .or_default()
+                            .insert(
+                                version.clone(),
+                                IncompletePackage::InconsistentMetadata(err.to_string()),
+                            );
+                        return Ok(Dependencies::Unavailable(
+                            UnavailableVersion::InconsistentMetadata,
+                        ));
+                    }
+                    MetadataResponse::InvalidStructure(err) => {
+                        warn!("Unable to extract metadata for {name}: {err}");
+                        self.incomplete_packages
+                            .entry(name.clone())
+                            .or_default()
+                            .insert(
+                                version.clone(),
+                                IncompletePackage::InvalidStructure(err.to_string()),
+                            );
+                        return Ok(Dependencies::Unavailable(
+                            UnavailableVersion::InvalidStructure,
+                        ));
+                    }
+                };
+
+                let requirements: Vec<_> = metadata
+                    .requires_dist
+                    .iter()
+                    .cloned()
+                    .map(Requirement::from_pep508)
+                    .collect::<Result<_, _>>()?;
+                let dependencies = PubGrubDependencies::from_requirements(
+                    &requirements,
                     &self.constraints,
                     &self.overrides,
-                    Some(package_name),
+                    Some(name),
                     extra.as_ref(),
                     &self.urls,
-                    self.markers,
+                    &self.locals,
+                    self.markers.as_ref(),
                 )?;
 
-                for (package, version) in constraints.iter() {
-                    debug!("Adding transitive dependency: {package}{version}");
+                for (dep_package, dep_version) in dependencies.iter() {
+                    debug!("Adding transitive dependency for {package}=={version}: {dep_package}{dep_version}");
 
-                    // Emit a request to fetch the metadata for this package.
-                    self.visit_package(package, priorities, request_sink)
-                        .await?;
-                }
+                    // Update the package priorities.
+                    priorities.insert(dep_package, dep_version);
 
-                // If a package has an extra, insert a constraint on the base package.
-                if extra.is_some() {
-                    constraints.push(
-                        PubGrubPackage::Package(package_name.clone(), None, url.clone()),
-                        Range::singleton(version.clone()),
-                    );
+                    // Emit a request to fetch the metadata for this package.
+                    self.visit_package(dep_package, request_sink)?;
                 }
 
-                Ok(Dependencies::Available(constraints.into()))
+                Ok(Dependencies::Available(dependencies.into()))
             }
+
+            // Add a dependency on both the extra and base package.
+            PubGrubPackageInner::Extra {
+                name,
+                extra,
+                marker: _marker,
+                url,
+            } => Ok(Dependencies::Available(vec![
+                (
+                    PubGrubPackage::from(PubGrubPackageInner::Package {
+                        name: name.clone(),
+                        extra: None,
+                        marker: None,
+                        url: url.clone(),
+                    }),
+                    Range::singleton(version.clone()),
+                ),
+                (
+                    PubGrubPackage::from(PubGrubPackageInner::Package {
+                        name: name.clone(),
+                        extra: Some(extra.clone()),
+                        marker: None,
+                        url: url.clone(),
+                    }),
+                    Range::singleton(version.clone()),
+                ),
+            ])),
         }
     }
 
     /// Fetch the metadata for a stream of packages and versions.
-    async fn fetch(
-        &self,
-        request_stream: tokio::sync::mpsc::Receiver<Request>,
+    async fn fetch<Provider: ResolverProvider>(
+        self: Arc<Self>,
+        provider: Arc<Provider>,
+        request_stream: Receiver<Request>,
     ) -> Result<(), ResolveError> {
         let mut response_stream = ReceiverStream::new(request_stream)
-            .map(|request| self.process_request(request).boxed())
-            .buffer_unordered(50);
+            .map(|request| self.process_request(request, &*provider).boxed_local())
+            // Allow as many futures as possible to start in the background.
+            // Backpressure is provided by at a more granular level by `DistributionDatabase`
+            // and `SourceDispatch`, as well as the bounded request channel.
+            .buffer_unordered(usize::MAX);
 
         while let Some(response) = response_stream.next().await {
             match response? {
                 Some(Response::Package(package_name, version_map)) => {
                     trace!("Received package metadata for: {package_name}");
-                    self.index.packages.done(package_name, version_map);
+                    self.index
+                        .packages()
+                        .done(package_name, Arc::new(version_map));
+                }
+                Some(Response::Installed { dist, metadata }) => {
+                    trace!("Received installed distribution metadata for: {dist}");
+                    self.index.distributions().done(
+                        dist.version_id(),
+                        Arc::new(MetadataResponse::Found(ArchiveMetadata::from(metadata))),
+                    );
                 }
                 Some(Response::Dist {
                     dist: Dist::Built(dist),
                     metadata,
-                    precise: _,
                 }) => {
                     trace!("Received built distribution metadata for: {dist}");
-                    self.index.distributions.done(dist.package_id(), metadata);
+                    match &metadata {
+                        MetadataResponse::InvalidMetadata(err) => {
+                            warn!("Unable to extract metadata for {dist}: {err}");
+                        }
+                        MetadataResponse::InvalidStructure(err) => {
+                            warn!("Unable to extract metadata for {dist}: {err}");
+                        }
+                        _ => {}
+                    }
+                    self.index
+                        .distributions()
+                        .done(dist.version_id(), Arc::new(metadata));
                 }
                 Some(Response::Dist {
-                    dist: Dist::Source(distribution),
+                    dist: Dist::Source(dist),
                     metadata,
-                    precise,
                 }) => {
-                    trace!("Received source distribution metadata for: {distribution}");
-                    self.index
-                        .distributions
-                        .done(distribution.package_id(), metadata);
-                    if let Some(precise) = precise {
-                        match distribution {
-                            SourceDist::DirectUrl(sdist) => {
-                                self.index.redirects.insert(sdist.url.to_url(), precise);
-                            }
-                            SourceDist::Git(sdist) => {
-                                self.index.redirects.insert(sdist.url.to_url(), precise);
-                            }
-                            SourceDist::Path(sdist) => {
-                                self.index.redirects.insert(sdist.url.to_url(), precise);
-                            }
-                            SourceDist::Registry(_) => {}
+                    trace!("Received source distribution metadata for: {dist}");
+                    match &metadata {
+                        MetadataResponse::InvalidMetadata(err) => {
+                            warn!("Unable to extract metadata for {dist}: {err}");
                         }
+                        MetadataResponse::InvalidStructure(err) => {
+                            warn!("Unable to extract metadata for {dist}: {err}");
+                        }
+                        _ => {}
                     }
+                    self.index
+                        .distributions()
+                        .done(dist.version_id(), Arc::new(metadata));
                 }
                 None => {}
             }
         }
 
         Ok::<(), ResolveError>(())
     }
 
     #[instrument(skip_all, fields(%request))]
-    async fn process_request(&self, request: Request) -> Result<Option<Response>, ResolveError> {
+    async fn process_request<Provider: ResolverProvider>(
+        &self,
+        request: Request,
+        provider: &Provider,
+    ) -> Result<Option<Response>, ResolveError> {
         match request {
             // Fetch package metadata from the registry.
             Request::Package(package_name) => {
-                let package_versions = self
-                    .provider
+                let package_versions = provider
                     .get_package_versions(&package_name)
-                    .boxed()
+                    .boxed_local()
                     .await
                     .map_err(ResolveError::Client)?;
 
                 Ok(Some(Response::Package(package_name, package_versions)))
             }
 
             // Fetch distribution metadata from the distribution database.
             Request::Dist(dist) => {
-                let (metadata, precise) = self
-                    .provider
+                let metadata = provider
                     .get_or_build_wheel_metadata(&dist)
-                    .boxed()
+                    .boxed_local()
                     .await
                     .map_err(|err| match dist.clone() {
-                        Dist::Built(BuiltDist::Path(built_dist)) => {
+                        Dist::Built(built_dist @ BuiltDist::Path(_)) => {
                             ResolveError::Read(Box::new(built_dist), err)
                         }
-                        Dist::Source(SourceDist::Path(source_dist)) => {
+                        Dist::Source(source_dist @ SourceDist::Path(_)) => {
+                            ResolveError::Build(Box::new(source_dist), err)
+                        }
+                        Dist::Source(source_dist @ SourceDist::Directory(_)) => {
                             ResolveError::Build(Box::new(source_dist), err)
                         }
                         Dist::Built(built_dist) => ResolveError::Fetch(Box::new(built_dist), err),
                         Dist::Source(source_dist) => {
                             ResolveError::FetchAndBuild(Box::new(source_dist), err)
                         }
                     })?;
-                Ok(Some(Response::Dist {
-                    dist,
-                    metadata,
-                    precise,
-                }))
+                Ok(Some(Response::Dist { dist, metadata }))
+            }
+
+            Request::Installed(dist) => {
+                let metadata = dist
+                    .metadata()
+                    .map_err(|err| ResolveError::ReadInstalled(Box::new(dist.clone()), err))?;
+                Ok(Some(Response::Installed { dist, metadata }))
             }
 
             // Pre-fetch the package and distribution metadata.
             Request::Prefetch(package_name, range) => {
                 // Wait for the package metadata to become available.
                 let versions_response = self
                     .index
-                    .packages
+                    .packages()
                     .wait(&package_name)
                     .await
                     .ok_or(ResolveError::Unregistered)?;
 
                 let version_map = match *versions_response {
                     VersionsResponse::Found(ref version_map) => version_map,
                     // Short-circuit if we did not find any versions for the package
@@ -1028,135 +1375,232 @@
                             .insert(package_name.clone(), UnavailablePackage::NotFound);
 
                         return Ok(None);
                     }
                 };
 
                 // Try to find a compatible version. If there aren't any compatible versions,
-                // short-circuit and return `None`.
-                let Some(candidate) = self.selector.select(&package_name, &range, version_map)
-                else {
+                // short-circuit.
+                let Some(candidate) = self.selector.select(
+                    &package_name,
+                    &range,
+                    version_map,
+                    &self.preferences,
+                    &self.installed_packages,
+                    &self.exclusions,
+                ) else {
                     return Ok(None);
                 };
 
                 // If there is not a compatible distribution, short-circuit.
                 let Some(dist) = candidate.compatible() else {
                     return Ok(None);
                 };
 
-                // If the Python version is incompatible, short-circuit.
-                if self.python_requirement.validate_dist(dist).is_some() {
-                    return Ok(None);
-                }
-
                 // Emit a request to fetch the metadata for this version.
-                if self.index.distributions.register(candidate.package_id()) {
-                    let dist = dist.for_resolution().dist.clone();
+                if self.index.distributions().register(candidate.version_id()) {
+                    let dist = dist.for_resolution().to_owned();
+
+                    let response = match dist {
+                        ResolvedDist::Installable(dist) => {
+                            let metadata = provider
+                                .get_or_build_wheel_metadata(&dist)
+                                .boxed_local()
+                                .await
+                                .map_err(|err| match dist.clone() {
+                                    Dist::Built(built_dist @ BuiltDist::Path(_)) => {
+                                        ResolveError::Read(Box::new(built_dist), err)
+                                    }
+                                    Dist::Source(source_dist @ SourceDist::Path(_)) => {
+                                        ResolveError::Build(Box::new(source_dist), err)
+                                    }
+                                    Dist::Source(source_dist @ SourceDist::Directory(_)) => {
+                                        ResolveError::Build(Box::new(source_dist), err)
+                                    }
+                                    Dist::Built(built_dist) => {
+                                        ResolveError::Fetch(Box::new(built_dist), err)
+                                    }
+                                    Dist::Source(source_dist) => {
+                                        ResolveError::FetchAndBuild(Box::new(source_dist), err)
+                                    }
+                                })?;
+                            Response::Dist { dist, metadata }
+                        }
+                        ResolvedDist::Installed(dist) => {
+                            let metadata = dist.metadata().map_err(|err| {
+                                ResolveError::ReadInstalled(Box::new(dist.clone()), err)
+                            })?;
+                            Response::Installed { dist, metadata }
+                        }
+                    };
 
-                    let (metadata, precise) = self
-                        .provider
-                        .get_or_build_wheel_metadata(&dist)
-                        .boxed()
-                        .await
-                        .map_err(|err| match dist.clone() {
-                            Dist::Built(BuiltDist::Path(built_dist)) => {
-                                ResolveError::Read(Box::new(built_dist), err)
-                            }
-                            Dist::Source(SourceDist::Path(source_dist)) => {
-                                ResolveError::Build(Box::new(source_dist), err)
-                            }
-                            Dist::Built(built_dist) => {
-                                ResolveError::Fetch(Box::new(built_dist), err)
-                            }
-                            Dist::Source(source_dist) => {
-                                ResolveError::FetchAndBuild(Box::new(source_dist), err)
-                            }
-                        })?;
-
-                    Ok(Some(Response::Dist {
-                        dist,
-                        metadata,
-                        precise,
-                    }))
+                    Ok(Some(response))
                 } else {
                     Ok(None)
                 }
             }
         }
     }
 
     fn on_progress(&self, package: &PubGrubPackage, version: &Version) {
         if let Some(reporter) = self.reporter.as_ref() {
-            match package {
-                PubGrubPackage::Root(_) => {}
-                PubGrubPackage::Python(_) => {}
-                PubGrubPackage::Package(package_name, _extra, Some(url)) => {
-                    reporter.on_progress(package_name, VersionOrUrl::Url(url));
-                }
-                PubGrubPackage::Package(package_name, _extra, None) => {
-                    reporter.on_progress(package_name, VersionOrUrl::Version(version));
+            match &**package {
+                PubGrubPackageInner::Root(_) => {}
+                PubGrubPackageInner::Python(_) => {}
+                PubGrubPackageInner::Extra { .. } => {}
+                PubGrubPackageInner::Package {
+                    name,
+                    url: Some(url),
+                    ..
+                } => {
+                    reporter.on_progress(name, &VersionOrUrlRef::Url(&url.verbatim));
+                }
+                PubGrubPackageInner::Package {
+                    name, url: None, ..
+                } => {
+                    reporter.on_progress(name, &VersionOrUrlRef::Version(version));
                 }
             }
         }
     }
 
     fn on_complete(&self) {
         if let Some(reporter) = self.reporter.as_ref() {
             reporter.on_complete();
         }
     }
 }
 
+/// State that is used during unit propagation in the resolver.
+#[derive(Clone)]
+struct SolveState {
+    /// The internal state used by the resolver.
+    ///
+    /// Note that not all parts of this state are strictly internal. For
+    /// example, the edges in the dependency graph generated as part of the
+    /// output of resolution are derived from the "incompatibilities" tracked
+    /// in this state. We also ultimately retrieve the final set of version
+    /// assignments (to packages) from this state's "partial solution."
+    pubgrub: State<UvDependencyProvider>,
+    /// The next package on which to run unit propagation.
+    next: PubGrubPackage,
+    /// The set of pinned versions we accrue throughout resolution.
+    ///
+    /// The key of this map is a package name, and each package name maps to
+    /// a set of versions for that package. Each version in turn is mapped
+    /// to a single `ResolvedDist`. That `ResolvedDist` represents, at time
+    /// of writing (2024/05/09), at most one wheel. The idea here is that
+    /// `FilePins` tracks precisely which wheel was selected during resolution.
+    /// After resolution is finished, this maps is consulted in order to select
+    /// the wheel chosen during resolution.
+    pins: FilePins,
+    /// When dependencies for a package are retrieved, this map of priorities
+    /// is updated based on how each dependency was specified. Certain types
+    /// of dependencies have more "priority" than others (like direct URL
+    /// dependencies). These priorities help determine which package to
+    /// consider next during resolution.
+    priorities: PubGrubPriorities,
+    /// This keeps track of the set of versions for each package that we've
+    /// already visited during resolution. This avoids doing redundant work.
+    added_dependencies: FxHashMap<PubGrubPackage, FxHashSet<Version>>,
+}
+
 /// Fetch the metadata for an item
 #[derive(Debug)]
 #[allow(clippy::large_enum_variant)]
 pub(crate) enum Request {
     /// A request to fetch the metadata for a package.
     Package(PackageName),
     /// A request to fetch the metadata for a built or source distribution.
     Dist(Dist),
+    /// A request to fetch the metadata from an already-installed distribution.
+    Installed(InstalledDist),
     /// A request to pre-fetch the metadata for a package and the best-guess distribution.
     Prefetch(PackageName, Range<Version>),
 }
 
+impl<'a> From<ResolvedDistRef<'a>> for Request {
+    fn from(dist: ResolvedDistRef<'a>) -> Request {
+        // N.B. This is almost identical to `ResolvedDistRef::to_owned`, but
+        // creates a `Request` instead of a `ResolvedDist`. There's probably
+        // some room for DRYing this up a bit. The obvious way would be to
+        // add a method to create a `Dist`, but a `Dist` cannot reprented an
+        // installed dist.
+        match dist {
+            ResolvedDistRef::InstallableRegistrySourceDist { sdist, prioritized } => {
+                // This is okay because we're only here if the prioritized dist
+                // has an sdist, so this always succeeds.
+                let source = prioritized.source_dist().expect("a source distribution");
+                assert_eq!(
+                    (&sdist.name, &sdist.version),
+                    (&source.name, &source.version),
+                    "expected chosen sdist to match prioritized sdist"
+                );
+                Request::Dist(Dist::Source(SourceDist::Registry(source)))
+            }
+            ResolvedDistRef::InstallableRegistryBuiltDist {
+                wheel, prioritized, ..
+            } => {
+                assert_eq!(
+                    Some(&wheel.filename),
+                    prioritized.best_wheel().map(|(wheel, _)| &wheel.filename),
+                    "expected chosen wheel to match best wheel"
+                );
+                // This is okay because we're only here if the prioritized dist
+                // has at least one wheel, so this always succeeds.
+                let built = prioritized.built_dist().expect("at least one wheel");
+                Request::Dist(Dist::Built(BuiltDist::Registry(built)))
+            }
+            ResolvedDistRef::Installed(dist) => Request::Installed(dist.clone()),
+        }
+    }
+}
+
 impl Display for Request {
     fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
         match self {
-            Request::Package(package_name) => {
+            Self::Package(package_name) => {
                 write!(f, "Versions {package_name}")
             }
-            Request::Dist(dist) => {
+            Self::Dist(dist) => {
                 write!(f, "Metadata {dist}")
             }
-            Request::Prefetch(package_name, range) => {
+            Self::Installed(dist) => {
+                write!(f, "Installed metadata {dist}")
+            }
+            Self::Prefetch(package_name, range) => {
                 write!(f, "Prefetch {package_name} {range}")
             }
         }
     }
 }
 
 #[derive(Debug)]
 #[allow(clippy::large_enum_variant)]
 enum Response {
     /// The returned metadata for a package hosted on a registry.
     Package(PackageName, VersionsResponse),
     /// The returned metadata for a distribution.
     Dist {
         dist: Dist,
-        metadata: Metadata21,
-        precise: Option<Url>,
+        metadata: MetadataResponse,
+    },
+    /// The returned metadata for an already-installed distribution.
+    Installed {
+        dist: InstalledDist,
+        metadata: Metadata23,
     },
 }
 
 /// An enum used by [`DependencyProvider`] that holds information about package dependencies.
 /// For each [Package] there is a set of versions allowed as a dependency.
 #[derive(Clone)]
 enum Dependencies {
     /// Package dependencies are not available.
-    Unavailable(String),
+    Unavailable(UnavailableVersion),
     /// Container for all available package versions.
     Available(Vec<(PubGrubPackage, Range<Version>)>),
 }
 
 fn uncapitalize<T: AsRef<str>>(string: T) -> String {
     let mut chars = string.as_ref().chars();
     match chars.next() {
```

### Comparing `uv-0.1.9/crates/uv-resolver/src/resolver/provider.rs` & `uv-0.2.0/crates/uv-resolver/src/resolver/provider.rs`

 * *Files 26% similar despite different names*

```diff
@@ -1,182 +1,218 @@
 use std::future::Future;
-use std::ops::Deref;
-use std::sync::Arc;
 
 use anyhow::Result;
-use chrono::{DateTime, Utc};
-use url::Url;
 
 use distribution_types::{Dist, IndexLocations};
 use platform_tags::Tags;
-use pypi_types::Metadata21;
-use uv_client::{FlatIndex, RegistryClient};
-use uv_distribution::DistributionDatabase;
+use uv_configuration::{NoBinary, NoBuild};
+use uv_distribution::{ArchiveMetadata, DistributionDatabase};
 use uv_normalize::PackageName;
-use uv_traits::{BuildContext, NoBinary};
+use uv_types::{BuildContext, HashStrategy};
 
+use crate::flat_index::FlatIndex;
 use crate::python_requirement::PythonRequirement;
 use crate::version_map::VersionMap;
+use crate::yanks::AllowedYanks;
+use crate::ExcludeNewer;
 
 pub type PackageVersionsResult = Result<VersionsResponse, uv_client::Error>;
-pub type WheelMetadataResult = Result<(Metadata21, Option<Url>), uv_distribution::Error>;
+pub type WheelMetadataResult = Result<MetadataResponse, uv_distribution::Error>;
 
 /// The response when requesting versions for a package
 #[derive(Debug)]
 pub enum VersionsResponse {
     /// The package was found in the registry with the included versions
-    Found(VersionMap),
+    Found(Vec<VersionMap>),
     /// The package was not found in the registry
     NotFound,
     /// The package was not found in the local registry
     NoIndex,
     /// The package was not found in the cache and the network is not available.
     Offline,
 }
 
-pub trait ResolverProvider: Send + Sync {
+#[derive(Debug)]
+pub enum MetadataResponse {
+    /// The wheel metadata was found and parsed successfully.
+    Found(ArchiveMetadata),
+    /// The wheel metadata was found, but could not be parsed.
+    InvalidMetadata(Box<pypi_types::MetadataError>),
+    /// The wheel metadata was found, but the metadata was inconsistent.
+    InconsistentMetadata(Box<uv_distribution::Error>),
+    /// The wheel has an invalid structure.
+    InvalidStructure(Box<install_wheel_rs::Error>),
+    /// The wheel metadata was not found in the cache and the network is not available.
+    Offline,
+}
+
+pub trait ResolverProvider {
     /// Get the version map for a package.
     fn get_package_versions<'io>(
         &'io self,
         package_name: &'io PackageName,
-    ) -> impl Future<Output = PackageVersionsResult> + Send + 'io;
+    ) -> impl Future<Output = PackageVersionsResult> + 'io;
 
     /// Get the metadata for a distribution.
     ///
     /// For a wheel, this is done by querying it's (remote) metadata, for a source dist we
     /// (fetch and) build the source distribution and return the metadata from the built
     /// distribution.
     fn get_or_build_wheel_metadata<'io>(
         &'io self,
         dist: &'io Dist,
-    ) -> impl Future<Output = WheelMetadataResult> + Send + 'io;
+    ) -> impl Future<Output = WheelMetadataResult> + 'io;
 
     fn index_locations(&self) -> &IndexLocations;
 
     /// Set the [`uv_distribution::Reporter`] to use for this installer.
     #[must_use]
     fn with_reporter(self, reporter: impl uv_distribution::Reporter + 'static) -> Self;
 }
 
 /// The main IO backend for the resolver, which does cached requests network requests using the
 /// [`RegistryClient`] and [`DistributionDatabase`].
-pub struct DefaultResolverProvider<'a, Context: BuildContext + Send + Sync> {
+pub struct DefaultResolverProvider<'a, Context: BuildContext> {
     /// The [`DistributionDatabase`] used to build source distributions.
     fetcher: DistributionDatabase<'a, Context>,
-    /// Allow moving the parameters to `VersionMap::from_metadata` to a different thread.
-    inner: Arc<DefaultResolverProviderInner>,
-}
-
-pub struct DefaultResolverProviderInner {
-    /// The [`RegistryClient`] used to query the index.
-    client: RegistryClient,
     /// These are the entries from `--find-links` that act as overrides for index responses.
     flat_index: FlatIndex,
     tags: Tags,
     python_requirement: PythonRequirement,
-    exclude_newer: Option<DateTime<Utc>>,
+    allowed_yanks: AllowedYanks,
+    hasher: HashStrategy,
+    exclude_newer: Option<ExcludeNewer>,
     no_binary: NoBinary,
+    no_build: NoBuild,
 }
 
-impl<'a, Context: BuildContext + Send + Sync> Deref for DefaultResolverProvider<'a, Context> {
-    type Target = DefaultResolverProviderInner;
-
-    fn deref(&self) -> &Self::Target {
-        self.inner.as_ref()
-    }
-}
-
-impl<'a, Context: BuildContext + Send + Sync> DefaultResolverProvider<'a, Context> {
+impl<'a, Context: BuildContext> DefaultResolverProvider<'a, Context> {
     /// Reads the flat index entries and builds the provider.
     #[allow(clippy::too_many_arguments)]
     pub fn new(
-        client: &'a RegistryClient,
         fetcher: DistributionDatabase<'a, Context>,
         flat_index: &'a FlatIndex,
         tags: &'a Tags,
         python_requirement: PythonRequirement,
-        exclude_newer: Option<DateTime<Utc>>,
+        allowed_yanks: AllowedYanks,
+        hasher: &'a HashStrategy,
+        exclude_newer: Option<ExcludeNewer>,
         no_binary: &'a NoBinary,
+        no_build: &'a NoBuild,
     ) -> Self {
         Self {
             fetcher,
-            inner: Arc::new(DefaultResolverProviderInner {
-                client: client.clone(),
-                flat_index: flat_index.clone(),
-                tags: tags.clone(),
-                python_requirement,
-                exclude_newer,
-                no_binary: no_binary.clone(),
-            }),
+            flat_index: flat_index.clone(),
+            tags: tags.clone(),
+            python_requirement,
+            allowed_yanks,
+            hasher: hasher.clone(),
+            exclude_newer,
+            no_binary: no_binary.clone(),
+            no_build: no_build.clone(),
         }
     }
 }
 
-impl<'a, Context: BuildContext + Send + Sync> ResolverProvider
-    for DefaultResolverProvider<'a, Context>
-{
+impl<'a, Context: BuildContext> ResolverProvider for DefaultResolverProvider<'a, Context> {
     /// Make a "Simple API" request for the package and convert the result to a [`VersionMap`].
     async fn get_package_versions<'io>(
         &'io self,
         package_name: &'io PackageName,
     ) -> PackageVersionsResult {
-        let result = self.client.simple(package_name).await;
+        let result = self
+            .fetcher
+            .client()
+            .managed(|client| client.simple(package_name))
+            .await;
 
-        // If the "Simple API" request was successful, convert to `VersionMap` on the Tokio
-        // threadpool, since it can be slow.
         match result {
-            Ok((index, metadata)) => {
-                let self_send = self.inner.clone();
-                let package_name_owned = package_name.clone();
-                Ok(tokio::task::spawn_blocking(move || {
-                    VersionsResponse::Found(VersionMap::from_metadata(
-                        metadata,
-                        &package_name_owned,
-                        &index,
-                        &self_send.tags,
-                        &self_send.python_requirement,
-                        self_send.exclude_newer.as_ref(),
-                        self_send.flat_index.get(&package_name_owned).cloned(),
-                        &self_send.no_binary,
-                    ))
-                })
-                .await
-                .expect("Tokio executor failed, was there a panic?"))
-            }
+            Ok(results) => Ok(VersionsResponse::Found(
+                results
+                    .into_iter()
+                    .map(|(index, metadata)| {
+                        VersionMap::from_metadata(
+                            metadata,
+                            package_name,
+                            &index,
+                            &self.tags,
+                            &self.python_requirement,
+                            &self.allowed_yanks,
+                            &self.hasher,
+                            self.exclude_newer.as_ref(),
+                            self.flat_index.get(package_name).cloned(),
+                            &self.no_binary,
+                            &self.no_build,
+                        )
+                    })
+                    .collect(),
+            )),
             Err(err) => match err.into_kind() {
                 uv_client::ErrorKind::PackageNotFound(_) => {
                     if let Some(flat_index) = self.flat_index.get(package_name).cloned() {
-                        Ok(VersionsResponse::Found(VersionMap::from(flat_index)))
+                        Ok(VersionsResponse::Found(vec![VersionMap::from(flat_index)]))
                     } else {
                         Ok(VersionsResponse::NotFound)
                     }
                 }
                 uv_client::ErrorKind::NoIndex(_) => {
                     if let Some(flat_index) = self.flat_index.get(package_name).cloned() {
-                        Ok(VersionsResponse::Found(VersionMap::from(flat_index)))
+                        Ok(VersionsResponse::Found(vec![VersionMap::from(flat_index)]))
                     } else if self.flat_index.offline() {
                         Ok(VersionsResponse::Offline)
                     } else {
                         Ok(VersionsResponse::NoIndex)
                     }
                 }
                 uv_client::ErrorKind::Offline(_) => {
                     if let Some(flat_index) = self.flat_index.get(package_name).cloned() {
-                        Ok(VersionsResponse::Found(VersionMap::from(flat_index)))
+                        Ok(VersionsResponse::Found(vec![VersionMap::from(flat_index)]))
                     } else {
                         Ok(VersionsResponse::Offline)
                     }
                 }
                 kind => Err(kind.into()),
             },
         }
     }
 
+    /// Fetch the metadata for a distribution, building it if necessary.
     async fn get_or_build_wheel_metadata<'io>(&'io self, dist: &'io Dist) -> WheelMetadataResult {
-        self.fetcher.get_or_build_wheel_metadata(dist).await
+        match self
+            .fetcher
+            .get_or_build_wheel_metadata(dist, self.hasher.get(dist))
+            .await
+        {
+            Ok(metadata) => Ok(MetadataResponse::Found(metadata)),
+            Err(err) => match err {
+                uv_distribution::Error::Client(client) => match client.into_kind() {
+                    uv_client::ErrorKind::Offline(_) => Ok(MetadataResponse::Offline),
+                    uv_client::ErrorKind::MetadataParseError(_, _, err) => {
+                        Ok(MetadataResponse::InvalidMetadata(err))
+                    }
+                    uv_client::ErrorKind::DistInfo(err) => {
+                        Ok(MetadataResponse::InvalidStructure(Box::new(err)))
+                    }
+                    kind => Err(uv_client::Error::from(kind).into()),
+                },
+                uv_distribution::Error::VersionMismatch { .. } => {
+                    Ok(MetadataResponse::InconsistentMetadata(Box::new(err)))
+                }
+                uv_distribution::Error::NameMismatch { .. } => {
+                    Ok(MetadataResponse::InconsistentMetadata(Box::new(err)))
+                }
+                uv_distribution::Error::Metadata(err) => {
+                    Ok(MetadataResponse::InvalidMetadata(Box::new(err)))
+                }
+                uv_distribution::Error::DistInfo(err) => {
+                    Ok(MetadataResponse::InvalidStructure(Box::new(err)))
+                }
+                err => Err(err),
+            },
+        }
     }
 
     fn index_locations(&self) -> &IndexLocations {
         self.fetcher.index_locations()
     }
 
     /// Set the [`uv_distribution::Reporter`] to use for this installer.
```

### Comparing `uv-0.1.9/crates/uv-resolver/src/resolver/reporter.rs` & `uv-0.2.0/crates/uv-resolver/src/resolver/reporter.rs`

 * *Files 10% similar despite different names*

```diff
@@ -1,48 +1,48 @@
 use std::sync::Arc;
 
 use url::Url;
 
-use distribution_types::{SourceDist, VersionOrUrl};
+use distribution_types::{BuildableSource, VersionOrUrlRef};
 use uv_normalize::PackageName;
 
 pub type BuildId = usize;
 
 pub trait Reporter: Send + Sync {
     /// Callback to invoke when a dependency is resolved.
-    fn on_progress(&self, name: &PackageName, version: VersionOrUrl);
+    fn on_progress(&self, name: &PackageName, version: &VersionOrUrlRef);
 
     /// Callback to invoke when the resolution is complete.
     fn on_complete(&self);
 
     /// Callback to invoke when a source distribution build is kicked off.
-    fn on_build_start(&self, dist: &SourceDist) -> usize;
+    fn on_build_start(&self, source: &BuildableSource) -> usize;
 
     /// Callback to invoke when a source distribution build is complete.
-    fn on_build_complete(&self, dist: &SourceDist, id: usize);
+    fn on_build_complete(&self, source: &BuildableSource, id: usize);
 
     /// Callback to invoke when a repository checkout begins.
     fn on_checkout_start(&self, url: &Url, rev: &str) -> usize;
 
     /// Callback to invoke when a repository checkout completes.
     fn on_checkout_complete(&self, url: &Url, rev: &str, index: usize);
 }
 
 /// A facade for converting from [`Reporter`] to [`uv_distribution::Reporter`].
 pub(crate) struct Facade {
     pub(crate) reporter: Arc<dyn Reporter>,
 }
 
 impl uv_distribution::Reporter for Facade {
-    fn on_build_start(&self, dist: &SourceDist) -> usize {
-        self.reporter.on_build_start(dist)
+    fn on_build_start(&self, source: &BuildableSource) -> usize {
+        self.reporter.on_build_start(source)
     }
 
-    fn on_build_complete(&self, dist: &SourceDist, id: usize) {
-        self.reporter.on_build_complete(dist, id);
+    fn on_build_complete(&self, source: &BuildableSource, id: usize) {
+        self.reporter.on_build_complete(source, id);
     }
 
     fn on_checkout_start(&self, url: &Url, rev: &str) -> usize {
         self.reporter.on_checkout_start(url, rev)
     }
 
     fn on_checkout_complete(&self, url: &Url, rev: &str, index: usize) {
```

### Comparing `uv-0.1.9/crates/uv-resolver/src/version_map.rs` & `uv-0.2.0/crates/uv-resolver/src/version_map.rs`

 * *Files 26% similar despite different names*

```diff
@@ -1,45 +1,61 @@
 use std::collections::btree_map::{BTreeMap, Entry};
 use std::sync::OnceLock;
 
-use chrono::{DateTime, Utc};
-use tracing::{instrument, warn};
-
-use distribution_filename::DistFilename;
-use distribution_types::{Dist, IncompatibleWheel, IndexUrl, PrioritizedDist, WheelCompatibility};
-use pep440_rs::Version;
-use platform_tags::Tags;
-use pypi_types::Hashes;
 use rkyv::{de::deserializers::SharedDeserializeMap, Deserialize};
-use uv_client::{FlatDistributions, OwnedArchive, SimpleMetadata, VersionFiles};
+use rustc_hash::FxHashSet;
+use tracing::instrument;
+
+use distribution_filename::{DistFilename, WheelFilename};
+use distribution_types::{
+    HashComparison, IncompatibleSource, IncompatibleWheel, IndexUrl, PrioritizedDist,
+    RegistryBuiltWheel, RegistrySourceDist, SourceDistCompatibility, WheelCompatibility,
+};
+use pep440_rs::{Version, VersionSpecifiers};
+use platform_tags::{TagCompatibility, Tags};
+use pypi_types::{HashDigest, Yanked};
+use uv_client::{OwnedArchive, SimpleMetadata, VersionFiles};
+use uv_configuration::{NoBinary, NoBuild};
 use uv_normalize::PackageName;
-use uv_traits::NoBinary;
+use uv_types::HashStrategy;
 use uv_warnings::warn_user_once;
 
-use crate::python_requirement::PythonRequirement;
+use crate::flat_index::FlatDistributions;
+use crate::{python_requirement::PythonRequirement, yanks::AllowedYanks, ExcludeNewer};
 
 /// A map from versions to distributions.
 #[derive(Debug)]
 pub struct VersionMap {
     inner: VersionMapInner,
 }
 
 impl VersionMap {
     /// Initialize a [`VersionMap`] from the given metadata.
+    ///
+    /// Note it is possible for files to have a different yank status per PEP 592 but in the official
+    /// PyPI warehouse this cannot happen.
+    ///
+    /// Here, we track if each file is yanked separately. If a release is partially yanked, the
+    /// unyanked distributions _can_ be used.
+    ///
+    /// PEP 592: <https://peps.python.org/pep-0592/#warehouse-pypi-implementation-notes>
     #[instrument(skip_all, fields(package_name))]
     #[allow(clippy::too_many_arguments)]
     pub(crate) fn from_metadata(
         simple_metadata: OwnedArchive<SimpleMetadata>,
         package_name: &PackageName,
         index: &IndexUrl,
         tags: &Tags,
         python_requirement: &PythonRequirement,
-        exclude_newer: Option<&DateTime<Utc>>,
+        allowed_yanks: &AllowedYanks,
+        hasher: &HashStrategy,
+        exclude_newer: Option<&ExcludeNewer>,
         flat_index: Option<FlatDistributions>,
         no_binary: &NoBinary,
+        no_build: &NoBuild,
     ) -> Self {
         let mut map = BTreeMap::new();
         // Create stubs for each entry in simple metadata. The full conversion
         // from a `VersionFiles` to a PrioritizedDist for each version
         // isn't done until that specific version is requested.
         for (datum_index, datum) in simple_metadata.iter().enumerate() {
             let version: Version = datum
@@ -81,23 +97,37 @@
         }
         // Check if binaries are allowed for this package.
         let no_binary = match no_binary {
             NoBinary::None => false,
             NoBinary::All => true,
             NoBinary::Packages(packages) => packages.contains(package_name),
         };
-        VersionMap {
+        // Check if source distributions are allowed for this package.
+        let no_build = match no_build {
+            NoBuild::None => false,
+            NoBuild::All => true,
+            NoBuild::Packages(packages) => packages.contains(package_name),
+        };
+        let allowed_yanks = allowed_yanks
+            .allowed_versions(package_name)
+            .cloned()
+            .unwrap_or_default();
+        let required_hashes = hasher.get_package(package_name).digests().to_vec();
+        Self {
             inner: VersionMapInner::Lazy(VersionMapLazy {
                 map,
                 simple_metadata,
                 no_binary,
+                no_build,
                 index: index.clone(),
                 tags: tags.clone(),
                 python_requirement: python_requirement.clone(),
                 exclude_newer: exclude_newer.copied(),
+                allowed_yanks,
+                required_hashes,
             }),
         }
     }
 
     /// Return the [`DistFile`] for the given version, if any.
     pub(crate) fn get(&self, version: &Version) -> Option<&PrioritizedDist> {
         self.get_with_version(version).map(|(_version, dist)| dist)
@@ -106,31 +136,33 @@
     /// Return the [`DistFile`] and the `Version` from the map for the given
     /// version, if any.
     ///
     /// This is useful when you depend on access to the specific `Version`
     /// stored in this map. For example, the versions `1.2.0` and `1.2` are
     /// semantically equivalent, but when converted to strings, they are
     /// distinct.
-    pub(crate) fn get_with_version<'a>(
-        &'a self,
+    pub(crate) fn get_with_version(
+        &self,
         version: &Version,
-    ) -> Option<(&'a Version, &'a PrioritizedDist)> {
+    ) -> Option<(&Version, &PrioritizedDist)> {
         match self.inner {
             VersionMapInner::Eager(ref map) => map.get_key_value(version),
             VersionMapInner::Lazy(ref lazy) => lazy.get_with_version(version),
         }
     }
 
     /// Return an iterator over the versions and distributions.
     ///
     /// Note that the value returned in this iterator is a [`VersionMapDist`],
     /// which can be used to lazily request a [`CompatibleDist`]. This is
     /// useful in cases where one can skip materializing a full distribution
     /// for each version.
-    pub(crate) fn iter(&self) -> impl DoubleEndedIterator<Item = (&Version, VersionMapDistHandle)> {
+    pub(crate) fn iter(
+        &self,
+    ) -> impl DoubleEndedIterator<Item = (&Version, VersionMapDistHandle)> + ExactSizeIterator {
         match self.inner {
             VersionMapInner::Eager(ref map) => {
                 either::Either::Left(map.iter().map(|(version, dist)| {
                     let version_map_dist = VersionMapDistHandle {
                         inner: VersionMapDistHandleInner::Eager(dist),
                     };
                     (version, version_map_dist)
@@ -144,24 +176,18 @@
                     (version, version_map_dist)
                 }))
             }
         }
     }
 
     /// Return the [`Hashes`] for the given version, if any.
-    pub(crate) fn hashes(&self, version: &Version) -> Vec<Hashes> {
+    pub(crate) fn hashes(&self, version: &Version) -> Option<Vec<HashDigest>> {
         match self.inner {
-            VersionMapInner::Eager(ref map) => map
-                .get(version)
-                .map(|file| file.hashes().to_vec())
-                .unwrap_or_default(),
-            VersionMapInner::Lazy(ref lazy) => lazy
-                .get(version)
-                .map(|file| file.hashes().to_vec())
-                .unwrap_or_default(),
+            VersionMapInner::Eager(ref map) => map.get(version).map(|file| file.hashes().to_vec()),
+            VersionMapInner::Lazy(ref lazy) => lazy.get(version).map(|file| file.hashes().to_vec()),
         }
     }
 
     /// Returns the total number of distinct versions in this map.
     ///
     /// Note that this may include versions of distributions that are not
     /// usable in the current environment.
@@ -169,17 +195,26 @@
         match self.inner {
             VersionMapInner::Eager(ref map) => map.len(),
             VersionMapInner::Lazy(VersionMapLazy { ref map, .. }) => map.len(),
         }
     }
 }
 
+impl Default for VersionMap {
+    /// Create an empty version map.
+    fn default() -> Self {
+        Self {
+            inner: VersionMapInner::Eager(BTreeMap::default()),
+        }
+    }
+}
+
 impl From<FlatDistributions> for VersionMap {
     fn from(flat_index: FlatDistributions) -> Self {
-        VersionMap {
+        Self {
             inner: VersionMapInner::Eager(flat_index.into()),
         }
     }
 }
 
 impl From<BTreeMap<Version, PrioritizedDist>> for VersionMap {
     fn from(value: BTreeMap<Version, PrioritizedDist>) -> Self {
@@ -252,25 +287,31 @@
     /// A map from version to possibly-initialized distribution.
     map: BTreeMap<Version, LazyPrioritizedDist>,
     /// The raw simple metadata from which `PrioritizedDist`s should
     /// be constructed.
     simple_metadata: OwnedArchive<SimpleMetadata>,
     /// When true, wheels aren't allowed.
     no_binary: bool,
+    /// When true, source dists aren't allowed.
+    no_build: bool,
     /// The URL of the index where this package came from.
     index: IndexUrl,
     /// The set of compatibility tags that determines whether a wheel is usable
     /// in the current environment.
     tags: Tags,
     /// The version of Python active in the current environment. This is used
     /// to determine whether a package's Python version constraint (if one
     /// exists) is satisfied or not.
     python_requirement: PythonRequirement,
     /// Whether files newer than this timestamp should be excluded or not.
-    exclude_newer: Option<DateTime<Utc>>,
+    exclude_newer: Option<ExcludeNewer>,
+    /// Which yanked versions are allowed
+    allowed_yanks: FxHashSet<Version>,
+    /// The hashes of allowed distributions.
+    required_hashes: Vec<HashDigest>,
 }
 
 impl VersionMapLazy {
     /// Returns the distribution for the given version, if it exists.
     fn get(&self, version: &Version) -> Option<&PrioritizedDist> {
         self.get_with_version(version)
             .map(|(_, prioritized_dist)| prioritized_dist)
@@ -315,98 +356,220 @@
                 .datum(simple.datum_index)
                 .expect("index to lazy dist is correct")
                 .files
                 .deserialize(&mut SharedDeserializeMap::new())
                 .expect("archived version files should deserialize");
             let mut priority_dist = init.cloned().unwrap_or_default();
             for (filename, file) in files.all() {
-                if let Some(exclude_newer) = self.exclude_newer {
+                // Support resolving as if it were an earlier timestamp, at least as long files have
+                // upload time information.
+                let (excluded, upload_time) = if let Some(exclude_newer) = self.exclude_newer {
                     match file.upload_time_utc_ms.as_ref() {
                         Some(&upload_time) if upload_time >= exclude_newer.timestamp_millis() => {
-                            priority_dist.set_exclude_newer();
-                            continue;
+                            (true, Some(upload_time))
                         }
                         None => {
                             warn_user_once!(
                                 "{} is missing an upload date, but user provided: {exclude_newer}",
                                 file.filename,
                             );
-                            priority_dist.set_exclude_newer();
-                            continue;
+                            (true, None)
                         }
-                        _ => {}
+                        _ => (false, None),
                     }
-                }
-                let yanked = file.yanked.clone().unwrap_or_default();
+                } else {
+                    (false, None)
+                };
+
+                // Prioritize amongst all available files.
+                let version = filename.version().clone();
                 let requires_python = file.requires_python.clone();
-                let hash = file.hashes.clone();
+                let yanked = file.yanked.clone();
+                let hashes = file.hashes.clone();
                 match filename {
                     DistFilename::WheelFilename(filename) => {
-                        // Determine a compatibility for the wheel based on tags
-                        let mut compatibility =
-                            WheelCompatibility::from(filename.compatibility(&self.tags));
-
-                        if compatibility.is_compatible() {
-                            // Check for Python version incompatibility
-                            if let Some(ref requires_python) = file.requires_python {
-                                if !requires_python.contains(self.python_requirement.target()) {
-                                    compatibility = WheelCompatibility::Incompatible(
-                                        IncompatibleWheel::RequiresPython,
-                                    );
-                                }
-                            }
-
-                            // Mark all wheels as incompatibility when binaries are disabled
-                            if self.no_binary {
-                                compatibility =
-                                    WheelCompatibility::Incompatible(IncompatibleWheel::NoBinary);
-                            }
-                        };
-
-                        let dist = Dist::from_registry(
-                            DistFilename::WheelFilename(filename),
-                            file,
-                            self.index.clone(),
-                        );
-                        priority_dist.insert_built(
-                            dist,
+                        let compatibility = self.wheel_compatibility(
+                            &filename,
+                            &version,
                             requires_python,
+                            &hashes,
                             yanked,
-                            Some(hash),
-                            compatibility,
+                            excluded,
+                            upload_time,
                         );
+                        let dist = RegistryBuiltWheel {
+                            filename,
+                            file: Box::new(file),
+                            index: self.index.clone(),
+                        };
+                        priority_dist.insert_built(dist, hashes, compatibility);
                     }
                     DistFilename::SourceDistFilename(filename) => {
-                        let dist = Dist::from_registry(
-                            DistFilename::SourceDistFilename(filename),
-                            file,
-                            self.index.clone(),
+                        let compatibility = self.source_dist_compatibility(
+                            &version,
+                            requires_python,
+                            &hashes,
+                            yanked,
+                            excluded,
+                            upload_time,
                         );
-                        priority_dist.insert_source(dist, requires_python, yanked, Some(hash));
+                        let dist = RegistrySourceDist {
+                            name: filename.name.clone(),
+                            version: filename.version.clone(),
+                            file: Box::new(file),
+                            index: self.index.clone(),
+                            wheels: vec![],
+                        };
+                        priority_dist.insert_source(dist, hashes, compatibility);
                     }
                 }
             }
             if priority_dist.is_empty() {
                 None
             } else {
                 Some(priority_dist)
             }
         };
         simple.dist.get_or_init(get_or_init).as_ref()
     }
+
+    #[allow(clippy::too_many_arguments)]
+    fn source_dist_compatibility(
+        &self,
+        version: &Version,
+        requires_python: Option<VersionSpecifiers>,
+        hashes: &[HashDigest],
+        yanked: Option<Yanked>,
+        excluded: bool,
+        upload_time: Option<i64>,
+    ) -> SourceDistCompatibility {
+        // Check if builds are disabled
+        if self.no_build {
+            return SourceDistCompatibility::Incompatible(IncompatibleSource::NoBuild);
+        }
+
+        // Check if after upload time cutoff
+        if excluded {
+            return SourceDistCompatibility::Incompatible(IncompatibleSource::ExcludeNewer(
+                upload_time,
+            ));
+        }
+
+        // Check if yanked
+        if let Some(yanked) = yanked {
+            if yanked.is_yanked() && !self.allowed_yanks.contains(version) {
+                return SourceDistCompatibility::Incompatible(IncompatibleSource::Yanked(yanked));
+            }
+        }
+
+        // Check if Python version is supported
+        // Source distributions must meet both the _target_ Python version and the
+        // _installed_ Python version (to build successfully)
+        if let Some(requires_python) = requires_python {
+            if !requires_python.contains(self.python_requirement.target())
+                || !requires_python.contains(self.python_requirement.installed())
+            {
+                return SourceDistCompatibility::Incompatible(IncompatibleSource::RequiresPython(
+                    requires_python,
+                ));
+            }
+        }
+
+        // Check if hashes line up. If hashes aren't required, they're considered matching.
+        let hash = if self.required_hashes.is_empty() {
+            HashComparison::Matched
+        } else {
+            if hashes.is_empty() {
+                HashComparison::Missing
+            } else if hashes
+                .iter()
+                .any(|hash| self.required_hashes.contains(hash))
+            {
+                HashComparison::Matched
+            } else {
+                HashComparison::Mismatched
+            }
+        };
+
+        SourceDistCompatibility::Compatible(hash)
+    }
+
+    #[allow(clippy::too_many_arguments)]
+    fn wheel_compatibility(
+        &self,
+        filename: &WheelFilename,
+        version: &Version,
+        requires_python: Option<VersionSpecifiers>,
+        hashes: &[HashDigest],
+        yanked: Option<Yanked>,
+        excluded: bool,
+        upload_time: Option<i64>,
+    ) -> WheelCompatibility {
+        // Check if binaries are disabled
+        if self.no_binary {
+            return WheelCompatibility::Incompatible(IncompatibleWheel::NoBinary);
+        }
+
+        // Check if after upload time cutoff
+        if excluded {
+            return WheelCompatibility::Incompatible(IncompatibleWheel::ExcludeNewer(upload_time));
+        }
+
+        // Check if yanked
+        if let Some(yanked) = yanked {
+            if yanked.is_yanked() && !self.allowed_yanks.contains(version) {
+                return WheelCompatibility::Incompatible(IncompatibleWheel::Yanked(yanked));
+            }
+        }
+
+        // Check for a Python version incompatibility`
+        if let Some(requires_python) = requires_python {
+            if !requires_python.contains(self.python_requirement.target()) {
+                return WheelCompatibility::Incompatible(IncompatibleWheel::RequiresPython(
+                    requires_python,
+                ));
+            }
+        }
+
+        // Determine a compatibility for the wheel based on tags.
+        let priority = match filename.compatibility(&self.tags) {
+            TagCompatibility::Incompatible(tag) => {
+                return WheelCompatibility::Incompatible(IncompatibleWheel::Tag(tag))
+            }
+            TagCompatibility::Compatible(priority) => priority,
+        };
+
+        // Check if hashes line up. If hashes aren't required, they're considered matching.
+        let hash = if self.required_hashes.is_empty() {
+            HashComparison::Matched
+        } else {
+            if hashes.is_empty() {
+                HashComparison::Missing
+            } else if hashes
+                .iter()
+                .any(|hash| self.required_hashes.contains(hash))
+            {
+                HashComparison::Matched
+            } else {
+                HashComparison::Mismatched
+            }
+        };
+
+        WheelCompatibility::Compatible(hash, priority)
+    }
 }
 
 /// Represents a possibly initialized [`PrioritizedDist`] for
 /// a single version of a package.
 #[derive(Debug)]
 enum LazyPrioritizedDist {
     /// Represents a eagerly constructed distribution from a
     /// `FlatDistributions`.
     OnlyFlat(PrioritizedDist),
-    /// Represents a lazyily constructed distribution from an index into a
+    /// Represents a lazily constructed distribution from an index into a
     /// `VersionFiles` from `SimpleMetadata`.
     OnlySimple(SimplePrioritizedDist),
     /// Combines the above. This occurs when we have data from both a flat
     /// distribution and a simple distribution.
     Both {
         flat: PrioritizedDist,
         simple: SimplePrioritizedDist,
```

### Comparing `uv-0.1.9/crates/uv-resolver/tests/resolver.rs` & `uv-0.2.0/crates/uv-resolver/tests/resolver.rs`

 * *Files 9% similar despite different names*

```diff
@@ -6,32 +6,40 @@
 use std::path::{Path, PathBuf};
 use std::str::FromStr;
 
 use anyhow::Result;
 use chrono::{DateTime, Utc};
 use once_cell::sync::Lazy;
 
-use distribution_types::{IndexLocations, Resolution, SourceDist};
-use pep508_rs::{MarkerEnvironment, Requirement, StringVersion};
-use platform_host::{Arch, Os, Platform};
-use platform_tags::Tags;
+use distribution_types::{IndexLocations, Requirement, Resolution, SourceDist};
+use pep508_rs::{MarkerEnvironment, MarkerEnvironmentBuilder};
+use platform_tags::{Arch, Os, Platform, Tags};
 use uv_cache::Cache;
-use uv_client::{FlatIndex, RegistryClientBuilder};
-use uv_interpreter::{Interpreter, Virtualenv};
+use uv_client::RegistryClientBuilder;
+use uv_configuration::{
+    BuildKind, Concurrency, Constraints, NoBinary, NoBuild, Overrides, SetupPyStrategy,
+};
+use uv_distribution::DistributionDatabase;
+use uv_interpreter::{find_default_interpreter, Interpreter, PythonEnvironment};
 use uv_resolver::{
-    DisplayResolutionGraph, InMemoryIndex, Manifest, Options, OptionsBuilder, PreReleaseMode,
-    ResolutionGraph, ResolutionMode, Resolver,
+    DisplayResolutionGraph, ExcludeNewer, Exclusions, FlatIndex, InMemoryIndex, Manifest, Options,
+    OptionsBuilder, PreReleaseMode, Preference, PythonRequirement, ResolutionGraph, ResolutionMode,
+    Resolver,
+};
+use uv_types::{
+    BuildContext, BuildIsolation, EmptyInstalledPackages, HashStrategy, SourceBuildTrait,
 };
-use uv_traits::{BuildContext, BuildKind, NoBinary, NoBuild, SetupPyStrategy, SourceBuildTrait};
 
 // Exclude any packages uploaded after this date.
-static EXCLUDE_NEWER: Lazy<DateTime<Utc>> = Lazy::new(|| {
-    DateTime::parse_from_rfc3339("2023-11-18T12:00:00Z")
-        .unwrap()
-        .with_timezone(&Utc)
+static EXCLUDE_NEWER: Lazy<ExcludeNewer> = Lazy::new(|| {
+    ExcludeNewer::from(
+        DateTime::parse_from_rfc3339("2023-11-18T12:00:00Z")
+            .unwrap()
+            .with_timezone(&Utc),
+    )
 });
 
 struct DummyContext {
     cache: Cache,
     interpreter: Interpreter,
     index_locations: IndexLocations,
 }
@@ -53,16 +61,16 @@
         &self.cache
     }
 
     fn interpreter(&self) -> &Interpreter {
         &self.interpreter
     }
 
-    fn base_python(&self) -> &Path {
-        panic!("The test should not need to build source distributions")
+    fn build_isolation(&self) -> BuildIsolation {
+        BuildIsolation::Isolated
     }
 
     fn no_build(&self) -> &NoBuild {
         &NoBuild::None
     }
 
     fn no_binary(&self) -> &NoBinary {
@@ -77,15 +85,15 @@
         &self.index_locations
     }
 
     async fn resolve<'a>(&'a self, _: &'a [Requirement]) -> Result<Resolution> {
         panic!("The test should not need to build source distributions")
     }
 
-    async fn install<'a>(&'a self, _: &'a Resolution, _: &'a Virtualenv) -> Result<()> {
+    async fn install<'a>(&'a self, _: &'a Resolution, _: &'a PythonEnvironment) -> Result<()> {
         panic!("The test should not need to build source distributions")
     }
 
     async fn setup_build<'a>(
         &'a self,
         _: &'a Path,
         _: Option<&'a Path>,
@@ -111,50 +119,58 @@
 
 async fn resolve(
     manifest: Manifest,
     options: Options,
     markers: &'static MarkerEnvironment,
     tags: &Tags,
 ) -> Result<ResolutionGraph> {
-    let client = RegistryClientBuilder::new(Cache::temp()?).build();
+    let cache = Cache::temp().unwrap().init().unwrap();
+    let client = RegistryClientBuilder::new(cache).build();
     let flat_index = FlatIndex::default();
     let index = InMemoryIndex::default();
-    let interpreter = Interpreter::artificial(
-        Platform::current()?,
-        markers.clone(),
-        PathBuf::from("/dev/null"),
-        PathBuf::from("/dev/null"),
-        PathBuf::from("/dev/null"),
-        PathBuf::from("/dev/null"),
-    );
-    let build_context = DummyContext::new(Cache::temp()?, interpreter.clone());
+    let real_interpreter = find_default_interpreter(&Cache::temp().unwrap())
+        .unwrap()
+        .expect("Python should be installed")
+        .into_interpreter();
+    let interpreter = Interpreter::artificial(real_interpreter.platform().clone(), markers.clone());
+    let python_requirement = PythonRequirement::from_marker_environment(&interpreter, markers);
+    let cache = Cache::temp().unwrap().init().unwrap();
+    let build_context = DummyContext::new(cache, interpreter.clone());
+    let hashes = HashStrategy::None;
+    let installed_packages = EmptyInstalledPackages;
+    let concurrency = Concurrency::default();
     let resolver = Resolver::new(
         manifest,
         options,
-        markers,
-        &interpreter,
+        &python_requirement,
+        Some(markers),
         tags,
-        &client,
         &flat_index,
         &index,
+        &hashes,
         &build_context,
+        installed_packages,
+        DistributionDatabase::new(&client, &build_context, concurrency.downloads),
     )?;
     Ok(resolver.resolve().await?)
 }
 
 macro_rules! assert_snapshot {
     ($value:expr, @$snapshot:literal) => {
         let snapshot = anstream::adapter::strip_str(&format!("{}", $value)).to_string();
         insta::assert_snapshot!(&snapshot, @$snapshot)
     };
 }
 
 #[tokio::test]
 async fn black() -> Result<()> {
-    let manifest = Manifest::simple(vec![Requirement::from_str("black<=23.9.1").unwrap()]);
+    let manifest = Manifest::simple(vec![Requirement::from_pep508(
+        pep508_rs::Requirement::from_str("black<=23.9.1").unwrap(),
+    )
+    .unwrap()]);
     let options = OptionsBuilder::new()
         .exclude_newer(Some(*EXCLUDE_NEWER))
         .build();
 
     let resolution = resolve(manifest, options, &MARKERS_311, &TAGS_311).await?;
 
     assert_snapshot!(DisplayResolutionGraph::from(&resolution), @r###"
@@ -172,17 +188,18 @@
     "###);
 
     Ok(())
 }
 
 #[tokio::test]
 async fn black_colorama() -> Result<()> {
-    let manifest = Manifest::simple(vec![
-        Requirement::from_str("black[colorama]<=23.9.1").unwrap()
-    ]);
+    let manifest = Manifest::simple(vec![Requirement::from_pep508(
+        pep508_rs::Requirement::from_str("black[colorama]<=23.9.1").unwrap(),
+    )
+    .unwrap()]);
     let options = OptionsBuilder::new()
         .exclude_newer(Some(*EXCLUDE_NEWER))
         .build();
 
     let resolution = resolve(manifest, options, &MARKERS_311, &TAGS_311).await?;
 
     assert_snapshot!(DisplayResolutionGraph::from(&resolution), @r###"
@@ -203,17 +220,18 @@
 
     Ok(())
 }
 
 /// Resolve Black with an invalid extra. The resolver should ignore the extra.
 #[tokio::test]
 async fn black_tensorboard() -> Result<()> {
-    let manifest = Manifest::simple(vec![
-        Requirement::from_str("black[tensorboard]<=23.9.1").unwrap()
-    ]);
+    let manifest = Manifest::simple(vec![Requirement::from_pep508(
+        pep508_rs::Requirement::from_str("black[tensorboard]<=23.9.1").unwrap(),
+    )
+    .unwrap()]);
     let options = OptionsBuilder::new()
         .exclude_newer(Some(*EXCLUDE_NEWER))
         .build();
 
     let resolution = resolve(manifest, options, &MARKERS_311, &TAGS_311).await?;
 
     assert_snapshot!(DisplayResolutionGraph::from(&resolution), @r###"
@@ -231,15 +249,18 @@
     "###);
 
     Ok(())
 }
 
 #[tokio::test]
 async fn black_python_310() -> Result<()> {
-    let manifest = Manifest::simple(vec![Requirement::from_str("black<=23.9.1").unwrap()]);
+    let manifest = Manifest::simple(vec![Requirement::from_pep508(
+        pep508_rs::Requirement::from_str("black<=23.9.1").unwrap(),
+    )
+    .unwrap()]);
     let options = OptionsBuilder::new()
         .exclude_newer(Some(*EXCLUDE_NEWER))
         .build();
 
     let resolution = resolve(manifest, options, &MARKERS_310, &TAGS_310).await?;
 
     assert_snapshot!(DisplayResolutionGraph::from(&resolution), @r###"
@@ -264,20 +285,28 @@
 }
 
 /// Resolve `black` with a constraint on `mypy-extensions`, to ensure that constraints are
 /// respected.
 #[tokio::test]
 async fn black_mypy_extensions() -> Result<()> {
     let manifest = Manifest::new(
-        vec![Requirement::from_str("black<=23.9.1").unwrap()],
-        vec![Requirement::from_str("mypy-extensions<0.4.4").unwrap()],
-        vec![],
+        vec![
+            Requirement::from_pep508(pep508_rs::Requirement::from_str("black<=23.9.1").unwrap())
+                .unwrap(),
+        ],
+        Constraints::from_requirements(vec![Requirement::from_pep508(
+            pep508_rs::Requirement::from_str("mypy-extensions<0.4.4").unwrap(),
+        )
+        .unwrap()]),
+        Overrides::default(),
         vec![],
         None,
         vec![],
+        Exclusions::default(),
+        vec![],
     );
     let options = OptionsBuilder::new()
         .exclude_newer(Some(*EXCLUDE_NEWER))
         .build();
 
     let resolution = resolve(manifest, options, &MARKERS_311, &TAGS_311).await?;
 
@@ -299,20 +328,28 @@
 }
 
 /// Resolve `black` with a constraint on `mypy-extensions[extra]`, to ensure that extras are
 /// ignored when resolving constraints.
 #[tokio::test]
 async fn black_mypy_extensions_extra() -> Result<()> {
     let manifest = Manifest::new(
-        vec![Requirement::from_str("black<=23.9.1").unwrap()],
-        vec![Requirement::from_str("mypy-extensions[extra]<0.4.4").unwrap()],
-        vec![],
+        vec![
+            Requirement::from_pep508(pep508_rs::Requirement::from_str("black<=23.9.1").unwrap())
+                .unwrap(),
+        ],
+        Constraints::from_requirements(vec![Requirement::from_pep508(
+            pep508_rs::Requirement::from_str("mypy-extensions[extra]<0.4.4").unwrap(),
+        )
+        .unwrap()]),
+        Overrides::default(),
         vec![],
         None,
         vec![],
+        Exclusions::default(),
+        vec![],
     );
     let options = OptionsBuilder::new()
         .exclude_newer(Some(*EXCLUDE_NEWER))
         .build();
 
     let resolution = resolve(manifest, options, &MARKERS_311, &TAGS_311).await?;
 
@@ -334,20 +371,28 @@
 }
 
 /// Resolve `black` with a redundant constraint on `flake8`, to ensure that constraints don't
 /// introduce new dependencies.
 #[tokio::test]
 async fn black_flake8() -> Result<()> {
     let manifest = Manifest::new(
-        vec![Requirement::from_str("black<=23.9.1").unwrap()],
-        vec![Requirement::from_str("flake8<1").unwrap()],
-        vec![],
+        vec![
+            Requirement::from_pep508(pep508_rs::Requirement::from_str("black<=23.9.1").unwrap())
+                .unwrap(),
+        ],
+        Constraints::from_requirements(vec![Requirement::from_pep508(
+            pep508_rs::Requirement::from_str("flake8<1").unwrap(),
+        )
+        .unwrap()]),
+        Overrides::default(),
         vec![],
         None,
         vec![],
+        Exclusions::default(),
+        vec![],
     );
     let options = OptionsBuilder::new()
         .exclude_newer(Some(*EXCLUDE_NEWER))
         .build();
 
     let resolution = resolve(manifest, options, &MARKERS_311, &TAGS_311).await?;
 
@@ -366,15 +411,18 @@
     "###);
 
     Ok(())
 }
 
 #[tokio::test]
 async fn black_lowest() -> Result<()> {
-    let manifest = Manifest::simple(vec![Requirement::from_str("black>21").unwrap()]);
+    let manifest = Manifest::simple(vec![Requirement::from_pep508(
+        pep508_rs::Requirement::from_str("black>21").unwrap(),
+    )
+    .unwrap()]);
     let options = OptionsBuilder::new()
         .resolution_mode(ResolutionMode::Lowest)
         .exclude_newer(Some(*EXCLUDE_NEWER))
         .build();
 
     let resolution = resolve(manifest, options, &MARKERS_311, &TAGS_311).await?;
 
@@ -393,15 +441,18 @@
     "###);
 
     Ok(())
 }
 
 #[tokio::test]
 async fn black_lowest_direct() -> Result<()> {
-    let manifest = Manifest::simple(vec![Requirement::from_str("black>21").unwrap()]);
+    let manifest = Manifest::simple(vec![Requirement::from_pep508(
+        pep508_rs::Requirement::from_str("black>21").unwrap(),
+    )
+    .unwrap()]);
     let options = OptionsBuilder::new()
         .resolution_mode(ResolutionMode::LowestDirect)
         .exclude_newer(Some(*EXCLUDE_NEWER))
         .build();
 
     let resolution = resolve(manifest, options, &MARKERS_311, &TAGS_311).await?;
 
@@ -421,21 +472,26 @@
 
     Ok(())
 }
 
 #[tokio::test]
 async fn black_respect_preference() -> Result<()> {
     let manifest = Manifest::new(
-        vec![Requirement::from_str("black<=23.9.1").unwrap()],
-        vec![],
-        vec![],
-        vec![Requirement::from_str("black==23.9.0").unwrap()],
+        vec![Requirement::from_pep508(pep508_rs::Requirement::from_str("black<=23.9.1")?).unwrap()],
+        Constraints::default(),
+        Overrides::default(),
+        vec![Preference::from_requirement(
+            Requirement::from_pep508(pep508_rs::Requirement::from_str("black==23.9.0")?).unwrap(),
+        )],
         None,
         vec![],
+        Exclusions::default(),
+        vec![],
     );
+
     let options = OptionsBuilder::new()
         .exclude_newer(Some(*EXCLUDE_NEWER))
         .build();
 
     let resolution = resolve(manifest, options, &MARKERS_311, &TAGS_311).await?;
 
     assert_snapshot!(DisplayResolutionGraph::from(&resolution), @r###"
@@ -454,20 +510,24 @@
 
     Ok(())
 }
 
 #[tokio::test]
 async fn black_ignore_preference() -> Result<()> {
     let manifest = Manifest::new(
-        vec![Requirement::from_str("black<=23.9.1").unwrap()],
-        vec![],
-        vec![],
-        vec![Requirement::from_str("black==23.9.2").unwrap()],
+        vec![Requirement::from_pep508(pep508_rs::Requirement::from_str("black<=23.9.1")?).unwrap()],
+        Constraints::default(),
+        Overrides::default(),
+        vec![Preference::from_requirement(
+            Requirement::from_pep508(pep508_rs::Requirement::from_str("black==23.9.2")?).unwrap(),
+        )],
         None,
         vec![],
+        Exclusions::default(),
+        vec![],
     );
     let options = OptionsBuilder::new()
         .exclude_newer(Some(*EXCLUDE_NEWER))
         .build();
 
     let resolution = resolve(manifest, options, &MARKERS_311, &TAGS_311).await?;
 
@@ -486,15 +546,18 @@
     "###);
 
     Ok(())
 }
 
 #[tokio::test]
 async fn black_disallow_prerelease() -> Result<()> {
-    let manifest = Manifest::simple(vec![Requirement::from_str("black<=20.0").unwrap()]);
+    let manifest = Manifest::simple(vec![Requirement::from_pep508(
+        pep508_rs::Requirement::from_str("black<=20.0").unwrap(),
+    )
+    .unwrap()]);
     let options = OptionsBuilder::new()
         .prerelease_mode(PreReleaseMode::Disallow)
         .exclude_newer(Some(*EXCLUDE_NEWER))
         .build();
 
     let err = resolve(manifest, options, &MARKERS_311, &TAGS_311)
         .await
@@ -507,15 +570,18 @@
     "###);
 
     Ok(())
 }
 
 #[tokio::test]
 async fn black_allow_prerelease_if_necessary() -> Result<()> {
-    let manifest = Manifest::simple(vec![Requirement::from_str("black<=20.0").unwrap()]);
+    let manifest = Manifest::simple(vec![Requirement::from_pep508(
+        pep508_rs::Requirement::from_str("black<=20.0").unwrap(),
+    )
+    .unwrap()]);
     let options = OptionsBuilder::new()
         .prerelease_mode(PreReleaseMode::IfNecessary)
         .exclude_newer(Some(*EXCLUDE_NEWER))
         .build();
 
     let err = resolve(manifest, options, &MARKERS_311, &TAGS_311)
         .await
@@ -528,15 +594,18 @@
     "###);
 
     Ok(())
 }
 
 #[tokio::test]
 async fn pylint_disallow_prerelease() -> Result<()> {
-    let manifest = Manifest::simple(vec![Requirement::from_str("pylint==2.3.0").unwrap()]);
+    let manifest = Manifest::simple(vec![Requirement::from_pep508(
+        pep508_rs::Requirement::from_str("pylint==2.3.0").unwrap(),
+    )
+    .unwrap()]);
     let options = OptionsBuilder::new()
         .prerelease_mode(PreReleaseMode::Disallow)
         .exclude_newer(Some(*EXCLUDE_NEWER))
         .build();
 
     let resolution = resolve(manifest, options, &MARKERS_311, &TAGS_311).await?;
 
@@ -551,15 +620,18 @@
     "###);
 
     Ok(())
 }
 
 #[tokio::test]
 async fn pylint_allow_prerelease() -> Result<()> {
-    let manifest = Manifest::simple(vec![Requirement::from_str("pylint==2.3.0").unwrap()]);
+    let manifest = Manifest::simple(vec![Requirement::from_pep508(
+        pep508_rs::Requirement::from_str("pylint==2.3.0").unwrap(),
+    )
+    .unwrap()]);
     let options = OptionsBuilder::new()
         .prerelease_mode(PreReleaseMode::Allow)
         .exclude_newer(Some(*EXCLUDE_NEWER))
         .build();
 
     let resolution = resolve(manifest, options, &MARKERS_311, &TAGS_311).await?;
 
@@ -575,16 +647,18 @@
 
     Ok(())
 }
 
 #[tokio::test]
 async fn pylint_allow_explicit_prerelease_without_marker() -> Result<()> {
     let manifest = Manifest::simple(vec![
-        Requirement::from_str("pylint==2.3.0").unwrap(),
-        Requirement::from_str("isort>=5.0.0").unwrap(),
+        Requirement::from_pep508(pep508_rs::Requirement::from_str("pylint==2.3.0").unwrap())
+            .unwrap(),
+        Requirement::from_pep508(pep508_rs::Requirement::from_str("isort>=5.0.0").unwrap())
+            .unwrap(),
     ]);
     let options = OptionsBuilder::new()
         .prerelease_mode(PreReleaseMode::Explicit)
         .exclude_newer(Some(*EXCLUDE_NEWER))
         .build();
 
     let resolution = resolve(manifest, options, &MARKERS_311, &TAGS_311).await?;
@@ -601,16 +675,18 @@
 
     Ok(())
 }
 
 #[tokio::test]
 async fn pylint_allow_explicit_prerelease_with_marker() -> Result<()> {
     let manifest = Manifest::simple(vec![
-        Requirement::from_str("pylint==2.3.0").unwrap(),
-        Requirement::from_str("isort>=5.0.0b").unwrap(),
+        Requirement::from_pep508(pep508_rs::Requirement::from_str("pylint==2.3.0").unwrap())
+            .unwrap(),
+        Requirement::from_pep508(pep508_rs::Requirement::from_str("isort>=5.0.0b").unwrap())
+            .unwrap(),
     ]);
     let options = OptionsBuilder::new()
         .prerelease_mode(PreReleaseMode::Explicit)
         .exclude_newer(Some(*EXCLUDE_NEWER))
         .build();
 
     let resolution = resolve(manifest, options, &MARKERS_311, &TAGS_311).await?;
@@ -628,15 +704,18 @@
     Ok(())
 }
 
 /// Resolve `msgraph-sdk==1.0.0`, which depends on `msgraph-core>=1.0.0a2`. The resolver should
 /// fail with a pre-release-centric hint.
 #[tokio::test]
 async fn msgraph_sdk() -> Result<()> {
-    let manifest = Manifest::simple(vec![Requirement::from_str("msgraph-sdk==1.0.0").unwrap()]);
+    let manifest = Manifest::simple(vec![Requirement::from_pep508(
+        pep508_rs::Requirement::from_str("msgraph-sdk==1.0.0").unwrap(),
+    )
+    .unwrap()]);
     let options = OptionsBuilder::new()
         .exclude_newer(Some(*EXCLUDE_NEWER))
         .build();
 
     let err = resolve(manifest, options, &MARKERS_311, &TAGS_311)
         .await
         .unwrap_err();
@@ -648,69 +727,71 @@
     hint: msgraph-core was requested with a pre-release marker (e.g., msgraph-core>=1.0.0a2), but pre-releases weren't enabled (try: `--prerelease=allow`)
     "###);
 
     Ok(())
 }
 
 static MARKERS_311: Lazy<MarkerEnvironment> = Lazy::new(|| {
-    MarkerEnvironment {
-        implementation_name: "cpython".to_string(),
-        implementation_version: StringVersion::from_str("3.11.5").unwrap(),
-        os_name: "posix".to_string(),
-        platform_machine: "arm64".to_string(),
-        platform_python_implementation: "CPython".to_string(),
-        platform_release: "21.6.0".to_string(),
-        platform_system: "Darwin".to_string(),
-        platform_version: "Darwin Kernel Version 21.6.0: Mon Aug 22 20:19:52 PDT 2022; root:xnu-8020.140.49~2/RELEASE_ARM64_T6000".to_string(),
-        python_full_version: StringVersion::from_str("3.11.5").unwrap(),
-        python_version: StringVersion::from_str("3.11").unwrap(),
-        sys_platform: "darwin".to_string(),
-    }
+    MarkerEnvironment::try_from(MarkerEnvironmentBuilder {
+        implementation_name: "cpython",
+        implementation_version: "3.11.5",
+        os_name: "posix",
+        platform_machine: "arm64",
+        platform_python_implementation: "CPython",
+        platform_release: "21.6.0",
+        platform_system: "Darwin",
+        platform_version: "Darwin Kernel Version 21.6.0: Mon Aug 22 20:19:52 PDT 2022; root:xnu-8020.140.49~2/RELEASE_ARM64_T6000",
+        python_full_version: "3.11.5",
+        python_version: "3.11",
+        sys_platform: "darwin",
+    }).unwrap()
 });
 
 static TAGS_311: Lazy<Tags> = Lazy::new(|| {
     Tags::from_env(
         &Platform::new(
             Os::Macos {
                 major: 21,
                 minor: 6,
             },
             Arch::Aarch64,
         ),
         (3, 11),
         "cpython",
         (3, 11),
+        false,
     )
     .unwrap()
 });
 
 static MARKERS_310: Lazy<MarkerEnvironment> = Lazy::new(|| {
-    MarkerEnvironment {
-        implementation_name: "cpython".to_string(),
-        implementation_version: StringVersion::from_str("3.10.5").unwrap(),
-        os_name: "posix".to_string(),
-        platform_machine: "arm64".to_string(),
-        platform_python_implementation: "CPython".to_string(),
-        platform_release: "21.6.0".to_string(),
-        platform_system: "Darwin".to_string(),
-        platform_version: "Darwin Kernel Version 21.6.0: Mon Aug 22 20:19:52 PDT 2022; root:xnu-8020.140.49~2/RELEASE_ARM64_T6000".to_string(),
-        python_full_version: StringVersion::from_str("3.10.5").unwrap(),
-        python_version: StringVersion::from_str("3.10").unwrap(),
-        sys_platform: "darwin".to_string(),
-    }
+    MarkerEnvironment::try_from(MarkerEnvironmentBuilder {
+        implementation_name: "cpython",
+        implementation_version: "3.10.5",
+        os_name: "posix",
+        platform_machine: "arm64",
+        platform_python_implementation: "CPython",
+        platform_release: "21.6.0",
+        platform_system: "Darwin",
+        platform_version: "Darwin Kernel Version 21.6.0: Mon Aug 22 20:19:52 PDT 2022; root:xnu-8020.140.49~2/RELEASE_ARM64_T6000",
+        python_full_version: "3.10.5",
+        python_version: "3.10",
+        sys_platform: "darwin",
+    }).unwrap()
 });
 
 static TAGS_310: Lazy<Tags> = Lazy::new(|| {
     Tags::from_env(
         &Platform::new(
             Os::Macos {
                 major: 21,
                 minor: 6,
             },
             Arch::Aarch64,
         ),
         (3, 10),
         "cpython",
         (3, 10),
+        false,
     )
     .unwrap()
 });
```

### Comparing `uv-0.1.9/crates/pep440-rs/Cargo.toml` & `uv-0.2.0/crates/uv-configuration/Cargo.toml`

 * *Files 20% similar despite different names*

```diff
@@ -1,30 +1,32 @@
 [package]
-name = "pep440_rs"
-version = "0.5.0"
-description = "A library for python version numbers and specifiers, implementing PEP 440"
-license = "Apache-2.0 OR BSD-2-Clause"
-include = ["/src", "Changelog.md", "License-Apache", "License-BSD", "Readme.md", "pyproject.toml"]
-
+name = "uv-configuration"
+version = "0.0.1"
 edition = { workspace = true }
 rust-version = { workspace = true }
 homepage = { workspace = true }
 documentation = { workspace = true }
 repository = { workspace = true }
 authors = { workspace = true }
+license = { workspace = true }
 
-[lib]
-name = "pep440_rs"
-crate-type = ["rlib", "cdylib"]
+[lints]
+workspace = true
 
 [dependencies]
-once_cell = { workspace = true }
-pubgrub = { workspace = true, optional = true }
-pyo3 = { workspace = true, optional = true, features = ["extension-module", "abi3-py37"] }
-serde = { workspace = true, features = ["derive"], optional = true }
-rkyv = { workspace = true, features = ["strict", "validation"], optional = true }
-tracing = { workspace = true, optional = true }
-unicode-width = { workspace = true }
-unscanny = { workspace = true }
+distribution-types = { workspace = true }
+pep508_rs = { workspace = true }
+platform-tags = { workspace = true }
+uv-auth = { workspace = true }
+uv-normalize = { workspace = true }
+
+anyhow = { workspace = true }
+clap = { workspace = true, features = ["derive"], optional = true }
+itertools = { workspace = true }
+rustc-hash = { workspace = true }
+schemars = { workspace = true, optional = true }
+serde = { workspace = true }
+serde_json = { workspace = true }
+tracing = { workspace = true }
 
-[dev-dependencies]
-indoc = { version = "2.0.4" }
+[features]
+default = []
```

### Comparing `uv-0.1.9/crates/pep440-rs/License-Apache` & `uv-0.2.0/crates/pep508-rs/License-Apache`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/pep440-rs/License-BSD` & `uv-0.2.0/crates/pep508-rs/License-BSD`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/pep440-rs/Readme.md` & `uv-0.2.0/crates/pep440-rs/Readme.md`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/pep440-rs/python/Readme.md` & `uv-0.2.0/crates/pep440-rs/python/Readme.md`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/pep440-rs/src/lib.rs` & `uv-0.2.0/crates/pep440-rs/src/lib.rs`

 * *Files 3% similar despite different names*

```diff
@@ -37,24 +37,30 @@
 #[cfg(feature = "pyo3")]
 pub use version::PyVersion;
 pub use {
     version::{
         LocalSegment, Operator, OperatorParseError, PreRelease, PreReleaseKind, Version,
         VersionParseError, VersionPattern, VersionPatternParseError, MIN_VERSION,
     },
-    version_specifier::{VersionSpecifier, VersionSpecifiers, VersionSpecifiersParseError},
+    version_specifier::{
+        VersionSpecifier, VersionSpecifierBuildError, VersionSpecifiers,
+        VersionSpecifiersParseError,
+    },
 };
 
 mod version;
 mod version_specifier;
 
 /// Python bindings shipped as `pep440_rs`
 #[cfg(feature = "pyo3")]
 #[pyo3::pymodule]
 #[pyo3(name = "_pep440_rs")]
-pub fn python_module(_py: pyo3::Python, module: &pyo3::types::PyModule) -> pyo3::PyResult<()> {
+pub fn python_module(
+    _py: pyo3::Python,
+    module: &pyo3::Bound<'_, pyo3::types::PyModule>,
+) -> pyo3::PyResult<()> {
     module.add_class::<PyVersion>()?;
     module.add_class::<Operator>()?;
     module.add_class::<VersionSpecifier>()?;
     module.add_class::<VersionSpecifiers>()?;
     Ok(())
 }
```

### Comparing `uv-0.1.9/crates/pep440-rs/src/version.rs` & `uv-0.2.0/crates/pep440-rs/src/version.rs`

 * *Files 3% similar despite different names*

```diff
@@ -7,28 +7,32 @@
 };
 
 #[cfg(feature = "pyo3")]
 use pyo3::{
     basic::CompareOp, exceptions::PyValueError, pyclass, pymethods, FromPyObject, IntoPy, PyAny,
     PyObject, PyResult, Python,
 };
-#[cfg(feature = "serde")]
 use serde::{de, Deserialize, Deserializer, Serialize, Serializer};
 
 /// One of `~=` `==` `!=` `<=` `>=` `<` `>` `===`
-#[derive(Eq, PartialEq, Debug, Hash, Clone, Copy)]
-#[cfg_attr(
-    feature = "rkyv",
-    derive(rkyv::Archive, rkyv::Deserialize, rkyv::Serialize)
-)]
-#[cfg_attr(feature = "rkyv", archive(check_bytes))]
-#[cfg_attr(
-    feature = "rkyv",
-    archive_attr(derive(Debug, Eq, PartialEq, PartialOrd, Ord))
+#[derive(
+    Eq,
+    Ord,
+    PartialEq,
+    PartialOrd,
+    Debug,
+    Hash,
+    Clone,
+    Copy,
+    rkyv::Archive,
+    rkyv::Deserialize,
+    rkyv::Serialize,
 )]
+#[archive(check_bytes)]
+#[archive_attr(derive(Debug, Eq, PartialEq, PartialOrd, Ord))]
 #[cfg_attr(feature = "pyo3", pyclass)]
 pub enum Operator {
     /// `== 1.2.3`
     Equal,
     /// `== 1.2.*`
     EqualStar,
     /// `===` (discouraged)
@@ -62,32 +66,32 @@
     /// NOT permitted in this version specifier." phrasing in the version
     /// specifiers [spec].
     ///
     /// [spec]: https://packaging.python.org/en/latest/specifications/version-specifiers/
     pub(crate) fn is_local_compatible(&self) -> bool {
         !matches!(
             *self,
-            Operator::GreaterThan
-                | Operator::GreaterThanEqual
-                | Operator::LessThan
-                | Operator::LessThanEqual
-                | Operator::TildeEqual
-                | Operator::EqualStar
-                | Operator::NotEqualStar
+            Self::GreaterThan
+                | Self::GreaterThanEqual
+                | Self::LessThan
+                | Self::LessThanEqual
+                | Self::TildeEqual
+                | Self::EqualStar
+                | Self::NotEqualStar
         )
     }
 
     /// Returns the wildcard version of this operator, if appropriate.
     ///
     /// This returns `None` when this operator doesn't have an analogous
     /// wildcard operator.
-    pub(crate) fn to_star(self) -> Option<Operator> {
+    pub(crate) fn to_star(self) -> Option<Self> {
         match self {
-            Operator::Equal => Some(Operator::EqualStar),
-            Operator::NotEqual => Some(Operator::NotEqualStar),
+            Self::Equal => Some(Self::EqualStar),
+            Self::NotEqual => Some(Self::NotEqualStar),
             _ => None,
         }
     }
 }
 
 impl FromStr for Operator {
     type Err = OperatorParseError;
@@ -120,26 +124,26 @@
     }
 }
 
 impl std::fmt::Display for Operator {
     /// Note the `EqualStar` is also `==`.
     fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
         let operator = match self {
-            Operator::Equal => "==",
+            Self::Equal => "==",
             // Beware, this doesn't print the star
-            Operator::EqualStar => "==",
+            Self::EqualStar => "==",
             #[allow(deprecated)]
-            Operator::ExactEqual => "===",
-            Operator::NotEqual => "!=",
-            Operator::NotEqualStar => "!=",
-            Operator::TildeEqual => "~=",
-            Operator::LessThan => "<",
-            Operator::LessThanEqual => "<=",
-            Operator::GreaterThan => ">",
-            Operator::GreaterThanEqual => ">=",
+            Self::ExactEqual => "===",
+            Self::NotEqual => "!=",
+            Self::NotEqualStar => "!=",
+            Self::TildeEqual => "~=",
+            Self::LessThan => "<",
+            Self::LessThanEqual => "<=",
+            Self::GreaterThan => ">",
+            Self::GreaterThanEqual => ">=",
         };
 
         write!(f, "{operator}")
     }
 }
 
 #[cfg(feature = "pyo3")]
@@ -244,57 +248,43 @@
 ///
 /// ```rust
 /// use std::str::FromStr;
 /// use pep440_rs::Version;
 ///
 /// let version = Version::from_str("1.19").unwrap();
 /// ```
-#[derive(Clone)]
-#[cfg_attr(
-    feature = "rkyv",
-    derive(rkyv::Archive, rkyv::Deserialize, rkyv::Serialize)
-)]
-#[cfg_attr(feature = "rkyv", archive(check_bytes))]
-#[cfg_attr(
-    feature = "rkyv",
-    archive_attr(derive(Debug, Eq, PartialEq, PartialOrd, Ord))
-)]
+#[derive(Clone, rkyv::Archive, rkyv::Deserialize, rkyv::Serialize)]
+#[archive(check_bytes)]
+#[archive_attr(derive(Debug, Eq, PartialEq, PartialOrd, Ord))]
 pub struct Version {
     inner: Arc<VersionInner>,
 }
 
-#[derive(Clone, Debug)]
-#[cfg_attr(
-    feature = "rkyv",
-    derive(rkyv::Archive, rkyv::Deserialize, rkyv::Serialize)
-)]
-#[cfg_attr(feature = "rkyv", archive(check_bytes))]
-#[cfg_attr(
-    feature = "rkyv",
-    archive_attr(derive(Debug, Eq, PartialEq, PartialOrd, Ord))
-)]
+#[derive(Clone, Debug, rkyv::Archive, rkyv::Deserialize, rkyv::Serialize)]
+#[archive(check_bytes)]
+#[archive_attr(derive(Debug, Eq, PartialEq, PartialOrd, Ord))]
 enum VersionInner {
     Small { small: VersionSmall },
     Full { full: VersionFull },
 }
 
 impl Version {
     /// Create a new version from an iterator of segments in the release part
     /// of a version.
     ///
     /// # Panics
     ///
     /// When the iterator yields no elements.
     #[inline]
-    pub fn new<I, R>(release_numbers: I) -> Version
+    pub fn new<I, R>(release_numbers: I) -> Self
     where
         I: IntoIterator<Item = R>,
         R: Borrow<u64>,
     {
-        Version {
+        Self {
             inner: Arc::new(VersionInner::Small {
                 small: VersionSmall::new(),
             }),
         }
         .with_release(release_numbers)
     }
 
@@ -381,26 +371,52 @@
     pub fn local(&self) -> &[LocalSegment] {
         match *self.inner {
             VersionInner::Small { ref small } => small.local(),
             VersionInner::Full { ref full } => &full.local,
         }
     }
 
+    /// Returns the min-release part of this version, if it exists.
+    ///
+    /// The "min" component is internal-only, and does not exist in PEP 440.
+    /// The version `1.0min0` is smaller than all other `1.0` versions,
+    /// like `1.0a1`, `1.0dev0`, etc.
+    #[inline]
+    pub fn min(&self) -> Option<u64> {
+        match *self.inner {
+            VersionInner::Small { ref small } => small.min(),
+            VersionInner::Full { ref full } => full.min,
+        }
+    }
+
+    /// Returns the max-release part of this version, if it exists.
+    ///
+    /// The "max" component is internal-only, and does not exist in PEP 440.
+    /// The version `1.0max0` is larger than all other `1.0` versions,
+    /// like `1.0.post1`, `1.0+local`, etc.
+    #[inline]
+    pub fn max(&self) -> Option<u64> {
+        match *self.inner {
+            VersionInner::Small { ref small } => small.max(),
+            VersionInner::Full { ref full } => full.max,
+        }
+    }
+
     /// Set the release numbers and return the updated version.
     ///
     /// Usually one can just use `Version::new` to create a new version with
     /// the updated release numbers, but this is useful when one wants to
     /// preserve the other components of a version number while only changing
     /// the release numbers.
     ///
     /// # Panics
     ///
     /// When the iterator yields no elements.
     #[inline]
-    pub fn with_release<I, R>(mut self, release_numbers: I) -> Version
+    pub fn with_release<I, R>(mut self, release_numbers: I) -> Self
     where
         I: IntoIterator<Item = R>,
         R: Borrow<u64>,
     {
         self.clear_release();
         for n in release_numbers {
             self.push_release(*n.borrow());
@@ -436,99 +452,140 @@
                 full.release.clear();
             }
         }
     }
 
     /// Set the epoch and return the updated version.
     #[inline]
-    pub fn with_epoch(mut self, value: u64) -> Version {
+    pub fn with_epoch(mut self, value: u64) -> Self {
         if let VersionInner::Small { ref mut small } = Arc::make_mut(&mut self.inner) {
             if small.set_epoch(value) {
                 return self;
             }
         }
         self.make_full().epoch = value;
         self
     }
 
     /// Set the pre-release component and return the updated version.
     #[inline]
-    pub fn with_pre(mut self, value: Option<PreRelease>) -> Version {
+    pub fn with_pre(mut self, value: Option<PreRelease>) -> Self {
         if let VersionInner::Small { ref mut small } = Arc::make_mut(&mut self.inner) {
             if small.set_pre(value) {
                 return self;
             }
         }
         self.make_full().pre = value;
         self
     }
 
     /// Set the post-release component and return the updated version.
     #[inline]
-    pub fn with_post(mut self, value: Option<u64>) -> Version {
+    pub fn with_post(mut self, value: Option<u64>) -> Self {
         if let VersionInner::Small { ref mut small } = Arc::make_mut(&mut self.inner) {
             if small.set_post(value) {
                 return self;
             }
         }
         self.make_full().post = value;
         self
     }
 
     /// Set the dev-release component and return the updated version.
     #[inline]
-    pub fn with_dev(mut self, value: Option<u64>) -> Version {
+    pub fn with_dev(mut self, value: Option<u64>) -> Self {
         if let VersionInner::Small { ref mut small } = Arc::make_mut(&mut self.inner) {
             if small.set_dev(value) {
                 return self;
             }
         }
         self.make_full().dev = value;
         self
     }
 
     /// Set the local segments and return the updated version.
     #[inline]
-    pub fn with_local(mut self, value: Vec<LocalSegment>) -> Version {
+    pub fn with_local(mut self, value: Vec<LocalSegment>) -> Self {
         if value.is_empty() {
             self.without_local()
         } else {
             self.make_full().local = value;
             self
         }
     }
 
     /// For PEP 440 specifier matching: "Except where specifically noted below,
     /// local version identifiers MUST NOT be permitted in version specifiers,
     /// and local version labels MUST be ignored entirely when checking if
     /// candidate versions match a given version specifier."
     #[inline]
-    pub fn without_local(mut self) -> Version {
+    pub fn without_local(mut self) -> Self {
         // A "small" version is already guaranteed not to have a local
         // component, so we only need to do anything if we have a "full"
         // version.
         if let VersionInner::Full { ref mut full } = Arc::make_mut(&mut self.inner) {
             full.local.clear();
         }
         self
     }
 
+    /// Set the min-release component and return the updated version.
+    ///
+    /// The "min" component is internal-only, and does not exist in PEP 440.
+    /// The version `1.0min0` is smaller than all other `1.0` versions,
+    /// like `1.0a1`, `1.0dev0`, etc.
+    #[inline]
+    pub fn with_min(mut self, value: Option<u64>) -> Self {
+        debug_assert!(!self.is_pre(), "min is not allowed on pre-release versions");
+        debug_assert!(!self.is_dev(), "min is not allowed on dev versions");
+        if let VersionInner::Small { ref mut small } = Arc::make_mut(&mut self.inner) {
+            if small.set_min(value) {
+                return self;
+            }
+        }
+        self.make_full().min = value;
+        self
+    }
+
+    /// Set the max-release component and return the updated version.
+    ///
+    /// The "max" component is internal-only, and does not exist in PEP 440.
+    /// The version `1.0max0` is larger than all other `1.0` versions,
+    /// like `1.0.post1`, `1.0+local`, etc.
+    #[inline]
+    pub fn with_max(mut self, value: Option<u64>) -> Self {
+        debug_assert!(
+            !self.is_post(),
+            "max is not allowed on post-release versions"
+        );
+        debug_assert!(!self.is_dev(), "max is not allowed on dev versions");
+        if let VersionInner::Small { ref mut small } = Arc::make_mut(&mut self.inner) {
+            if small.set_max(value) {
+                return self;
+            }
+        }
+        self.make_full().max = value;
+        self
+    }
+
     /// Convert this version to a "full" representation in-place and return a
     /// mutable borrow to the full type.
     fn make_full(&mut self) -> &mut VersionFull {
         if let VersionInner::Small { ref small } = *self.inner {
             let full = VersionFull {
                 epoch: small.epoch(),
                 release: small.release().to_vec(),
+                min: small.min(),
+                max: small.max(),
                 pre: small.pre(),
                 post: small.post(),
                 dev: small.dev(),
                 local: vec![],
             };
-            *self = Version {
+            *self = Self {
                 inner: Arc::new(VersionInner::Full { full }),
             };
         }
         match Arc::make_mut(&mut self.inner) {
             VersionInner::Full { ref mut full } => full,
             VersionInner::Small { .. } => unreachable!(),
         }
@@ -538,15 +595,15 @@
     ///
     /// This comparison is done using only the public API of a `Version`, and
     /// is thus independent of its specific representation. This is useful
     /// to use when comparing two versions that aren't *both* the small
     /// representation.
     #[cold]
     #[inline(never)]
-    fn cmp_slow(&self, other: &Version) -> Ordering {
+    fn cmp_slow(&self, other: &Self) -> Ordering {
         match self.epoch().cmp(&other.epoch()) {
             Ordering::Less => {
                 return Ordering::Less;
             }
             Ordering::Equal => {}
             Ordering::Greater => {
                 return Ordering::Greater;
@@ -565,27 +622,25 @@
 
         // release is equal, so compare the other parts
         sortable_tuple(self).cmp(&sortable_tuple(other))
     }
 }
 
 /// <https://github.com/serde-rs/serde/issues/1316#issue-332908452>
-#[cfg(feature = "serde")]
 impl<'de> Deserialize<'de> for Version {
     fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
     where
         D: Deserializer<'de>,
     {
         let s = String::deserialize(deserializer)?;
         FromStr::from_str(&s).map_err(de::Error::custom)
     }
 }
 
 /// <https://github.com/serde-rs/serde/issues/1316#issue-332908452>
-#[cfg(feature = "serde")]
 impl Serialize for Version {
     fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
     where
         S: Serializer,
     {
         serializer.collect_str(self)
     }
@@ -632,15 +687,15 @@
         };
         write!(f, "{epoch}{release}{pre}{post}{dev}{local}")
     }
 }
 
 impl std::fmt::Debug for Version {
     fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-        write!(f, "\"{}\"", self)
+        write!(f, "\"{self}\"")
     }
 }
 
 impl PartialEq<Self> for Version {
     #[inline]
     fn eq(&self, other: &Self) -> bool {
         self.cmp(other) == Ordering::Equal
@@ -740,47 +795,45 @@
 /// that it preserves the ordering between versions as prescribed in PEP 440.
 /// Namely:
 ///
 /// * Bytes 6 and 7 correspond to the first release segment as a `u16`.
 /// * Bytes 5, 4 and 3 correspond to the second, third and fourth release
 /// segments, respectively.
 /// * Bytes 2, 1 and 0 represent *one* of the following:
-///   `.devN, aN, bN, rcN, <no suffix>, .postN`. Its representation is thus:
+///   `min, .devN, aN, bN, rcN, <no suffix>, .postN, max`.
+///   Its representation is thus:
 ///   * The most significant 3 bits of Byte 2 corresponds to a value in
-///   the range 0-5 inclusive, corresponding to dev, pre-a, pre-b, pre-rc,
-///   no-suffix or post releases, respectively.
+///   the range 0-6 inclusive, corresponding to min, dev, pre-a, pre-b, pre-rc,
+///   no-suffix or post releases, respectively. `min` is a special version that
+///   does not exist in PEP 440, but is used here to represent the smallest
+///   possible version, preceding any `dev`, `pre`, `post` or releases. `max` is
+///   an analogous concept for the largest possible version, following any `post`
+///   or local releases.
 ///   * The low 5 bits combined with the bits in bytes 1 and 0 correspond
 ///   to the release number of the suffix, if one exists. If there is no
 ///   suffix, then this bits are always 0.
 ///
 /// The order of the encoding above is significant. For example, suffixes are
 /// encoded at a less significant location than the release numbers, so that
 /// `1.2.3 < 1.2.3.post4`.
 ///
-/// In a previous representation, we tried to enocode the suffixes in different
+/// In a previous representation, we tried to encode the suffixes in different
 /// locations so that, in theory, you could represent `1.2.3.dev2.post3` in the
 /// packed form. But getting the ordering right for this is difficult (perhaps
 /// impossible without extra space?). So we limited to only storing one suffix.
 /// But even then, we wound up with a bug where `1.0dev1 > 1.0a1`, when of
 /// course, all dev releases should compare less than pre releases. This was
 /// because the encoding recorded the pre-release as "absent", and this in turn
 /// screwed up the order comparisons.
 ///
 /// Thankfully, such versions are incredibly rare. Virtually all versions have
 /// zero or one pre, dev or post release components.
-#[derive(Clone, Debug)]
-#[cfg_attr(
-    feature = "rkyv",
-    derive(rkyv::Archive, rkyv::Deserialize, rkyv::Serialize)
-)]
-#[cfg_attr(feature = "rkyv", archive(check_bytes))]
-#[cfg_attr(
-    feature = "rkyv",
-    archive_attr(derive(Debug, Eq, PartialEq, PartialOrd, Ord))
-)]
+#[derive(Clone, Debug, rkyv::Archive, rkyv::Deserialize, rkyv::Serialize)]
+#[archive(check_bytes)]
+#[archive_attr(derive(Debug, Eq, PartialEq, PartialOrd, Ord))]
 struct VersionSmall {
     /// The representation discussed above.
     repr: u64,
     /// The `u64` numbers in the release component.
     ///
     /// These are *only* used to implement the public API `Version::release`
     /// method. This is necessary in order to provide a `&[u64]` to the caller.
@@ -806,26 +859,28 @@
     /// to truncate the zero components. And always filling out to 4
     /// places somewhat exposes internal details, since the "full" version
     /// representation would not do that.
     len: u8,
 }
 
 impl VersionSmall {
-    const SUFFIX_DEV: u64 = 0;
-    const SUFFIX_PRE_ALPHA: u64 = 1;
-    const SUFFIX_PRE_BETA: u64 = 2;
-    const SUFFIX_PRE_RC: u64 = 3;
-    const SUFFIX_NONE: u64 = 4;
-    const SUFFIX_POST: u64 = 5;
+    const SUFFIX_MIN: u64 = 0;
+    const SUFFIX_DEV: u64 = 1;
+    const SUFFIX_PRE_ALPHA: u64 = 2;
+    const SUFFIX_PRE_BETA: u64 = 3;
+    const SUFFIX_PRE_RC: u64 = 4;
+    const SUFFIX_NONE: u64 = 5;
+    const SUFFIX_POST: u64 = 6;
+    const SUFFIX_MAX: u64 = 7;
     const SUFFIX_MAX_VERSION: u64 = 0x1FFFFF;
 
     #[inline]
-    fn new() -> VersionSmall {
-        VersionSmall {
-            repr: 0x00000000_00800000,
+    fn new() -> Self {
+        Self {
+            repr: 0x00000000_00A00000,
             release: [0, 0, 0, 0],
             len: 0,
         }
     }
 
     #[inline]
     fn epoch(&self) -> u64 {
@@ -875,117 +930,195 @@
             self.len += 1;
             true
         }
     }
 
     #[inline]
     fn post(&self) -> Option<u64> {
-        if self.suffix_kind() == VersionSmall::SUFFIX_POST {
+        if self.suffix_kind() == Self::SUFFIX_POST {
             Some(self.suffix_version())
         } else {
             None
         }
     }
 
     #[inline]
     fn set_post(&mut self, value: Option<u64>) -> bool {
-        if self.pre().is_some() || self.dev().is_some() {
+        if self.min().is_some()
+            || self.pre().is_some()
+            || self.dev().is_some()
+            || self.max().is_some()
+        {
             return value.is_none();
         }
         match value {
             None => {
-                self.set_suffix_kind(VersionSmall::SUFFIX_NONE);
+                self.set_suffix_kind(Self::SUFFIX_NONE);
             }
             Some(number) => {
-                if number > VersionSmall::SUFFIX_MAX_VERSION {
+                if number > Self::SUFFIX_MAX_VERSION {
                     return false;
                 }
-                self.set_suffix_kind(VersionSmall::SUFFIX_POST);
+                self.set_suffix_kind(Self::SUFFIX_POST);
                 self.set_suffix_version(number);
             }
         }
         true
     }
 
     #[inline]
     fn pre(&self) -> Option<PreRelease> {
         let (kind, number) = (self.suffix_kind(), self.suffix_version());
-        if kind == VersionSmall::SUFFIX_PRE_ALPHA {
+        if kind == Self::SUFFIX_PRE_ALPHA {
             Some(PreRelease {
                 kind: PreReleaseKind::Alpha,
                 number,
             })
-        } else if kind == VersionSmall::SUFFIX_PRE_BETA {
+        } else if kind == Self::SUFFIX_PRE_BETA {
             Some(PreRelease {
                 kind: PreReleaseKind::Beta,
                 number,
             })
-        } else if kind == VersionSmall::SUFFIX_PRE_RC {
+        } else if kind == Self::SUFFIX_PRE_RC {
             Some(PreRelease {
                 kind: PreReleaseKind::Rc,
                 number,
             })
         } else {
             None
         }
     }
 
     #[inline]
     fn set_pre(&mut self, value: Option<PreRelease>) -> bool {
-        if self.dev().is_some() || self.post().is_some() {
+        if self.min().is_some()
+            || self.dev().is_some()
+            || self.post().is_some()
+            || self.max().is_some()
+        {
             return value.is_none();
         }
         match value {
             None => {
-                self.set_suffix_kind(VersionSmall::SUFFIX_NONE);
+                self.set_suffix_kind(Self::SUFFIX_NONE);
             }
             Some(PreRelease { kind, number }) => {
-                if number > VersionSmall::SUFFIX_MAX_VERSION {
+                if number > Self::SUFFIX_MAX_VERSION {
                     return false;
                 }
                 match kind {
                     PreReleaseKind::Alpha => {
-                        self.set_suffix_kind(VersionSmall::SUFFIX_PRE_ALPHA);
+                        self.set_suffix_kind(Self::SUFFIX_PRE_ALPHA);
                     }
                     PreReleaseKind::Beta => {
-                        self.set_suffix_kind(VersionSmall::SUFFIX_PRE_BETA);
+                        self.set_suffix_kind(Self::SUFFIX_PRE_BETA);
                     }
                     PreReleaseKind::Rc => {
-                        self.set_suffix_kind(VersionSmall::SUFFIX_PRE_RC);
+                        self.set_suffix_kind(Self::SUFFIX_PRE_RC);
                     }
                 }
                 self.set_suffix_version(number);
             }
         }
         true
     }
 
     #[inline]
     fn dev(&self) -> Option<u64> {
-        if self.suffix_kind() == VersionSmall::SUFFIX_DEV {
+        if self.suffix_kind() == Self::SUFFIX_DEV {
             Some(self.suffix_version())
         } else {
             None
         }
     }
 
     #[inline]
     fn set_dev(&mut self, value: Option<u64>) -> bool {
-        if self.pre().is_some() || self.post().is_some() {
+        if self.min().is_some()
+            || self.pre().is_some()
+            || self.post().is_some()
+            || self.max().is_some()
+        {
             return value.is_none();
         }
         match value {
             None => {
-                self.set_suffix_kind(VersionSmall::SUFFIX_NONE);
+                self.set_suffix_kind(Self::SUFFIX_NONE);
             }
             Some(number) => {
-                if number > VersionSmall::SUFFIX_MAX_VERSION {
+                if number > Self::SUFFIX_MAX_VERSION {
                     return false;
                 }
-                self.set_suffix_kind(VersionSmall::SUFFIX_DEV);
+                self.set_suffix_kind(Self::SUFFIX_DEV);
+                self.set_suffix_version(number);
+            }
+        }
+        true
+    }
+
+    #[inline]
+    fn min(&self) -> Option<u64> {
+        if self.suffix_kind() == Self::SUFFIX_MIN {
+            Some(self.suffix_version())
+        } else {
+            None
+        }
+    }
+
+    #[inline]
+    fn set_min(&mut self, value: Option<u64>) -> bool {
+        if self.dev().is_some()
+            || self.pre().is_some()
+            || self.post().is_some()
+            || self.max().is_some()
+        {
+            return value.is_none();
+        }
+        match value {
+            None => {
+                self.set_suffix_kind(Self::SUFFIX_NONE);
+            }
+            Some(number) => {
+                if number > Self::SUFFIX_MAX_VERSION {
+                    return false;
+                }
+                self.set_suffix_kind(Self::SUFFIX_MIN);
+                self.set_suffix_version(number);
+            }
+        }
+        true
+    }
+
+    #[inline]
+    fn max(&self) -> Option<u64> {
+        if self.suffix_kind() == Self::SUFFIX_MAX {
+            Some(self.suffix_version())
+        } else {
+            None
+        }
+    }
+
+    #[inline]
+    fn set_max(&mut self, value: Option<u64>) -> bool {
+        if self.dev().is_some()
+            || self.pre().is_some()
+            || self.post().is_some()
+            || self.min().is_some()
+        {
+            return value.is_none();
+        }
+        match value {
+            None => {
+                self.set_suffix_kind(Self::SUFFIX_NONE);
+            }
+            Some(number) => {
+                if number > Self::SUFFIX_MAX_VERSION {
+                    return false;
+                }
+                self.set_suffix_kind(Self::SUFFIX_MAX);
                 self.set_suffix_version(number);
             }
         }
         true
     }
 
     #[inline]
@@ -994,24 +1127,24 @@
         // of local segments.
         &[]
     }
 
     #[inline]
     fn suffix_kind(&self) -> u64 {
         let kind = (self.repr >> 21) & 0b111;
-        debug_assert!(kind <= VersionSmall::SUFFIX_POST);
+        debug_assert!(kind <= Self::SUFFIX_MAX);
         kind
     }
 
     #[inline]
     fn set_suffix_kind(&mut self, kind: u64) {
-        debug_assert!(kind <= VersionSmall::SUFFIX_POST);
+        debug_assert!(kind <= Self::SUFFIX_MAX);
         self.repr &= !0xE00000;
         self.repr |= kind << 21;
-        if kind == VersionSmall::SUFFIX_NONE {
+        if kind == Self::SUFFIX_NONE {
             self.set_suffix_version(0);
         }
     }
 
     #[inline]
     fn suffix_version(&self) -> u64 {
         self.repr & 0x1FFFFF
@@ -1029,24 +1162,17 @@
 ///
 /// This can represent all possible versions, but is a bit beefier because of
 /// it. It also uses some indirection for variable length data such as the
 /// release numbers and the local segments.
 ///
 /// In general, the "full" representation is rarely used in practice since most
 /// versions will fit into the "small" representation.
-#[derive(Clone, Debug)]
-#[cfg_attr(
-    feature = "rkyv",
-    derive(rkyv::Archive, rkyv::Deserialize, rkyv::Serialize)
-)]
-#[cfg_attr(feature = "rkyv", archive(check_bytes))]
-#[cfg_attr(
-    feature = "rkyv",
-    archive_attr(derive(Debug, Eq, PartialEq, PartialOrd, Ord))
-)]
+#[derive(Clone, Debug, rkyv::Archive, rkyv::Deserialize, rkyv::Serialize)]
+#[archive(check_bytes)]
+#[archive_attr(derive(Debug, Eq, PartialEq, PartialOrd, Ord))]
 struct VersionFull {
     /// The [versioning
     /// epoch](https://peps.python.org/pep-0440/#version-epochs). Normally
     /// just 0, but you can increment it if you switched the versioning
     /// scheme.
     epoch: u64,
     /// The normal number part of the version (["final
@@ -1075,14 +1201,22 @@
     ///
     /// > They consist of a normal public version identifier (as defined
     /// > in the previous section), along with an arbitrary “local version
     /// > label”, separated from the public version identifier by a plus.
     /// > Local version labels have no specific semantics assigned, but
     /// > some syntactic restrictions are imposed.
     local: Vec<LocalSegment>,
+    /// An internal-only segment that does not exist in PEP 440, used to
+    /// represent the smallest possible version of a release, preceding any
+    /// `dev`, `pre`, `post` or releases.
+    min: Option<u64>,
+    /// An internal-only segment that does not exist in PEP 440, used to
+    /// represent the largest possible version of a release, following any
+    /// `post` or local releases.
+    max: Option<u64>,
 }
 
 /// A version number pattern.
 ///
 /// A version pattern appears in a
 /// [`VersionSpecifier`](crate::VersionSpecifier). It is just like a version,
 /// except that it permits a trailing `*` (wildcard) at the end of the version
@@ -1105,26 +1239,26 @@
     wildcard: bool,
 }
 
 impl VersionPattern {
     /// Creates a new verbatim version pattern that matches the given
     /// version exactly.
     #[inline]
-    pub fn verbatim(version: Version) -> VersionPattern {
-        VersionPattern {
+    pub fn verbatim(version: Version) -> Self {
+        Self {
             version,
             wildcard: false,
         }
     }
 
     /// Creates a new wildcard version pattern that matches any version with
     /// the given version as a prefix.
     #[inline]
-    pub fn wildcard(version: Version) -> VersionPattern {
-        VersionPattern {
+    pub fn wildcard(version: Version) -> Self {
+        Self {
             version,
             wildcard: true,
         }
     }
 
     /// Returns the underlying version.
     #[inline]
@@ -1144,52 +1278,62 @@
         self.wildcard
     }
 }
 
 impl FromStr for VersionPattern {
     type Err = VersionPatternParseError;
 
-    fn from_str(version: &str) -> Result<VersionPattern, VersionPatternParseError> {
+    fn from_str(version: &str) -> Result<Self, VersionPatternParseError> {
         Parser::new(version.as_bytes()).parse_pattern()
     }
 }
 
 /// An optional pre-release modifier and number applied to a version.
-#[derive(PartialEq, Eq, Debug, Hash, Clone, Copy, Ord, PartialOrd)]
-#[cfg_attr(feature = "pyo3", pyclass)]
-#[cfg_attr(
-    feature = "rkyv",
-    derive(rkyv::Archive, rkyv::Deserialize, rkyv::Serialize)
-)]
-#[cfg_attr(feature = "rkyv", archive(check_bytes))]
-#[cfg_attr(
-    feature = "rkyv",
-    archive_attr(derive(Debug, Eq, PartialEq, PartialOrd, Ord))
+#[derive(
+    PartialEq,
+    Eq,
+    Debug,
+    Hash,
+    Clone,
+    Copy,
+    Ord,
+    PartialOrd,
+    rkyv::Archive,
+    rkyv::Deserialize,
+    rkyv::Serialize,
 )]
+#[archive(check_bytes)]
+#[archive_attr(derive(Debug, Eq, PartialEq, PartialOrd, Ord))]
+#[cfg_attr(feature = "pyo3", pyclass)]
 pub struct PreRelease {
     /// The kind of pre-release.
     pub kind: PreReleaseKind,
     /// The number associated with the pre-release.
     pub number: u64,
 }
 
 /// Optional prerelease modifier (alpha, beta or release candidate) appended to version
 ///
 /// <https://peps.python.org/pep-0440/#pre-releases>
-#[derive(PartialEq, Eq, Debug, Hash, Clone, Copy, Ord, PartialOrd)]
-#[cfg_attr(feature = "pyo3", pyclass)]
-#[cfg_attr(
-    feature = "rkyv",
-    derive(rkyv::Archive, rkyv::Deserialize, rkyv::Serialize)
-)]
-#[cfg_attr(feature = "rkyv", archive(check_bytes))]
-#[cfg_attr(
-    feature = "rkyv",
-    archive_attr(derive(Debug, Eq, PartialEq, PartialOrd, Ord))
+#[derive(
+    PartialEq,
+    Eq,
+    Debug,
+    Hash,
+    Clone,
+    Copy,
+    Ord,
+    PartialOrd,
+    rkyv::Archive,
+    rkyv::Deserialize,
+    rkyv::Serialize,
 )]
+#[archive(check_bytes)]
+#[archive_attr(derive(Debug, Eq, PartialEq, PartialOrd, Ord))]
+#[cfg_attr(feature = "pyo3", pyclass)]
 pub enum PreReleaseKind {
     /// alpha prerelease
     Alpha,
     /// beta prerelease
     Beta,
     /// release candidate prerelease
     Rc,
@@ -1216,24 +1360,17 @@
 /// > comparing a numeric and lexicographic segment, the numeric section always compares as greater
 /// > than the lexicographic segment. Additionally a local version with a great number of segments
 /// > will always compare as greater than a local version with fewer segments, as long as the
 /// > shorter local version’s segments match the beginning of the longer local version’s segments
 /// > exactly.
 ///
 /// Luckily the default `Ord` implementation for `Vec<LocalSegment>` matches the PEP 440 rules.
-#[derive(Eq, PartialEq, Debug, Clone, Hash)]
-#[cfg_attr(
-    feature = "rkyv",
-    derive(rkyv::Archive, rkyv::Deserialize, rkyv::Serialize)
-)]
-#[cfg_attr(feature = "rkyv", archive(check_bytes))]
-#[cfg_attr(
-    feature = "rkyv",
-    archive_attr(derive(Debug, Eq, PartialEq, PartialOrd, Ord))
-)]
+#[derive(Eq, PartialEq, Debug, Clone, Hash, rkyv::Archive, rkyv::Deserialize, rkyv::Serialize)]
+#[archive(check_bytes)]
+#[archive_attr(derive(Debug, Eq, PartialEq, PartialOrd, Ord))]
 pub enum LocalSegment {
     /// Not-parseable as integer segment of local version
     String(String),
     /// Inferred integer segment of local version
     Number(u64),
 }
 
@@ -1356,16 +1493,16 @@
         }
         self.parse_pre()?;
         self.parse_post()?;
         self.parse_dev()?;
         self.parse_local()?;
         self.bump_while(|byte| byte.is_ascii_whitespace());
         if !self.is_done() {
+            let version = String::from_utf8_lossy(&self.v[..self.i]).into_owned();
             let remaining = String::from_utf8_lossy(&self.v[self.i..]).into_owned();
-            let version = self.into_pattern().version;
             return Err(ErrorKind::UnexpectedEnd { version, remaining }.into());
         }
         Ok(self.into_pattern())
     }
 
     /// Attempts to do a "fast parse" of a version.
     ///
@@ -1406,15 +1543,15 @@
             // think it makes the bit logic much clearer, and makes it
             // explicit that nothing was forgotten.
             #[allow(clippy::identity_op)]
             repr: (u64::from(release[0]) << 48)
                 | (u64::from(release[1]) << 40)
                 | (u64::from(release[2]) << 32)
                 | (u64::from(release[3]) << 24)
-                | (0x80 << 16)
+                | (0xA0 << 16)
                 | (0x00 << 8)
                 | (0x00 << 0),
             release: [
                 u64::from(release[0]),
                 u64::from(release[1]),
                 u64::from(release[2]),
                 u64::from(release[3]),
@@ -1764,55 +1901,55 @@
 enum ReleaseNumbers {
     Inline { numbers: [u64; 4], len: usize },
     Vec(Vec<u64>),
 }
 
 impl ReleaseNumbers {
     /// Create a new empty set of release numbers.
-    fn new() -> ReleaseNumbers {
-        ReleaseNumbers::Inline {
+    fn new() -> Self {
+        Self::Inline {
             numbers: [0; 4],
             len: 0,
         }
     }
 
     /// Push a new release number. This automatically switches over to the heap
     /// when the lengths grow too big.
     fn push(&mut self, n: u64) {
         match *self {
-            ReleaseNumbers::Inline {
+            Self::Inline {
                 ref mut numbers,
                 ref mut len,
             } => {
                 assert!(*len <= 4);
                 if *len == 4 {
                     let mut numbers = numbers.to_vec();
                     numbers.push(n);
-                    *self = ReleaseNumbers::Vec(numbers.to_vec());
+                    *self = Self::Vec(numbers.clone());
                 } else {
                     numbers[*len] = n;
                     *len += 1;
                 }
             }
-            ReleaseNumbers::Vec(ref mut numbers) => {
+            Self::Vec(ref mut numbers) => {
                 numbers.push(n);
             }
         }
     }
 
     /// Returns the number of components in this release component.
     fn len(&self) -> usize {
         self.as_slice().len()
     }
 
     /// Returns the release components as a slice.
     fn as_slice(&self) -> &[u64] {
         match *self {
-            ReleaseNumbers::Inline { ref numbers, len } => &numbers[..len],
-            ReleaseNumbers::Vec(ref vec) => vec,
+            Self::Inline { ref numbers, len } => &numbers[..len],
+            Self::Vec(ref vec) => vec,
         }
     }
 }
 
 /// Represents a set of strings for prefix searching.
 ///
 /// This can be built as a constant and is useful for quickly looking for one
@@ -1828,15 +1965,15 @@
 
 impl StringSet {
     /// Create a new string set for prefix searching from the given strings.
     ///
     /// # Panics
     ///
     /// When the number of strings is too big.
-    const fn new(strings: &'static [&'static str]) -> StringSet {
+    const fn new(strings: &'static [&'static str]) -> Self {
         assert!(
             strings.len() <= 20,
             "only a small number of strings are supported"
         );
         let (mut firsts, mut firsts_len) = ([0u8; 20], 0);
         let mut i = 0;
         while i < strings.len() {
@@ -1845,15 +1982,15 @@
                 "every string in set should be non-empty",
             );
             firsts[firsts_len] = strings[i].as_bytes()[0];
             firsts_len += 1;
             i += 1;
         }
         let first_byte = ByteSet::new(&firsts);
-        StringSet {
+        Self {
             first_byte,
             strings,
         }
     }
 
     /// Returns the index of the first string in this set that is a prefix of
     /// the given haystack, or `None` if no elements are a prefix.
@@ -1874,23 +2011,23 @@
 /// A set of bytes for searching case insensitively (ASCII only).
 struct ByteSet {
     set: [bool; 256],
 }
 
 impl ByteSet {
     /// Create a new byte set for searching from the given bytes.
-    const fn new(bytes: &[u8]) -> ByteSet {
+    const fn new(bytes: &[u8]) -> Self {
         let mut set = [false; 256];
         let mut i = 0;
         while i < bytes.len() {
             set[bytes[i].to_ascii_uppercase() as usize] = true;
             set[bytes[i].to_ascii_lowercase() as usize] = true;
             i += 1;
         }
-        ByteSet { set }
+        Self { set }
     }
 
     /// Returns the first byte in the haystack if and only if that byte is in
     /// this set (ignoring ASCII case).
     fn starts_with(&self, haystack: &[u8]) -> Option<u8> {
         let byte = *haystack.first()?;
         if self.contains(byte) {
@@ -1932,16 +2069,15 @@
             ErrorKind::Wildcard => write!(f, "wildcards are not allowed in a version"),
             ErrorKind::InvalidDigit { got } if got.is_ascii() => {
                 write!(f, "expected ASCII digit, but found {:?}", char::from(got))
             }
             ErrorKind::InvalidDigit { got } => {
                 write!(
                     f,
-                    "expected ASCII digit, but found non-ASCII byte \\x{:02X}",
-                    got
+                    "expected ASCII digit, but found non-ASCII byte \\x{got:02X}"
                 )
             }
             ErrorKind::NumberTooBig { ref bytes } => {
                 let string = match std::str::from_utf8(bytes) {
                     Ok(v) => v,
                     Err(err) => {
                         std::str::from_utf8(&bytes[..err.valid_up_to()]).expect("valid UTF-8")
@@ -1968,25 +2104,25 @@
                      but no ASCII digits after the epoch were found"
                 )
             }
             ErrorKind::LocalEmpty { precursor } => {
                 write!(
                     f,
                     "found a `{precursor}` indicating the start of a local \
-                     component in a version, but did not find any alpha-numeric \
+                     component in a version, but did not find any alphanumeric \
                      ASCII segment following the `{precursor}`",
                 )
             }
             ErrorKind::UnexpectedEnd {
                 ref version,
                 ref remaining,
             } => {
                 write!(
                     f,
-                    "after parsing {version}, found {remaining:?} after it, \
+                    "after parsing '{version}', found '{remaining}', \
                      which is not part of a valid version",
                 )
             }
         }
     }
 }
 
@@ -2008,34 +2144,34 @@
         bytes: Vec<u8>,
     },
     /// Occurs when a version does not start with a leading number.
     NoLeadingNumber,
     /// Occurs when an epoch version does not have a number after the `!`.
     NoLeadingReleaseNumber,
     /// Occurs when a `+` (or a `.` after the first local segment) is seen
-    /// (indicating a local component of a version), but no alpha-numeric ASCII
+    /// (indicating a local component of a version), but no alphanumeric ASCII
     /// string is found following it.
     LocalEmpty {
         /// Either a `+` or a `[-_.]` indicating what was found that demands a
         /// non-empty local segment following it.
         precursor: char,
     },
     /// Occurs when a version has been parsed but there is some unexpected
     /// trailing data in the string.
     UnexpectedEnd {
         /// The version that has been parsed so far.
-        version: Version,
+        version: String,
         /// The bytes that were remaining and not parsed.
         remaining: String,
     },
 }
 
 impl From<ErrorKind> for VersionParseError {
-    fn from(kind: ErrorKind) -> VersionParseError {
-        VersionParseError {
+    fn from(kind: ErrorKind) -> Self {
+        Self {
             kind: Box::new(kind),
         }
     }
 }
 
 /// An error that occurs when parsing a [`VersionPattern`] string fails.
 #[derive(Clone, Debug, Eq, PartialEq)]
@@ -2060,30 +2196,30 @@
 #[derive(Clone, Debug, Eq, PartialEq)]
 pub(crate) enum PatternErrorKind {
     Version(VersionParseError),
     WildcardNotTrailing,
 }
 
 impl From<PatternErrorKind> for VersionPatternParseError {
-    fn from(kind: PatternErrorKind) -> VersionPatternParseError {
-        VersionPatternParseError {
+    fn from(kind: PatternErrorKind) -> Self {
+        Self {
             kind: Box::new(kind),
         }
     }
 }
 
 impl From<ErrorKind> for VersionPatternParseError {
-    fn from(kind: ErrorKind) -> VersionPatternParseError {
-        VersionPatternParseError::from(VersionParseError::from(kind))
+    fn from(kind: ErrorKind) -> Self {
+        Self::from(VersionParseError::from(kind))
     }
 }
 
 impl From<VersionParseError> for VersionPatternParseError {
-    fn from(err: VersionParseError) -> VersionPatternParseError {
-        VersionPatternParseError {
+    fn from(err: VersionParseError) -> Self {
+        Self {
             kind: Box::new(PatternErrorKind::Version(err)),
         }
     }
 }
 
 /// Workaround for <https://github.com/PyO3/pyo3/pull/2786>
 #[cfg(feature = "pyo3")]
@@ -2239,59 +2375,72 @@
 }
 
 /// Compare the parts attached after the release, given equal release
 ///
 /// According to [a summary of permitted suffixes and relative
 /// ordering][pep440-suffix-ordering] the order of pre/post-releases is: .devN,
 /// aN, bN, rcN, <no suffix (final)>, .postN but also, you can have dev/post
-/// releases on beta releases, so we make a three stage ordering: ({dev: 0, a:
-/// 1, b: 2, rc: 3, (): 4, post: 5}, <preN>, <postN or None as smallest>, <devN
-/// or Max as largest>, <local>)
+/// releases on beta releases, so we make a three stage ordering: ({min: 0,
+/// dev: 1, a: 2, b: 3, rc: 4, (): 5, post: 6}, <preN>, <postN or None as
+/// smallest>, <devN or Max as largest>, <local>)
 ///
 /// For post, any number is better than none (so None defaults to None<0),
 /// but for dev, no number is better (so None default to the maximum). For
 /// local the Option<Vec<T>> luckily already has the correct default Ord
 /// implementation
 ///
 /// [pep440-suffix-ordering]: https://peps.python.org/pep-0440/#summary-of-permitted-suffixes-and-relative-ordering
 fn sortable_tuple(version: &Version) -> (u64, u64, Option<u64>, u64, &[LocalSegment]) {
-    match (version.pre(), version.post(), version.dev()) {
+    // If the version is a "max" version, use a post version larger than any possible post version.
+    let post = if version.max().is_some() {
+        Some(u64::MAX)
+    } else {
+        version.post()
+    };
+    match (version.pre(), post, version.dev(), version.min()) {
+        // min release
+        (_pre, post, _dev, Some(n)) => (0, 0, post, n, version.local()),
         // dev release
-        (None, None, Some(n)) => (0, 0, None, n, version.local()),
+        (None, None, Some(n), None) => (1, 0, None, n, version.local()),
         // alpha release
         (
             Some(PreRelease {
                 kind: PreReleaseKind::Alpha,
                 number: n,
             }),
             post,
             dev,
-        ) => (1, n, post, dev.unwrap_or(u64::MAX), version.local()),
+            None,
+        ) => (2, n, post, dev.unwrap_or(u64::MAX), version.local()),
         // beta release
         (
             Some(PreRelease {
                 kind: PreReleaseKind::Beta,
                 number: n,
             }),
             post,
             dev,
-        ) => (2, n, post, dev.unwrap_or(u64::MAX), version.local()),
+            None,
+        ) => (3, n, post, dev.unwrap_or(u64::MAX), version.local()),
         // alpha release
         (
             Some(PreRelease {
                 kind: PreReleaseKind::Rc,
                 number: n,
             }),
             post,
             dev,
-        ) => (3, n, post, dev.unwrap_or(u64::MAX), version.local()),
+            None,
+        ) => (4, n, post, dev.unwrap_or(u64::MAX), version.local()),
         // final release
-        (None, None, None) => (4, 0, None, 0, version.local()),
+        (None, None, None, None) => (5, 0, None, 0, version.local()),
         // post release
-        (None, Some(post), dev) => (5, 0, Some(post), dev.unwrap_or(u64::MAX), version.local()),
+        (None, Some(post), dev, None) => {
+            (6, 0, Some(post), dev.unwrap_or(u64::MAX), version.local())
+        }
     }
 }
 
 /// Returns true only when, ignoring ASCII case, `needle` is a prefix of
 /// `haystack`.
 fn starts_with_ignore_ascii_case(needle: &[u8], haystack: &[u8]) -> bool {
     needle.len() <= haystack.len()
@@ -2333,33 +2482,14 @@
     Ok(n)
 }
 
 /// The minimum version that can be represented by a [`Version`]: `0a0.dev0`.
 pub static MIN_VERSION: once_cell::sync::Lazy<Version> =
     once_cell::sync::Lazy::new(|| Version::from_str("0a0.dev0").unwrap());
 
-#[cfg(feature = "pubgrub")]
-impl pubgrub::version::Version for Version {
-    fn lowest() -> Self {
-        MIN_VERSION.to_owned()
-    }
-
-    fn bump(&self) -> Self {
-        let mut next = self.clone();
-        if let Some(dev) = next.dev() {
-            next = next.with_dev(Some(dev + 1));
-        } else if let Some(post) = next.post() {
-            next = next.with_post(Some(post + 1));
-        } else {
-            next = next.with_post(Some(0)).with_dev(Some(0));
-        }
-        next
-    }
-}
-
 #[cfg(test)]
 mod tests {
     use std::str::FromStr;
 
     #[cfg(feature = "pyo3")]
     use pyo3::pyfunction;
 
@@ -3017,34 +3147,31 @@
         assert_eq!(
             p("1.2.*.4.*").unwrap_err(),
             PatternErrorKind::WildcardNotTrailing.into(),
         );
         assert_eq!(
             p("1.0-dev1.*").unwrap_err(),
             ErrorKind::UnexpectedEnd {
-                version: Version::new([1, 0]).with_dev(Some(1)),
+                version: "1.0-dev1".to_string(),
                 remaining: ".*".to_string()
             }
             .into(),
         );
         assert_eq!(
             p("1.0a1.*").unwrap_err(),
             ErrorKind::UnexpectedEnd {
-                version: Version::new([1, 0]).with_pre(Some(PreRelease {
-                    kind: PreReleaseKind::Alpha,
-                    number: 1
-                })),
+                version: "1.0a1".to_string(),
                 remaining: ".*".to_string()
             }
             .into(),
         );
         assert_eq!(
             p("1.0.post1.*").unwrap_err(),
             ErrorKind::UnexpectedEnd {
-                version: Version::new([1, 0]).with_post(Some(1)),
+                version: "1.0.post1".to_string(),
                 remaining: ".*".to_string()
             }
             .into(),
         );
         assert_eq!(
             p("1.0+lolwat.*").unwrap_err(),
             ErrorKind::LocalEmpty { precursor: '.' }.into(),
@@ -3363,14 +3490,17 @@
             Version::new([5, 6, 7]).with_local(vec![
                 LocalSegment::String("abc".to_string()),
                 LocalSegment::Number(123),
                 LocalSegment::String("xyz".to_string())
             ])
         );
         assert_eq!(p("  \n5\n \t"), Version::new([5]));
+
+        // min tests
+        assert!(Parser::new("1.min0".as_bytes()).parse().is_err())
     }
 
     // Tests the error cases of our version parser.
     //
     // I wrote these with the intent to cover every possible error
     // case.
     //
@@ -3398,23 +3528,23 @@
             }
             .into()
         );
         assert_eq!(p("5!"), ErrorKind::NoLeadingReleaseNumber.into());
         assert_eq!(
             p("5.6./"),
             ErrorKind::UnexpectedEnd {
-                version: Version::new([5, 6]),
+                version: "5.6".to_string(),
                 remaining: "./".to_string()
             }
             .into()
         );
         assert_eq!(
             p("5.6.-alpha2"),
             ErrorKind::UnexpectedEnd {
-                version: Version::new([5, 6]),
+                version: "5.6".to_string(),
                 remaining: ".-alpha2".to_string()
             }
             .into()
         );
         assert_eq!(
             p("1.2.3a18446744073709551616"),
             ErrorKind::NumberTooBig {
@@ -3430,15 +3560,15 @@
         assert_eq!(
             p("5+abc. "),
             ErrorKind::LocalEmpty { precursor: '.' }.into()
         );
         assert_eq!(
             p("5.6-"),
             ErrorKind::UnexpectedEnd {
-                version: Version::new([5, 6]),
+                version: "5.6".to_string(),
                 remaining: "-".to_string()
             }
             .into()
         );
     }
 
     #[test]
@@ -3506,14 +3636,135 @@
                     less.as_bloated_debug(),
                     greater.as_bloated_debug()
                 );
             }
         }
     }
 
+    #[test]
+    fn min_version() {
+        // Ensure that the `.min` suffix precedes all other suffixes.
+        let less = Version::new([1, 0]).with_min(Some(0));
+
+        let versions = &[
+            "1.dev0",
+            "1.0.dev456",
+            "1.0a1",
+            "1.0a2.dev456",
+            "1.0a12.dev456",
+            "1.0a12",
+            "1.0b1.dev456",
+            "1.0b2",
+            "1.0b2.post345.dev456",
+            "1.0b2.post345",
+            "1.0rc1.dev456",
+            "1.0rc1",
+            "1.0",
+            "1.0+abc.5",
+            "1.0+abc.7",
+            "1.0+5",
+            "1.0.post456.dev34",
+            "1.0.post456",
+            "1.0.15",
+            "1.1.dev1",
+        ];
+
+        for greater in versions.iter() {
+            let greater = greater.parse::<Version>().unwrap();
+            assert_eq!(
+                less.cmp(&greater),
+                Ordering::Less,
+                "less: {:?}\ngreater: {:?}",
+                less.as_bloated_debug(),
+                greater.as_bloated_debug()
+            );
+        }
+    }
+
+    #[test]
+    fn max_version() {
+        // Ensure that the `.max` suffix succeeds all other suffixes.
+        let greater = Version::new([1, 0]).with_max(Some(0));
+
+        let versions = &[
+            "1.dev0",
+            "1.0.dev456",
+            "1.0a1",
+            "1.0a2.dev456",
+            "1.0a12.dev456",
+            "1.0a12",
+            "1.0b1.dev456",
+            "1.0b2",
+            "1.0b2.post345.dev456",
+            "1.0b2.post345",
+            "1.0rc1.dev456",
+            "1.0rc1",
+            "1.0",
+            "1.0+abc.5",
+            "1.0+abc.7",
+            "1.0+5",
+            "1.0.post456.dev34",
+            "1.0.post456",
+            "1.0",
+        ];
+
+        for less in versions.iter() {
+            let less = less.parse::<Version>().unwrap();
+            assert_eq!(
+                less.cmp(&greater),
+                Ordering::Less,
+                "less: {:?}\ngreater: {:?}",
+                less.as_bloated_debug(),
+                greater.as_bloated_debug()
+            );
+        }
+
+        // Ensure that the `.max` suffix plays nicely with pre-release versions.
+        let greater = Version::new([1, 0])
+            .with_pre(Some(PreRelease {
+                kind: PreReleaseKind::Alpha,
+                number: 1,
+            }))
+            .with_max(Some(0));
+
+        let versions = &["1.0a1", "1.0a1+local", "1.0a1.post1"];
+
+        for less in versions.iter() {
+            let less = less.parse::<Version>().unwrap();
+            assert_eq!(
+                less.cmp(&greater),
+                Ordering::Less,
+                "less: {:?}\ngreater: {:?}",
+                less.as_bloated_debug(),
+                greater.as_bloated_debug()
+            );
+        }
+
+        // Ensure that the `.max` suffix plays nicely with pre-release versions.
+        let less = Version::new([1, 0])
+            .with_pre(Some(PreRelease {
+                kind: PreReleaseKind::Alpha,
+                number: 1,
+            }))
+            .with_max(Some(0));
+
+        let versions = &["1.0b1", "1.0b1+local", "1.0b1.post1", "1.0"];
+
+        for greater in versions.iter() {
+            let greater = greater.parse::<Version>().unwrap();
+            assert_eq!(
+                less.cmp(&greater),
+                Ordering::Less,
+                "less: {:?}\ngreater: {:?}",
+                less.as_bloated_debug(),
+                greater.as_bloated_debug()
+            );
+        }
+    }
+
     // Tests our bespoke u64 decimal integer parser.
     #[test]
     fn parse_number_u64() {
         let p = |s: &str| parse_u64(s.as_bytes());
         assert_eq!(p("0"), Ok(0));
         assert_eq!(p("00"), Ok(0));
         assert_eq!(p("1"), Ok(1));
@@ -3573,14 +3824,16 @@
             f.debug_struct("Version")
                 .field("epoch", &self.0.epoch())
                 .field("release", &self.0.release())
                 .field("pre", &self.0.pre())
                 .field("post", &self.0.post())
                 .field("dev", &self.0.dev())
                 .field("local", &self.0.local())
+                .field("min", &self.0.min())
+                .field("max", &self.0.max())
                 .finish()
         }
     }
 
     impl Version {
         pub(crate) fn as_bloated_debug(&self) -> impl std::fmt::Debug + '_ {
             VersionBloatedDebug(self)
```

### Comparing `uv-0.1.9/crates/pep440-rs/src/version_specifier.rs` & `uv-0.2.0/crates/pep440-rs/src/version_specifier.rs`

 * *Files 2% similar despite different names*

```diff
@@ -5,15 +5,14 @@
 #[cfg(feature = "pyo3")]
 use pyo3::{
     exceptions::{PyIndexError, PyNotImplementedError, PyValueError},
     pyclass,
     pyclass::CompareOp,
     pymethods, Py, PyRef, PyRefMut, PyResult,
 };
-#[cfg(feature = "serde")]
 use serde::{de, Deserialize, Deserializer, Serialize, Serializer};
 
 #[cfg(feature = "pyo3")]
 use crate::version::PyVersion;
 use crate::{
     version, Operator, OperatorParseError, Version, VersionPattern, VersionPatternParseError,
 };
@@ -31,34 +30,35 @@
 ///
 /// let version = Version::from_str("1.19").unwrap();
 /// let version_specifiers = VersionSpecifiers::from_str(">=1.16, <2.0").unwrap();
 /// assert!(version_specifiers.contains(&version));
 /// // VersionSpecifiers derefs into a list of specifiers
 /// assert_eq!(version_specifiers.iter().position(|specifier| *specifier.operator() == Operator::LessThan), Some(1));
 /// ```
-#[derive(Eq, PartialEq, Debug, Clone, Hash)]
-#[cfg_attr(
-    feature = "rkyv",
-    derive(rkyv::Archive, rkyv::Deserialize, rkyv::Serialize)
-)]
-#[cfg_attr(feature = "rkyv", archive(check_bytes))]
-#[cfg_attr(feature = "rkyv", archive_attr(derive(Debug)))]
+#[derive(Eq, PartialEq, Debug, Clone, Hash, rkyv::Archive, rkyv::Deserialize, rkyv::Serialize)]
+#[archive(check_bytes)]
+#[archive_attr(derive(Debug))]
 #[cfg_attr(feature = "pyo3", pyclass(sequence))]
 pub struct VersionSpecifiers(Vec<VersionSpecifier>);
 
 impl std::ops::Deref for VersionSpecifiers {
     type Target = [VersionSpecifier];
 
     fn deref(&self) -> &Self::Target {
         &self.0
     }
 }
 
 impl VersionSpecifiers {
-    /// Whether all specifiers match the given version
+    /// Matches all versions.
+    pub fn empty() -> Self {
+        Self(Vec::new())
+    }
+
+    /// Whether all specifiers match the given version.
     pub fn contains(&self, version: &Version) -> bool {
         self.iter().all(|specifier| specifier.contains(version))
     }
 }
 
 impl FromIterator<VersionSpecifier> for VersionSpecifiers {
     fn from_iter<T: IntoIterator<Item = VersionSpecifier>>(iter: T) -> Self {
@@ -159,26 +159,24 @@
 
     /// Whether the version matches all the specifiers
     pub fn __contains__(&self, version: &PyVersion) -> bool {
         self.contains(&version.0)
     }
 }
 
-#[cfg(feature = "serde")]
 impl<'de> Deserialize<'de> for VersionSpecifiers {
     fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
     where
         D: Deserializer<'de>,
     {
         let s = String::deserialize(deserializer)?;
         Self::from_str(&s).map_err(de::Error::custom)
     }
 }
 
-#[cfg(feature = "serde")]
 impl Serialize for VersionSpecifiers {
     #[allow(unstable_name_collisions)]
     fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
     where
         S: Serializer,
     {
         serializer.collect_str(
@@ -219,16 +217,16 @@
 
         let VersionSpecifiersParseErrorInner {
             ref err,
             ref line,
             start,
             end,
         } = *self.inner;
-        writeln!(f, "Failed to parse version: {}:", err)?;
-        writeln!(f, "{}", line)?;
+        writeln!(f, "Failed to parse version: {err}:")?;
+        writeln!(f, "{line}")?;
         let indent = line[..start].width();
         let point = line[start..end].width();
         writeln!(f, "{}{}", " ".repeat(indent), "^".repeat(point))?;
         Ok(())
     }
 }
 
@@ -248,21 +246,28 @@
 /// use std::str::FromStr;
 /// use pep440_rs::{Version, VersionSpecifier};
 ///
 /// let version = Version::from_str("1.19").unwrap();
 /// let version_specifier = VersionSpecifier::from_str("== 1.*").unwrap();
 /// assert!(version_specifier.contains(&version));
 /// ```
-#[derive(Eq, PartialEq, Debug, Clone, Hash)]
-#[cfg_attr(
-    feature = "rkyv",
-    derive(rkyv::Archive, rkyv::Deserialize, rkyv::Serialize)
+#[derive(
+    Eq,
+    Ord,
+    PartialEq,
+    PartialOrd,
+    Debug,
+    Clone,
+    Hash,
+    rkyv::Archive,
+    rkyv::Deserialize,
+    rkyv::Serialize,
 )]
-#[cfg_attr(feature = "rkyv", archive(check_bytes))]
-#[cfg_attr(feature = "rkyv", archive_attr(derive(Debug)))]
+#[archive(check_bytes)]
+#[archive_attr(derive(Debug))]
 #[cfg_attr(feature = "pyo3", pyclass(get_all))]
 pub struct VersionSpecifier {
     /// ~=|==|!=|<=|>=|<|>|===, plus whether the version ended with a star
     pub(crate) operator: Operator,
     /// The whole version part behind the operator
     pub(crate) version: Version,
 }
@@ -313,62 +318,69 @@
         let mut hasher = std::collections::hash_map::DefaultHasher::new();
         self.hash(&mut hasher);
         hasher.finish()
     }
 }
 
 /// <https://github.com/serde-rs/serde/issues/1316#issue-332908452>
-#[cfg(feature = "serde")]
 impl<'de> Deserialize<'de> for VersionSpecifier {
     fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
     where
         D: Deserializer<'de>,
     {
         let s = String::deserialize(deserializer)?;
         FromStr::from_str(&s).map_err(de::Error::custom)
     }
 }
 
 /// <https://github.com/serde-rs/serde/issues/1316#issue-332908452>
-#[cfg(feature = "serde")]
 impl Serialize for VersionSpecifier {
     fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
     where
         S: Serializer,
     {
         serializer.collect_str(self)
     }
 }
 
 impl VersionSpecifier {
     /// Build from parts, validating that the operator is allowed with that version. The last
     /// parameter indicates a trailing `.*`, to differentiate between `1.1.*` and `1.1`
-    pub fn new(
+    pub fn from_pattern(
         operator: Operator,
         version_pattern: VersionPattern,
     ) -> Result<Self, VersionSpecifierBuildError> {
         let star = version_pattern.is_wildcard();
         let version = version_pattern.into_version();
-        // "Local version identifiers are NOT permitted in this version specifier."
-        if version.is_local() && !operator.is_local_compatible() {
-            return Err(BuildErrorKind::OperatorLocalCombo { operator, version }.into());
-        }
 
         // Check if there are star versions and if so, switch operator to star version
         let operator = if star {
             match operator.to_star() {
                 Some(starop) => starop,
                 None => {
                     return Err(BuildErrorKind::OperatorWithStar { operator }.into());
                 }
             }
         } else {
             operator
         };
 
+        Self::from_version(operator, version)
+    }
+
+    /// Create a new version specifier from an operator and a version.
+    pub fn from_version(
+        operator: Operator,
+        version: Version,
+    ) -> Result<Self, VersionSpecifierBuildError> {
+        // "Local version identifiers are NOT permitted in this version specifier."
+        if version.is_local() && !operator.is_local_compatible() {
+            return Err(BuildErrorKind::OperatorLocalCombo { operator, version }.into());
+        }
+
         if operator == Operator::TildeEqual && version.release().len() < 2 {
             return Err(BuildErrorKind::CompatibleRelease.into());
         }
 
         Ok(Self { operator, version })
     }
 
@@ -413,18 +425,15 @@
         // "Except where specifically noted below, local version identifiers MUST NOT be permitted
         // in version specifiers, and local version labels MUST be ignored entirely when checking
         // if candidate versions match a given version specifier."
         let (this, other) = if !self.version.local().is_empty() {
             (self.version.clone(), version.clone())
         } else {
             // self is already without local
-            (
-                self.version.clone().without_local(),
-                version.clone().without_local(),
-            )
+            (self.version.clone(), version.clone().without_local())
         };
 
         match self.operator {
             Operator::Equal => other == this,
             Operator::EqualStar => {
                 this.epoch() == other.epoch()
                     && self
@@ -544,15 +553,15 @@
         s.eat_while(|c: char| c.is_whitespace());
         let version = s.eat_while(|c: char| !c.is_whitespace());
         if version.is_empty() {
             return Err(ParseErrorKind::MissingVersion.into());
         }
         let vpat = version.parse().map_err(ParseErrorKind::InvalidVersion)?;
         let version_specifier =
-            VersionSpecifier::new(operator, vpat).map_err(ParseErrorKind::InvalidSpecifier)?;
+            Self::from_pattern(operator, vpat).map_err(ParseErrorKind::InvalidSpecifier)?;
         s.eat_while(|c: char| c.is_whitespace());
         if !s.done() {
             return Err(ParseErrorKind::InvalidTrailing(s.after().to_string()).into());
         }
         Ok(version_specifier)
     }
 }
@@ -632,16 +641,16 @@
     },
     /// Occurs when the compatible release operator (`~=`) is used with a
     /// version that has fewer than 2 segments in its release version.
     CompatibleRelease,
 }
 
 impl From<BuildErrorKind> for VersionSpecifierBuildError {
-    fn from(kind: BuildErrorKind) -> VersionSpecifierBuildError {
-        VersionSpecifierBuildError {
+    fn from(kind: BuildErrorKind) -> Self {
+        Self {
             kind: Box::new(kind),
         }
     }
 }
 
 /// An error that can occur when parsing or constructing a version specifier.
 #[derive(Clone, Debug, Eq, PartialEq)]
@@ -686,16 +695,16 @@
     InvalidSpecifier(VersionSpecifierBuildError),
     MissingOperator,
     MissingVersion,
     InvalidTrailing(String),
 }
 
 impl From<ParseErrorKind> for VersionSpecifierParseError {
-    fn from(kind: ParseErrorKind) -> VersionSpecifierParseError {
-        VersionSpecifierParseError {
+    fn from(kind: ParseErrorKind) -> Self {
+        Self {
             kind: Box::new(kind),
         }
     }
 }
 
 /// Parse a list of specifiers such as `>= 1.0, != 1.3.*, < 2.0`.
 pub(crate) fn parse_version_specifiers(
@@ -731,15 +740,15 @@
 
 #[cfg(test)]
 mod tests {
     use std::{cmp::Ordering, str::FromStr};
 
     use indoc::indoc;
 
-    use crate::{LocalSegment, PreRelease, PreReleaseKind};
+    use crate::LocalSegment;
 
     use super::*;
 
     /// <https://peps.python.org/pep-0440/#version-matching>
     #[test]
     fn test_equal() {
         let version = Version::from_str("1.1.post1").unwrap();
@@ -824,15 +833,15 @@
         let versions: Vec<Version> = VERSIONS_ALL
             .iter()
             .map(|version| Version::from_str(version).unwrap())
             .collect();
 
         // Below we'll generate every possible combination of VERSIONS_ALL that
         // should be true for the given operator
-        let operations: Vec<_> = [
+        let operations = [
             // Verify that the less than (<) operator works correctly
             versions
                 .iter()
                 .enumerate()
                 .flat_map(|(i, x)| {
                     versions[i + 1..]
                         .iter()
@@ -848,16 +857,15 @@
             versions
                 .iter()
                 .enumerate()
                 .flat_map(|(i, x)| versions[..i].iter().map(move |y| (x, y, Ordering::Greater)))
                 .collect::<Vec<_>>(),
         ]
         .into_iter()
-        .flatten()
-        .collect();
+        .flatten();
 
         for (a, b, ordering) in operations {
             assert_eq!(a.cmp(b), ordering, "{a} {ordering:?} {b}");
         }
     }
 
     const VERSIONS_0: &[&str] = &[
@@ -980,32 +988,28 @@
     ];
 
     /// Test for tilde equal (~=) and star equal (== x.y.*) recorded from pypa/packaging
     ///
     /// Well, except for <https://github.com/pypa/packaging/issues/617>
     #[test]
     fn test_operators_other() {
-        let versions: Vec<Version> = VERSIONS_0
+        let versions = VERSIONS_0
             .iter()
-            .map(|version| Version::from_str(version).unwrap())
-            .collect();
+            .map(|version| Version::from_str(version).unwrap());
         let specifiers: Vec<_> = SPECIFIERS_OTHER
             .iter()
             .map(|specifier| VersionSpecifier::from_str(specifier).unwrap())
             .collect();
 
-        for (version, expected) in versions.iter().zip(EXPECTED_OTHER) {
+        for (version, expected) in versions.zip(EXPECTED_OTHER) {
             let actual = specifiers
                 .iter()
-                .map(|specifier| specifier.contains(version))
-                .collect::<Vec<bool>>();
-            for ((actual, expected), _specifier) in
-                actual.iter().zip(expected).zip(SPECIFIERS_OTHER)
-            {
-                assert_eq!(actual, expected);
+                .map(|specifier| specifier.contains(&version));
+            for ((actual, expected), _specifier) in actual.zip(expected).zip(SPECIFIERS_OTHER) {
+                assert_eq!(actual, *expected);
             }
         }
     }
 
     #[test]
     fn test_arbitrary_equality() {
         assert!(VersionSpecifier::from_str("=== 1.2a1")
@@ -1249,19 +1253,19 @@
     }
 
     #[test]
     fn test_parse_error() {
         let result = VersionSpecifiers::from_str("~= 0.9, %‍= 1.0, != 1.3.4.*");
         assert_eq!(
             result.unwrap_err().to_string(),
-            indoc! {r#"
+            indoc! {r"
                 Failed to parse version: Unexpected end of version specifier, expected operator:
                 ~= 0.9, %‍= 1.0, != 1.3.4.*
                        ^^^^^^^
-            "#}
+            "}
         );
     }
 
     #[test]
     fn test_non_star_after_star() {
         let result = VersionSpecifiers::from_str("== 0.9.*.1");
         assert_eq!(
@@ -1440,76 +1444,70 @@
             ),
             // Prefix matching cannot be used with a pre-release, post-release,
             // dev or local version
             (
                 "==2.0a1.*",
                 ParseErrorKind::InvalidVersion(
                     version::ErrorKind::UnexpectedEnd {
-                        version: Version::new([2, 0]).with_pre(Some(PreRelease {
-                            kind: PreReleaseKind::Alpha,
-                            number: 1,
-                        })),
+                        version: "2.0a1".to_string(),
                         remaining: ".*".to_string(),
                     }
                     .into(),
                 )
                 .into(),
             ),
             (
                 "!=2.0a1.*",
                 ParseErrorKind::InvalidVersion(
                     version::ErrorKind::UnexpectedEnd {
-                        version: Version::new([2, 0]).with_pre(Some(PreRelease {
-                            kind: PreReleaseKind::Alpha,
-                            number: 1,
-                        })),
+                        version: "2.0a1".to_string(),
                         remaining: ".*".to_string(),
                     }
                     .into(),
                 )
                 .into(),
             ),
             (
                 "==2.0.post1.*",
                 ParseErrorKind::InvalidVersion(
                     version::ErrorKind::UnexpectedEnd {
-                        version: Version::new([2, 0]).with_post(Some(1)),
+                        version: "2.0.post1".to_string(),
                         remaining: ".*".to_string(),
                     }
                     .into(),
                 )
                 .into(),
             ),
             (
                 "!=2.0.post1.*",
                 ParseErrorKind::InvalidVersion(
                     version::ErrorKind::UnexpectedEnd {
-                        version: Version::new([2, 0]).with_post(Some(1)),
+                        version: "2.0.post1".to_string(),
                         remaining: ".*".to_string(),
                     }
                     .into(),
                 )
                 .into(),
             ),
             (
                 "==2.0.dev1.*",
                 ParseErrorKind::InvalidVersion(
                     version::ErrorKind::UnexpectedEnd {
-                        version: Version::new([2, 0]).with_dev(Some(1)),
+                        version: "2.0.dev1".to_string(),
                         remaining: ".*".to_string(),
                     }
                     .into(),
                 )
                 .into(),
             ),
             (
                 "!=2.0.dev1.*",
                 ParseErrorKind::InvalidVersion(
                     version::ErrorKind::UnexpectedEnd {
-                        version: Version::new([2, 0]).with_dev(Some(1)),
+                        version: "2.0.dev1".to_string(),
                         remaining: ".*".to_string(),
                     }
                     .into(),
                 )
                 .into(),
             ),
             (
@@ -1540,26 +1538,26 @@
                 ParseErrorKind::InvalidSpecifier(BuildErrorKind::CompatibleRelease.into()).into(),
             ),
             // Cannot use a prefix matching after a .devN version
             (
                 "==1.0.dev1.*",
                 ParseErrorKind::InvalidVersion(
                     version::ErrorKind::UnexpectedEnd {
-                        version: Version::new([1, 0]).with_dev(Some(1)),
+                        version: "1.0.dev1".to_string(),
                         remaining: ".*".to_string(),
                     }
                     .into(),
                 )
                 .into(),
             ),
             (
                 "!=1.0.dev1.*",
                 ParseErrorKind::InvalidVersion(
                     version::ErrorKind::UnexpectedEnd {
-                        version: Version::new([1, 0]).with_dev(Some(1)),
+                        version: "1.0.dev1".to_string(),
                         remaining: ".*".to_string(),
                     }
                     .into(),
                 )
                 .into(),
             ),
         ];
@@ -1668,15 +1666,15 @@
     fn error_message_version_specifier_build_error() {
         let err = VersionSpecifierBuildError {
             kind: Box::new(BuildErrorKind::CompatibleRelease),
         };
         let op = Operator::TildeEqual;
         let v = Version::new([5]);
         let vpat = VersionPattern::verbatim(v);
-        assert_eq!(err, VersionSpecifier::new(op, vpat).unwrap_err());
+        assert_eq!(err, VersionSpecifier::from_pattern(op, vpat).unwrap_err());
         assert_eq!(
             err.to_string(),
             "The ~= operator requires at least two segments in the release version"
         );
     }
 
     /// Tests the human readable error messages generated from parsing invalid
```

### Comparing `uv-0.1.9/crates/pep508-rs/Cargo.toml` & `uv-0.2.0/crates/pep508-rs/Cargo.toml`

 * *Files 13% similar despite different names*

```diff
@@ -13,43 +13,42 @@
 authors = { workspace = true }
 
 [lib]
 name = "pep508_rs"
 crate-type = ["cdylib", "rlib"]
 
 [dependencies]
-pep440_rs = { path = "../pep440-rs" }
-uv-fs = { path = "../uv-fs" }
-uv-normalize = { path = "../uv-normalize" }
+pep440_rs = { workspace = true }
+uv-fs = { workspace = true }
+uv-normalize = { workspace = true }
 
 derivative = { workspace = true }
 once_cell = { workspace = true }
 pyo3 = { workspace = true, optional = true, features = ["abi3", "extension-module"] }
 pyo3-log = { workspace = true, optional = true }
 regex = { workspace = true }
-rkyv = { workspace = true, features = ["strict"], optional = true }
-serde = { workspace = true, features = ["derive"], optional = true }
+serde = { workspace = true, features = ["derive", "rc"] }
 serde_json = { workspace = true, optional = true }
 thiserror = { workspace = true }
 tracing = { workspace = true, optional = true }
 unicode-width = { workspace = true }
-url = { workspace = true }
+url = { workspace = true, features = ["serde"] }
 
 [dev-dependencies]
-indoc = { version = "2.0.4" }
-log = { version = "0.4.20" }
-serde_json = { version = "1.0.111" }
+insta = { version = "1.36.1" }
+log = { version = "0.4.21" }
+serde_json = { version = "1.0.114" }
 testing_logger = { version = "0.1.1" }
 
 [features]
 pyo3 = ["dep:pyo3", "pep440_rs/pyo3", "pyo3-log", "tracing", "tracing/log"]
-rkyv = ["dep:rkyv", "pep440_rs/rkyv", "uv-normalize/rkyv"]
-serde = ["dep:serde", "pep440_rs/serde", "uv-normalize/serde", "url/serde"]
 tracing = ["dep:tracing", "pep440_rs/tracing"]
 # PEP 508 allows only URLs such as `foo @ https://example.org/foo` or `foo @ file:///home/ferris/foo`, and
 # arguably does not allow relative paths in file URLs (`foo @ file://./foo`,
 # `foo @ file:foo-3.0.0-py3-none-any.whl`, `foo @ file://foo-3.0.0-py3-none-any.whl`), as they are not part of the
 # relevant RFCs, even though widely supported. Pip accepts relative file URLs and paths instead of urls
 # (`foo @ ./foo-3.0.0-py3-none-any.whl`). The `non-pep508-features` controls whether these non-spec features will
 # be supported.
 non-pep508-extensions = []
 default = []
+# Match the API of the published crate, for compatibility.
+serde = []
```

### Comparing `uv-0.1.9/crates/pep508-rs/License-Apache` & `uv-0.2.0/crates/pep440-rs/License-Apache`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/pep508-rs/License-BSD` & `uv-0.2.0/crates/pep440-rs/License-BSD`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/pep508-rs/Readme.md` & `uv-0.2.0/crates/pep508-rs/Readme.md`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/pep508-rs/src/lib.rs` & `uv-0.2.0/crates/pep508-rs/src/lib.rs`

 * *Files 14% similar despite different names*

```diff
@@ -1,88 +1,100 @@
 //! A library for python [dependency specifiers](https://packaging.python.org/en/latest/specifications/dependency-specifiers/)
 //! better known as [PEP 508](https://peps.python.org/pep-0508/)
 //!
 //! ## Usage
 //!
 //! ```
 //! use std::str::FromStr;
-//! use pep508_rs::Requirement;
+//! use pep508_rs::{Requirement, VerbatimUrl};
 //! use uv_normalize::ExtraName;
 //!
 //! let marker = r#"requests [security,tests] >= 2.8.1, == 2.8.* ; python_version > "3.8""#;
-//! let dependency_specification = Requirement::from_str(marker).unwrap();
+//! let dependency_specification = Requirement::<VerbatimUrl>::from_str(marker).unwrap();
 //! assert_eq!(dependency_specification.name.as_ref(), "requests");
 //! assert_eq!(dependency_specification.extras, vec![ExtraName::from_str("security").unwrap(), ExtraName::from_str("tests").unwrap()]);
 //! ```
 
 #![warn(missing_docs)]
 
+use cursor::Cursor;
 #[cfg(feature = "pyo3")]
 use std::collections::hash_map::DefaultHasher;
 use std::collections::HashSet;
-use std::fmt::{Display, Formatter};
+use std::error::Error;
+use std::fmt::{Debug, Display, Formatter};
 #[cfg(feature = "pyo3")]
 use std::hash::{Hash, Hasher};
+#[cfg(feature = "pyo3")]
+use std::ops::Deref;
 use std::path::Path;
-use std::str::{Chars, FromStr};
+use std::str::FromStr;
 
 #[cfg(feature = "pyo3")]
-use pep440_rs::PyVersion;
-#[cfg(feature = "pyo3")]
 use pyo3::{
     create_exception, exceptions::PyNotImplementedError, pyclass, pyclass::CompareOp, pymethods,
     pymodule, types::PyModule, IntoPy, PyObject, PyResult, Python,
 };
-#[cfg(feature = "serde")]
 use serde::{de, Deserialize, Deserializer, Serialize, Serializer};
 use thiserror::Error;
 use unicode_width::UnicodeWidthChar;
+use url::Url;
 
 pub use marker::{
-    MarkerEnvironment, MarkerExpression, MarkerOperator, MarkerTree, MarkerValue,
-    MarkerValueString, MarkerValueVersion, MarkerWarningKind, StringVersion,
+    ExtraOperator, MarkerEnvironment, MarkerEnvironmentBuilder, MarkerExpression, MarkerOperator,
+    MarkerTree, MarkerValue, MarkerValueString, MarkerValueVersion, MarkerWarningKind,
+    StringVersion,
 };
-use pep440_rs::{Version, VersionSpecifier, VersionSpecifiers};
-use uv_fs::normalize_url_path;
 #[cfg(feature = "pyo3")]
-use uv_normalize::InvalidNameError;
-use uv_normalize::{ExtraName, PackageName};
-pub use verbatim_url::{split_scheme, Scheme, VerbatimUrl};
+use pep440_rs::PyVersion;
+use pep440_rs::{Version, VersionSpecifier, VersionSpecifiers};
+#[cfg(feature = "non-pep508-extensions")]
+pub use unnamed::UnnamedRequirement;
+// Parity with the crates.io version of pep508_rs
+pub use origin::RequirementOrigin;
+pub use uv_normalize::{ExtraName, InvalidNameError, PackageName};
+pub use verbatim_url::{
+    expand_env_vars, split_scheme, strip_host, Scheme, VerbatimUrl, VerbatimUrlError,
+};
 
+mod cursor;
 mod marker;
+mod origin;
+#[cfg(feature = "non-pep508-extensions")]
+mod unnamed;
 mod verbatim_url;
 
 /// Error with a span attached. Not that those aren't `String` but `Vec<char>` indices.
 #[derive(Debug)]
-pub struct Pep508Error {
+pub struct Pep508Error<T: Pep508Url = VerbatimUrl> {
     /// Either we have an error string from our parser or an upstream error from `url`
-    pub message: Pep508ErrorSource,
+    pub message: Pep508ErrorSource<T>,
     /// Span start index
     pub start: usize,
     /// Span length
     pub len: usize,
     /// The input string so we can print it underlined
     pub input: String,
 }
 
 /// Either we have an error string from our parser or an upstream error from `url`
 #[derive(Debug, Error)]
-pub enum Pep508ErrorSource {
+pub enum Pep508ErrorSource<T: Pep508Url = VerbatimUrl> {
     /// An error from our parser.
     #[error("{0}")]
     String(String),
     /// A URL parsing error.
     #[error(transparent)]
-    UrlError(#[from] verbatim_url::VerbatimUrlError),
+    UrlError(T::Err),
     /// The version requirement is not supported.
     #[error("{0}")]
     UnsupportedRequirement(String),
 }
 
-impl Display for Pep508Error {
+impl<T: Pep508Url> Display for Pep508Error<T> {
     /// Pretty formatting with underline.
     fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
         // We can use char indices here since it's a Vec<char>
         let start_offset = self.input[..self.start]
             .chars()
             .flat_map(|c| c.width())
             .sum::<usize>();
@@ -108,45 +120,57 @@
             " ".repeat(start_offset),
             "^".repeat(underline_len)
         )
     }
 }
 
 /// We need this to allow e.g. anyhow's `.context()`
-impl std::error::Error for Pep508Error {}
+impl<E: Error + Debug, T: Pep508Url<Err = E>> std::error::Error for Pep508Error<T> {}
 
 #[cfg(feature = "pyo3")]
 create_exception!(
     pep508,
     PyPep508Error,
     pyo3::exceptions::PyValueError,
     "A PEP 508 parser error with span information"
 );
 
-/// A PEP 508 dependency specification
+/// A PEP 508 dependency specifier.
 #[derive(Hash, Debug, Clone, Eq, PartialEq)]
-#[cfg_attr(feature = "pyo3", pyclass(module = "pep508"))]
-pub struct Requirement {
-    /// The distribution name such as `numpy` in
-    /// `requests [security,tests] >= 2.8.1, == 2.8.* ; python_version > "3.8"`
+pub struct Requirement<T: Pep508Url = VerbatimUrl> {
+    /// The distribution name such as `requests` in
+    /// `requests [security,tests] >= 2.8.1, == 2.8.* ; python_version > "3.8"`.
     pub name: PackageName,
     /// The list of extras such as `security`, `tests` in
-    /// `requests [security,tests] >= 2.8.1, == 2.8.* ; python_version > "3.8"`
+    /// `requests [security,tests] >= 2.8.1, == 2.8.* ; python_version > "3.8"`.
     pub extras: Vec<ExtraName>,
     /// The version specifier such as `>= 2.8.1`, `== 2.8.*` in
-    /// `requests [security,tests] >= 2.8.1, == 2.8.* ; python_version > "3.8"`
-    /// or a url
-    pub version_or_url: Option<VersionOrUrl>,
+    /// `requests [security,tests] >= 2.8.1, == 2.8.* ; python_version > "3.8"`.
+    /// or a URL.
+    pub version_or_url: Option<VersionOrUrl<T>>,
     /// The markers such as `python_version > "3.8"` in
     /// `requests [security,tests] >= 2.8.1, == 2.8.* ; python_version > "3.8"`.
-    /// Those are a nested and/or tree
+    /// Those are a nested and/or tree.
     pub marker: Option<MarkerTree>,
+    /// The source file containing the requirement.
+    pub origin: Option<RequirementOrigin>,
+}
+
+impl Requirement {
+    /// Set the source file containing the requirement.
+    #[must_use]
+    pub fn with_origin(self, origin: RequirementOrigin) -> Self {
+        Self {
+            origin: Some(origin),
+            ..self
+        }
+    }
 }
 
-impl Display for Requirement {
+impl<T: Pep508Url + Display> Display for Requirement<T> {
     fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
         write!(f, "{}", self.name)?;
         if !self.extras.is_empty() {
             write!(
                 f,
                 "[{}]",
                 self.extras
@@ -157,15 +181,15 @@
             )?;
         }
         if let Some(version_or_url) = &self.version_or_url {
             match version_or_url {
                 VersionOrUrl::VersionSpecifier(version_specifier) => {
                     let version_specifier: Vec<String> =
                         version_specifier.iter().map(ToString::to_string).collect();
-                    write!(f, " {}", version_specifier.join(", "))?;
+                    write!(f, "{}", version_specifier.join(","))?;
                 }
                 VersionOrUrl::Url(url) => {
                     // We add the space for markers later if necessary
                     write!(f, " @ {url}")?;
                 }
             }
         }
@@ -173,42 +197,55 @@
             write!(f, " ; {marker}")?;
         }
         Ok(())
     }
 }
 
 /// <https://github.com/serde-rs/serde/issues/908#issuecomment-298027413>
-#[cfg(feature = "serde")]
-impl<'de> Deserialize<'de> for Requirement {
+impl<'de, T: Pep508Url + Deserialize<'de>> Deserialize<'de> for Requirement<T> {
     fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
     where
         D: Deserializer<'de>,
     {
         let s = String::deserialize(deserializer)?;
         FromStr::from_str(&s).map_err(de::Error::custom)
     }
 }
 
 /// <https://github.com/serde-rs/serde/issues/1316#issue-332908452>
-#[cfg(feature = "serde")]
-impl Serialize for Requirement {
+impl<T: Pep508Url> Serialize for Requirement<T> {
     fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
     where
         S: Serializer,
     {
         serializer.collect_str(self)
     }
 }
 
-type MarkerWarning = (MarkerWarningKind, String, String);
+type MarkerWarning = (MarkerWarningKind, String);
+
+#[cfg(feature = "pyo3")]
+#[pyclass(module = "pep508", name = "Requirement")]
+#[derive(Hash, Debug, Clone, Eq, PartialEq)]
+/// A PEP 508 dependency specifier.
+pub struct PyRequirement(Requirement);
+
+#[cfg(feature = "pyo3")]
+impl Deref for PyRequirement {
+    type Target = Requirement;
+
+    fn deref(&self) -> &Self::Target {
+        &self.0
+    }
+}
 
 #[cfg(feature = "pyo3")]
 #[pymethods]
-impl Requirement {
-    /// The distribution name such as `numpy` in
+impl PyRequirement {
+    /// The distribution name such as `requests` in
     /// `requests [security,tests] >= 2.8.1, == 2.8.* ; python_version > "3.8"`
     #[getter]
     pub fn name(&self) -> String {
         self.name.to_string()
     }
 
     /// The list of extras such as `security`, `tests` in
@@ -224,15 +261,18 @@
     pub fn marker(&self) -> Option<String> {
         self.marker.as_ref().map(std::string::ToString::to_string)
     }
 
     /// Parses a PEP 440 string
     #[new]
     pub fn py_new(requirement: &str) -> PyResult<Self> {
-        Self::from_str(requirement).map_err(|err| PyPep508Error::new_err(err.to_string()))
+        Ok(Self(
+            Requirement::from_str(requirement)
+                .map_err(|err| PyPep508Error::new_err(err.to_string()))?,
+        ))
     }
 
     #[getter]
     fn version_or_url(&self, py: Python<'_>) -> PyObject {
         match &self.version_or_url {
             None => py.None(),
             Some(VersionOrUrl::VersionSpecifier(version_specifier)) => version_specifier
@@ -245,15 +285,15 @@
     }
 
     fn __str__(&self) -> String {
         self.to_string()
     }
 
     fn __repr__(&self) -> String {
-        format!(r#""{self}""#)
+        self.to_string()
     }
 
     fn __richcmp__(&self, other: &Self, op: CompareOp) -> PyResult<bool> {
         let err = PyNotImplementedError::new_err("Requirement only supports equality comparisons");
         match op {
             CompareOp::Lt => Err(err),
             CompareOp::Le => Err(err),
@@ -328,15 +368,15 @@
             .collect::<Result<Vec<_>, InvalidNameError>>()
             .map_err(|err| PyPep508Error::new_err(err.to_string()))?;
 
         Ok(self.evaluate_markers_and_report(env, &extras))
     }
 }
 
-impl Requirement {
+impl<T: Pep508Url> Requirement<T> {
     /// Returns `true` if the [`Version`] satisfies the [`Requirement`].
     pub fn is_satisfied_by(&self, version: &Version) -> bool {
         let Some(version_or_url) = self.version_or_url.as_ref() else {
             return true;
         };
 
         let specifiers = match version_or_url {
@@ -373,215 +413,231 @@
         if let Some(marker) = &self.marker {
             marker.evaluate_extras_and_python_version(extras, python_versions)
         } else {
             true
         }
     }
 
-    /// Returns whether the markers apply for the given environment
+    /// Returns whether the markers apply for the given environment.
     pub fn evaluate_markers_and_report(
         &self,
         env: &MarkerEnvironment,
         extras: &[ExtraName],
     ) -> (bool, Vec<MarkerWarning>) {
         if let Some(marker) = &self.marker {
             marker.evaluate_collect_warnings(env, extras)
         } else {
             (true, Vec::new())
         }
     }
+
+    /// Return the requirement with an additional marker added, to require the given extra.
+    ///
+    /// For example, given `flask >= 2.0.2`, calling `with_extra_marker("dotenv")` would return
+    /// `flask >= 2.0.2 ; extra == "dotenv"`.
+    pub fn with_extra_marker(self, extra: &ExtraName) -> Self {
+        let marker = match self.marker {
+            Some(expression) => MarkerTree::And(vec![
+                expression,
+                MarkerTree::Expression(MarkerExpression::Extra {
+                    operator: ExtraOperator::Equal,
+                    name: extra.clone(),
+                }),
+            ]),
+            None => MarkerTree::Expression(MarkerExpression::Extra {
+                operator: ExtraOperator::Equal,
+                name: extra.clone(),
+            }),
+        };
+        Self {
+            marker: Some(marker),
+            ..self
+        }
+    }
 }
 
-impl FromStr for Requirement {
-    type Err = Pep508Error;
+/// Type to parse URLs from `name @ <url>` into. Defaults to [`url::Url`].
+pub trait Pep508Url: Clone + Display + Debug {
+    /// String to URL parsing error
+    type Err: Error + Debug;
 
-    /// Parse a [Dependency Specifier](https://packaging.python.org/en/latest/specifications/dependency-specifiers/)
+    /// Parse a url from `name @ <url>`. Defaults to [`url::Url::parse_url`].
+    fn parse_url(url: &str, working_dir: Option<&Path>) -> Result<Self, Self::Err>;
+}
+
+impl Pep508Url for Url {
+    type Err = url::ParseError;
+
+    fn parse_url(url: &str, _working_dir: Option<&Path>) -> Result<Self, Self::Err> {
+        Url::parse(url)
+    }
+}
+
+/// A reporter for warnings that occur during marker parsing or evaluation.
+pub trait Reporter {
+    /// Report a warning.
+    fn report(&mut self, kind: MarkerWarningKind, warning: String);
+}
+
+impl<F> Reporter for F
+where
+    F: FnMut(MarkerWarningKind, String),
+{
+    fn report(&mut self, kind: MarkerWarningKind, warning: String) {
+        (self)(kind, warning)
+    }
+}
+
+/// A simple [`Reporter`] that logs to tracing when the `tracing` feature is enabled.
+pub struct TracingReporter;
+
+impl Reporter for TracingReporter {
+    fn report(&mut self, _kind: MarkerWarningKind, _message: String) {
+        #[cfg(feature = "tracing")]
+        {
+            tracing::warn!("{}", _message);
+        }
+    }
+}
+
+impl<T: Pep508Url> FromStr for Requirement<T> {
+    type Err = Pep508Error<T>;
+
+    /// Parse a [Dependency Specifier](https://packaging.python.org/en/latest/specifications/dependency-specifiers/).
     fn from_str(input: &str) -> Result<Self, Self::Err> {
-        parse(&mut Cursor::new(input), None)
+        parse_pep508_requirement::<T>(&mut Cursor::new(input), None, &mut TracingReporter)
     }
 }
 
-impl Requirement {
+impl<T: Pep508Url> Requirement<T> {
+    /// Parse a [Dependency Specifier](https://packaging.python.org/en/latest/specifications/dependency-specifiers/).
+    pub fn parse(input: &str, working_dir: impl AsRef<Path>) -> Result<Self, Pep508Error<T>> {
+        parse_pep508_requirement(
+            &mut Cursor::new(input),
+            Some(working_dir.as_ref()),
+            &mut TracingReporter,
+        )
+    }
+
     /// Parse a [Dependency Specifier](https://packaging.python.org/en/latest/specifications/dependency-specifiers/)
-    pub fn parse(input: &str, working_dir: impl AsRef<Path>) -> Result<Self, Pep508Error> {
-        parse(&mut Cursor::new(input), Some(working_dir.as_ref()))
+    /// with the given reporter for warnings.
+    pub fn parse_reporter(
+        input: &str,
+        working_dir: impl AsRef<Path>,
+        reporter: &mut impl Reporter,
+    ) -> Result<Self, Pep508Error<T>> {
+        parse_pep508_requirement(
+            &mut Cursor::new(input),
+            Some(working_dir.as_ref()),
+            reporter,
+        )
+    }
+
+    /// Convert a requirement with one URL type into one with another URL type.
+    ///
+    /// Example: `Requirement<Url>` to `Requirement<VerbatimUrl>`.
+    pub fn convert_url<U: Pep508Url + From<T>>(self) -> Requirement<U> {
+        let Requirement {
+            name,
+            extras,
+            version_or_url,
+            marker,
+            origin,
+        } = self;
+        Requirement {
+            name,
+            extras,
+            version_or_url: match version_or_url {
+                None => None,
+                Some(VersionOrUrl::VersionSpecifier(specifier)) => {
+                    Some(VersionOrUrl::VersionSpecifier(specifier))
+                }
+                Some(VersionOrUrl::Url(url)) => Some(VersionOrUrl::Url(U::from(url))),
+            },
+            marker,
+            origin,
+        }
     }
 }
 
 /// A list of [`ExtraName`] that can be attached to a [`Requirement`].
 #[derive(Debug, Clone, Eq, Hash, PartialEq)]
 pub struct Extras(Vec<ExtraName>);
 
 impl Extras {
     /// Parse a list of extras.
-    pub fn parse(input: &str) -> Result<Self, Pep508Error> {
-        Ok(Self(parse_extras(&mut Cursor::new(input))?))
+    pub fn parse<T: Pep508Url>(input: &str) -> Result<Self, Pep508Error<T>> {
+        Ok(Self(parse_extras_cursor(&mut Cursor::new(input))?))
     }
 
     /// Convert the [`Extras`] into a [`Vec`] of [`ExtraName`].
     pub fn into_vec(self) -> Vec<ExtraName> {
         self.0
     }
 }
 
-/// The actual version specifier or url to install
+/// The actual version specifier or URL to install.
 #[derive(Debug, Clone, Eq, Hash, PartialEq)]
-pub enum VersionOrUrl {
+pub enum VersionOrUrl<T: Pep508Url = VerbatimUrl> {
     /// A PEP 440 version specifier set
     VersionSpecifier(VersionSpecifiers),
     /// A installable URL
-    Url(VerbatimUrl),
+    Url(T),
 }
 
-/// A [`Cursor`] over a string.
-#[derive(Debug, Clone)]
-pub struct Cursor<'a> {
-    input: &'a str,
-    chars: Chars<'a>,
-    pos: usize,
+/// Unowned version specifier or URL to install.
+#[derive(Debug, Clone, Copy, Eq, Hash, PartialEq)]
+pub enum VersionOrUrlRef<'a, T: Pep508Url = VerbatimUrl> {
+    /// A PEP 440 version specifier set
+    VersionSpecifier(&'a VersionSpecifiers),
+    /// A installable URL
+    Url(&'a T),
 }
 
-impl<'a> Cursor<'a> {
-    /// Convert from `&str`.
-    pub fn new(input: &'a str) -> Self {
-        Self {
-            input,
-            chars: input.chars(),
-            pos: 0,
-        }
-    }
-
-    /// Returns a new cursor starting at the given position.
-    pub fn at(self, pos: usize) -> Self {
-        Self {
-            input: self.input,
-            chars: self.input[pos..].chars(),
-            pos,
-        }
-    }
-
-    /// Returns the current byte position of the cursor.
-    fn pos(&self) -> usize {
-        self.pos
-    }
-
-    /// Returns a slice over the input string.
-    fn slice(&self, start: usize, len: usize) -> &str {
-        &self.input[start..start + len]
-    }
-
-    /// Peeks the next character and position from the input stream without consuming it.
-    fn peek(&self) -> Option<(usize, char)> {
-        self.chars.clone().next().map(|char| (self.pos, char))
-    }
-
-    /// Peeks the next character from the input stream without consuming it.
-    fn peek_char(&self) -> Option<char> {
-        self.chars.clone().next()
-    }
-
-    /// Eats the next character from the input stream if it matches the given token.
-    fn eat_char(&mut self, token: char) -> Option<usize> {
-        let (start_pos, peek_char) = self.peek()?;
-        if peek_char == token {
-            self.next();
-            Some(start_pos)
-        } else {
-            None
-        }
-    }
-
-    /// Consumes whitespace from the cursor.
-    fn eat_whitespace(&mut self) {
-        while let Some(char) = self.peek_char() {
-            if char.is_whitespace() {
-                self.next();
-            } else {
-                return;
-            }
-        }
-    }
-
-    /// Returns the next character and position from the input stream and consumes it.
-    fn next(&mut self) -> Option<(usize, char)> {
-        let pos = self.pos;
-        let char = self.chars.next()?;
-        self.pos += char.len_utf8();
-        Some((pos, char))
-    }
-
-    /// Peeks over the cursor as long as the condition is met, without consuming it.
-    fn peek_while(&mut self, condition: impl Fn(char) -> bool) -> (usize, usize) {
-        let peeker = self.chars.clone();
-        let start = self.pos();
-        let len = peeker.take_while(|c| condition(*c)).count();
-        (start, len)
-    }
-
-    /// Consumes characters from the cursor as long as the condition is met.
-    fn take_while(&mut self, condition: impl Fn(char) -> bool) -> (usize, usize) {
-        let start = self.pos();
-        let mut len = 0;
-        while let Some(char) = self.peek_char() {
-            if !condition(char) {
-                break;
+impl<'a> From<&'a VersionOrUrl> for VersionOrUrlRef<'a> {
+    fn from(value: &'a VersionOrUrl) -> Self {
+        match value {
+            VersionOrUrl::VersionSpecifier(version_specifier) => {
+                VersionOrUrlRef::VersionSpecifier(version_specifier)
             }
-
-            self.next();
-            len += char.len_utf8();
-        }
-        (start, len)
-    }
-
-    /// Consumes characters from the cursor, raising an error if it doesn't match the given token.
-    fn next_expect_char(&mut self, expected: char, span_start: usize) -> Result<(), Pep508Error> {
-        match self.next() {
-            None => Err(Pep508Error {
-                message: Pep508ErrorSource::String(format!(
-                    "Expected '{expected}', found end of dependency specification"
-                )),
-                start: span_start,
-                len: 1,
-                input: self.to_string(),
-            }),
-            Some((_, value)) if value == expected => Ok(()),
-            Some((pos, other)) => Err(Pep508Error {
-                message: Pep508ErrorSource::String(format!(
-                    "Expected '{expected}', found '{other}'"
-                )),
-                start: pos,
-                len: other.len_utf8(),
-                input: self.to_string(),
-            }),
+            VersionOrUrl::Url(url) => VersionOrUrlRef::Url(url),
         }
     }
 }
 
-impl Display for Cursor<'_> {
-    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
-        write!(f, "{}", self.input)
-    }
-}
-
-fn parse_name(cursor: &mut Cursor) -> Result<PackageName, Pep508Error> {
+fn parse_name<T: Pep508Url>(cursor: &mut Cursor) -> Result<PackageName, Pep508Error<T>> {
     // https://peps.python.org/pep-0508/#names
     // ^([A-Z0-9]|[A-Z0-9][A-Z0-9._-]*[A-Z0-9])$ with re.IGNORECASE
+    let start = cursor.pos();
     let mut name = String::new();
+
     if let Some((index, char)) = cursor.next() {
         if matches!(char, 'A'..='Z' | 'a'..='z' | '0'..='9') {
             name.push(char);
         } else {
-            return Err(Pep508Error {
-                message: Pep508ErrorSource::String(format!(
-                    "Expected package name starting with an alphanumeric character, found '{char}'"
-                )),
-                start: index,
-                len: char.len_utf8(),
-                input: cursor.to_string(),
-            });
+            // Check if the user added a filesystem path without a package name. pip supports this
+            // in `requirements.txt`, but it doesn't adhere to the PEP 508 grammar.
+            let mut clone = cursor.clone().at(start);
+            return if looks_like_unnamed_requirement(&mut clone) {
+                Err(Pep508Error {
+                    message: Pep508ErrorSource::UnsupportedRequirement("URL requirement must be preceded by a package name. Add the name of the package before the URL (e.g., `package_name @ /path/to/file`).".to_string()),
+                    start,
+                    len: clone.pos() - start,
+                    input: clone.to_string(),
+                })
+            } else {
+                Err(Pep508Error {
+                    message: Pep508ErrorSource::String(format!(
+                        "Expected package name starting with an alphanumeric character, found '{char}'"
+                    )),
+                    start: index,
+                    len: char.len_utf8(),
+                    input: cursor.to_string(),
+                })
+            };
         }
     } else {
         return Err(Pep508Error {
             message: Pep508ErrorSource::String("Empty field is not allowed for PEP508".to_string()),
             start: 0,
             len: 1,
             input: cursor.to_string(),
@@ -609,36 +665,108 @@
                 return Ok(PackageName::new(name)
                     .expect("`PackageName` validation should match PEP 508 parsing"));
             }
         }
     }
 }
 
+/// Parse a potential URL from the [`Cursor`], advancing the [`Cursor`] to the end of the URL.
+///
+/// Returns `true` if the URL appears to be a viable unnamed requirement, and `false` otherwise.
+fn looks_like_unnamed_requirement(cursor: &mut Cursor) -> bool {
+    // Read the entire path.
+    let (start, len) = cursor.take_while(|char| !char.is_whitespace());
+    let url = cursor.slice(start, len);
+
+    // Expand any environment variables in the path.
+    let expanded = expand_env_vars(url);
+
+    // Analyze the path.
+    let mut chars = expanded.chars();
+
+    let Some(first_char) = chars.next() else {
+        return false;
+    };
+
+    // Ex) `/bin/ls`
+    if first_char == '\\' || first_char == '/' || first_char == '.' {
+        return true;
+    }
+
+    // Ex) `https://` or `C:`
+    if split_scheme(&expanded).is_some() {
+        return true;
+    }
+
+    false
+}
+
 /// parses extras in the `[extra1,extra2] format`
-fn parse_extras(cursor: &mut Cursor) -> Result<Vec<ExtraName>, Pep508Error> {
+fn parse_extras_cursor<T: Pep508Url>(
+    cursor: &mut Cursor,
+) -> Result<Vec<ExtraName>, Pep508Error<T>> {
     let Some(bracket_pos) = cursor.eat_char('[') else {
         return Ok(vec![]);
     };
+    cursor.eat_whitespace();
+
     let mut extras = Vec::new();
+    let mut is_first_iteration = true;
 
     loop {
+        // End of the extras section. (Empty extras are allowed.)
+        if let Some(']') = cursor.peek_char() {
+            cursor.next();
+            break;
+        }
+
+        // Comma separator
+        match (cursor.peek(), is_first_iteration) {
+            // For the first iteration, we don't expect a comma.
+            (Some((pos, ',')), true) => {
+                return Err(Pep508Error {
+                    message: Pep508ErrorSource::String(
+                        "Expected either alphanumerical character (starting the extra name) or ']' (ending the extras section), found ','".to_string()
+                    ),
+                    start: pos,
+                    len: 1,
+                    input: cursor.to_string(),
+                });
+            }
+            // For the other iterations, the comma is required.
+            (Some((_, ',')), false) => {
+                cursor.next();
+            }
+            (Some((pos, other)), false) => {
+                return Err(Pep508Error {
+                    message: Pep508ErrorSource::String(
+                        format!("Expected either ',' (separating extras) or ']' (ending the extras section), found '{other}'")
+                    ),
+                    start: pos,
+                    len: 1,
+                    input: cursor.to_string(),
+                });
+            }
+            _ => {}
+        }
+
         // wsp* before the identifier
         cursor.eat_whitespace();
         let mut buffer = String::new();
         let early_eof_error = Pep508Error {
             message: Pep508ErrorSource::String(
                 "Missing closing bracket (expected ']', found end of dependency specification)"
                     .to_string(),
             ),
             start: bracket_pos,
             len: 1,
             input: cursor.to_string(),
         };
 
-        // First char of the identifier
+        // First char of the identifier.
         match cursor.next() {
             // letterOrDigit
             Some((_, alphanumeric @ ('a'..='z' | 'A'..='Z' | '0'..='9'))) => {
                 buffer.push(alphanumeric);
             }
             Some((pos, other)) => {
                 return Err(Pep508Error {
@@ -670,175 +798,107 @@
                     input: cursor.to_string(),
                 });
             }
             _ => {}
         };
         // wsp* after the identifier
         cursor.eat_whitespace();
-        // end or next identifier?
-        match cursor.next() {
-            Some((_, ',')) => {
-                extras.push(
-                    ExtraName::new(buffer)
-                        .expect("`ExtraName` validation should match PEP 508 parsing"),
-                );
-            }
-            Some((_, ']')) => {
-                extras.push(
-                    ExtraName::new(buffer)
-                        .expect("`ExtraName` validation should match PEP 508 parsing"),
-                );
-                break;
-            }
-            Some((pos, other)) => {
-                return Err(Pep508Error {
-                    message: Pep508ErrorSource::String(format!(
-                        "Expected either ',' (separating extras) or ']' (ending the extras section), found '{other}'"
-                    )),
-                    start: pos,
-                    len: other.len_utf8(),
-                    input: cursor.to_string(),
-                });
-            }
-            None => return Err(early_eof_error),
-        }
+
+        // Add the parsed extra
+        extras.push(
+            ExtraName::new(buffer).expect("`ExtraName` validation should match PEP 508 parsing"),
+        );
+        is_first_iteration = false;
     }
 
     Ok(extras)
 }
 
 /// Parse a raw string for a URL requirement, which could be either a URL or a local path, and which
 /// could contain unexpanded environment variables.
 ///
 /// For example:
 /// - `https://pypi.org/project/requests/...`
 /// - `file:///home/ferris/project/scripts/...`
 /// - `file:../editable/`
 /// - `../editable/`
 /// - `https://download.pytorch.org/whl/torch_stable.html`
-fn parse_url(cursor: &mut Cursor, working_dir: Option<&Path>) -> Result<VerbatimUrl, Pep508Error> {
+fn parse_url<T: Pep508Url>(
+    cursor: &mut Cursor,
+    working_dir: Option<&Path>,
+) -> Result<T, Pep508Error<T>> {
     // wsp*
     cursor.eat_whitespace();
     // <URI_reference>
     let (start, len) = cursor.take_while(|char| !char.is_whitespace());
     let url = cursor.slice(start, len);
     if url.is_empty() {
         return Err(Pep508Error {
             message: Pep508ErrorSource::String("Expected URL".to_string()),
             start,
             len,
             input: cursor.to_string(),
         });
     }
 
-    let url = preprocess_url(url, working_dir, cursor, start, len)?;
+    let url = T::parse_url(url, working_dir).map_err(|err| Pep508Error {
+        message: Pep508ErrorSource::UrlError(err),
+        start,
+        len,
+        input: cursor.to_string(),
+    })?;
 
     Ok(url)
 }
 
-/// Create a `VerbatimUrl` to represent the requirement.
-fn preprocess_url(
-    url: &str,
-    #[cfg_attr(not(feature = "non-pep508-extensions"), allow(unused))] working_dir: Option<&Path>,
-    cursor: &Cursor,
-    start: usize,
-    len: usize,
-) -> Result<VerbatimUrl, Pep508Error> {
-    if let Some((scheme, path)) = split_scheme(url) {
-        match Scheme::parse(scheme) {
-            // Ex) `file:///home/ferris/project/scripts/...` or `file:../editable/`.
-            Some(Scheme::File) => {
-                let path = path.strip_prefix("//").unwrap_or(path);
-
-                // Transform, e.g., `/C:/Users/ferris/wheel-0.42.0.tar.gz` to `C:\Users\ferris\wheel-0.42.0.tar.gz`.
-                let path = normalize_url_path(path);
-
-                #[cfg(feature = "non-pep508-extensions")]
-                if let Some(working_dir) = working_dir {
-                    return Ok(
-                        VerbatimUrl::from_path(path, working_dir).with_given(url.to_string())
-                    );
-                }
-
-                Ok(VerbatimUrl::from_absolute_path(path)
-                    .map_err(|err| Pep508Error {
-                        message: Pep508ErrorSource::UrlError(err),
-                        start,
-                        len,
-                        input: cursor.to_string(),
-                    })?
-                    .with_given(url.to_string()))
-            }
-            // Ex) `https://download.pytorch.org/whl/torch_stable.html`
-            Some(_) => {
-                // Ex) `https://download.pytorch.org/whl/torch_stable.html`
-                Ok(VerbatimUrl::from_str(url).map_err(|err| Pep508Error {
-                    message: Pep508ErrorSource::UrlError(err),
-                    start,
-                    len,
-                    input: cursor.to_string(),
-                })?)
-            }
-
-            // Ex) `C:\Users\ferris\wheel-0.42.0.tar.gz`
-            _ => {
-                #[cfg(feature = "non-pep508-extensions")]
-                if let Some(working_dir) = working_dir {
-                    return Ok(VerbatimUrl::from_path(url, working_dir).with_given(url.to_string()));
-                }
-
-                Ok(VerbatimUrl::from_absolute_path(url)
-                    .map_err(|err| Pep508Error {
-                        message: Pep508ErrorSource::UrlError(err),
-                        start,
-                        len,
-                        input: cursor.to_string(),
-                    })?
-                    .with_given(url.to_string()))
-            }
-        }
-    } else {
-        // Ex) `../editable/`
-        #[cfg(feature = "non-pep508-extensions")]
-        if let Some(working_dir) = working_dir {
-            return Ok(VerbatimUrl::from_path(url, working_dir).with_given(url.to_string()));
-        }
+/// Identify the extras in a relative URL (e.g., `../editable[dev]`).
+///
+/// Pip uses `m = re.match(r'^(.+)(\[[^]]+])$', path)`. Our strategy is:
+/// - If the string ends with a closing bracket (`]`)...
+/// - Iterate backwards until you find the open bracket (`[`)...
+/// - But abort if you find another closing bracket (`]`) first.
+pub fn split_extras(given: &str) -> Option<(&str, &str)> {
+    let mut chars = given.char_indices().rev();
+
+    // If the string ends with a closing bracket (`]`)...
+    if !matches!(chars.next(), Some((_, ']'))) {
+        return None;
+    }
+
+    // Iterate backwards until you find the open bracket (`[`)...
+    let (index, _) = chars
+        .take_while(|(_, c)| *c != ']')
+        .find(|(_, c)| *c == '[')?;
 
-        Ok(VerbatimUrl::from_absolute_path(url)
-            .map_err(|err| Pep508Error {
-                message: Pep508ErrorSource::UrlError(err),
-                start,
-                len,
-                input: cursor.to_string(),
-            })?
-            .with_given(url.to_string()))
-    }
+    Some(given.split_at(index))
 }
 
 /// PEP 440 wrapper
-fn parse_specifier(
+fn parse_specifier<T: Pep508Url>(
     cursor: &mut Cursor,
     buffer: &str,
     start: usize,
     end: usize,
-) -> Result<VersionSpecifier, Pep508Error> {
+) -> Result<VersionSpecifier, Pep508Error<T>> {
     VersionSpecifier::from_str(buffer).map_err(|err| Pep508Error {
         message: Pep508ErrorSource::String(err.to_string()),
         start,
         len: end - start,
         input: cursor.to_string(),
     })
 }
 
 /// Such as `>=1.19,<2.0`, either delimited by the end of the specifier or a `;` for the marker part
 ///
 /// ```text
 /// version_one (wsp* ',' version_one)*
 /// ```
-fn parse_version_specifier(cursor: &mut Cursor) -> Result<Option<VersionOrUrl>, Pep508Error> {
+fn parse_version_specifier<T: Pep508Url>(
+    cursor: &mut Cursor,
+) -> Result<Option<VersionOrUrl<T>>, Pep508Error<T>> {
     let mut start = cursor.pos();
     let mut specifiers = Vec::new();
     let mut buffer = String::new();
     let requirement_kind = loop {
         match cursor.peek() {
             Some((end, ',')) => {
                 let specifier = parse_specifier(cursor, &buffer, start, end)?;
@@ -865,17 +925,17 @@
 }
 
 /// Such as `(>=1.19,<2.0)`
 ///
 /// ```text
 /// '(' version_one (wsp* ',' version_one)* ')'
 /// ```
-fn parse_version_specifier_parentheses(
+fn parse_version_specifier_parentheses<T: Pep508Url>(
     cursor: &mut Cursor,
-) -> Result<Option<VersionOrUrl>, Pep508Error> {
+) -> Result<Option<VersionOrUrl<T>>, Pep508Error<T>> {
     let brace_pos = cursor.pos();
     cursor.next();
     // Makes for slightly better error underline
     cursor.eat_whitespace();
     let mut start = cursor.pos();
     let mut specifiers = Vec::new();
     let mut buffer = String::new();
@@ -901,16 +961,20 @@
                 input: cursor.to_string(),
             }),
         }
     };
     Ok(requirement_kind)
 }
 
-/// Parse a [dependency specifier](https://packaging.python.org/en/latest/specifications/dependency-specifiers)
-fn parse(cursor: &mut Cursor, working_dir: Option<&Path>) -> Result<Requirement, Pep508Error> {
+/// Parse a PEP 508-compliant [dependency specifier](https://packaging.python.org/en/latest/specifications/dependency-specifiers).
+fn parse_pep508_requirement<T: Pep508Url>(
+    cursor: &mut Cursor,
+    working_dir: Option<&Path>,
+    reporter: &mut impl Reporter,
+) -> Result<Requirement<T>, Pep508Error<T>> {
     let start = cursor.pos();
 
     // Technically, the grammar is:
     // ```text
     // name_req      = name wsp* extras? wsp* versionspec? wsp* quoted_marker?
     // url_req       = name wsp* extras? wsp* urlspec wsp+ quoted_marker?
     // specification = wsp* ( url_req | name_req ) wsp*
@@ -924,15 +988,15 @@
     // wsp*
     cursor.eat_whitespace();
     // name
     let name = parse_name(cursor)?;
     // wsp*
     cursor.eat_whitespace();
     // extras?
-    let extras = parse_extras(cursor)?;
+    let extras = parse_extras_cursor(cursor)?;
     // wsp*
     cursor.eat_whitespace();
 
     // ( url_req | name_req )?
     let requirement_kind = match cursor.peek_char() {
         // url_req
         Some('@') => {
@@ -946,15 +1010,15 @@
         // No requirements / any version
         Some(';') | None => None,
         Some(other) => {
             // Rewind to the start of the version specifier, to see if the user added a URL without
             // a package name. pip supports this in `requirements.txt`, but it doesn't adhere to
             // the PEP 508 grammar.
             let mut clone = cursor.clone().at(start);
-            return if parse_url(&mut clone, working_dir).is_ok() {
+            return if parse_url::<T>(&mut clone, working_dir).is_ok() {
                 Err(Pep508Error {
                     message: Pep508ErrorSource::UnsupportedRequirement("URL requirement must be preceded by a package name. Add the name of the package before the URL (e.g., `package_name @ https://...`).".to_string()),
                     start,
                     len: clone.pos() - start,
                     input: clone.to_string(),
                 })
             } else {
@@ -966,41 +1030,39 @@
                     len: other.len_utf8(),
                     input: cursor.to_string(),
                 })
             };
         }
     };
 
-    let requirement_end = cursor.pos;
+    let requirement_end = cursor.pos();
 
     // wsp*
     cursor.eat_whitespace();
     // quoted_marker?
     let marker = if cursor.peek_char() == Some(';') {
         // Skip past the semicolon
         cursor.next();
-        Some(marker::parse_markers_impl(cursor)?)
+        Some(marker::parse_markers_cursor(cursor, reporter)?)
     } else {
         None
     };
     // wsp*
     cursor.eat_whitespace();
     if let Some((pos, char)) = cursor.next() {
         if let Some(VersionOrUrl::Url(url)) = requirement_kind {
-            if let Some(given) = url.given() {
-                if given.ends_with(';') && marker.is_none() {
-                    return Err(Pep508Error {
-                        message: Pep508ErrorSource::String(
-                            "Missing space before ';', the end of the URL is ambiguous".to_string(),
-                        ),
-                        start: requirement_end - ';'.len_utf8(),
-                        len: ';'.len_utf8(),
-                        input: cursor.to_string(),
-                    });
-                }
+            if marker.is_none() && url.to_string().ends_with(';') {
+                return Err(Pep508Error {
+                    message: Pep508ErrorSource::String(
+                        "Missing space before ';', the end of the URL is ambiguous".to_string(),
+                    ),
+                    start: requirement_end - ';'.len_utf8(),
+                    len: ';'.len_utf8(),
+                    input: cursor.to_string(),
+                });
             }
         }
         let message = if marker.is_none() {
             format!(r#"Expected end of input or ';', found '{char}'"#)
         } else {
             format!(r#"Expected end of input, found '{char}'"#)
         };
@@ -1013,628 +1075,709 @@
     }
 
     Ok(Requirement {
         name,
         extras,
         version_or_url: requirement_kind,
         marker,
+        origin: None,
     })
 }
 
 /// A library for [dependency specifiers](https://packaging.python.org/en/latest/specifications/dependency-specifiers/)
 /// as originally specified in [PEP 508](https://peps.python.org/pep-0508/)
 ///
 /// This has `Version` and `VersionSpecifier` included. That is because
 /// `pep440_rs.Version("1.2.3") != pep508_rs.Requirement("numpy==1.2.3").version_or_url` as the
 /// `Version`s come from two different binaries and can therefore never be equal.
 #[cfg(feature = "pyo3")]
 #[pymodule]
 #[pyo3(name = "pep508_rs")]
-pub fn python_module(py: Python<'_>, m: &PyModule) -> PyResult<()> {
+pub fn python_module(py: Python<'_>, m: &pyo3::Bound<'_, PyModule>) -> PyResult<()> {
     // Allowed to fail if we embed this module in another
+
     #[allow(unused_must_use)]
     {
         pyo3_log::try_init();
     }
 
     m.add_class::<PyVersion>()?;
     m.add_class::<VersionSpecifier>()?;
 
-    m.add_class::<Requirement>()?;
+    m.add_class::<PyRequirement>()?;
     m.add_class::<MarkerEnvironment>()?;
-    m.add("Pep508Error", py.get_type::<PyPep508Error>())?;
+    m.add("Pep508Error", py.get_type_bound::<PyPep508Error>())?;
     Ok(())
 }
 
 /// Half of these tests are copied from <https://github.com/pypa/packaging/pull/624>
 #[cfg(test)]
 mod tests {
     use std::env;
     use std::str::FromStr;
 
-    use indoc::indoc;
+    use insta::assert_snapshot;
+    use url::Url;
 
     use pep440_rs::{Operator, Version, VersionPattern, VersionSpecifier};
-    use uv_normalize::{ExtraName, PackageName};
+    use uv_normalize::{ExtraName, InvalidNameError, PackageName};
 
+    use crate::cursor::Cursor;
     use crate::marker::{
-        parse_markers_impl, MarkerExpression, MarkerOperator, MarkerTree, MarkerValue,
-        MarkerValueString, MarkerValueVersion,
+        parse_markers_cursor, MarkerExpression, MarkerOperator, MarkerTree, MarkerValueString,
+        MarkerValueVersion,
     };
-    use crate::{Cursor, Requirement, VerbatimUrl, VersionOrUrl};
+    use crate::{Requirement, TracingReporter, VerbatimUrl, VersionOrUrl};
+
+    fn parse_pep508_err(input: &str) -> String {
+        Requirement::<VerbatimUrl>::from_str(input)
+            .unwrap_err()
+            .to_string()
+    }
 
-    fn assert_err(input: &str, error: &str) {
-        assert_eq!(Requirement::from_str(input).unwrap_err().to_string(), error);
+    #[cfg(feature = "non-pep508-extensions")]
+    fn parse_unnamed_err(input: &str) -> String {
+        crate::UnnamedRequirement::from_str(input)
+            .unwrap_err()
+            .to_string()
     }
 
     #[cfg(windows)]
     #[test]
     fn test_preprocess_url_windows() {
         use std::path::PathBuf;
 
-        let actual = crate::preprocess_url(
-            "file:///C:/Users/ferris/wheel-0.42.0.tar.gz",
+        let actual = crate::parse_url::<VerbatimUrl>(
+            &mut Cursor::new("file:///C:/Users/ferris/wheel-0.42.0.tar.gz"),
             None,
-            &Cursor::new(""),
-            0,
-            0,
         )
         .unwrap()
         .to_file_path();
         let expected = PathBuf::from(r"C:\Users\ferris\wheel-0.42.0.tar.gz");
         assert_eq!(actual, Ok(expected));
     }
 
     #[test]
     fn error_empty() {
-        assert_err(
-            "",
-            indoc! {"\
-            Empty field is not allowed for PEP508
+        assert_snapshot!(
+            parse_pep508_err(""),
+            @r"
+        Empty field is not allowed for PEP508
 
-            ^"
-            },
+        ^"
         );
     }
 
     #[test]
     fn error_start() {
-        assert_err(
-            "_name",
-            indoc! {"
-                Expected package name starting with an alphanumeric character, found '_'
-                _name
-                ^"
-            },
+        assert_snapshot!(
+            parse_pep508_err("_name"),
+            @"
+            Expected package name starting with an alphanumeric character, found '_'
+            _name
+            ^"
         );
     }
 
     #[test]
     fn error_end() {
-        assert_err(
-            "name_",
-            indoc! {"
-                Package name must end with an alphanumeric character, not '_'
-                name_
-                    ^"
-            },
+        assert_snapshot!(
+            parse_pep508_err("name_"),
+            @"
+            Package name must end with an alphanumeric character, not '_'
+            name_
+                ^"
         );
     }
 
     #[test]
     fn basic_examples() {
-        let input = r#"requests[security,tests] >=2.8.1, ==2.8.* ; python_version < '2.7'"#;
-        let requests = Requirement::from_str(input).unwrap();
+        let input = r"requests[security,tests]>=2.8.1,==2.8.* ; python_version < '2.7'";
+        let requests = Requirement::<Url>::from_str(input).unwrap();
         assert_eq!(input, requests.to_string());
         let expected = Requirement {
             name: PackageName::from_str("requests").unwrap(),
             extras: vec![
                 ExtraName::from_str("security").unwrap(),
                 ExtraName::from_str("tests").unwrap(),
             ],
             version_or_url: Some(VersionOrUrl::VersionSpecifier(
                 [
-                    VersionSpecifier::new(
+                    VersionSpecifier::from_pattern(
                         Operator::GreaterThanEqual,
                         VersionPattern::verbatim(Version::new([2, 8, 1])),
                     )
                     .unwrap(),
-                    VersionSpecifier::new(
+                    VersionSpecifier::from_pattern(
                         Operator::Equal,
                         VersionPattern::wildcard(Version::new([2, 8])),
                     )
                     .unwrap(),
                 ]
                 .into_iter()
                 .collect(),
             )),
-            marker: Some(MarkerTree::Expression(MarkerExpression {
-                l_value: MarkerValue::MarkerEnvVersion(MarkerValueVersion::PythonVersion),
-                operator: MarkerOperator::LessThan,
-                r_value: MarkerValue::QuotedString("2.7".to_string()),
+            marker: Some(MarkerTree::Expression(MarkerExpression::Version {
+                key: MarkerValueVersion::PythonVersion,
+                specifier: VersionSpecifier::from_pattern(
+                    pep440_rs::Operator::LessThan,
+                    "2.7".parse().unwrap(),
+                )
+                .unwrap(),
             })),
+            origin: None,
         };
         assert_eq!(requests, expected);
     }
 
     #[test]
     fn parenthesized_single() {
-        let numpy = Requirement::from_str("numpy ( >=1.19 )").unwrap();
+        let numpy = Requirement::<Url>::from_str("numpy ( >=1.19 )").unwrap();
         assert_eq!(numpy.name.as_ref(), "numpy");
     }
 
     #[test]
     fn parenthesized_double() {
-        let numpy = Requirement::from_str("numpy ( >=1.19, <2.0 )").unwrap();
+        let numpy = Requirement::<Url>::from_str("numpy ( >=1.19, <2.0 )").unwrap();
         assert_eq!(numpy.name.as_ref(), "numpy");
     }
 
     #[test]
     fn versions_single() {
-        let numpy = Requirement::from_str("numpy >=1.19 ").unwrap();
+        let numpy = Requirement::<Url>::from_str("numpy >=1.19 ").unwrap();
         assert_eq!(numpy.name.as_ref(), "numpy");
     }
 
     #[test]
     fn versions_double() {
-        let numpy = Requirement::from_str("numpy >=1.19, <2.0 ").unwrap();
+        let numpy = Requirement::<Url>::from_str("numpy >=1.19, <2.0 ").unwrap();
         assert_eq!(numpy.name.as_ref(), "numpy");
     }
 
     #[test]
+    #[cfg(feature = "non-pep508-extensions")]
+    fn direct_url_no_extras() {
+        let numpy = crate::UnnamedRequirement::from_str("https://files.pythonhosted.org/packages/28/4a/46d9e65106879492374999e76eb85f87b15328e06bd1550668f79f7b18c6/numpy-1.26.4-cp312-cp312-win32.whl").unwrap();
+        assert_eq!(numpy.url.to_string(), "https://files.pythonhosted.org/packages/28/4a/46d9e65106879492374999e76eb85f87b15328e06bd1550668f79f7b18c6/numpy-1.26.4-cp312-cp312-win32.whl");
+        assert_eq!(numpy.extras, vec![]);
+    }
+
+    #[test]
+    #[cfg(all(unix, feature = "non-pep508-extensions"))]
+    fn direct_url_extras() {
+        let numpy =
+            crate::UnnamedRequirement::from_str("/path/to/numpy-1.26.4-cp312-cp312-win32.whl[dev]")
+                .unwrap();
+        assert_eq!(
+            numpy.url.to_string(),
+            "file:///path/to/numpy-1.26.4-cp312-cp312-win32.whl"
+        );
+        assert_eq!(numpy.extras, vec![ExtraName::from_str("dev").unwrap()]);
+    }
+
+    #[test]
+    #[cfg(all(windows, feature = "non-pep508-extensions"))]
+    fn direct_url_extras() {
+        let numpy = crate::UnnamedRequirement::from_str(
+            "C:\\path\\to\\numpy-1.26.4-cp312-cp312-win32.whl[dev]",
+        )
+        .unwrap();
+        assert_eq!(
+            numpy.url.to_string(),
+            "file:///C:/path/to/numpy-1.26.4-cp312-cp312-win32.whl"
+        );
+        assert_eq!(numpy.extras, vec![ExtraName::from_str("dev").unwrap()]);
+    }
+
+    #[test]
     fn error_extras_eof1() {
-        assert_err(
-            "black[",
-            indoc! {"
-                Missing closing bracket (expected ']', found end of dependency specification)
-                black[
-                     ^"
-            },
+        assert_snapshot!(
+            parse_pep508_err("black["),
+            @"
+            Missing closing bracket (expected ']', found end of dependency specification)
+            black[
+                 ^"
         );
     }
 
     #[test]
     fn error_extras_eof2() {
-        assert_err(
-            "black[d",
-            indoc! {"
-                Missing closing bracket (expected ']', found end of dependency specification)
-                black[d
-                     ^"
-            },
+        assert_snapshot!(
+            parse_pep508_err("black[d"),
+            @"
+            Missing closing bracket (expected ']', found end of dependency specification)
+            black[d
+                 ^"
         );
     }
 
     #[test]
     fn error_extras_eof3() {
-        assert_err(
-            "black[d,",
-            indoc! {"
-                Missing closing bracket (expected ']', found end of dependency specification)
-                black[d,
-                     ^"
-            },
+        assert_snapshot!(
+            parse_pep508_err("black[d,"),
+            @"
+            Missing closing bracket (expected ']', found end of dependency specification)
+            black[d,
+                 ^"
         );
     }
 
     #[test]
     fn error_extras_illegal_start1() {
-        assert_err(
-            "black[ö]",
-            indoc! {"
-                Expected an alphanumeric character starting the extra name, found 'ö'
-                black[ö]
-                      ^"
-            },
+        assert_snapshot!(
+            parse_pep508_err("black[ö]"),
+            @"
+            Expected an alphanumeric character starting the extra name, found 'ö'
+            black[ö]
+                  ^"
         );
     }
 
     #[test]
     fn error_extras_illegal_start2() {
-        assert_err(
-            "black[_d]",
-            indoc! {"
-                Expected an alphanumeric character starting the extra name, found '_'
-                black[_d]
-                      ^"
-            },
+        assert_snapshot!(
+            parse_pep508_err("black[_d]"),
+            @"
+            Expected an alphanumeric character starting the extra name, found '_'
+            black[_d]
+                  ^"
+        );
+    }
+
+    #[test]
+    fn error_extras_illegal_start3() {
+        assert_snapshot!(
+            parse_pep508_err("black[,]"),
+            @"
+            Expected either alphanumerical character (starting the extra name) or ']' (ending the extras section), found ','
+            black[,]
+                  ^"
         );
     }
 
     #[test]
     fn error_extras_illegal_character() {
-        assert_err(
-            "black[jüpyter]",
-            indoc! {"
-                Invalid character in extras name, expected an alphanumeric character, '-', '_', '.', ',' or ']', found 'ü'
-                black[jüpyter]
-                       ^"
-            },
+        assert_snapshot!(
+            parse_pep508_err("black[jüpyter]"),
+            @"
+            Invalid character in extras name, expected an alphanumeric character, '-', '_', '.', ',' or ']', found 'ü'
+            black[jüpyter]
+                   ^"
         );
     }
 
     #[test]
     fn error_extras1() {
-        let numpy = Requirement::from_str("black[d]").unwrap();
+        let numpy = Requirement::<Url>::from_str("black[d]").unwrap();
         assert_eq!(numpy.extras, vec![ExtraName::from_str("d").unwrap()]);
     }
 
     #[test]
     fn error_extras2() {
-        let numpy = Requirement::from_str("black[d,jupyter]").unwrap();
+        let numpy = Requirement::<Url>::from_str("black[d,jupyter]").unwrap();
         assert_eq!(
             numpy.extras,
             vec![
                 ExtraName::from_str("d").unwrap(),
                 ExtraName::from_str("jupyter").unwrap(),
             ]
         );
     }
 
     #[test]
+    fn empty_extras() {
+        let black = Requirement::<Url>::from_str("black[]").unwrap();
+        assert_eq!(black.extras, vec![]);
+    }
+
+    #[test]
+    fn empty_extras_with_spaces() {
+        let black = Requirement::<Url>::from_str("black[  ]").unwrap();
+        assert_eq!(black.extras, vec![]);
+    }
+
+    #[test]
+    fn error_extra_with_trailing_comma() {
+        assert_snapshot!(
+            parse_pep508_err("black[d,]"),
+            @"
+            Expected an alphanumeric character starting the extra name, found ']'
+            black[d,]
+                    ^"
+        );
+    }
+
+    #[test]
     fn error_parenthesized_pep440() {
-        assert_err(
-            "numpy ( ><1.19 )",
-            indoc! {"
-                no such comparison operator \"><\", must be one of ~= == != <= >= < > ===
-                numpy ( ><1.19 )
-                        ^^^^^^^"
-            },
+        assert_snapshot!(
+            parse_pep508_err("numpy ( ><1.19 )"),
+            @"
+            no such comparison operator \"><\", must be one of ~= == != <= >= < > ===
+            numpy ( ><1.19 )
+                    ^^^^^^^"
         );
     }
 
     #[test]
     fn error_parenthesized_parenthesis() {
-        assert_err(
-            "numpy ( >=1.19",
-            indoc! {"
-                Missing closing parenthesis (expected ')', found end of dependency specification)
-                numpy ( >=1.19
-                      ^"
-            },
+        assert_snapshot!(
+            parse_pep508_err("numpy ( >=1.19"),
+            @"
+            Missing closing parenthesis (expected ')', found end of dependency specification)
+            numpy ( >=1.19
+                  ^"
         );
     }
 
     #[test]
     fn error_whats_that() {
-        assert_err(
-            "numpy % 1.16",
-            indoc! {"
-                Expected one of `@`, `(`, `<`, `=`, `>`, `~`, `!`, `;`, found `%`
-                numpy % 1.16
-                      ^"
-            },
+        assert_snapshot!(
+            parse_pep508_err("numpy % 1.16"),
+            @"
+            Expected one of `@`, `(`, `<`, `=`, `>`, `~`, `!`, `;`, found `%`
+            numpy % 1.16
+                  ^"
         );
     }
 
     #[test]
     fn url() {
         let pip_url =
             Requirement::from_str("pip @ https://github.com/pypa/pip/archive/1.3.1.zip#sha1=da9234ee9982d4bbb3c72346a6de940a148ea686")
                 .unwrap();
         let url = "https://github.com/pypa/pip/archive/1.3.1.zip#sha1=da9234ee9982d4bbb3c72346a6de940a148ea686";
         let expected = Requirement {
             name: PackageName::from_str("pip").unwrap(),
             extras: vec![],
             marker: None,
-            version_or_url: Some(VersionOrUrl::Url(VerbatimUrl::from_str(url).unwrap())),
+            version_or_url: Some(VersionOrUrl::Url(Url::parse(url).unwrap())),
+            origin: None,
         };
         assert_eq!(pip_url, expected);
     }
 
     #[test]
     fn test_marker_parsing() {
         let marker = r#"python_version == "2.7" and (sys_platform == "win32" or (os_name == "linux" and implementation_name == 'cpython'))"#;
-        let actual = parse_markers_impl(&mut Cursor::new(marker)).unwrap();
+        let actual =
+            parse_markers_cursor::<Url>(&mut Cursor::new(marker), &mut TracingReporter).unwrap();
         let expected = MarkerTree::And(vec![
-            MarkerTree::Expression(MarkerExpression {
-                l_value: MarkerValue::MarkerEnvVersion(MarkerValueVersion::PythonVersion),
-                operator: MarkerOperator::Equal,
-                r_value: MarkerValue::QuotedString("2.7".to_string()),
+            MarkerTree::Expression(MarkerExpression::Version {
+                key: MarkerValueVersion::PythonVersion,
+                specifier: VersionSpecifier::from_pattern(
+                    pep440_rs::Operator::Equal,
+                    "2.7".parse().unwrap(),
+                )
+                .unwrap(),
             }),
             MarkerTree::Or(vec![
-                MarkerTree::Expression(MarkerExpression {
-                    l_value: MarkerValue::MarkerEnvString(MarkerValueString::SysPlatform),
+                MarkerTree::Expression(MarkerExpression::String {
+                    key: MarkerValueString::SysPlatform,
                     operator: MarkerOperator::Equal,
-                    r_value: MarkerValue::QuotedString("win32".to_string()),
+                    value: "win32".to_string(),
                 }),
                 MarkerTree::And(vec![
-                    MarkerTree::Expression(MarkerExpression {
-                        l_value: MarkerValue::MarkerEnvString(MarkerValueString::OsName),
+                    MarkerTree::Expression(MarkerExpression::String {
+                        key: MarkerValueString::OsName,
                         operator: MarkerOperator::Equal,
-                        r_value: MarkerValue::QuotedString("linux".to_string()),
+                        value: "linux".to_string(),
                     }),
-                    MarkerTree::Expression(MarkerExpression {
-                        l_value: MarkerValue::MarkerEnvString(
-                            MarkerValueString::ImplementationName,
-                        ),
+                    MarkerTree::Expression(MarkerExpression::String {
+                        key: MarkerValueString::ImplementationName,
                         operator: MarkerOperator::Equal,
-                        r_value: MarkerValue::QuotedString("cpython".to_string()),
+                        value: "cpython".to_string(),
                     }),
                 ]),
             ]),
         ]);
         assert_eq!(expected, actual);
     }
 
     #[test]
     fn name_and_marker() {
-        Requirement::from_str(r#"numpy; sys_platform == "win32" or (os_name == "linux" and implementation_name == 'cpython')"#).unwrap();
+        Requirement::<Url>::from_str(r#"numpy; sys_platform == "win32" or (os_name == "linux" and implementation_name == 'cpython')"#).unwrap();
     }
 
     #[test]
     fn error_marker_incomplete1() {
-        assert_err(
-            r#"numpy; sys_platform"#,
-            indoc! {"
+        assert_snapshot!(
+            parse_pep508_err(r"numpy; sys_platform"),
+            @"
                 Expected a valid marker operator (such as '>=' or 'not in'), found ''
                 numpy; sys_platform
                                    ^"
-            },
         );
     }
 
     #[test]
     fn error_marker_incomplete2() {
-        assert_err(
-            r#"numpy; sys_platform =="#,
-            indoc! {"\
-                Expected marker value, found end of dependency specification
-                numpy; sys_platform ==
-                                      ^"
-            },
+        assert_snapshot!(
+            parse_pep508_err(r"numpy; sys_platform =="),
+            @r"
+            Expected marker value, found end of dependency specification
+            numpy; sys_platform ==
+                                  ^"
         );
     }
 
     #[test]
     fn error_marker_incomplete3() {
-        assert_err(
-            r#"numpy; sys_platform == "win32" or"#,
-            indoc! {"
-                Expected marker value, found end of dependency specification
-                numpy; sys_platform == \"win32\" or
-                                                 ^"},
+        assert_snapshot!(
+            parse_pep508_err(r#"numpy; sys_platform == "win32" or"#),
+            @r#"
+            Expected marker value, found end of dependency specification
+            numpy; sys_platform == "win32" or
+                                             ^"#
         );
     }
 
     #[test]
     fn error_marker_incomplete4() {
-        assert_err(
-            r#"numpy; sys_platform == "win32" or (os_name == "linux""#,
-            indoc! {r#"
-                Expected ')', found end of dependency specification
-                numpy; sys_platform == "win32" or (os_name == "linux"
-                                                  ^"#},
+        assert_snapshot!(
+            parse_pep508_err(r#"numpy; sys_platform == "win32" or (os_name == "linux""#),
+            @r#"
+            Expected ')', found end of dependency specification
+            numpy; sys_platform == "win32" or (os_name == "linux"
+                                              ^"#
         );
     }
 
     #[test]
     fn error_marker_incomplete5() {
-        assert_err(
-            r#"numpy; sys_platform == "win32" or (os_name == "linux" and"#,
-            indoc! {"
-                Expected marker value, found end of dependency specification
-                numpy; sys_platform == \"win32\" or (os_name == \"linux\" and
-                                                                         ^"},
+        assert_snapshot!(
+            parse_pep508_err(r#"numpy; sys_platform == "win32" or (os_name == "linux" and"#),
+            @r#"
+            Expected marker value, found end of dependency specification
+            numpy; sys_platform == "win32" or (os_name == "linux" and
+                                                                     ^"#
         );
     }
 
     #[test]
     fn error_pep440() {
-        assert_err(
-            r#"numpy >=1.1.*"#,
-            indoc! {"
-                Operator >= cannot be used with a wildcard version specifier
-                numpy >=1.1.*
-                      ^^^^^^^"
-            },
+        assert_snapshot!(
+            parse_pep508_err(r"numpy >=1.1.*"),
+            @r"
+            Operator >= cannot be used with a wildcard version specifier
+            numpy >=1.1.*
+                  ^^^^^^^"
         );
     }
 
     #[test]
     fn error_no_name() {
-        assert_err(
-            r#"==0.0"#,
-            indoc! {"
-                Expected package name starting with an alphanumeric character, found '='
-                ==0.0
-                ^"
-            },
+        assert_snapshot!(
+            parse_pep508_err(r"==0.0"),
+            @r"
+        Expected package name starting with an alphanumeric character, found '='
+        ==0.0
+        ^
+        "
         );
     }
 
     #[test]
-    fn error_bare_url() {
-        assert_err(
-            r#"git+https://github.com/pallets/flask.git"#,
-            indoc! {"
-                URL requirement must be preceded by a package name. Add the name of the package before the URL (e.g., `package_name @ https://...`).
-                git+https://github.com/pallets/flask.git
-                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
-            },
+    fn error_unnamedunnamed_url() {
+        assert_snapshot!(
+            parse_pep508_err(r"git+https://github.com/pallets/flask.git"),
+            @"
+            URL requirement must be preceded by a package name. Add the name of the package before the URL (e.g., `package_name @ https://...`).
+            git+https://github.com/pallets/flask.git
+            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
+        );
+    }
+
+    #[test]
+    fn error_unnamed_file_path() {
+        assert_snapshot!(
+            parse_pep508_err(r"/path/to/flask.tar.gz"),
+            @r###"
+        URL requirement must be preceded by a package name. Add the name of the package before the URL (e.g., `package_name @ /path/to/file`).
+        /path/to/flask.tar.gz
+        ^^^^^^^^^^^^^^^^^^^^^
+        "###
         );
     }
 
     #[test]
     fn error_no_comma_between_extras() {
-        assert_err(
-            r#"name[bar baz]"#,
-            indoc! {"
-                Expected either ',' (separating extras) or ']' (ending the extras section), found 'b'
-                name[bar baz]
-                         ^"
-            },
+        assert_snapshot!(
+            parse_pep508_err(r"name[bar baz]"),
+            @"
+            Expected either ',' (separating extras) or ']' (ending the extras section), found 'b'
+            name[bar baz]
+                     ^"
         );
     }
 
     #[test]
     fn error_extra_comma_after_extras() {
-        assert_err(
-            r#"name[bar, baz,]"#,
-            indoc! {"
-                Expected an alphanumeric character starting the extra name, found ']'
-                name[bar, baz,]
-                              ^"
-            },
+        assert_snapshot!(
+            parse_pep508_err(r"name[bar, baz,]"),
+            @"
+            Expected an alphanumeric character starting the extra name, found ']'
+            name[bar, baz,]
+                          ^"
         );
     }
 
     #[test]
     fn error_extras_not_closed() {
-        assert_err(
-            r#"name[bar, baz >= 1.0"#,
-            indoc! {"
-                Expected either ',' (separating extras) or ']' (ending the extras section), found '>'
-                name[bar, baz >= 1.0
-                              ^"
-            },
+        assert_snapshot!(
+            parse_pep508_err(r"name[bar, baz >= 1.0"),
+            @"
+            Expected either ',' (separating extras) or ']' (ending the extras section), found '>'
+            name[bar, baz >= 1.0
+                          ^"
         );
     }
 
     #[test]
     fn error_no_space_after_url() {
-        assert_err(
-            r#"name @ https://example.com/; extra == 'example'"#,
-            indoc! {"
-                Missing space before ';', the end of the URL is ambiguous
-                name @ https://example.com/; extra == 'example'
-                                           ^"
-            },
+        assert_snapshot!(
+            parse_pep508_err(r"name @ https://example.com/; extra == 'example'"),
+            @"
+            Missing space before ';', the end of the URL is ambiguous
+            name @ https://example.com/; extra == 'example'
+                                       ^"
         );
     }
 
     #[test]
     fn error_name_at_nothing() {
-        assert_err(
-            r#"name @"#,
-            indoc! {"
-                Expected URL
-                name @
-                      ^"
-            },
+        assert_snapshot!(
+            parse_pep508_err(r"name @"),
+            @"
+            Expected URL
+            name @
+                  ^"
         );
     }
 
     #[test]
     fn test_error_invalid_marker_key() {
-        assert_err(
-            r#"name; invalid_name"#,
-            indoc! {"
-                Expected a valid marker name, found 'invalid_name'
-                name; invalid_name
-                      ^^^^^^^^^^^^"
-            },
+        assert_snapshot!(
+            parse_pep508_err(r"name; invalid_name"),
+            @"
+            Expected a valid marker name, found 'invalid_name'
+            name; invalid_name
+                  ^^^^^^^^^^^^"
         );
     }
 
     #[test]
     fn error_markers_invalid_order() {
-        assert_err(
-            "name; '3.7' <= invalid_name",
-            indoc! {"
-                Expected a valid marker name, found 'invalid_name'
-                name; '3.7' <= invalid_name
-                               ^^^^^^^^^^^^"
-            },
+        assert_snapshot!(
+            parse_pep508_err("name; '3.7' <= invalid_name"),
+            @"
+            Expected a valid marker name, found 'invalid_name'
+            name; '3.7' <= invalid_name
+                           ^^^^^^^^^^^^"
         );
     }
 
     #[test]
     fn error_markers_notin() {
-        assert_err(
-            "name; '3.7' notin python_version",
-            indoc! {"
-                Expected a valid marker operator (such as '>=' or 'not in'), found 'notin'
-                name; '3.7' notin python_version
-                            ^^^^^"
-            },
+        assert_snapshot!(
+            parse_pep508_err("name; '3.7' notin python_version"),
+            @"
+            Expected a valid marker operator (such as '>=' or 'not in'), found 'notin'
+            name; '3.7' notin python_version
+                        ^^^^^"
         );
     }
 
     #[test]
     fn error_markers_inpython_version() {
-        assert_err(
-            "name; '3.6'inpython_version",
-            indoc! {"
-                Expected a valid marker operator (such as '>=' or 'not in'), found 'inpython_version'
-                name; '3.6'inpython_version
-                           ^^^^^^^^^^^^^^^^"
-            },
+        assert_snapshot!(
+            parse_pep508_err("name; '3.6'inpython_version"),
+            @"
+            Expected a valid marker operator (such as '>=' or 'not in'), found 'inpython_version'
+            name; '3.6'inpython_version
+                       ^^^^^^^^^^^^^^^^"
         );
     }
 
     #[test]
     fn error_markers_not_python_version() {
-        assert_err(
-            "name; '3.7' not python_version",
-            indoc! {"
-                Expected 'i', found 'p'
-                name; '3.7' not python_version
-                                ^"
-            },
+        assert_snapshot!(
+            parse_pep508_err("name; '3.7' not python_version"),
+            @"
+            Expected 'i', found 'p'
+            name; '3.7' not python_version
+                            ^"
         );
     }
 
     #[test]
     fn error_markers_invalid_operator() {
-        assert_err(
-            "name; '3.7' ~ python_version",
-            indoc! {"
-                Expected a valid marker operator (such as '>=' or 'not in'), found '~'
-                name; '3.7' ~ python_version
-                            ^"
-            },
+        assert_snapshot!(
+            parse_pep508_err("name; '3.7' ~ python_version"),
+            @"
+            Expected a valid marker operator (such as '>=' or 'not in'), found '~'
+            name; '3.7' ~ python_version
+                        ^"
         );
     }
 
     #[test]
     fn error_invalid_prerelease() {
-        assert_err(
-            "name==1.0.org1",
-            indoc! {"
-                after parsing 1.0, found \".org1\" after it, which is not part of a valid version
-                name==1.0.org1
-                    ^^^^^^^^^^"
-            },
+        assert_snapshot!(
+            parse_pep508_err("name==1.0.org1"),
+            @r###"
+        after parsing '1.0', found '.org1', which is not part of a valid version
+        name==1.0.org1
+            ^^^^^^^^^^
+        "###
         );
     }
 
     #[test]
     fn error_no_version_value() {
-        assert_err(
-            "name==",
-            indoc! {"
-                Unexpected end of version specifier, expected version
-                name==
-                    ^^"
-            },
+        assert_snapshot!(
+            parse_pep508_err("name=="),
+            @"
+            Unexpected end of version specifier, expected version
+            name==
+                ^^"
         );
     }
 
     #[test]
     fn error_no_version_operator() {
-        assert_err(
-            "name 1.0",
-            indoc! {"
-                Expected one of `@`, `(`, `<`, `=`, `>`, `~`, `!`, `;`, found `1`
-                name 1.0
-                     ^"
-            },
+        assert_snapshot!(
+            parse_pep508_err("name 1.0"),
+            @"
+            Expected one of `@`, `(`, `<`, `=`, `>`, `~`, `!`, `;`, found `1`
+            name 1.0
+                 ^"
         );
     }
 
     #[test]
     fn error_random_char() {
-        assert_err(
-            "name >= 1.0 #",
-            indoc! {"
-                Trailing `#` is not allowed
-                name >= 1.0 #
-                     ^^^^^^^^"
-            },
+        assert_snapshot!(
+            parse_pep508_err("name >= 1.0 #"),
+            @"
+            Trailing `#` is not allowed
+            name >= 1.0 #
+                 ^^^^^^^^"
+        );
+    }
+
+    #[test]
+    #[cfg(feature = "non-pep508-extensions")]
+    fn error_invalid_extra_unnamed_url() {
+        assert_snapshot!(
+            parse_unnamed_err("/foo-3.0.0-py3-none-any.whl[d,]"),
+            @r###"
+        Expected an alphanumeric character starting the extra name, found ']'
+        /foo-3.0.0-py3-none-any.whl[d,]
+                                      ^
+        "###
         );
     }
 
     /// Check that the relative path support feature toggle works.
     #[test]
     fn non_pep508_paths() {
         let requirements = &[
@@ -1643,16 +1786,82 @@
             "foo @ file:foo-3.0.0-py3-none-any.whl",
             "foo @ ./foo-3.0.0-py3-none-any.whl",
         ];
         let cwd = env::current_dir().unwrap();
 
         for requirement in requirements {
             assert_eq!(
-                Requirement::parse(requirement, &cwd).is_ok(),
+                Requirement::<VerbatimUrl>::parse(requirement, &cwd).is_ok(),
                 cfg!(feature = "non-pep508-extensions"),
                 "{}: {:?}",
                 requirement,
-                Requirement::parse(requirement, &cwd)
+                Requirement::<VerbatimUrl>::parse(requirement, &cwd)
             );
         }
     }
+
+    #[test]
+    fn no_space_after_operator() {
+        let requirement = Requirement::<Url>::from_str("pytest;'4.0'>=python_version").unwrap();
+        assert_eq!(requirement.to_string(), "pytest ; '4.0' >= python_version");
+    }
+
+    #[test]
+    fn path_with_fragment() {
+        let requirements = if cfg!(windows) {
+            &[
+                "wheel @ file:///C:/Users/ferris/wheel-0.42.0.whl#hash=somehash",
+                "wheel @ C:/Users/ferris/wheel-0.42.0.whl#hash=somehash",
+            ]
+        } else {
+            &[
+                "wheel @ file:///Users/ferris/wheel-0.42.0.whl#hash=somehash",
+                "wheel @ /Users/ferris/wheel-0.42.0.whl#hash=somehash",
+            ]
+        };
+
+        for requirement in requirements {
+            // Extract the URL.
+            let Some(VersionOrUrl::Url(url)) = Requirement::<VerbatimUrl>::from_str(requirement)
+                .unwrap()
+                .version_or_url
+            else {
+                unreachable!("Expected a URL")
+            };
+
+            // Assert that the fragment and path have been separated correctly.
+            assert_eq!(url.fragment(), Some("hash=somehash"));
+            assert!(
+                url.path().ends_with("/Users/ferris/wheel-0.42.0.whl"),
+                "Expected the path to end with '/Users/ferris/wheel-0.42.0.whl', found '{}'",
+                url.path()
+            );
+        }
+    }
+
+    #[test]
+    fn add_extra_marker() -> Result<(), InvalidNameError> {
+        let requirement = Requirement::<Url>::from_str("pytest").unwrap();
+        let expected = Requirement::<Url>::from_str("pytest; extra == 'dotenv'").unwrap();
+        let actual = requirement.with_extra_marker(&ExtraName::from_str("dotenv")?);
+        assert_eq!(actual, expected);
+
+        let requirement = Requirement::<Url>::from_str("pytest; '4.0' >= python_version").unwrap();
+        let expected =
+            Requirement::from_str("pytest; '4.0' >= python_version and extra == 'dotenv'").unwrap();
+        let actual = requirement.with_extra_marker(&ExtraName::from_str("dotenv")?);
+        assert_eq!(actual, expected);
+
+        let requirement = Requirement::<Url>::from_str(
+            "pytest; '4.0' >= python_version or sys_platform == 'win32'",
+        )
+        .unwrap();
+        let expected = Requirement::from_str(
+            "pytest; ('4.0' >= python_version or sys_platform == 'win32') and extra == 'dotenv'",
+        )
+        .unwrap();
+        let actual = requirement.with_extra_marker(&ExtraName::from_str("dotenv")?);
+        assert_eq!(actual, expected);
+
+        Ok(())
+    }
 }
```

### Comparing `uv-0.1.9/crates/pep508-rs/src/marker.rs` & `uv-0.2.0/crates/pep508-rs/src/marker.rs`

 * *Files 20% similar despite different names*

```diff
@@ -5,28 +5,33 @@
 //! say `importlib-metadata ; python_version < "3.8"` or
 //! `itsdangerous (>=1.1.0) ; extra == 'security'`. Unfortunately, the marker grammar has some
 //! oversights (e.g. <https://github.com/pypa/packaging.python.org/pull/1181>) and
 //! the design of comparisons (PEP 440 comparisons with lexicographic fallback) leads to confusing
 //! outcomes. This implementation tries to carefully validate everything and emit warnings whenever
 //! bogus comparisons with unintended semantics are made.
 
-use crate::{Cursor, Pep508Error, Pep508ErrorSource};
-use pep440_rs::{Version, VersionPattern, VersionSpecifier};
-#[cfg(feature = "pyo3")]
-use pyo3::{
-    basic::CompareOp, exceptions::PyValueError, pyclass, pymethods, PyAny, PyResult, Python,
-};
-#[cfg(feature = "serde")]
-use serde::{de, Deserialize, Deserializer, Serialize, Serializer};
 use std::collections::HashSet;
 use std::fmt::{Display, Formatter};
 use std::ops::Deref;
 use std::str::FromStr;
+use std::sync::Arc;
+
+#[cfg(feature = "pyo3")]
+use pyo3::{
+    basic::CompareOp, exceptions::PyValueError, pyclass, pymethods, types::PyAnyMethods, PyResult,
+    Python,
+};
+use serde::{de, Deserialize, Deserializer, Serialize, Serializer};
+
+use pep440_rs::{Version, VersionParseError, VersionPattern, VersionSpecifier};
 use uv_normalize::ExtraName;
 
+use crate::cursor::Cursor;
+use crate::{Pep508Error, Pep508ErrorSource, Pep508Url, Reporter, TracingReporter};
+
 /// Ways in which marker evaluation can fail
 #[derive(Debug, Eq, Hash, Ord, PartialOrd, PartialEq, Clone, Copy)]
 #[cfg_attr(feature = "pyo3", pyclass(module = "pep508"))]
 pub enum MarkerWarningKind {
     /// Using an old name from PEP 345 instead of the modern equivalent
     /// <https://peps.python.org/pep-0345/#environment-markers>
     DeprecatedMarkerName,
@@ -54,15 +59,15 @@
     #[allow(clippy::trivially_copy_pass_by_ref)]
     fn __richcmp__(&self, other: Self, op: CompareOp) -> bool {
         op.matches(self.cmp(&other))
     }
 }
 
 /// Those environment markers with a PEP 440 version as value such as `python_version`
-#[derive(Clone, Debug, Eq, Hash, PartialEq)]
+#[derive(Clone, Debug, Eq, Hash, PartialEq, PartialOrd, Ord)]
 #[allow(clippy::enum_variant_names)]
 pub enum MarkerValueVersion {
     /// `implementation_version`
     ImplementationVersion,
     /// `python_full_version`
     PythonFullVersion,
     /// `python_version`
@@ -76,15 +81,15 @@
             Self::PythonFullVersion => f.write_str("python_full_version"),
             Self::PythonVersion => f.write_str("python_version"),
         }
     }
 }
 
 /// Those environment markers with an arbitrary string as value such as `sys_platform`
-#[derive(Clone, Debug, Eq, Hash, PartialEq)]
+#[derive(Clone, Debug, Eq, Hash, PartialEq, PartialOrd, Ord)]
 pub enum MarkerValueString {
     /// `implementation_name`
     ImplementationName,
     /// `os_name`
     OsName,
     /// Deprecated `os.name` from <https://peps.python.org/pep-0345/#environment-markers>
     OsNameDeprecated,
@@ -92,14 +97,16 @@
     PlatformMachine,
     /// Deprecated `platform.machine` from <https://peps.python.org/pep-0345/#environment-markers>
     PlatformMachineDeprecated,
     /// `platform_python_implementation`
     PlatformPythonImplementation,
     /// Deprecated `platform.python_implementation` from <https://peps.python.org/pep-0345/#environment-markers>
     PlatformPythonImplementationDeprecated,
+    /// Deprecated `python_implementation` from <https://github.com/pypa/packaging/issues/72>
+    PythonImplementationDeprecated,
     /// `platform_release`
     PlatformRelease,
     /// `platform_system`
     PlatformSystem,
     /// `platform_version`
     PlatformVersion,
     /// Deprecated `platform.version` from <https://peps.python.org/pep-0345/#environment-markers>
@@ -115,31 +122,31 @@
     fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
         match self {
             Self::ImplementationName => f.write_str("implementation_name"),
             Self::OsName | Self::OsNameDeprecated => f.write_str("os_name"),
             Self::PlatformMachine | Self::PlatformMachineDeprecated => {
                 f.write_str("platform_machine")
             }
-            Self::PlatformPythonImplementation | Self::PlatformPythonImplementationDeprecated => {
-                f.write_str("platform_python_implementation")
-            }
+            Self::PlatformPythonImplementation
+            | Self::PlatformPythonImplementationDeprecated
+            | Self::PythonImplementationDeprecated => f.write_str("platform_python_implementation"),
             Self::PlatformRelease => f.write_str("platform_release"),
             Self::PlatformSystem => f.write_str("platform_system"),
             Self::PlatformVersion | Self::PlatformVersionDeprecated => {
                 f.write_str("platform_version")
             }
             Self::SysPlatform | Self::SysPlatformDeprecated => f.write_str("sys_platform"),
         }
     }
 }
 
 /// One of the predefined environment values
 ///
 /// <https://packaging.python.org/en/latest/specifications/dependency-specifiers/#environment-markers>
-#[derive(Clone, Debug, Eq, Hash, PartialEq)]
+#[derive(Clone, Debug, Eq, Hash, PartialEq, PartialOrd, Ord)]
 pub enum MarkerValue {
     /// Those environment markers with a PEP 440 version as value such as `python_version`
     MarkerEnvVersion(MarkerValueVersion),
     /// Those environment markers with an arbitrary string as value such as `sys_platform`
     MarkerEnvString(MarkerValueString),
     /// `extra`. This one is special because it's a list and not env but user given
     Extra,
@@ -171,14 +178,17 @@
             }
             "platform_python_implementation" => {
                 Self::MarkerEnvString(MarkerValueString::PlatformPythonImplementation)
             }
             "platform.python_implementation" => {
                 Self::MarkerEnvString(MarkerValueString::PlatformPythonImplementationDeprecated)
             }
+            "python_implementation" => {
+                Self::MarkerEnvString(MarkerValueString::PythonImplementationDeprecated)
+            }
             "platform_release" => Self::MarkerEnvString(MarkerValueString::PlatformRelease),
             "platform_system" => Self::MarkerEnvString(MarkerValueString::PlatformSystem),
             "platform_version" => Self::MarkerEnvString(MarkerValueString::PlatformVersion),
             "platform.version" => {
                 Self::MarkerEnvString(MarkerValueString::PlatformVersionDeprecated)
             }
             "python_full_version" => Self::MarkerEnvVersion(MarkerValueVersion::PythonFullVersion),
@@ -200,15 +210,15 @@
             Self::Extra => f.write_str("extra"),
             Self::QuotedString(value) => write!(f, "'{value}'"),
         }
     }
 }
 
 /// How to compare key and value, such as by `==`, `>` or `not in`
-#[derive(Clone, Debug, Eq, Hash, PartialEq)]
+#[derive(Copy, Clone, Debug, Eq, Hash, PartialEq, PartialOrd, Ord)]
 pub enum MarkerOperator {
     /// `==`
     Equal,
     /// `!=`
     NotEqual,
     /// `>`
     GreaterThan,
@@ -224,25 +234,25 @@
     In,
     /// `not in`
     NotIn,
 }
 
 impl MarkerOperator {
     /// Compare two versions, returning None for `in` and `not in`
-    fn to_pep440_operator(&self) -> Option<pep440_rs::Operator> {
+    fn to_pep440_operator(self) -> Option<pep440_rs::Operator> {
         match self {
-            MarkerOperator::Equal => Some(pep440_rs::Operator::Equal),
-            MarkerOperator::NotEqual => Some(pep440_rs::Operator::NotEqual),
-            MarkerOperator::GreaterThan => Some(pep440_rs::Operator::GreaterThan),
-            MarkerOperator::GreaterEqual => Some(pep440_rs::Operator::GreaterThanEqual),
-            MarkerOperator::LessThan => Some(pep440_rs::Operator::LessThan),
-            MarkerOperator::LessEqual => Some(pep440_rs::Operator::LessThanEqual),
-            MarkerOperator::TildeEqual => Some(pep440_rs::Operator::TildeEqual),
-            MarkerOperator::In => None,
-            MarkerOperator::NotIn => None,
+            Self::Equal => Some(pep440_rs::Operator::Equal),
+            Self::NotEqual => Some(pep440_rs::Operator::NotEqual),
+            Self::GreaterThan => Some(pep440_rs::Operator::GreaterThan),
+            Self::GreaterEqual => Some(pep440_rs::Operator::GreaterThanEqual),
+            Self::LessThan => Some(pep440_rs::Operator::LessThan),
+            Self::LessEqual => Some(pep440_rs::Operator::LessThanEqual),
+            Self::TildeEqual => Some(pep440_rs::Operator::TildeEqual),
+            Self::In => None,
+            Self::NotIn => None,
         }
     }
 }
 
 impl FromStr for MarkerOperator {
     type Err = String;
 
@@ -298,41 +308,39 @@
     /// Original unchanged string
     pub string: String,
     /// Parsed version
     pub version: Version,
 }
 
 impl FromStr for StringVersion {
-    type Err = String;
+    type Err = VersionParseError;
 
     fn from_str(s: &str) -> Result<Self, Self::Err> {
         Ok(Self {
             string: s.to_string(),
-            version: Version::from_str(s).map_err(|e| e.to_string())?,
+            version: Version::from_str(s)?,
         })
     }
 }
 
 impl Display for StringVersion {
     fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
-        self.version.fmt(f)
+        self.string.fmt(f)
     }
 }
 
-#[cfg(feature = "serde")]
 impl Serialize for StringVersion {
     fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
     where
         S: Serializer,
     {
         serializer.serialize_str(&self.string)
     }
 }
 
-#[cfg(feature = "serde")]
 impl<'de> Deserialize<'de> for StringVersion {
     fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
     where
         D: Deserializer<'de>,
     {
         let string = String::deserialize(deserializer)?;
         Self::from_str(&string).map_err(de::Error::custom)
@@ -349,65 +357,306 @@
 
 /// The marker values for a python interpreter, normally the current one
 ///
 /// <https://packaging.python.org/en/latest/specifications/dependency-specifiers/#environment-markers>
 ///
 /// Some are `(String, Version)` because we have to support version comparison
 #[allow(missing_docs, clippy::unsafe_derive_deserialize)]
-#[cfg_attr(feature = "serde", derive(serde::Deserialize, serde::Serialize))]
-#[cfg_attr(feature = "pyo3", pyclass(get_all, module = "pep508"))]
-#[derive(Clone, Debug, Eq, Hash, PartialEq)]
+#[derive(Clone, Debug, Eq, Hash, PartialEq, serde::Deserialize, serde::Serialize)]
+#[cfg_attr(feature = "pyo3", pyclass(module = "pep508"))]
 pub struct MarkerEnvironment {
-    pub implementation_name: String,
-    pub implementation_version: StringVersion,
-    pub os_name: String,
-    pub platform_machine: String,
-    pub platform_python_implementation: String,
-    pub platform_release: String,
-    pub platform_system: String,
-    pub platform_version: String,
-    pub python_full_version: StringVersion,
-    pub python_version: StringVersion,
-    pub sys_platform: String,
+    #[serde(flatten)]
+    inner: Arc<MarkerEnvironmentInner>,
+}
+
+#[derive(Clone, Debug, Eq, Hash, PartialEq, serde::Deserialize, serde::Serialize)]
+struct MarkerEnvironmentInner {
+    implementation_name: String,
+    implementation_version: StringVersion,
+    os_name: String,
+    platform_machine: String,
+    platform_python_implementation: String,
+    platform_release: String,
+    platform_system: String,
+    platform_version: String,
+    python_full_version: StringVersion,
+    python_version: StringVersion,
+    sys_platform: String,
 }
 
 impl MarkerEnvironment {
     /// Returns of the PEP 440 version typed value of the key in the current environment
-    fn get_version(&self, key: &MarkerValueVersion) -> &Version {
+    pub fn get_version(&self, key: &MarkerValueVersion) -> &Version {
         match key {
-            MarkerValueVersion::ImplementationVersion => &self.implementation_version.version,
-            MarkerValueVersion::PythonFullVersion => &self.python_full_version.version,
-            MarkerValueVersion::PythonVersion => &self.python_version.version,
+            MarkerValueVersion::ImplementationVersion => &self.implementation_version().version,
+            MarkerValueVersion::PythonFullVersion => &self.python_full_version().version,
+            MarkerValueVersion::PythonVersion => &self.python_version().version,
         }
     }
 
     /// Returns of the stringly typed value of the key in the current environment
-    fn get_string(&self, key: &MarkerValueString) -> &str {
+    pub fn get_string(&self, key: &MarkerValueString) -> &str {
         match key {
-            MarkerValueString::ImplementationName => &self.implementation_name,
-            MarkerValueString::OsName | MarkerValueString::OsNameDeprecated => &self.os_name,
+            MarkerValueString::ImplementationName => self.implementation_name(),
+            MarkerValueString::OsName | MarkerValueString::OsNameDeprecated => self.os_name(),
             MarkerValueString::PlatformMachine | MarkerValueString::PlatformMachineDeprecated => {
-                &self.platform_machine
+                self.platform_machine()
             }
             MarkerValueString::PlatformPythonImplementation
-            | MarkerValueString::PlatformPythonImplementationDeprecated => {
-                &self.platform_python_implementation
+            | MarkerValueString::PlatformPythonImplementationDeprecated
+            | MarkerValueString::PythonImplementationDeprecated => {
+                self.platform_python_implementation()
             }
-            MarkerValueString::PlatformRelease => &self.platform_release,
-            MarkerValueString::PlatformSystem => &self.platform_system,
+            MarkerValueString::PlatformRelease => self.platform_release(),
+            MarkerValueString::PlatformSystem => self.platform_system(),
             MarkerValueString::PlatformVersion | MarkerValueString::PlatformVersionDeprecated => {
-                &self.platform_version
+                self.platform_version()
             }
             MarkerValueString::SysPlatform | MarkerValueString::SysPlatformDeprecated => {
-                &self.sys_platform
+                self.sys_platform()
             }
         }
     }
 }
 
+/// APIs for retrieving specific parts of a marker environment.
+impl MarkerEnvironment {
+    /// Returns the name of the Python implementation for this environment.
+    ///
+    /// This is equivalent to `sys.implementation.name`.
+    ///
+    /// Some example values are: `cpython`.
+    #[inline]
+    pub fn implementation_name(&self) -> &str {
+        &self.inner.implementation_name
+    }
+
+    /// Returns the Python implementation version for this environment.
+    ///
+    /// This value is derived from `sys.implementation.version`. See [PEP 508
+    /// environment markers] for full details.
+    ///
+    /// This is equivalent to `sys.implementation.name`.
+    ///
+    /// Some example values are: `3.4.0`, `3.5.0b1`.
+    ///
+    /// [PEP 508 environment markers]: https://peps.python.org/pep-0508/#environment-markers
+    #[inline]
+    pub fn implementation_version(&self) -> &StringVersion {
+        &self.inner.implementation_version
+    }
+
+    /// Returns the name of the operating system for this environment.
+    ///
+    /// This is equivalent to `os.name`.
+    ///
+    /// Some example values are: `posix`, `java`.
+    #[inline]
+    pub fn os_name(&self) -> &str {
+        &self.inner.os_name
+    }
+
+    /// Returns the name of the machine for this environment's platform.
+    ///
+    /// This is equivalent to `platform.machine()`.
+    ///
+    /// Some example values are: `x86_64`.
+    #[inline]
+    pub fn platform_machine(&self) -> &str {
+        &self.inner.platform_machine
+    }
+
+    /// Returns the name of the Python implementation for this environment's
+    /// platform.
+    ///
+    /// This is equivalent to `platform.python_implementation()`.
+    ///
+    /// Some example values are: `CPython`, `Jython`.
+    #[inline]
+    pub fn platform_python_implementation(&self) -> &str {
+        &self.inner.platform_python_implementation
+    }
+
+    /// Returns the release for this environment's platform.
+    ///
+    /// This is equivalent to `platform.release()`.
+    ///
+    /// Some example values are: `3.14.1-x86_64-linode39`, `14.5.0`, `1.8.0_51`.
+    #[inline]
+    pub fn platform_release(&self) -> &str {
+        &self.inner.platform_release
+    }
+
+    /// Returns the system for this environment's platform.
+    ///
+    /// This is equivalent to `platform.system()`.
+    ///
+    /// Some example values are: `Linux`, `Windows`, `Java`.
+    #[inline]
+    pub fn platform_system(&self) -> &str {
+        &self.inner.platform_system
+    }
+
+    /// Returns the version for this environment's platform.
+    ///
+    /// This is equivalent to `platform.version()`.
+    ///
+    /// Some example values are: `#1 SMP Fri Apr 25 13:07:35 EDT 2014`,
+    /// `Java HotSpot(TM) 64-Bit Server VM, 25.51-b03, Oracle Corporation`,
+    /// `Darwin Kernel Version 14.5.0: Wed Jul 29 02:18:53 PDT 2015;
+    /// root:xnu-2782.40.9~2/RELEASE_X86_64`.
+    #[inline]
+    pub fn platform_version(&self) -> &str {
+        &self.inner.platform_version
+    }
+
+    /// Returns the full version of Python for this environment.
+    ///
+    /// This is equivalent to `platform.python_version()`.
+    ///
+    /// Some example values are: `3.4.0`, `3.5.0b1`.
+    #[inline]
+    pub fn python_full_version(&self) -> &StringVersion {
+        &self.inner.python_full_version
+    }
+
+    /// Returns the version of Python for this environment.
+    ///
+    /// This is equivalent to `'.'.join(platform.python_version_tuple()[:2])`.
+    ///
+    /// Some example values are: `3.4`, `2.7`.
+    #[inline]
+    pub fn python_version(&self) -> &StringVersion {
+        &self.inner.python_version
+    }
+
+    /// Returns the name of the system platform for this environment.
+    ///
+    /// This is equivalent to `sys.platform`.
+    ///
+    /// Some example values are: `linux`, `linux2`, `darwin`, `java1.8.0_51`
+    /// (note that `linux` is from Python3 and `linux2` from Python2).
+    #[inline]
+    pub fn sys_platform(&self) -> &str {
+        &self.inner.sys_platform
+    }
+}
+
+/// APIs for setting specific parts of a marker environment.
+impl MarkerEnvironment {
+    /// Set the name of the Python implementation for this environment.
+    ///
+    /// See also [`MarkerEnvironment::implementation_name`].
+    #[inline]
+    pub fn with_implementation_name(mut self, value: impl Into<String>) -> MarkerEnvironment {
+        Arc::make_mut(&mut self.inner).implementation_name = value.into();
+        self
+    }
+
+    /// Set the Python implementation version for this environment.
+    ///
+    /// See also [`MarkerEnvironment::implementation_version`].
+    #[inline]
+    pub fn with_implementation_version(
+        mut self,
+        value: impl Into<StringVersion>,
+    ) -> MarkerEnvironment {
+        Arc::make_mut(&mut self.inner).implementation_version = value.into();
+        self
+    }
+
+    /// Set the name of the operating system for this environment.
+    ///
+    /// See also [`MarkerEnvironment::os_name`].
+    #[inline]
+    pub fn with_os_name(mut self, value: impl Into<String>) -> MarkerEnvironment {
+        Arc::make_mut(&mut self.inner).os_name = value.into();
+        self
+    }
+
+    /// Set the name of the machine for this environment's platform.
+    ///
+    /// See also [`MarkerEnvironment::platform_machine`].
+    #[inline]
+    pub fn with_platform_machine(mut self, value: impl Into<String>) -> MarkerEnvironment {
+        Arc::make_mut(&mut self.inner).platform_machine = value.into();
+        self
+    }
+
+    /// Set the name of the Python implementation for this environment's
+    /// platform.
+    ///
+    /// See also [`MarkerEnvironment::platform_python_implementation`].
+    #[inline]
+    pub fn with_platform_python_implementation(
+        mut self,
+        value: impl Into<String>,
+    ) -> MarkerEnvironment {
+        Arc::make_mut(&mut self.inner).platform_python_implementation = value.into();
+        self
+    }
+
+    /// Set the release for this environment's platform.
+    ///
+    /// See also [`MarkerEnvironment::platform_release`].
+    #[inline]
+    pub fn with_platform_release(mut self, value: impl Into<String>) -> MarkerEnvironment {
+        Arc::make_mut(&mut self.inner).platform_release = value.into();
+        self
+    }
+
+    /// Set the system for this environment's platform.
+    ///
+    /// See also [`MarkerEnvironment::platform_system`].
+    #[inline]
+    pub fn with_platform_system(mut self, value: impl Into<String>) -> MarkerEnvironment {
+        Arc::make_mut(&mut self.inner).platform_system = value.into();
+        self
+    }
+
+    /// Set the version for this environment's platform.
+    ///
+    /// See also [`MarkerEnvironment::platform_version`].
+    #[inline]
+    pub fn with_platform_version(mut self, value: impl Into<String>) -> MarkerEnvironment {
+        Arc::make_mut(&mut self.inner).platform_version = value.into();
+        self
+    }
+
+    /// Set the full version of Python for this environment.
+    ///
+    /// See also [`MarkerEnvironment::python_full_version`].
+    #[inline]
+    pub fn with_python_full_version(
+        mut self,
+        value: impl Into<StringVersion>,
+    ) -> MarkerEnvironment {
+        Arc::make_mut(&mut self.inner).python_full_version = value.into();
+        self
+    }
+
+    /// Set the version of Python for this environment.
+    ///
+    /// See also [`MarkerEnvironment::python_full_version`].
+    #[inline]
+    pub fn with_python_version(mut self, value: impl Into<StringVersion>) -> MarkerEnvironment {
+        Arc::make_mut(&mut self.inner).python_version = value.into();
+        self
+    }
+
+    /// Set the name of the system platform for this environment.
+    ///
+    /// See also [`MarkerEnvironment::sys_platform`].
+    #[inline]
+    pub fn with_sys_platform(mut self, value: impl Into<String>) -> MarkerEnvironment {
+        Arc::make_mut(&mut self.inner).sys_platform = value.into();
+        self
+    }
+}
+
 #[cfg(feature = "pyo3")]
 #[pymethods]
 impl MarkerEnvironment {
     /// Construct your own marker environment
     #[new]
     #[pyo3(signature = (*,
         implementation_name,
@@ -449,43 +698,45 @@
         })?;
         let python_version = StringVersion::from_str(python_version).map_err(|err| {
             PyValueError::new_err(format!(
                 "python_version is not a valid PEP440 version: {err}"
             ))
         })?;
         Ok(Self {
-            implementation_name: implementation_name.to_string(),
-            implementation_version,
-            os_name: os_name.to_string(),
-            platform_machine: platform_machine.to_string(),
-            platform_python_implementation: platform_python_implementation.to_string(),
-            platform_release: platform_release.to_string(),
-            platform_system: platform_system.to_string(),
-            platform_version: platform_version.to_string(),
-            python_full_version,
-            python_version,
-            sys_platform: sys_platform.to_string(),
+            inner: Arc::new(MarkerEnvironmentInner {
+                implementation_name: implementation_name.to_string(),
+                implementation_version,
+                os_name: os_name.to_string(),
+                platform_machine: platform_machine.to_string(),
+                platform_python_implementation: platform_python_implementation.to_string(),
+                platform_release: platform_release.to_string(),
+                platform_system: platform_system.to_string(),
+                platform_version: platform_version.to_string(),
+                python_full_version,
+                python_version,
+                sys_platform: sys_platform.to_string(),
+            }),
         })
     }
 
     /// Query the current python interpreter to get the correct marker value
     #[staticmethod]
     fn current(py: Python<'_>) -> PyResult<Self> {
-        let os = py.import("os")?;
-        let platform = py.import("platform")?;
-        let sys = py.import("sys")?;
+        let os = py.import_bound("os")?;
+        let platform = py.import_bound("platform")?;
+        let sys = py.import_bound("sys")?;
         let python_version_tuple: (String, String, String) = platform
             .getattr("python_version_tuple")?
             .call0()?
             .extract()?;
 
         // See pseudocode at
         // https://packaging.python.org/en/latest/specifications/dependency-specifiers/#environment-markers
         let name = sys.getattr("implementation")?.getattr("name")?.extract()?;
-        let info: &PyAny = sys.getattr("implementation")?.getattr("version")?;
+        let info = sys.getattr("implementation")?.getattr("version")?;
         let kind = info.getattr("releaselevel")?.extract::<String>()?;
         let implementation_version: String = format!(
             "{}.{}.{}{}",
             info.getattr("major")?.extract::<usize>()?,
             info.getattr("minor")?.extract::<usize>()?,
             info.getattr("micro")?.extract::<usize>()?,
             if kind == "final" {
@@ -511,213 +762,597 @@
         })?;
         let python_version = StringVersion::from_str(&python_version).map_err(|err| {
             PyValueError::new_err(format!(
                 "Broken python implementation, python_version is not a valid PEP440 version: {err}"
             ))
         })?;
         Ok(Self {
-            implementation_name: name,
-            implementation_version,
-            os_name: os.getattr("name")?.extract()?,
-            platform_machine: platform.getattr("machine")?.call0()?.extract()?,
-            platform_python_implementation: platform
-                .getattr("python_implementation")?
-                .call0()?
-                .extract()?,
-            platform_release: platform.getattr("release")?.call0()?.extract()?,
-            platform_system: platform.getattr("system")?.call0()?.extract()?,
-            platform_version: platform.getattr("version")?.call0()?.extract()?,
-            python_full_version,
-            python_version,
-            sys_platform: sys.getattr("platform")?.extract()?,
+            inner: Arc::new(MarkerEnvironmentInner {
+                implementation_name: name,
+                implementation_version,
+                os_name: os.getattr("name")?.extract()?,
+                platform_machine: platform.getattr("machine")?.call0()?.extract()?,
+                platform_python_implementation: platform
+                    .getattr("python_implementation")?
+                    .call0()?
+                    .extract()?,
+                platform_release: platform.getattr("release")?.call0()?.extract()?,
+                platform_system: platform.getattr("system")?.call0()?.extract()?,
+                platform_version: platform.getattr("version")?.call0()?.extract()?,
+                python_full_version,
+                python_version,
+                sys_platform: sys.getattr("platform")?.extract()?,
+            }),
         })
     }
+
+    /// Returns the name of the Python implementation for this environment.
+    #[getter]
+    pub fn py_implementation_name(&self) -> String {
+        self.implementation_name().to_string()
+    }
+
+    /// Returns the Python implementation version for this environment.
+    #[getter]
+    pub fn py_implementation_version(&self) -> StringVersion {
+        self.implementation_version().clone()
+    }
+
+    /// Returns the name of the operating system for this environment.
+    #[getter]
+    pub fn py_os_name(&self) -> String {
+        self.os_name().to_string()
+    }
+
+    /// Returns the name of the machine for this environment's platform.
+    #[getter]
+    pub fn py_platform_machine(&self) -> String {
+        self.platform_machine().to_string()
+    }
+
+    /// Returns the name of the Python implementation for this environment's
+    /// platform.
+    #[getter]
+    pub fn py_platform_python_implementation(&self) -> String {
+        self.platform_python_implementation().to_string()
+    }
+
+    /// Returns the release for this environment's platform.
+    #[getter]
+    pub fn py_platform_release(&self) -> String {
+        self.platform_release().to_string()
+    }
+
+    /// Returns the system for this environment's platform.
+    #[getter]
+    pub fn py_platform_system(&self) -> String {
+        self.platform_system().to_string()
+    }
+
+    /// Returns the version for this environment's platform.
+    #[getter]
+    pub fn py_platform_version(&self) -> String {
+        self.platform_version().to_string()
+    }
+
+    /// Returns the full version of Python for this environment.
+    #[getter]
+    pub fn py_python_full_version(&self) -> StringVersion {
+        self.python_full_version().clone()
+    }
+
+    /// Returns the version of Python for this environment.
+    #[getter]
+    pub fn py_python_version(&self) -> StringVersion {
+        self.python_version().clone()
+    }
+
+    /// Returns the name of the system platform for this environment.
+    #[getter]
+    pub fn py_sys_platform(&self) -> String {
+        self.sys_platform().to_string()
+    }
 }
 
-/// Represents one clause such as `python_version > "3.8"` in the form
-/// ```text
-/// <a name from the PEP508 list | a string> <an operator> <a name from the PEP508 list | a string>
-/// ```
+/// A builder for constructing a marker environment.
+///
+/// A value of this type can be fallibly converted to a full
+/// [`MarkerEnvironment`] via [`MarkerEnvironment::try_from`]. This can fail when
+/// the version strings given aren't valid.
+///
+/// The main utility of this type is for constructing dummy or test environment
+/// values.
+#[allow(missing_docs)]
 #[derive(Clone, Debug, Eq, Hash, PartialEq)]
-pub struct MarkerExpression {
-    /// A name from the PEP508 list or a string
-    pub l_value: MarkerValue,
-    /// an operator, such as `>=` or `not in`
-    pub operator: MarkerOperator,
-    /// A name from the PEP508 list or a string
-    pub r_value: MarkerValue,
+pub struct MarkerEnvironmentBuilder<'a> {
+    pub implementation_name: &'a str,
+    pub implementation_version: &'a str,
+    pub os_name: &'a str,
+    pub platform_machine: &'a str,
+    pub platform_python_implementation: &'a str,
+    pub platform_release: &'a str,
+    pub platform_system: &'a str,
+    pub platform_version: &'a str,
+    pub python_full_version: &'a str,
+    pub python_version: &'a str,
+    pub sys_platform: &'a str,
+}
+
+impl<'a> TryFrom<MarkerEnvironmentBuilder<'a>> for MarkerEnvironment {
+    type Error = VersionParseError;
+
+    fn try_from(builder: MarkerEnvironmentBuilder<'a>) -> Result<Self, Self::Error> {
+        Ok(MarkerEnvironment {
+            inner: Arc::new(MarkerEnvironmentInner {
+                implementation_name: builder.implementation_name.to_string(),
+                implementation_version: builder.implementation_version.parse()?,
+                os_name: builder.os_name.to_string(),
+                platform_machine: builder.platform_machine.to_string(),
+                platform_python_implementation: builder.platform_python_implementation.to_string(),
+                platform_release: builder.platform_release.to_string(),
+                platform_system: builder.platform_system.to_string(),
+                platform_version: builder.platform_version.to_string(),
+                python_full_version: builder.python_full_version.parse()?,
+                python_version: builder.python_version.parse()?,
+                sys_platform: builder.sys_platform.to_string(),
+            }),
+        })
+    }
 }
 
-impl MarkerExpression {
-    /// Evaluate a <`marker_value`> <`marker_op`> <`marker_value`> expression
-    fn evaluate(
-        &self,
-        env: &MarkerEnvironment,
-        extras: &[ExtraName],
-        reporter: &mut impl FnMut(MarkerWarningKind, String, &MarkerExpression),
-    ) -> bool {
-        match &self.l_value {
-            // The only sound choice for this is `<version key> <version op> <quoted PEP 440 version>`
-            MarkerValue::MarkerEnvVersion(l_key) => {
-                let value = &self.r_value;
-                let r_vpat = if let MarkerValue::QuotedString(r_string) = &value {
-                    match r_string.parse::<VersionPattern>() {
-                        Ok(vpat) => vpat,
-                        Err(err) => {
-                            reporter(MarkerWarningKind::Pep440Error, format!(
-                                "Expected PEP 440 version to compare with {}, found {}, evaluating to false: {}",
-                                l_key, self.r_value, err
-                            ), self);
-                            return false;
-                        }
-                    }
-                } else {
-                    reporter(MarkerWarningKind::Pep440Error, format!(
-                        "Expected double quoted PEP 440 version to compare with {}, found {}, evaluating to false",
-                        l_key, self.r_value
-                    ), self);
-                    return false;
-                };
+/// Represents one clause such as `python_version > "3.8"`.
+#[derive(Clone, Debug, Eq, Hash, PartialEq, PartialOrd, Ord)]
+#[allow(missing_docs)]
+pub enum MarkerExpression {
+    /// A version expression, e.g. `<version key> <version op> <quoted PEP 440 version>`.
+    Version {
+        key: MarkerValueVersion,
+        specifier: VersionSpecifier,
+    },
+    /// An inverted version expression, e.g `<quoted PEP 440 version> <version op> <version key>`.
+    VersionInverted {
+        /// No star allowed here, `'3.*' == python_version` is not a valid PEP 440 comparison.
+        version: Version,
+        operator: pep440_rs::Operator,
+        key: MarkerValueVersion,
+    },
+    /// An string marker comparison, e.g. `sys_platform == '...'`.
+    String {
+        key: MarkerValueString,
+        operator: MarkerOperator,
+        value: String,
+    },
+    /// An inverted string marker comparison, e.g. `'...' == sys_platform`.
+    StringInverted {
+        value: String,
+        operator: MarkerOperator,
+        key: MarkerValueString,
+    },
+    /// `extra <extra op> '...'` or `'...' <extra op> extra`
+    Extra {
+        operator: ExtraOperator,
+        name: ExtraName,
+    },
+    /// An invalid or meaningless expression, such as '...' == '...'.
+    ///
+    /// Invalid expressions always evaluate to `false`, and are warned for during parsing.
+    Arbitrary {
+        l_value: MarkerValue,
+        operator: MarkerOperator,
+        r_value: MarkerValue,
+    },
+}
 
-                let operator = match self.operator.to_pep440_operator() {
-                    None => {
-                        reporter(MarkerWarningKind::Pep440Error, format!(
-                            "Expected PEP 440 version operator to compare {} with '{}', found '{}', evaluating to false",
-                            l_key, r_vpat.version(), self.operator
-                        ), self);
-                        return false;
-                    }
-                    Some(operator) => operator,
-                };
+/// The operator for an extra expression, either '==' or '!='.
+#[derive(Clone, Debug, Eq, Hash, PartialEq, PartialOrd, Ord)]
+pub enum ExtraOperator {
+    /// `==`
+    Equal,
+    /// `!=`
+    NotEqual,
+}
 
-                let specifier = match VersionSpecifier::new(operator, r_vpat) {
-                    Ok(specifier) => specifier,
-                    Err(err) => {
-                        reporter(
-                            MarkerWarningKind::Pep440Error,
-                            format!("Invalid operator/version combination: {err}"),
-                            self,
-                        );
-                        return false;
-                    }
+impl ExtraOperator {
+    fn from_marker_operator(operator: MarkerOperator) -> Option<ExtraOperator> {
+        match operator {
+            MarkerOperator::Equal => Some(ExtraOperator::Equal),
+            MarkerOperator::NotEqual => Some(ExtraOperator::NotEqual),
+            _ => None,
+        }
+    }
+}
+
+impl Display for ExtraOperator {
+    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
+        f.write_str(match self {
+            Self::Equal => "==",
+            Self::NotEqual => "!=",
+        })
+    }
+}
+
+impl MarkerExpression {
+    /// Parse a [`MarkerExpression`] from a string with the given reporter.
+    pub fn parse_reporter(s: &str, reporter: &mut impl Reporter) -> Result<Self, Pep508Error> {
+        let mut chars = Cursor::new(s);
+        let expression = parse_marker_key_op_value(&mut chars, reporter)?;
+        chars.eat_whitespace();
+        if let Some((pos, unexpected)) = chars.next() {
+            return Err(Pep508Error {
+                message: Pep508ErrorSource::String(format!(
+                    "Unexpected character '{unexpected}', expected end of input"
+                )),
+                start: pos,
+                len: chars.remaining(),
+                input: chars.to_string(),
+            });
+        }
+        Ok(expression)
+    }
+
+    /// Convert a <`marker_value`> <`marker_op`> <`marker_value`> expression into it's
+    /// typed equivalent.
+    fn new(
+        l_value: MarkerValue,
+        operator: MarkerOperator,
+        r_value: MarkerValue,
+        reporter: &mut impl Reporter,
+    ) -> MarkerExpression {
+        match l_value {
+            // The only sound choice for this is `<version key> <version op> <quoted PEP 440 version>`
+            MarkerValue::MarkerEnvVersion(key) => {
+                let MarkerValue::QuotedString(value) = r_value else {
+                    reporter.report(
+                        MarkerWarningKind::Pep440Error,
+                        format!(
+                            "Expected double quoted PEP 440 version to compare with {}, found {},
+                            will evaluate to false",
+                            key, r_value
+                        ),
+                    );
+
+                    return MarkerExpression::arbitrary(
+                        MarkerValue::MarkerEnvVersion(key),
+                        operator,
+                        r_value,
+                    );
                 };
 
-                let l_version = env.get_version(l_key);
-                specifier.contains(l_version)
+                match MarkerExpression::version(key.clone(), operator, &value, reporter) {
+                    Some(expr) => expr,
+                    None => MarkerExpression::arbitrary(
+                        MarkerValue::MarkerEnvVersion(key),
+                        operator,
+                        MarkerValue::QuotedString(value),
+                    ),
+                }
             }
-            // This is half the same block as above inverted
-            MarkerValue::MarkerEnvString(l_key) => {
-                let r_string = match &self.r_value {
+            // The only sound choice for this is `<env key> <op> <string>`
+            MarkerValue::MarkerEnvString(key) => {
+                let value = match r_value {
                     MarkerValue::Extra
                     | MarkerValue::MarkerEnvVersion(_)
                     | MarkerValue::MarkerEnvString(_) => {
-                        reporter(MarkerWarningKind::MarkerMarkerComparison, "Comparing two markers with each other doesn't make any sense, evaluating to false".to_string(), self);
-                        return false;
+                        reporter.report(
+                            MarkerWarningKind::MarkerMarkerComparison,
+                            "Comparing two markers with each other doesn't make any sense,
+                            will evaluate to false"
+                                .to_string(),
+                        );
+
+                        return MarkerExpression::arbitrary(
+                            MarkerValue::MarkerEnvString(key),
+                            operator,
+                            r_value,
+                        );
                     }
                     MarkerValue::QuotedString(r_string) => r_string,
                 };
 
-                let l_string = env.get_string(l_key);
-                self.compare_strings(l_string, r_string, reporter)
+                MarkerExpression::String {
+                    key,
+                    operator,
+                    value,
+                }
             }
             // `extra == '...'`
             MarkerValue::Extra => {
-                let r_value_string = match &self.r_value {
+                let value = match r_value {
                     MarkerValue::MarkerEnvVersion(_)
                     | MarkerValue::MarkerEnvString(_)
                     | MarkerValue::Extra => {
-                        reporter(MarkerWarningKind::ExtraInvalidComparison, "Comparing extra with something other than a quoted string is wrong, evaluating to false".to_string(), self);
-                        return false;
+                        reporter.report(
+                            MarkerWarningKind::ExtraInvalidComparison,
+                            "Comparing extra with something other than a quoted string is wrong,
+                            will evaluate to false"
+                                .to_string(),
+                        );
+                        return MarkerExpression::arbitrary(l_value, operator, r_value);
                     }
-                    MarkerValue::QuotedString(r_value_string) => r_value_string,
+                    MarkerValue::QuotedString(value) => value,
                 };
-                match ExtraName::from_str(r_value_string) {
-                    Ok(r_extra) => extras.contains(&r_extra),
-                    Err(err) => {
-                        reporter(MarkerWarningKind::ExtraInvalidComparison, format!(
-                            "Expected extra name, found '{r_value_string}', evaluating to false: {err}"
-                        ), self);
-                        false
-                    }
+
+                match MarkerExpression::extra(operator, &value, reporter) {
+                    Some(expr) => expr,
+                    None => MarkerExpression::arbitrary(
+                        MarkerValue::Extra,
+                        operator,
+                        MarkerValue::QuotedString(value),
+                    ),
                 }
             }
             // This is either MarkerEnvVersion, MarkerEnvString or Extra inverted
             MarkerValue::QuotedString(l_string) => {
-                match &self.r_value {
+                match r_value {
                     // The only sound choice for this is `<quoted PEP 440 version> <version op>` <version key>
-                    MarkerValue::MarkerEnvVersion(r_key) => {
-                        let l_version = match Version::from_str(l_string) {
-                            Ok(l_version) => l_version,
-                            Err(err) => {
-                                reporter(MarkerWarningKind::Pep440Error, format!(
-                                    "Expected double quoted PEP 440 version to compare with {}, found {}, evaluating to false: {}",
-                                    l_string, self.r_value, err
-                                ), self);
-                                return false;
-                            }
-                        };
-                        let r_version = env.get_version(r_key);
-
-                        let operator = match self.operator.to_pep440_operator() {
-                            None => {
-                                reporter(MarkerWarningKind::Pep440Error, format!(
-                                    "Expected PEP 440 version operator to compare '{}' with {}, found '{}', evaluating to false",
-                                    l_string, r_key, self.operator
-                                ), self);
-                                return false;
-                            }
-                            Some(operator) => operator,
-                        };
-
-                        let specifier = match VersionSpecifier::new(
+                    MarkerValue::MarkerEnvVersion(key) => {
+                        match MarkerExpression::version_inverted(
+                            &l_string,
                             operator,
-                            VersionPattern::verbatim(r_version.clone()),
+                            key.clone(),
+                            reporter,
                         ) {
-                            Ok(specifier) => specifier,
-                            Err(err) => {
-                                reporter(
-                                    MarkerWarningKind::Pep440Error,
-                                    format!("Invalid operator/version combination: {err}"),
-                                    self,
-                                );
-                                return false;
-                            }
-                        };
-
-                        specifier.contains(&l_version)
-                    }
-                    // This is half the same block as above inverted
-                    MarkerValue::MarkerEnvString(r_key) => {
-                        let r_string = env.get_string(r_key);
-                        self.compare_strings(l_string, r_string, reporter)
+                            Some(expr) => expr,
+                            None => MarkerExpression::arbitrary(
+                                MarkerValue::QuotedString(l_string),
+                                operator,
+                                MarkerValue::MarkerEnvVersion(key),
+                            ),
+                        }
                     }
+                    // '...' == <env key>
+                    MarkerValue::MarkerEnvString(key) => MarkerExpression::StringInverted {
+                        key,
+                        operator,
+                        value: l_string,
+                    },
                     // `'...' == extra`
-                    MarkerValue::Extra => match ExtraName::from_str(l_string) {
-                        Ok(l_extra) => self.marker_compare(&l_extra, extras, reporter),
-                        Err(err) => {
-                            reporter(MarkerWarningKind::ExtraInvalidComparison, format!(
-                                    "Expected extra name, found '{l_string}', evaluating to false: {err}"
-                                ), self);
-                            false
+                    MarkerValue::Extra => {
+                        match MarkerExpression::extra(operator, &l_string, reporter) {
+                            Some(expr) => expr,
+                            None => MarkerExpression::arbitrary(
+                                MarkerValue::QuotedString(l_string),
+                                operator,
+                                MarkerValue::Extra,
+                            ),
                         }
-                    },
+                    }
                     // `'...' == '...'`, doesn't make much sense
                     MarkerValue::QuotedString(_) => {
                         // Not even pypa/packaging 22.0 supports this
                         // https://github.com/pypa/packaging/issues/632
-                        reporter(MarkerWarningKind::StringStringComparison, format!(
-                            "Comparing two quoted strings with each other doesn't make sense: {self}, evaluating to false"
-                        ), self);
-                        false
+                        let expr = MarkerExpression::arbitrary(
+                            MarkerValue::QuotedString(l_string),
+                            operator,
+                            r_value,
+                        );
+
+                        reporter.report(MarkerWarningKind::StringStringComparison, format!(
+                            "Comparing two quoted strings with each other doesn't make sense: {expr},
+                            will evaluate to false"
+                        ));
+
+                        expr
                     }
                 }
             }
         }
     }
 
+    /// Creates an instance of [`MarkerExpression::Arbitrary`] with the given values.
+    fn arbitrary(
+        l_value: MarkerValue,
+        operator: MarkerOperator,
+        r_value: MarkerValue,
+    ) -> MarkerExpression {
+        MarkerExpression::Arbitrary {
+            l_value,
+            operator,
+            r_value,
+        }
+    }
+
+    /// Creates an instance of [`MarkerExpression::Version`] with the given values.
+    ///
+    /// Reports a warning on failure, and returns `None`.
+    pub fn version(
+        key: MarkerValueVersion,
+        marker_operator: MarkerOperator,
+        value: &str,
+        reporter: &mut impl Reporter,
+    ) -> Option<MarkerExpression> {
+        let pattern = match value.parse::<VersionPattern>() {
+            Ok(pattern) => pattern,
+            Err(err) => {
+                reporter.report(
+                    MarkerWarningKind::Pep440Error,
+                    format!(
+                        "Expected PEP 440 version to compare with {}, found {}, will evaluate to false: {}",
+                        key, value, err
+                    ),
+                );
+
+                return None;
+            }
+        };
+
+        let Some(operator) = marker_operator.to_pep440_operator() else {
+            reporter.report(
+                MarkerWarningKind::Pep440Error,
+                format!(
+                    "Expected PEP 440 version operator to compare {} with '{}',
+                    found '{}', will evaluate to false",
+                    key,
+                    pattern.version(),
+                    marker_operator
+                ),
+            );
+
+            return None;
+        };
+
+        let specifier = match VersionSpecifier::from_pattern(operator, pattern) {
+            Ok(specifier) => specifier,
+            Err(err) => {
+                reporter.report(
+                    MarkerWarningKind::Pep440Error,
+                    format!("Invalid operator/version combination: {err}"),
+                );
+                return None;
+            }
+        };
+
+        Some(MarkerExpression::Version { key, specifier })
+    }
+
+    /// Creates an instance of [`MarkerExpression::VersionInverted`] with the given values.
+    ///
+    /// Reports a warning on failure, and returns `None`.
+    fn version_inverted(
+        value: &str,
+        marker_operator: MarkerOperator,
+        key: MarkerValueVersion,
+        reporter: &mut impl Reporter,
+    ) -> Option<MarkerExpression> {
+        let version = match value.parse::<Version>() {
+            Ok(version) => version,
+            Err(err) => {
+                reporter.report(
+                    MarkerWarningKind::Pep440Error,
+                    format!(
+                        "Expected PEP 440 version to compare with {}, found {}, will evaluate to false: {}",
+                        key, value, err
+                    ),
+                );
+
+                return None;
+            }
+        };
+
+        let Some(operator) = marker_operator.to_pep440_operator() else {
+            reporter.report(
+                MarkerWarningKind::Pep440Error,
+                format!(
+                    "Expected PEP 440 version operator to compare {} with '{}',
+                    found '{}', will evaluate to false",
+                    key, version, marker_operator
+                ),
+            );
+
+            return None;
+        };
+
+        Some(MarkerExpression::VersionInverted {
+            version,
+            operator,
+            key,
+        })
+    }
+
+    /// Creates an instance of [`MarkerExpression::Extra`] with the given values, falling back to
+    /// [`MarkerExpression::Arbitrary`] on failure.
+    fn extra(
+        operator: MarkerOperator,
+        value: &str,
+        reporter: &mut impl Reporter,
+    ) -> Option<MarkerExpression> {
+        let name = match ExtraName::from_str(value) {
+            Ok(name) => name,
+            Err(err) => {
+                reporter.report(
+                    MarkerWarningKind::ExtraInvalidComparison,
+                    format!("Expected extra name, found '{value}', will evaluate to false: {err}"),
+                );
+
+                return None;
+            }
+        };
+
+        match ExtraOperator::from_marker_operator(operator) {
+            Some(operator) => Some(MarkerExpression::Extra { operator, name }),
+            None => {
+                reporter.report(
+                    MarkerWarningKind::ExtraInvalidComparison,
+                    "Comparing extra with something other than a quoted string is wrong,
+                    will evaluate to false"
+                        .to_string(),
+                );
+                None
+            }
+        }
+    }
+
+    /// Evaluate a <`marker_value`> <`marker_op`> <`marker_value`> expression
+    ///
+    /// When `env` is `None`, all expressions that reference the environment
+    /// will evaluate as `true`.
+    fn evaluate(
+        &self,
+        env: Option<&MarkerEnvironment>,
+        extras: &[ExtraName],
+        reporter: &mut impl Reporter,
+    ) -> bool {
+        match self {
+            MarkerExpression::Version { key, specifier } => env
+                .map(|env| specifier.contains(env.get_version(key)))
+                .unwrap_or(true),
+            MarkerExpression::VersionInverted {
+                key,
+                operator,
+                version,
+            } => env
+                .map(|env| {
+                    let r_version = VersionPattern::verbatim(env.get_version(key).clone());
+                    let specifier = match VersionSpecifier::from_pattern(*operator, r_version) {
+                        Ok(specifier) => specifier,
+                        Err(err) => {
+                            reporter.report(
+                                MarkerWarningKind::Pep440Error,
+                                format!("Invalid operator/version combination: {err}"),
+                            );
+
+                            return false;
+                        }
+                    };
+
+                    specifier.contains(version)
+                })
+                .unwrap_or(true),
+            MarkerExpression::String {
+                key,
+                operator,
+                value,
+            } => env
+                .map(|env| {
+                    let l_string = env.get_string(key);
+                    self.compare_strings(l_string, operator, value, reporter)
+                })
+                .unwrap_or(true),
+            MarkerExpression::StringInverted {
+                key,
+                operator,
+                value,
+            } => env
+                .map(|env| {
+                    let r_string = env.get_string(key);
+                    self.compare_strings(value, operator, r_string, reporter)
+                })
+                .unwrap_or(true),
+            MarkerExpression::Extra {
+                operator: ExtraOperator::Equal,
+                name,
+            } => extras.contains(name),
+            MarkerExpression::Extra {
+                operator: ExtraOperator::NotEqual,
+                name,
+            } => !extras.contains(name),
+            MarkerExpression::Arbitrary { .. } => false,
+        }
+    }
+
     /// Evaluates only the extras and python version part of the markers. We use this during
     /// dependency resolution when we want to have packages for all possible environments but
     /// already know the extras and the possible python versions (from `requires-python`)
     ///
     /// This considers only expression in the from `extra == '...'`, `'...' == extra`,
     /// `python_version <pep PEP 440 operator> '...'` and
     /// `'...' <pep PEP 440 operator>  python_version`.
@@ -746,185 +1381,159 @@
     /// # }
     /// ```
     fn evaluate_extras_and_python_version(
         &self,
         extras: &HashSet<ExtraName>,
         python_versions: &[Version],
     ) -> bool {
-        match (&self.l_value, &self.operator, &self.r_value) {
-            // `extra == '...'`
-            (MarkerValue::Extra, MarkerOperator::Equal, MarkerValue::QuotedString(r_string)) => {
-                ExtraName::from_str(r_string).is_ok_and(|r_extra| extras.contains(&r_extra))
-            }
-            // `'...' == extra`
-            (MarkerValue::QuotedString(l_string), MarkerOperator::Equal, MarkerValue::Extra) => {
-                ExtraName::from_str(l_string).is_ok_and(|l_extra| extras.contains(&l_extra))
-            }
-            // `extra != '...'`
-            (MarkerValue::Extra, MarkerOperator::NotEqual, MarkerValue::QuotedString(r_string)) => {
-                ExtraName::from_str(r_string).is_ok_and(|r_extra| !extras.contains(&r_extra))
-            }
-            // `'...' != extra`
-            (MarkerValue::QuotedString(l_string), MarkerOperator::NotEqual, MarkerValue::Extra) => {
-                ExtraName::from_str(l_string).is_ok_and(|l_extra| !extras.contains(&l_extra))
-            }
-            (
-                MarkerValue::MarkerEnvVersion(MarkerValueVersion::PythonVersion),
-                operator,
-                MarkerValue::QuotedString(r_string),
-            ) => {
-                // ignore all errors block
-                (|| {
-                    // The right hand side is allowed to contain a star, e.g. `python_version == '3.*'`
-                    let r_vpat = r_string.parse::<VersionPattern>().ok()?;
-                    let operator = operator.to_pep440_operator()?;
-                    // operator and right hand side make the specifier
-                    let specifier = VersionSpecifier::new(operator, r_vpat).ok()?;
-
-                    let compatible = python_versions
-                        .iter()
-                        .any(|l_version| specifier.contains(l_version));
-                    Some(compatible)
-                })()
-                .unwrap_or(true)
-            }
-            (
-                MarkerValue::QuotedString(l_string),
+        match self {
+            MarkerExpression::Version {
+                key: MarkerValueVersion::PythonVersion,
+                specifier,
+            } => python_versions
+                .iter()
+                .any(|l_version| specifier.contains(l_version)),
+            MarkerExpression::VersionInverted {
+                key: MarkerValueVersion::PythonVersion,
                 operator,
-                MarkerValue::MarkerEnvVersion(MarkerValueVersion::PythonVersion),
-            ) => {
-                // ignore all errors block
-                (|| {
-                    // Not star allowed here, `'3.*' == python_version` is not a valid PEP 440
-                    // comparison
-                    let l_version = Version::from_str(l_string).ok()?;
-                    let operator = operator.to_pep440_operator()?;
-
-                    let compatible = python_versions.iter().any(|r_version| {
-                        // operator and right hand side make the specifier and in this case the
-                        // right hand is `python_version` so changes every iteration
-                        match VersionSpecifier::new(
-                            operator,
-                            VersionPattern::verbatim(r_version.clone()),
-                        ) {
-                            Ok(specifier) => specifier.contains(&l_version),
-                            Err(_) => true,
-                        }
-                    });
+                version,
+            } => {
+                python_versions.iter().any(|r_version| {
+                    // operator and right hand side make the specifier and in this case the
+                    // right hand is `python_version` so changes every iteration
+                    let Ok(specifier) = VersionSpecifier::from_pattern(
+                        *operator,
+                        VersionPattern::verbatim(r_version.clone()),
+                    ) else {
+                        return true;
+                    };
 
-                    Some(compatible)
-                })()
-                .unwrap_or(true)
+                    specifier.contains(version)
+                })
             }
+            MarkerExpression::Extra {
+                operator: ExtraOperator::Equal,
+                name,
+            } => extras.contains(name),
+            MarkerExpression::Extra {
+                operator: ExtraOperator::NotEqual,
+                name,
+            } => !extras.contains(name),
             _ => true,
         }
     }
 
     /// Compare strings by PEP 508 logic, with warnings
     fn compare_strings(
         &self,
         l_string: &str,
+        operator: &MarkerOperator,
         r_string: &str,
-        reporter: &mut impl FnMut(MarkerWarningKind, String, &MarkerExpression),
+        reporter: &mut impl Reporter,
     ) -> bool {
-        match self.operator {
+        match operator {
             MarkerOperator::Equal => l_string == r_string,
             MarkerOperator::NotEqual => l_string != r_string,
             MarkerOperator::GreaterThan => {
-                reporter(
+                reporter.report(
                     MarkerWarningKind::LexicographicComparison,
                     format!("Comparing {l_string} and {r_string} lexicographically"),
-                    self,
                 );
                 l_string > r_string
             }
             MarkerOperator::GreaterEqual => {
-                reporter(
+                reporter.report(
                     MarkerWarningKind::LexicographicComparison,
                     format!("Comparing {l_string} and {r_string} lexicographically"),
-                    self,
                 );
                 l_string >= r_string
             }
             MarkerOperator::LessThan => {
-                reporter(
+                reporter.report(
                     MarkerWarningKind::LexicographicComparison,
                     format!("Comparing {l_string} and {r_string} lexicographically"),
-                    self,
                 );
                 l_string < r_string
             }
             MarkerOperator::LessEqual => {
-                reporter(
+                reporter.report(
                     MarkerWarningKind::LexicographicComparison,
                     format!("Comparing {l_string} and {r_string} lexicographically"),
-                    self,
                 );
                 l_string <= r_string
             }
             MarkerOperator::TildeEqual => {
-                reporter(
+                reporter.report(
                     MarkerWarningKind::LexicographicComparison,
                     format!("Can't compare {l_string} and {r_string} with `~=`"),
-                    self,
                 );
                 false
             }
             MarkerOperator::In => r_string.contains(l_string),
             MarkerOperator::NotIn => !r_string.contains(l_string),
         }
     }
-
-    // The `marker <op> '...'` comparison
-    fn marker_compare(
-        &self,
-        value: &ExtraName,
-        extras: &[ExtraName],
-        reporter: &mut impl FnMut(MarkerWarningKind, String, &MarkerExpression),
-    ) -> bool {
-        match self.operator {
-            MarkerOperator::Equal => extras.contains(value),
-            MarkerOperator::NotEqual => !extras.contains(value),
-            _ => {
-                reporter(MarkerWarningKind::ExtraInvalidComparison, "Comparing extra with something other than equal (`==`) or unequal (`!=`) is wrong, evaluating to false".to_string(), self);
-                false
-            }
-        }
-    }
 }
 
 impl FromStr for MarkerExpression {
     type Err = Pep508Error;
 
     fn from_str(s: &str) -> Result<Self, Self::Err> {
-        let mut chars = Cursor::new(s);
-        let expression = parse_marker_key_op_value(&mut chars)?;
-        chars.eat_whitespace();
-        if let Some((pos, unexpected)) = chars.next() {
-            return Err(Pep508Error {
-                message: Pep508ErrorSource::String(format!(
-                    "Unexpected character '{unexpected}', expected end of input"
-                )),
-                start: pos,
-                len: chars.chars.clone().count(),
-                input: chars.to_string(),
-            });
-        }
-        Ok(expression)
+        MarkerExpression::parse_reporter(s, &mut TracingReporter)
     }
 }
 
 impl Display for MarkerExpression {
     fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
-        write!(f, "{} {} {}", self.l_value, self.operator, self.r_value)
+        match self {
+            MarkerExpression::Version { key, specifier } => {
+                write!(
+                    f,
+                    "{key} {} '{}'",
+                    specifier.operator(),
+                    specifier.version()
+                )
+            }
+            MarkerExpression::VersionInverted {
+                version,
+                operator,
+                key,
+            } => {
+                write!(f, "'{version}' {operator} {key}")
+            }
+            MarkerExpression::String {
+                key,
+                operator,
+                value,
+            } => {
+                write!(f, "{key} {operator} '{value}'")
+            }
+            MarkerExpression::StringInverted {
+                value,
+                operator,
+                key,
+            } => {
+                write!(f, "'{value}' {operator} {key}")
+            }
+            MarkerExpression::Extra { operator, name } => {
+                write!(f, "extra {operator} '{name}'")
+            }
+            MarkerExpression::Arbitrary {
+                l_value,
+                operator,
+                r_value,
+            } => {
+                write!(f, "{l_value} {operator} {r_value}")
+            }
+        }
     }
 }
 
 /// Represents one of the nested marker expressions with and/or/parentheses
-#[derive(Clone, Debug, Eq, Hash, PartialEq)]
+#[derive(Clone, Debug, Eq, Hash, PartialEq, PartialOrd, Ord)]
 pub enum MarkerTree {
     /// A simple expression such as `python_version > "3.8"`
     Expression(MarkerExpression),
     /// An and between nested expressions, such as
     /// `python_version > "3.8" and implementation_name == 'cpython'`
     And(Vec<MarkerTree>),
     /// An or between nested expressions, such as
@@ -932,63 +1541,156 @@
     Or(Vec<MarkerTree>),
 }
 
 impl FromStr for MarkerTree {
     type Err = Pep508Error;
 
     fn from_str(markers: &str) -> Result<Self, Self::Err> {
-        parse_markers(markers)
+        parse_markers(markers, &mut TracingReporter)
     }
 }
 
 impl MarkerTree {
+    /// Parse a [`MarkerTree`] from a string with the given reporter.
+    pub fn parse_reporter(
+        markers: &str,
+        reporter: &mut impl Reporter,
+    ) -> Result<Self, Pep508Error> {
+        parse_markers(markers, reporter)
+    }
+
     /// Does this marker apply in the given environment?
     pub fn evaluate(&self, env: &MarkerEnvironment, extras: &[ExtraName]) -> bool {
-        let mut reporter = |_kind, _message, _marker_expression: &MarkerExpression| {
-            #[cfg(feature = "tracing")]
-            {
-                tracing::warn!("{}", _message);
-            }
-        };
-        self.report_deprecated_options(&mut reporter);
+        self.evaluate_optional_environment(Some(env), extras)
+    }
+
+    /// Evaluates this marker tree against an optional environment and a
+    /// possibly empty sequence of extras.
+    ///
+    /// When an environment is not provided, all marker expressions based on
+    /// the environment evaluate to `true`. That is, this provides environment
+    /// independent marker evaluation. In practice, this means only the extras
+    /// are evaluated when an environment is not provided.
+    pub fn evaluate_optional_environment(
+        &self,
+        env: Option<&MarkerEnvironment>,
+        extras: &[ExtraName],
+    ) -> bool {
+        self.report_deprecated_options(&mut TracingReporter);
         match self {
-            MarkerTree::Expression(expression) => expression.evaluate(env, extras, &mut reporter),
-            MarkerTree::And(expressions) => expressions
+            Self::Expression(expression) => expression.evaluate(env, extras, &mut TracingReporter),
+            Self::And(expressions) => expressions
                 .iter()
-                .all(|x| x.evaluate_reporter_impl(env, extras, &mut reporter)),
-            MarkerTree::Or(expressions) => expressions
+                .all(|x| x.evaluate_reporter_impl(env, extras, &mut TracingReporter)),
+            Self::Or(expressions) => expressions
                 .iter()
-                .any(|x| x.evaluate_reporter_impl(env, extras, &mut reporter)),
+                .any(|x| x.evaluate_reporter_impl(env, extras, &mut TracingReporter)),
+        }
+    }
+
+    /// Remove the extras from a marker, returning `None` if the marker tree evaluates to `true`.
+    ///
+    /// Any `extra` markers that are always `true` given the provided extras will be removed.
+    /// Any `extra` markers that are always `false` given the provided extras will be left
+    /// unchanged.
+    ///
+    /// For example, if `dev` is a provided extra, given `sys_platform == 'linux' and extra == 'dev'`,
+    /// the marker will be simplified to `sys_platform == 'linux'`.
+    pub fn simplify_extras(self, extras: &[ExtraName]) -> Option<MarkerTree> {
+        /// Returns `true` if the given expression is always `true` given the set of extras.
+        pub fn is_true(expression: &MarkerExpression, extras: &[ExtraName]) -> bool {
+            match expression {
+                MarkerExpression::Extra {
+                    operator: ExtraOperator::Equal,
+                    name,
+                } => extras.contains(name),
+                MarkerExpression::Extra {
+                    operator: ExtraOperator::NotEqual,
+                    name,
+                } => !extras.contains(name),
+                _ => false,
+            }
+        }
+
+        match self {
+            Self::Expression(expression) => {
+                // If the expression is true, we can remove the marker entirely.
+                if is_true(&expression, extras) {
+                    None
+                } else {
+                    // If not, return the original marker.
+                    Some(Self::Expression(expression))
+                }
+            }
+            Self::And(expressions) => {
+                // Remove any expressions that are _true_ due to the presence of an extra.
+                let simplified = expressions
+                    .into_iter()
+                    .filter_map(|marker| marker.simplify_extras(extras))
+                    .collect::<Vec<_>>();
+
+                // If there are no expressions left, return None.
+                if simplified.is_empty() {
+                    None
+                } else if simplified.len() == 1 {
+                    // If there is only one expression left, return the remaining expression.
+                    simplified.into_iter().next()
+                } else {
+                    // If there are still expressions left, return the simplified marker.
+                    Some(Self::And(simplified))
+                }
+            }
+            Self::Or(expressions) => {
+                let num_expressions = expressions.len();
+
+                // Remove any expressions that are _true_ due to the presence of an extra.
+                let simplified = expressions
+                    .into_iter()
+                    .filter_map(|marker| marker.simplify_extras(extras))
+                    .collect::<Vec<_>>();
+
+                // If _any_ of the expressions are true (i.e., if any of the markers were filtered
+                // out in the above filter step), the entire marker is true.
+                if simplified.len() < num_expressions {
+                    None
+                } else if simplified.len() == 1 {
+                    // If there is only one expression left, return the remaining expression.
+                    simplified.into_iter().next()
+                } else {
+                    // If there are still expressions left, return the simplified marker.
+                    Some(Self::Or(simplified))
+                }
+            }
         }
     }
 
     /// Same as [`Self::evaluate`], but instead of using logging to warn, you can pass your own
     /// handler for warnings
     pub fn evaluate_reporter(
         &self,
         env: &MarkerEnvironment,
         extras: &[ExtraName],
-        reporter: &mut impl FnMut(MarkerWarningKind, String, &MarkerExpression),
+        reporter: &mut impl Reporter,
     ) -> bool {
         self.report_deprecated_options(reporter);
-        self.evaluate_reporter_impl(env, extras, reporter)
+        self.evaluate_reporter_impl(Some(env), extras, reporter)
     }
 
     fn evaluate_reporter_impl(
         &self,
-        env: &MarkerEnvironment,
+        env: Option<&MarkerEnvironment>,
         extras: &[ExtraName],
-        reporter: &mut impl FnMut(MarkerWarningKind, String, &MarkerExpression),
+        reporter: &mut impl Reporter,
     ) -> bool {
         match self {
-            MarkerTree::Expression(expression) => expression.evaluate(env, extras, reporter),
-            MarkerTree::And(expressions) => expressions
+            Self::Expression(expression) => expression.evaluate(env, extras, reporter),
+            Self::And(expressions) => expressions
                 .iter()
                 .all(|x| x.evaluate_reporter_impl(env, extras, reporter)),
-            MarkerTree::Or(expressions) => expressions
+            Self::Or(expressions) => expressions
                 .iter()
                 .any(|x| x.evaluate_reporter_impl(env, extras, reporter)),
         }
     }
 
     /// Checks if the requirement should be activated with the given set of active extras and a set
     /// of possible python versions (from `requires-python`) without evaluating the remaining
@@ -1000,148 +1702,150 @@
     /// and forward all warnings.
     pub fn evaluate_extras_and_python_version(
         &self,
         extras: &HashSet<ExtraName>,
         python_versions: &[Version],
     ) -> bool {
         match self {
-            MarkerTree::Expression(expression) => {
+            Self::Expression(expression) => {
                 expression.evaluate_extras_and_python_version(extras, python_versions)
             }
-            MarkerTree::And(expressions) => expressions
+            Self::And(expressions) => expressions
                 .iter()
                 .all(|x| x.evaluate_extras_and_python_version(extras, python_versions)),
-            MarkerTree::Or(expressions) => expressions
+            Self::Or(expressions) => expressions
                 .iter()
                 .any(|x| x.evaluate_extras_and_python_version(extras, python_versions)),
         }
     }
 
     /// Same as [`Self::evaluate`], but instead of using logging to warn, you get a Vec with all
     /// warnings collected
     pub fn evaluate_collect_warnings(
         &self,
         env: &MarkerEnvironment,
         extras: &[ExtraName],
-    ) -> (bool, Vec<(MarkerWarningKind, String, String)>) {
+    ) -> (bool, Vec<(MarkerWarningKind, String)>) {
         let mut warnings = Vec::new();
-        let mut reporter = |kind, warning, marker: &MarkerExpression| {
-            warnings.push((kind, warning, marker.to_string()));
+        let mut reporter = |kind, warning| {
+            warnings.push((kind, warning));
         };
         self.report_deprecated_options(&mut reporter);
-        let result = self.evaluate_reporter_impl(env, extras, &mut reporter);
+        let result = self.evaluate_reporter_impl(Some(env), extras, &mut reporter);
         (result, warnings)
     }
 
     /// Report the deprecated marker from <https://peps.python.org/pep-0345/#environment-markers>
-    fn report_deprecated_options(
-        &self,
-        reporter: &mut impl FnMut(MarkerWarningKind, String, &MarkerExpression),
-    ) {
+    fn report_deprecated_options(&self, reporter: &mut impl Reporter) {
         match self {
-            MarkerTree::Expression(expression) => {
-                for value in [&expression.l_value, &expression.r_value] {
-                    match value {
-                        MarkerValue::MarkerEnvString(MarkerValueString::OsNameDeprecated) => {
-                            reporter(
-                                MarkerWarningKind::DeprecatedMarkerName,
-                                "os.name is deprecated in favor of os_name".to_string(),
-                                expression,
-                            );
-                        }
-                        MarkerValue::MarkerEnvString(
-                            MarkerValueString::PlatformMachineDeprecated,
-                        ) => {
-                            reporter(
-                                MarkerWarningKind::DeprecatedMarkerName,
-                                "platform.machine is deprecated in favor of platform_machine"
-                                    .to_string(),
-                                expression,
-                            );
-                        }
-                        MarkerValue::MarkerEnvString(
-                            MarkerValueString::PlatformPythonImplementationDeprecated,
-                        ) => {
-                            reporter(
+            Self::Expression(expression) => {
+                let MarkerExpression::String { key, .. } = expression else {
+                    return;
+                };
+
+                match key {
+                    MarkerValueString::OsNameDeprecated => {
+                        reporter.report(
+                            MarkerWarningKind::DeprecatedMarkerName,
+                            "os.name is deprecated in favor of os_name".to_string(),
+                        );
+                    }
+                    MarkerValueString::PlatformMachineDeprecated => {
+                        reporter.report(
+                            MarkerWarningKind::DeprecatedMarkerName,
+                            "platform.machine is deprecated in favor of platform_machine"
+                                .to_string(),
+                        );
+                    }
+                    MarkerValueString::PlatformPythonImplementationDeprecated => {
+                        reporter.report(
                                 MarkerWarningKind::DeprecatedMarkerName,
                                 "platform.python_implementation is deprecated in favor of platform_python_implementation".to_string(),
-                                expression,
-                            );
-                        }
-                        MarkerValue::MarkerEnvString(
-                            MarkerValueString::PlatformVersionDeprecated,
-                        ) => {
-                            reporter(
-                                MarkerWarningKind::DeprecatedMarkerName,
-                                "platform.version is deprecated in favor of platform_version"
-                                    .to_string(),
-                                expression,
                             );
-                        }
-                        MarkerValue::MarkerEnvString(MarkerValueString::SysPlatformDeprecated) => {
-                            reporter(
+                    }
+                    MarkerValueString::PythonImplementationDeprecated => {
+                        reporter.report(
                                 MarkerWarningKind::DeprecatedMarkerName,
-                                "sys.platform  is deprecated in favor of sys_platform".to_string(),
-                                expression,
+                                "python_implementation is deprecated in favor of platform_python_implementation".to_string(),
                             );
-                        }
-                        _ => {}
                     }
+                    MarkerValueString::PlatformVersionDeprecated => {
+                        reporter.report(
+                            MarkerWarningKind::DeprecatedMarkerName,
+                            "platform.version is deprecated in favor of platform_version"
+                                .to_string(),
+                        );
+                    }
+                    MarkerValueString::SysPlatformDeprecated => {
+                        reporter.report(
+                            MarkerWarningKind::DeprecatedMarkerName,
+                            "sys.platform  is deprecated in favor of sys_platform".to_string(),
+                        );
+                    }
+                    _ => {}
                 }
             }
-            MarkerTree::And(expressions) => {
+            Self::And(expressions) => {
                 for expression in expressions {
                     expression.report_deprecated_options(reporter);
                 }
             }
-            MarkerTree::Or(expressions) => {
+            Self::Or(expressions) => {
                 for expression in expressions {
                     expression.report_deprecated_options(reporter);
                 }
             }
         }
     }
 }
 
 impl Display for MarkerTree {
     fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
-        let format_inner = |expression: &MarkerTree| {
-            if matches!(expression, MarkerTree::Expression(_)) {
+        let format_inner = |expression: &Self| {
+            if matches!(expression, Self::Expression(_)) {
                 format!("{expression}")
             } else {
                 format!("({expression})")
             }
         };
         match self {
-            MarkerTree::Expression(expression) => write!(f, "{expression}"),
-            MarkerTree::And(and_list) => f.write_str(
+            Self::Expression(expression) => write!(f, "{expression}"),
+            Self::And(and_list) => f.write_str(
                 &and_list
                     .iter()
                     .map(format_inner)
                     .collect::<Vec<String>>()
                     .join(" and "),
             ),
-            MarkerTree::Or(or_list) => f.write_str(
+            Self::Or(or_list) => f.write_str(
                 &or_list
                     .iter()
                     .map(format_inner)
                     .collect::<Vec<String>>()
                     .join(" or "),
             ),
         }
     }
 }
 
 /// ```text
 /// version_cmp   = wsp* <'<=' | '<' | '!=' | '==' | '>=' | '>' | '~=' | '==='>
 /// marker_op     = version_cmp | (wsp* 'in') | (wsp* 'not' wsp+ 'in')
 /// ```
-fn parse_marker_operator(cursor: &mut Cursor) -> Result<MarkerOperator, Pep508Error> {
-    let (start, len) =
-        cursor.take_while(|char| !char.is_whitespace() && char != '\'' && char != '"');
+/// The `wsp*` has already been consumed by the caller.
+fn parse_marker_operator<T: Pep508Url>(
+    cursor: &mut Cursor,
+) -> Result<MarkerOperator, Pep508Error<T>> {
+    let (start, len) = if cursor.peek_char().is_some_and(|c| c.is_alphabetic()) {
+        // "in" or "not"
+        cursor.take_while(|char| !char.is_whitespace() && char != '\'' && char != '"')
+    } else {
+        // A mathematical operator
+        cursor.take_while(|char| matches!(char, '<' | '=' | '>' | '~' | '!'))
+    };
     let operator = cursor.slice(start, len);
     if operator == "not" {
         // 'not' wsp+ 'in'
         match cursor.next() {
             None => {
                 return Err(Pep508Error {
                     message: Pep508ErrorSource::String(
@@ -1179,15 +1883,15 @@
     })
 }
 
 /// Either a single or double quoted string or one of '`python_version`', '`python_full_version`',
 /// '`os_name`', '`sys_platform`', '`platform_release`', '`platform_system`', '`platform_version`',
 /// '`platform_machine`', '`platform_python_implementation`', '`implementation_name`',
 /// '`implementation_version`', 'extra'
-fn parse_marker_value(cursor: &mut Cursor) -> Result<MarkerValue, Pep508Error> {
+fn parse_marker_value<T: Pep508Url>(cursor: &mut Cursor) -> Result<MarkerValue, Pep508Error<T>> {
     // > User supplied constants are always encoded as strings with either ' or " quote marks. Note
     // > that backslash escapes are not defined, but existing implementations do support them. They
     // > are not included in this specification because they add complexity and there is no observable
     // > need for them today. Similarly we do not define non-ASCII character support: all the runtime
     // > variables we are referencing are expected to be ASCII-only.
     match cursor.peek() {
         None => Err(Pep508Error {
@@ -1223,71 +1927,83 @@
         }
     }
 }
 
 /// ```text
 /// marker_var:l marker_op:o marker_var:r
 /// ```
-fn parse_marker_key_op_value(cursor: &mut Cursor) -> Result<MarkerExpression, Pep508Error> {
+fn parse_marker_key_op_value<T: Pep508Url>(
+    cursor: &mut Cursor,
+    reporter: &mut impl Reporter,
+) -> Result<MarkerExpression, Pep508Error<T>> {
     cursor.eat_whitespace();
     let lvalue = parse_marker_value(cursor)?;
     cursor.eat_whitespace();
     // "not in" and "in" must be preceded by whitespace. We must already have matched a whitespace
     // when we're here because other `parse_marker_key` would have pulled the characters in and
     // errored
     let operator = parse_marker_operator(cursor)?;
     cursor.eat_whitespace();
     let rvalue = parse_marker_value(cursor)?;
-    Ok(MarkerExpression {
-        l_value: lvalue,
-        operator,
-        r_value: rvalue,
-    })
+
+    Ok(MarkerExpression::new(lvalue, operator, rvalue, reporter))
 }
 
 /// ```text
 /// marker_expr   = marker_var:l marker_op:o marker_var:r -> (o, l, r)
 ///               | wsp* '(' marker:m wsp* ')' -> m
 /// ```
-fn parse_marker_expr(cursor: &mut Cursor) -> Result<MarkerTree, Pep508Error> {
+fn parse_marker_expr<T: Pep508Url>(
+    cursor: &mut Cursor,
+    reporter: &mut impl Reporter,
+) -> Result<MarkerTree, Pep508Error<T>> {
     cursor.eat_whitespace();
     if let Some(start_pos) = cursor.eat_char('(') {
-        let marker = parse_marker_or(cursor)?;
+        let marker = parse_marker_or(cursor, reporter)?;
         cursor.next_expect_char(')', start_pos)?;
         Ok(marker)
     } else {
-        Ok(MarkerTree::Expression(parse_marker_key_op_value(cursor)?))
+        Ok(MarkerTree::Expression(parse_marker_key_op_value(
+            cursor, reporter,
+        )?))
     }
 }
 
 /// ```text
 /// marker_and    = marker_expr:l wsp* 'and' marker_expr:r -> ('and', l, r)
 ///               | marker_expr:m -> m
 /// ```
-fn parse_marker_and(cursor: &mut Cursor) -> Result<MarkerTree, Pep508Error> {
-    parse_marker_op(cursor, "and", MarkerTree::And, parse_marker_expr)
+fn parse_marker_and<T: Pep508Url>(
+    cursor: &mut Cursor,
+    reporter: &mut impl Reporter,
+) -> Result<MarkerTree, Pep508Error<T>> {
+    parse_marker_op(cursor, "and", MarkerTree::And, parse_marker_expr, reporter)
 }
 
 /// ```text
 /// marker_or     = marker_and:l wsp* 'or' marker_and:r -> ('or', l, r)
 ///                   | marker_and:m -> m
 /// ```
-fn parse_marker_or(cursor: &mut Cursor) -> Result<MarkerTree, Pep508Error> {
-    parse_marker_op(cursor, "or", MarkerTree::Or, parse_marker_and)
+fn parse_marker_or<T: Pep508Url>(
+    cursor: &mut Cursor,
+    reporter: &mut impl Reporter,
+) -> Result<MarkerTree, Pep508Error<T>> {
+    parse_marker_op(cursor, "or", MarkerTree::Or, parse_marker_and, reporter)
 }
 
 /// Parses both `marker_and` and `marker_or`
-fn parse_marker_op(
+fn parse_marker_op<T: Pep508Url, R: Reporter>(
     cursor: &mut Cursor,
     op: &str,
     op_constructor: fn(Vec<MarkerTree>) -> MarkerTree,
-    parse_inner: fn(&mut Cursor) -> Result<MarkerTree, Pep508Error>,
-) -> Result<MarkerTree, Pep508Error> {
+    parse_inner: fn(&mut Cursor, &mut R) -> Result<MarkerTree, Pep508Error<T>>,
+    reporter: &mut R,
+) -> Result<MarkerTree, Pep508Error<T>> {
     // marker_and or marker_expr
-    let first_element = parse_inner(cursor)?;
+    let first_element = parse_inner(cursor, reporter)?;
     // wsp*
     cursor.eat_whitespace();
     // Check if we're done here instead of invoking the whole vec allocating loop
     if matches!(cursor.peek_char(), None | Some(')')) {
         return Ok(first_element);
     }
 
@@ -1297,15 +2013,15 @@
         // wsp*
         cursor.eat_whitespace();
         // ('or' marker_and) or ('and' marker_or)
         let (start, len) = cursor.peek_while(|c| !c.is_whitespace());
         match cursor.slice(start, len) {
             value if value == op => {
                 cursor.take_while(|c| !c.is_whitespace());
-                let expression = parse_inner(cursor)?;
+                let expression = parse_inner(cursor, reporter)?;
                 expressions.push(expression);
             }
             _ => {
                 // Build minimal trees
                 return if expressions.len() == 1 {
                     Ok(expressions.remove(0))
                 } else {
@@ -1313,75 +2029,87 @@
                 };
             }
         }
     }
 }
 
 /// ```text
-/// marker        = marker_or
+/// marker        = marker_or^
 /// ```
-pub(crate) fn parse_markers_impl(cursor: &mut Cursor) -> Result<MarkerTree, Pep508Error> {
-    let marker = parse_marker_or(cursor)?;
+pub(crate) fn parse_markers_cursor<T: Pep508Url>(
+    cursor: &mut Cursor,
+    reporter: &mut impl Reporter,
+) -> Result<MarkerTree, Pep508Error<T>> {
+    let marker = parse_marker_or(cursor, reporter)?;
     cursor.eat_whitespace();
     if let Some((pos, unexpected)) = cursor.next() {
         // If we're here, both parse_marker_or and parse_marker_and returned because the next
         // character was neither "and" nor "or"
         return Err(Pep508Error {
             message: Pep508ErrorSource::String(format!(
                 "Unexpected character '{unexpected}', expected 'and', 'or' or end of input"
             )),
             start: pos,
-            len: cursor.chars.clone().count(),
+            len: cursor.remaining(),
             input: cursor.to_string(),
         });
     };
     Ok(marker)
 }
 
 /// Parses markers such as `python_version < '3.8'` or
 /// `python_version == "3.10" and (sys_platform == "win32" or (os_name == "linux" and implementation_name == 'cpython'))`
-fn parse_markers(markers: &str) -> Result<MarkerTree, Pep508Error> {
+fn parse_markers<T: Pep508Url>(
+    markers: &str,
+    reporter: &mut impl Reporter,
+) -> Result<MarkerTree, Pep508Error<T>> {
     let mut chars = Cursor::new(markers);
-    parse_markers_impl(&mut chars)
+    parse_markers_cursor(&mut chars, reporter)
 }
 
 #[cfg(test)]
 mod test {
-    use crate::marker::{MarkerEnvironment, StringVersion};
-    use crate::{MarkerExpression, MarkerOperator, MarkerTree, MarkerValue, MarkerValueString};
-    use indoc::indoc;
     use std::str::FromStr;
 
-    fn assert_err(input: &str, error: &str) {
-        assert_eq!(MarkerTree::from_str(input).unwrap_err().to_string(), error);
+    use insta::assert_snapshot;
+
+    use pep440_rs::VersionSpecifier;
+    use uv_normalize::ExtraName;
+
+    use crate::marker::{ExtraOperator, MarkerEnvironment, MarkerEnvironmentBuilder};
+    use crate::{
+        MarkerExpression, MarkerOperator, MarkerTree, MarkerValueString, MarkerValueVersion,
+    };
+
+    fn parse_err(input: &str) -> String {
+        MarkerTree::from_str(input).unwrap_err().to_string()
     }
 
     fn env37() -> MarkerEnvironment {
-        let v37 = StringVersion::from_str("3.7").unwrap();
-
-        MarkerEnvironment {
-            implementation_name: String::new(),
-            implementation_version: v37.clone(),
-            os_name: "linux".to_string(),
-            platform_machine: String::new(),
-            platform_python_implementation: String::new(),
-            platform_release: String::new(),
-            platform_system: String::new(),
-            platform_version: String::new(),
-            python_full_version: v37.clone(),
-            python_version: v37,
-            sys_platform: "linux".to_string(),
-        }
+        MarkerEnvironment::try_from(MarkerEnvironmentBuilder {
+            implementation_name: "",
+            implementation_version: "3.7",
+            os_name: "linux",
+            platform_machine: "",
+            platform_python_implementation: "",
+            platform_release: "",
+            platform_system: "",
+            platform_version: "",
+            python_full_version: "3.7",
+            python_version: "3.7",
+            sys_platform: "linux",
+        })
+        .unwrap()
     }
 
     /// Copied from <https://github.com/pypa/packaging/blob/85ff971a250dc01db188ef9775499c15553a8c95/tests/test_markers.py#L175-L221>
     #[test]
     fn test_marker_equivalence() {
         let values = [
-            (r#"python_version == '2.7'"#, r#"python_version == "2.7""#),
+            (r"python_version == '2.7'", r#"python_version == "2.7""#),
             (r#"python_version == "2.7""#, r#"python_version == "2.7""#),
             (
                 r#"python_version == "2.7" and os_name == "posix""#,
                 r#"python_version == "2.7" and os_name == "posix""#,
             ),
             (
                 r#"python_version == "2.7" or os_name == "posix""#,
@@ -1408,28 +2136,28 @@
                 "{a} {b}"
             );
         }
     }
 
     #[test]
     fn test_marker_evaluation() {
-        let v27 = StringVersion::from_str("2.7").unwrap();
-        let env27 = MarkerEnvironment {
-            implementation_name: String::new(),
-            implementation_version: v27.clone(),
-            os_name: "linux".to_string(),
-            platform_machine: String::new(),
-            platform_python_implementation: String::new(),
-            platform_release: String::new(),
-            platform_system: String::new(),
-            platform_version: String::new(),
-            python_full_version: v27.clone(),
-            python_version: v27,
-            sys_platform: "linux".to_string(),
-        };
+        let env27 = MarkerEnvironment::try_from(MarkerEnvironmentBuilder {
+            implementation_name: "",
+            implementation_version: "2.7",
+            os_name: "linux",
+            platform_machine: "",
+            platform_python_implementation: "",
+            platform_release: "",
+            platform_system: "",
+            platform_version: "",
+            python_full_version: "2.7",
+            python_version: "2.7",
+            sys_platform: "linux",
+        })
+        .unwrap();
         let env37 = env37();
         let marker1 = MarkerTree::from_str("python_version == '2.7'").unwrap();
         let marker2 = MarkerTree::from_str(
             "os_name == \"linux\" or python_version == \"3.7\" and sys_platform == \"win32\"",
         )
         .unwrap();
         let marker3 = MarkerTree::from_str(
@@ -1449,37 +2177,37 @@
         let env37 = env37();
         testing_logger::setup();
         let compare_keys = MarkerTree::from_str("platform_version == sys_platform").unwrap();
         compare_keys.evaluate(&env37, &[]);
         testing_logger::validate(|captured_logs| {
             assert_eq!(
                 captured_logs[0].body,
-                "Comparing two markers with each other doesn't make any sense, evaluating to false"
+                "Comparing two markers with each other doesn't make any sense, will evaluate to false"
             );
             assert_eq!(captured_logs[0].level, log::Level::Warn);
             assert_eq!(captured_logs.len(), 1);
         });
         let non_pep440 = MarkerTree::from_str("python_version >= '3.9.'").unwrap();
         non_pep440.evaluate(&env37, &[]);
         testing_logger::validate(|captured_logs| {
             assert_eq!(
                 captured_logs[0].body,
                 "Expected PEP 440 version to compare with python_version, found '3.9.', \
-                 evaluating to false: after parsing 3.9, found \".\" after it, \
-                 which is not part of a valid version"
+                 will evaluate to false: after parsing '3.9', found '.', which is \
+                 not part of a valid version"
             );
             assert_eq!(captured_logs[0].level, log::Level::Warn);
             assert_eq!(captured_logs.len(), 1);
         });
         let string_string = MarkerTree::from_str("'b' >= 'a'").unwrap();
         string_string.evaluate(&env37, &[]);
         testing_logger::validate(|captured_logs| {
             assert_eq!(
                 captured_logs[0].body,
-                "Comparing two quoted strings with each other doesn't make sense: 'b' >= 'a', evaluating to false"
+                "Comparing two quoted strings with each other doesn't make sense: 'b' >= 'a', will evaluate to false"
             );
             assert_eq!(captured_logs[0].level, log::Level::Warn);
             assert_eq!(captured_logs.len(), 1);
         });
         let string_string = MarkerTree::from_str(r#"os.name == 'posix' and platform.machine == 'x86_64' and platform.python_implementation == 'CPython' and 'Ubuntu' in platform.version and sys.platform == 'linux'"#).unwrap();
         string_string.evaluate(&env37, &[]);
         testing_logger::validate(|captured_logs| {
@@ -1503,14 +2231,46 @@
 
     #[test]
     fn test_not_in() {
         MarkerTree::from_str("'posix' not in os_name").unwrap();
     }
 
     #[test]
+    fn test_marker_version_inverted() {
+        let env37 = env37();
+        let (result, warnings) = MarkerTree::from_str("python_version > '3.6'")
+            .unwrap()
+            .evaluate_collect_warnings(&env37, &[]);
+        assert_eq!(warnings, &[]);
+        assert!(result);
+
+        let (result, warnings) = MarkerTree::from_str("'3.6' > python_version")
+            .unwrap()
+            .evaluate_collect_warnings(&env37, &[]);
+        assert_eq!(warnings, &[]);
+        assert!(!result);
+    }
+
+    #[test]
+    fn test_marker_string_inverted() {
+        let env37 = env37();
+        let (result, warnings) = MarkerTree::from_str("'nux' in sys_platform")
+            .unwrap()
+            .evaluate_collect_warnings(&env37, &[]);
+        assert_eq!(warnings, &[]);
+        assert!(result);
+
+        let (result, warnings) = MarkerTree::from_str("sys_platform in 'nux'")
+            .unwrap()
+            .evaluate_collect_warnings(&env37, &[]);
+        assert_eq!(warnings, &[]);
+        assert!(!result);
+    }
+
+    #[test]
     fn test_marker_version_star() {
         let env37 = env37();
         let (result, warnings) = MarkerTree::from_str("python_version == '3.7.*'")
             .unwrap()
             .evaluate_collect_warnings(&env37, &[]);
         assert_eq!(warnings, &[]);
         assert!(result);
@@ -1529,59 +2289,86 @@
     #[test]
     fn test_closing_parentheses() {
         MarkerTree::from_str(r#"( "linux" in sys_platform) and extra == 'all'"#).unwrap();
     }
 
     #[test]
     fn wrong_quotes_dot_star() {
-        assert_err(
-            r#"python_version == "3.8".* and python_version >= "3.8""#,
-            indoc! {r#"
-                Unexpected character '.', expected 'and', 'or' or end of input
-                python_version == "3.8".* and python_version >= "3.8"
-                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"#
-            },
+        assert_snapshot!(
+            parse_err(r#"python_version == "3.8".* and python_version >= "3.8""#),
+            @r#"
+            Unexpected character '.', expected 'and', 'or' or end of input
+            python_version == "3.8".* and python_version >= "3.8"
+                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"#
         );
-        assert_err(
-            r#"python_version == "3.8".*"#,
-            indoc! {r#"
-                Unexpected character '.', expected 'and', 'or' or end of input
-                python_version == "3.8".*
-                                       ^"#
-            },
+        assert_snapshot!(
+            parse_err(r#"python_version == "3.8".*"#),
+            @r#"
+            Unexpected character '.', expected 'and', 'or' or end of input
+            python_version == "3.8".*
+                                   ^"#
         );
     }
 
     #[test]
     fn test_marker_expression() {
         assert_eq!(
             MarkerExpression::from_str(r#"os_name == "nt""#).unwrap(),
-            MarkerExpression {
-                l_value: MarkerValue::MarkerEnvString(MarkerValueString::OsName),
+            MarkerExpression::String {
+                key: MarkerValueString::OsName,
                 operator: MarkerOperator::Equal,
-                r_value: MarkerValue::QuotedString("nt".to_string()),
+                value: "nt".to_string(),
             }
         );
     }
 
     #[test]
-    fn test_marker_expression_to_long() {
+    fn test_marker_expression_inverted() {
         assert_eq!(
-            MarkerExpression::from_str(r#"os_name == "nt" and python_version >= "3.8""#)
-                .unwrap_err()
-                .to_string(),
-            indoc! {r#"
-                Unexpected character 'a', expected end of input
-                os_name == "nt" and python_version >= "3.8"
-                                ^^^^^^^^^^^^^^^^^^^^^^^^^^"#
-            },
+            MarkerTree::from_str(
+                r#""nt" in os_name and '3.7' >= python_version and python_full_version >= '3.7'"#
+            )
+            .unwrap(),
+            MarkerTree::And(vec![
+                MarkerTree::Expression(MarkerExpression::StringInverted {
+                    value: "nt".to_string(),
+                    operator: MarkerOperator::In,
+                    key: MarkerValueString::OsName,
+                }),
+                MarkerTree::Expression(MarkerExpression::VersionInverted {
+                    key: MarkerValueVersion::PythonVersion,
+                    operator: pep440_rs::Operator::GreaterThanEqual,
+                    version: "3.7".parse().unwrap(),
+                }),
+                MarkerTree::Expression(MarkerExpression::Version {
+                    key: MarkerValueVersion::PythonFullVersion,
+                    specifier: VersionSpecifier::from_pattern(
+                        pep440_rs::Operator::GreaterThanEqual,
+                        "3.7".parse().unwrap()
+                    )
+                    .unwrap()
+                }),
+            ])
+        );
+    }
+
+    #[test]
+    fn test_marker_expression_to_long() {
+        let err = MarkerExpression::from_str(r#"os_name == "nt" and python_version >= "3.8""#)
+            .unwrap_err()
+            .to_string();
+        assert_snapshot!(
+            err,
+            @r#"
+            Unexpected character 'a', expected end of input
+            os_name == "nt" and python_version >= "3.8"
+                            ^^^^^^^^^^^^^^^^^^^^^^^^^^"#
         );
     }
 
-    #[cfg(feature = "serde")]
     #[test]
     fn test_marker_environment_from_json() {
         let _env: MarkerEnvironment = serde_json::from_str(
             r##"{
                 "implementation_name": "cpython",
                 "implementation_version": "3.7.13",
                 "os_name": "posix",
@@ -1593,8 +2380,104 @@
                 "python_full_version": "3.7.13",
                 "python_version": "3.7",
                 "sys_platform": "linux"
             }"##,
         )
         .unwrap();
     }
+
+    #[test]
+    fn test_simplify_extras() {
+        // Given `os_name == "nt" and extra == "dev"`, simplify to `os_name == "nt"`.
+        let markers = MarkerTree::from_str(r#"os_name == "nt" and extra == "dev""#).unwrap();
+        let simplified = markers.simplify_extras(&[ExtraName::from_str("dev").unwrap()]);
+        assert_eq!(
+            simplified,
+            Some(MarkerTree::Expression(MarkerExpression::String {
+                key: MarkerValueString::OsName,
+                operator: MarkerOperator::Equal,
+                value: "nt".to_string(),
+            }))
+        );
+
+        // Given `os_name == "nt" or extra == "dev"`, remove the marker entirely.
+        let markers = MarkerTree::from_str(r#"os_name == "nt" or extra == "dev""#).unwrap();
+        let simplified = markers.simplify_extras(&[ExtraName::from_str("dev").unwrap()]);
+        assert_eq!(simplified, None);
+
+        // Given `extra == "dev"`, remove the marker entirely.
+        let markers = MarkerTree::from_str(r#"extra == "dev""#).unwrap();
+        let simplified = markers.simplify_extras(&[ExtraName::from_str("dev").unwrap()]);
+        assert_eq!(simplified, None);
+
+        // Given `extra == "dev" and extra == "test"`, simplify to `extra == "test"`.
+        let markers = MarkerTree::from_str(r#"extra == "dev" and extra == "test""#).unwrap();
+        let simplified = markers.simplify_extras(&[ExtraName::from_str("dev").unwrap()]);
+        assert_eq!(
+            simplified,
+            Some(MarkerTree::Expression(MarkerExpression::Extra {
+                operator: ExtraOperator::Equal,
+                name: ExtraName::from_str("test").unwrap(),
+            }))
+        );
+
+        // Given `os_name == "nt" and extra == "test"`, don't simplify.
+        let markers = MarkerTree::from_str(r#"os_name == "nt" and extra == "test""#).unwrap();
+        let simplified = markers.simplify_extras(&[ExtraName::from_str("dev").unwrap()]);
+        assert_eq!(
+            simplified,
+            Some(MarkerTree::And(vec![
+                MarkerTree::Expression(MarkerExpression::String {
+                    key: MarkerValueString::OsName,
+                    operator: MarkerOperator::Equal,
+                    value: "nt".to_string(),
+                }),
+                MarkerTree::Expression(MarkerExpression::Extra {
+                    operator: ExtraOperator::Equal,
+                    name: ExtraName::from_str("test").unwrap(),
+                }),
+            ]))
+        );
+
+        // Given `os_name == "nt" and (python_version == "3.7" or extra == "dev")`, simplify to
+        // `os_name == "nt".
+        let markers = MarkerTree::from_str(
+            r#"os_name == "nt" and (python_version == "3.7" or extra == "dev")"#,
+        )
+        .unwrap();
+        let simplified = markers.simplify_extras(&[ExtraName::from_str("dev").unwrap()]);
+        assert_eq!(
+            simplified,
+            Some(MarkerTree::Expression(MarkerExpression::String {
+                key: MarkerValueString::OsName,
+                operator: MarkerOperator::Equal,
+                value: "nt".to_string(),
+            }))
+        );
+
+        // Given `os_name == "nt" or (python_version == "3.7" and extra == "dev")`, simplify to
+        // `os_name == "nt" or python_version == "3.7"`.
+        let markers = MarkerTree::from_str(
+            r#"os_name == "nt" or (python_version == "3.7" and extra == "dev")"#,
+        )
+        .unwrap();
+        let simplified = markers.simplify_extras(&[ExtraName::from_str("dev").unwrap()]);
+        assert_eq!(
+            simplified,
+            Some(MarkerTree::Or(vec![
+                MarkerTree::Expression(MarkerExpression::String {
+                    key: MarkerValueString::OsName,
+                    operator: MarkerOperator::Equal,
+                    value: "nt".to_string(),
+                }),
+                MarkerTree::Expression(MarkerExpression::Version {
+                    key: MarkerValueVersion::PythonVersion,
+                    specifier: VersionSpecifier::from_pattern(
+                        pep440_rs::Operator::Equal,
+                        "3.7".parse().unwrap()
+                    )
+                    .unwrap(),
+                }),
+            ]))
+        );
+    }
 }
```

### Comparing `uv-0.1.9/crates/pep508-rs/src/verbatim_url.rs` & `uv-0.2.0/crates/distribution-types/src/index_url.rs`

 * *Files 22% similar despite different names*

```diff
@@ -1,396 +1,477 @@
 use std::borrow::Cow;
-use std::fmt::Debug;
+use std::fmt::{Display, Formatter};
 use std::ops::Deref;
-use std::path::{Component, Path, PathBuf};
+use std::path::PathBuf;
+use std::str::FromStr;
 
+use itertools::Either;
 use once_cell::sync::Lazy;
-use regex::Regex;
 use url::Url;
 
-/// A wrapper around [`Url`] that preserves the original string.
-#[derive(Debug, Clone, Eq, derivative::Derivative)]
-#[derivative(PartialEq, Hash)]
-#[cfg_attr(feature = "serde", derive(serde::Serialize, serde::Deserialize))]
-pub struct VerbatimUrl {
-    /// The parsed URL.
-    #[cfg_attr(
-        feature = "serde",
-        serde(
-            serialize_with = "Url::serialize_internal",
-            deserialize_with = "Url::deserialize_internal"
-        )
-    )]
-    url: Url,
-    /// The URL as it was provided by the user.
-    #[derivative(PartialEq = "ignore")]
-    #[derivative(Hash = "ignore")]
-    given: Option<String>,
-}
-
-impl VerbatimUrl {
-    /// Parse a URL from a string, expanding any environment variables.
-    pub fn parse(given: impl AsRef<str>) -> Result<Self, VerbatimUrlError> {
-        let url = Url::parse(&expand_env_vars(given.as_ref(), true))
-            .map_err(|err| VerbatimUrlError::Url(given.as_ref().to_owned(), err))?;
-        Ok(Self { url, given: None })
-    }
-
-    /// Parse a URL from an absolute or relative path.
-    #[cfg(feature = "non-pep508-extensions")] // PEP 508 arguably only allows absolute file URLs.
-    pub fn from_path(path: impl AsRef<str>, working_dir: impl AsRef<Path>) -> Self {
-        // Expand any environment variables.
-        let path = PathBuf::from(expand_env_vars(path.as_ref(), false).as_ref());
-
-        // Convert the path to an absolute path, if necessary.
-        let path = if path.is_absolute() {
-            path
-        } else {
-            working_dir.as_ref().join(path)
-        };
+use pep508_rs::{expand_env_vars, split_scheme, strip_host, Scheme, VerbatimUrl};
+use uv_fs::normalize_url_path;
 
-        // Normalize the path.
-        let path = normalize_path(&path);
+use crate::Verbatim;
 
-        // Convert to a URL.
-        let url = Url::from_file_path(path).expect("path is absolute");
+static PYPI_URL: Lazy<Url> = Lazy::new(|| Url::parse("https://pypi.org/simple").unwrap());
 
-        Self { url, given: None }
+static DEFAULT_INDEX_URL: Lazy<IndexUrl> =
+    Lazy::new(|| IndexUrl::Pypi(VerbatimUrl::from_url(PYPI_URL.clone())));
+
+/// The URL of an index to use for fetching packages (e.g., PyPI).
+#[derive(Debug, Clone, Hash, Eq, PartialEq)]
+pub enum IndexUrl {
+    Pypi(VerbatimUrl),
+    Url(VerbatimUrl),
+    Path(VerbatimUrl),
+}
+
+#[cfg(feature = "schemars")]
+impl schemars::JsonSchema for IndexUrl {
+    fn schema_name() -> String {
+        "IndexUrl".to_string()
+    }
+
+    fn json_schema(_gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {
+        schemars::schema::SchemaObject {
+            instance_type: Some(schemars::schema::InstanceType::String.into()),
+            format: Some("uri".to_owned()),
+            metadata: Some(Box::new(schemars::schema::Metadata {
+                description: Some("The URL of an index to use for fetching packages (e.g., `https://pypi.org/simple`).".to_string()),
+              ..schemars::schema::Metadata::default()
+            })),
+            ..schemars::schema::SchemaObject::default()
+        }
+        .into()
     }
+}
 
-    /// Parse a URL from an absolute path.
-    pub fn from_absolute_path(path: impl AsRef<str>) -> Result<Self, VerbatimUrlError> {
-        // Expand any environment variables.
-        let path = PathBuf::from(expand_env_vars(path.as_ref(), false).as_ref());
+impl IndexUrl {
+    /// Return the raw URL for the index.
+    pub fn url(&self) -> &Url {
+        match self {
+            Self::Pypi(url) => url.raw(),
+            Self::Url(url) => url.raw(),
+            Self::Path(url) => url.raw(),
+        }
+    }
 
-        // Convert the path to an absolute path, if necessary.
-        let path = if path.is_absolute() {
-            path
+    /// Return the redacted URL for the index, omitting any sensitive credentials.
+    pub fn redacted(&self) -> Cow<'_, Url> {
+        let url = self.url();
+        if url.username().is_empty() && url.password().is_none() {
+            Cow::Borrowed(url)
         } else {
-            return Err(VerbatimUrlError::RelativePath(path));
-        };
+            let mut url = url.clone();
+            let _ = url.set_username("");
+            let _ = url.set_password(None);
+            Cow::Owned(url)
+        }
+    }
+}
+
+impl Display for IndexUrl {
+    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
+        match self {
+            Self::Pypi(url) => Display::fmt(url, f),
+            Self::Url(url) => Display::fmt(url, f),
+            Self::Path(url) => Display::fmt(url, f),
+        }
+    }
+}
+
+impl Verbatim for IndexUrl {
+    fn verbatim(&self) -> Cow<'_, str> {
+        match self {
+            Self::Pypi(url) => url.verbatim(),
+            Self::Url(url) => url.verbatim(),
+            Self::Path(url) => url.verbatim(),
+        }
+    }
+}
 
-        // Normalize the path.
-        let path = normalize_path(&path);
+impl FromStr for IndexUrl {
+    type Err = url::ParseError;
 
-        // Convert to a URL.
-        let url = Url::from_file_path(path).expect("path is absolute");
+    fn from_str(s: &str) -> Result<Self, Self::Err> {
+        let url = Url::parse(s)?;
+        let url = VerbatimUrl::from_url(url).with_given(s.to_owned());
+        if *url.raw() == *PYPI_URL {
+            Ok(Self::Pypi(url))
+        } else {
+            Ok(Self::Url(url))
+        }
+    }
+}
 
-        Ok(Self { url, given: None })
+impl serde::ser::Serialize for IndexUrl {
+    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
+    where
+        S: serde::ser::Serializer,
+    {
+        self.to_string().serialize(serializer)
     }
+}
 
-    /// Set the verbatim representation of the URL.
-    #[must_use]
-    pub fn with_given(self, given: String) -> Self {
-        Self {
-            given: Some(given),
-            ..self
+impl<'de> serde::de::Deserialize<'de> for IndexUrl {
+    fn deserialize<D>(deserializer: D) -> Result<IndexUrl, D::Error>
+    where
+        D: serde::de::Deserializer<'de>,
+    {
+        let s = String::deserialize(deserializer)?;
+        IndexUrl::from_str(&s).map_err(serde::de::Error::custom)
+    }
+}
+
+impl From<VerbatimUrl> for IndexUrl {
+    fn from(url: VerbatimUrl) -> Self {
+        if *url.raw() == *PYPI_URL {
+            Self::Pypi(url)
+        } else {
+            Self::Url(url)
         }
     }
+}
 
-    /// Return the original string as given by the user, if available.
-    pub fn given(&self) -> Option<&str> {
-        self.given.as_deref()
+impl From<IndexUrl> for Url {
+    fn from(index: IndexUrl) -> Self {
+        match index {
+            IndexUrl::Pypi(url) => url.to_url(),
+            IndexUrl::Url(url) => url.to_url(),
+            IndexUrl::Path(url) => url.to_url(),
+        }
     }
+}
 
-    /// Return the underlying [`Url`].
-    pub fn raw(&self) -> &Url {
-        &self.url
+impl Deref for IndexUrl {
+    type Target = Url;
+
+    fn deref(&self) -> &Self::Target {
+        match &self {
+            Self::Pypi(url) => url,
+            Self::Url(url) => url,
+            Self::Path(url) => url,
+        }
     }
+}
 
-    /// Convert a [`VerbatimUrl`] into a [`Url`].
-    pub fn to_url(&self) -> Url {
-        self.url.clone()
+/// A directory with distributions or a URL to an HTML file with a flat listing of distributions.
+///
+/// Also known as `--find-links`.
+#[derive(Debug, Clone, Hash, Eq, PartialEq)]
+pub enum FlatIndexLocation {
+    Path(PathBuf),
+    Url(Url),
+}
+
+#[cfg(feature = "schemars")]
+impl schemars::JsonSchema for FlatIndexLocation {
+    fn schema_name() -> String {
+        "FlatIndexLocation".to_string()
+    }
+
+    fn json_schema(_gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {
+        schemars::schema::SchemaObject {
+            instance_type: Some(schemars::schema::InstanceType::String.into()),
+            format: Some("uri".to_owned()),
+            metadata: Some(Box::new(schemars::schema::Metadata {
+                description: Some("The path to a directory of distributions, or a URL to an HTML file with a flat listing of distributions.".to_string()),
+              ..schemars::schema::Metadata::default()
+            })),
+            ..schemars::schema::SchemaObject::default()
+        }
+        .into()
     }
+}
 
-    /// Create a [`VerbatimUrl`] from a [`Url`].
-    ///
-    /// This method should be used sparingly (ideally, not at all), as it represents a loss of the
-    /// verbatim representation.
-    pub fn unknown(url: Url) -> Self {
-        Self { given: None, url }
+impl serde::ser::Serialize for FlatIndexLocation {
+    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
+    where
+        S: serde::ser::Serializer,
+    {
+        self.to_string().serialize(serializer)
     }
 }
 
-impl std::str::FromStr for VerbatimUrl {
-    type Err = VerbatimUrlError;
+impl<'de> serde::de::Deserialize<'de> for FlatIndexLocation {
+    fn deserialize<D>(deserializer: D) -> Result<FlatIndexLocation, D::Error>
+    where
+        D: serde::de::Deserializer<'de>,
+    {
+        let s = String::deserialize(deserializer)?;
+        FlatIndexLocation::from_str(&s).map_err(serde::de::Error::custom)
+    }
+}
 
+impl FromStr for FlatIndexLocation {
+    type Err = url::ParseError;
+
+    /// Parse a raw string for a `--find-links` entry, which could be a URL or a local path.
+    ///
+    /// For example:
+    /// - `file:///home/ferris/project/scripts/...`
+    /// - `file:../ferris/`
+    /// - `../ferris/`
+    /// - `https://download.pytorch.org/whl/torch_stable.html`
     fn from_str(s: &str) -> Result<Self, Self::Err> {
-        Self::parse(s).map(|url| url.with_given(s.to_owned()))
+        // Expand environment variables.
+        let expanded = expand_env_vars(s);
+
+        // Parse the expanded path.
+        if let Some((scheme, path)) = split_scheme(&expanded) {
+            match Scheme::parse(scheme) {
+                // Ex) `file:///home/ferris/project/scripts/...`, `file://localhost/home/ferris/project/scripts/...`, or `file:../ferris/`
+                Some(Scheme::File) => {
+                    // Strip the leading slashes, along with the `localhost` host, if present.
+                    let path = strip_host(path);
+
+                    // Transform, e.g., `/C:/Users/ferris/wheel-0.42.0.tar.gz` to `C:\Users\ferris\wheel-0.42.0.tar.gz`.
+                    let path = normalize_url_path(path);
+
+                    let path = PathBuf::from(path.as_ref());
+                    Ok(Self::Path(path))
+                }
+
+                // Ex) `https://download.pytorch.org/whl/torch_stable.html`
+                Some(_) => {
+                    let url = Url::parse(expanded.as_ref())?;
+                    Ok(Self::Url(url))
+                }
+
+                // Ex) `C:\Users\ferris\wheel-0.42.0.tar.gz`
+                None => {
+                    let path = PathBuf::from(expanded.as_ref());
+                    Ok(Self::Path(path))
+                }
+            }
+        } else {
+            // Ex) `../ferris/`
+            let path = PathBuf::from(expanded.as_ref());
+            Ok(Self::Path(path))
+        }
     }
 }
 
-impl std::fmt::Display for VerbatimUrl {
-    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-        std::fmt::Display::fmt(&self.url, f)
+impl Display for FlatIndexLocation {
+    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
+        match self {
+            Self::Path(path) => Display::fmt(&path.display(), f),
+            Self::Url(url) => Display::fmt(url, f),
+        }
     }
 }
 
-impl Deref for VerbatimUrl {
-    type Target = Url;
+/// The index locations to use for fetching packages. By default, uses the PyPI index.
+///
+/// From a pip perspective, this type merges `--index-url`, `--extra-index-url`, and `--find-links`.
+#[derive(Debug, Clone)]
+pub struct IndexLocations {
+    index: Option<IndexUrl>,
+    extra_index: Vec<IndexUrl>,
+    flat_index: Vec<FlatIndexLocation>,
+    no_index: bool,
+}
 
-    fn deref(&self) -> &Self::Target {
-        &self.url
+impl Default for IndexLocations {
+    /// By default, use the `PyPI` index.
+    fn default() -> Self {
+        Self {
+            index: Some(DEFAULT_INDEX_URL.clone()),
+            extra_index: Vec::new(),
+            flat_index: Vec::new(),
+            no_index: false,
+        }
     }
 }
 
-/// An error that can occur when parsing a [`VerbatimUrl`].
-#[derive(thiserror::Error, Debug)]
-pub enum VerbatimUrlError {
-    /// Failed to parse a URL.
-    #[error("{0}")]
-    Url(String, #[source] url::ParseError),
+impl IndexLocations {
+    /// Determine the index URLs to use for fetching packages.
+    pub fn new(
+        index: Option<IndexUrl>,
+        extra_index: Vec<IndexUrl>,
+        flat_index: Vec<FlatIndexLocation>,
+        no_index: bool,
+    ) -> Self {
+        Self {
+            index,
+            extra_index,
+            flat_index,
+            no_index,
+        }
+    }
 
-    /// Received a relative path, but no working directory was provided.
-    #[error("relative path without a working directory: {0}")]
-    RelativePath(PathBuf),
+    /// Combine a set of index locations.
+    ///
+    /// If either the current or the other index locations have `no_index` set, the result will
+    /// have `no_index` set.
+    ///
+    /// If the current index location has an `index` set, it will be preserved.
+    #[must_use]
+    pub fn combine(
+        self,
+        index: Option<IndexUrl>,
+        extra_index: Vec<IndexUrl>,
+        flat_index: Vec<FlatIndexLocation>,
+        no_index: bool,
+    ) -> Self {
+        Self {
+            index: self.index.or(index),
+            extra_index: self.extra_index.into_iter().chain(extra_index).collect(),
+            flat_index: self.flat_index.into_iter().chain(flat_index).collect(),
+            no_index: self.no_index || no_index,
+        }
+    }
 }
 
-/// Expand all available environment variables.
-///
-/// This is modeled off of pip's environment variable expansion, which states:
-///
-///   The only allowed format for environment variables defined in the
-///   requirement file is `${MY_VARIABLE_1}` to ensure two things:
-///
-///   1. Strings that contain a `$` aren't accidentally (partially) expanded.
-///   2. Ensure consistency across platforms for requirement files.
-///
-///   ...
-///
-///   Valid characters in variable names follow the `POSIX standard
-///   <http://pubs.opengroup.org/onlinepubs/9699919799/>`_ and are limited
-///   to uppercase letter, digits and the `_` (underscore).
-fn expand_env_vars(s: &str, escape: bool) -> Cow<'_, str> {
-    // Generate the project root, to be used via the `${PROJECT_ROOT}`
-    // environment variable.
-    static PROJECT_ROOT_FRAGMENT: Lazy<String> = Lazy::new(|| {
-        let project_root = std::env::current_dir().unwrap();
-        project_root.to_string_lossy().to_string()
-    });
-
-    static RE: Lazy<Regex> =
-        Lazy::new(|| Regex::new(r"(?P<var>\$\{(?P<name>[A-Z0-9_]+)})").unwrap());
-
-    RE.replace_all(s, |caps: &regex::Captures<'_>| {
-        let name = caps.name("name").unwrap().as_str();
-        std::env::var(name).unwrap_or_else(|_| match name {
-            // Ensure that the variable is URL-escaped, if necessary.
-            "PROJECT_ROOT" => {
-                if escape {
-                    PROJECT_ROOT_FRAGMENT.replace(' ', "%20")
-                } else {
-                    PROJECT_ROOT_FRAGMENT.to_string()
-                }
+impl<'a> IndexLocations {
+    /// Return the primary [`IndexUrl`] entry.
+    ///
+    /// If `--no-index` is set, return `None`.
+    ///
+    /// If no index is provided, use the `PyPI` index.
+    pub fn index(&'a self) -> Option<&'a IndexUrl> {
+        if self.no_index {
+            None
+        } else {
+            match self.index.as_ref() {
+                Some(index) => Some(index),
+                None => Some(&DEFAULT_INDEX_URL),
             }
-            _ => caps["var"].to_owned(),
-        })
-    })
+        }
+    }
+
+    /// Return an iterator over the extra [`IndexUrl`] entries.
+    pub fn extra_index(&'a self) -> impl Iterator<Item = &'a IndexUrl> + 'a {
+        if self.no_index {
+            Either::Left(std::iter::empty())
+        } else {
+            Either::Right(self.extra_index.iter())
+        }
+    }
+
+    /// Return an iterator over all [`IndexUrl`] entries.
+    pub fn indexes(&'a self) -> impl Iterator<Item = &'a IndexUrl> + 'a {
+        self.index().into_iter().chain(self.extra_index())
+    }
+
+    /// Return an iterator over the [`FlatIndexLocation`] entries.
+    pub fn flat_index(&'a self) -> impl Iterator<Item = &'a FlatIndexLocation> + 'a {
+        self.flat_index.iter()
+    }
+
+    /// Clone the index locations into a [`IndexUrls`] instance.
+    pub fn index_urls(&'a self) -> IndexUrls {
+        IndexUrls {
+            index: self.index.clone(),
+            extra_index: self.extra_index.clone(),
+            no_index: self.no_index,
+        }
+    }
+
+    /// Return an iterator over all [`Url`] entries.
+    pub fn urls(&'a self) -> impl Iterator<Item = &'a Url> + 'a {
+        self.indexes()
+            .map(IndexUrl::url)
+            .chain(self.flat_index.iter().filter_map(|index| match index {
+                FlatIndexLocation::Path(_) => None,
+                FlatIndexLocation::Url(url) => Some(url),
+            }))
+    }
 }
 
-/// Normalize a path, removing things like `.` and `..`.
+/// The index URLs to use for fetching packages.
 ///
-/// Source: <https://github.com/rust-lang/cargo/blob/b48c41aedbd69ee3990d62a0e2006edbb506a480/crates/cargo-util/src/paths.rs#L76C1-L109C2>
-fn normalize_path(path: &Path) -> PathBuf {
-    let mut components = path.components().peekable();
-    let mut ret = if let Some(c @ Component::Prefix(..)) = components.peek().copied() {
-        components.next();
-        PathBuf::from(c.as_os_str())
-    } else {
-        PathBuf::new()
-    };
-
-    for component in components {
-        match component {
-            Component::Prefix(..) => unreachable!(),
-            Component::RootDir => {
-                ret.push(component.as_os_str());
-            }
-            Component::CurDir => {}
-            Component::ParentDir => {
-                ret.pop();
-            }
-            Component::Normal(c) => {
-                ret.push(c);
-            }
+/// From a pip perspective, this type merges `--index-url` and `--extra-index-url`.
+#[derive(Debug, Clone)]
+pub struct IndexUrls {
+    index: Option<IndexUrl>,
+    extra_index: Vec<IndexUrl>,
+    no_index: bool,
+}
+
+impl Default for IndexUrls {
+    /// By default, use the `PyPI` index.
+    fn default() -> Self {
+        Self {
+            index: Some(DEFAULT_INDEX_URL.clone()),
+            extra_index: Vec::new(),
+            no_index: false,
         }
     }
-    ret
 }
 
-/// Like [`Url::parse`], but only splits the scheme. Derived from the `url` crate.
-pub fn split_scheme(s: &str) -> Option<(&str, &str)> {
-    /// <https://url.spec.whatwg.org/#c0-controls-and-space>
-    #[inline]
-    fn c0_control_or_space(ch: char) -> bool {
-        ch <= ' ' // U+0000 to U+0020
-    }
-
-    /// <https://url.spec.whatwg.org/#ascii-alpha>
-    #[inline]
-    fn ascii_alpha(ch: char) -> bool {
-        ch.is_ascii_alphabetic()
-    }
-
-    // Trim control characters and spaces from the start and end.
-    let s = s.trim_matches(c0_control_or_space);
-    if s.is_empty() || !s.starts_with(ascii_alpha) {
-        return None;
-    }
-
-    // Find the `:` following any alpha characters.
-    let mut iter = s.char_indices();
-    let end = loop {
-        match iter.next() {
-            Some((_i, 'a'..='z' | 'A'..='Z' | '0'..='9' | '+' | '-' | '.')) => {}
-            Some((i, ':')) => break i,
-            _ => return None,
-        }
-    };
-
-    let scheme = &s[..end];
-    let rest = &s[end + 1..];
-    Some((scheme, rest))
-}
-
-/// A supported URL scheme for PEP 508 direct-URL requirements.
-#[derive(Debug, Clone, Copy, PartialEq, Eq)]
-pub enum Scheme {
-    /// `file://...`
-    File,
-    /// `git+git://...`
-    GitGit,
-    /// `git+http://...`
-    GitHttp,
-    /// `git+file://...`
-    GitFile,
-    /// `git+ssh://...`
-    GitSsh,
-    /// `git+https://...`
-    GitHttps,
-    /// `bzr+http://...`
-    BzrHttp,
-    /// `bzr+https://...`
-    BzrHttps,
-    /// `bzr+ssh://...`
-    BzrSsh,
-    /// `bzr+sftp://...`
-    BzrSftp,
-    /// `bzr+ftp://...`
-    BzrFtp,
-    /// `bzr+lp://...`
-    BzrLp,
-    /// `bzr+file://...`
-    BzrFile,
-    /// `hg+file://...`
-    HgFile,
-    /// `hg+http://...`
-    HgHttp,
-    /// `hg+https://...`
-    HgHttps,
-    /// `hg+ssh://...`
-    HgSsh,
-    /// `hg+static-http://...`
-    HgStaticHttp,
-    /// `svn+ssh://...`
-    SvnSsh,
-    /// `svn+http://...`
-    SvnHttp,
-    /// `svn+https://...`
-    SvnHttps,
-    /// `svn+svn://...`
-    SvnSvn,
-    /// `svn+file://...`
-    SvnFile,
-    /// `http://...`
-    Http,
-    /// `https://...`
-    Https,
-}
-
-impl Scheme {
-    /// Determine the [`Scheme`] from the given string, if possible.
-    pub fn parse(s: &str) -> Option<Self> {
-        match s {
-            "file" => Some(Self::File),
-            "git+git" => Some(Self::GitGit),
-            "git+http" => Some(Self::GitHttp),
-            "git+file" => Some(Self::GitFile),
-            "git+ssh" => Some(Self::GitSsh),
-            "git+https" => Some(Self::GitHttps),
-            "bzr+http" => Some(Self::BzrHttp),
-            "bzr+https" => Some(Self::BzrHttps),
-            "bzr+ssh" => Some(Self::BzrSsh),
-            "bzr+sftp" => Some(Self::BzrSftp),
-            "bzr+ftp" => Some(Self::BzrFtp),
-            "bzr+lp" => Some(Self::BzrLp),
-            "bzr+file" => Some(Self::BzrFile),
-            "hg+file" => Some(Self::HgFile),
-            "hg+http" => Some(Self::HgHttp),
-            "hg+https" => Some(Self::HgHttps),
-            "hg+ssh" => Some(Self::HgSsh),
-            "hg+static-http" => Some(Self::HgStaticHttp),
-            "svn+ssh" => Some(Self::SvnSsh),
-            "svn+http" => Some(Self::SvnHttp),
-            "svn+https" => Some(Self::SvnHttps),
-            "svn+svn" => Some(Self::SvnSvn),
-            "svn+file" => Some(Self::SvnFile),
-            "http" => Some(Self::Http),
-            "https" => Some(Self::Https),
-            _ => None,
+impl<'a> IndexUrls {
+    /// Return the fallback [`IndexUrl`] entry.
+    ///
+    /// If `--no-index` is set, return `None`.
+    ///
+    /// If no index is provided, use the `PyPI` index.
+    fn index(&'a self) -> Option<&'a IndexUrl> {
+        if self.no_index {
+            None
+        } else {
+            match self.index.as_ref() {
+                Some(index) => Some(index),
+                None => Some(&DEFAULT_INDEX_URL),
+            }
         }
     }
+
+    /// Return an iterator over the extra [`IndexUrl`] entries.
+    fn extra_index(&'a self) -> impl Iterator<Item = &'a IndexUrl> + 'a {
+        if self.no_index {
+            Either::Left(std::iter::empty())
+        } else {
+            Either::Right(self.extra_index.iter())
+        }
+    }
+
+    /// Return an iterator over all [`IndexUrl`] entries in order.
+    ///
+    /// Prioritizes the extra indexes over the main index.
+    ///
+    /// If `no_index` was enabled, then this always returns an empty
+    /// iterator.
+    pub fn indexes(&'a self) -> impl Iterator<Item = &'a IndexUrl> + 'a {
+        self.extra_index().chain(self.index())
+    }
 }
 
-impl std::fmt::Display for Scheme {
-    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-        match self {
-            Self::File => write!(f, "file"),
-            Self::GitGit => write!(f, "git+git"),
-            Self::GitHttp => write!(f, "git+http"),
-            Self::GitFile => write!(f, "git+file"),
-            Self::GitSsh => write!(f, "git+ssh"),
-            Self::GitHttps => write!(f, "git+https"),
-            Self::BzrHttp => write!(f, "bzr+http"),
-            Self::BzrHttps => write!(f, "bzr+https"),
-            Self::BzrSsh => write!(f, "bzr+ssh"),
-            Self::BzrSftp => write!(f, "bzr+sftp"),
-            Self::BzrFtp => write!(f, "bzr+ftp"),
-            Self::BzrLp => write!(f, "bzr+lp"),
-            Self::BzrFile => write!(f, "bzr+file"),
-            Self::HgFile => write!(f, "hg+file"),
-            Self::HgHttp => write!(f, "hg+http"),
-            Self::HgHttps => write!(f, "hg+https"),
-            Self::HgSsh => write!(f, "hg+ssh"),
-            Self::HgStaticHttp => write!(f, "hg+static-http"),
-            Self::SvnSsh => write!(f, "svn+ssh"),
-            Self::SvnHttp => write!(f, "svn+http"),
-            Self::SvnHttps => write!(f, "svn+https"),
-            Self::SvnSvn => write!(f, "svn+svn"),
-            Self::SvnFile => write!(f, "svn+file"),
-            Self::Http => write!(f, "http"),
-            Self::Https => write!(f, "https"),
+impl From<IndexLocations> for IndexUrls {
+    fn from(locations: IndexLocations) -> Self {
+        Self {
+            index: locations.index,
+            extra_index: locations.extra_index,
+            no_index: locations.no_index,
         }
     }
 }
 
 #[cfg(test)]
-mod tests {
+#[cfg(unix)]
+mod test {
     use super::*;
 
     #[test]
-    fn scheme() {
+    fn parse_find_links() {
+        assert_eq!(
+            FlatIndexLocation::from_str("file:///home/ferris/project/scripts/...").unwrap(),
+            FlatIndexLocation::Path(PathBuf::from("/home/ferris/project/scripts/..."))
+        );
         assert_eq!(
-            split_scheme("file:///home/ferris/project/scripts"),
-            Some(("file", "///home/ferris/project/scripts"))
+            FlatIndexLocation::from_str("file:../ferris/").unwrap(),
+            FlatIndexLocation::Path(PathBuf::from("../ferris/"))
         );
         assert_eq!(
-            split_scheme("file:home/ferris/project/scripts"),
-            Some(("file", "home/ferris/project/scripts"))
+            FlatIndexLocation::from_str("../ferris/").unwrap(),
+            FlatIndexLocation::Path(PathBuf::from("../ferris/"))
         );
         assert_eq!(
-            split_scheme("https://example.com"),
-            Some(("https", "//example.com"))
+            FlatIndexLocation::from_str("https://download.pytorch.org/whl/torch_stable.html")
+                .unwrap(),
+            FlatIndexLocation::Url(
+                Url::parse("https://download.pytorch.org/whl/torch_stable.html").unwrap()
+            )
         );
-        assert_eq!(split_scheme("https:"), Some(("https", "")));
     }
 }
```

### Comparing `uv-0.1.9/crates/cache-key/src/cache_key.rs` & `uv-0.2.0/crates/cache-key/src/cache_key.rs`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/cache-key/src/canonical_url.rs` & `uv-0.2.0/crates/cache-key/src/canonical_url.rs`

 * *Files 15% similar despite different names*

```diff
@@ -14,17 +14,31 @@
 /// This is **only** for internal purposes and provides no means to actually read the underlying
 /// string value of the `Url` it contains. This is intentional, because all fetching should still
 /// happen within the context of the original URL.
 #[derive(Debug, PartialEq, Eq, PartialOrd, Ord, Clone)]
 pub struct CanonicalUrl(Url);
 
 impl CanonicalUrl {
-    pub fn new(url: &Url) -> CanonicalUrl {
+    pub fn new(url: &Url) -> Self {
         let mut url = url.clone();
 
+        // If the URL cannot be a base, then it's not a valid URL anyway.
+        if url.cannot_be_a_base() {
+            return Self(url);
+        }
+
+        // If the URL has no host, then it's not a valid URL anyway.
+        if !url.has_host() {
+            return Self(url);
+        }
+
+        // Strip credentials.
+        let _ = url.set_password(None);
+        let _ = url.set_username("");
+
         // Strip a trailing slash.
         if url.path().ends_with('/') {
             url.path_segments_mut().unwrap().pop_if_empty();
         }
 
         // For GitHub URLs specifically, just lower-case everything. GitHub
         // treats both the same, but they hash differently, and we're gonna be
@@ -58,15 +72,15 @@
                     let last = url.path_segments().unwrap().next_back().unwrap();
                     last[..last.len() - 4].to_owned()
                 };
                 url.path_segments_mut().unwrap().pop().push(&last);
             }
         }
 
-        CanonicalUrl(url)
+        Self(url)
     }
 
     pub fn parse(url: &str) -> Result<Self, url::ParseError> {
         Ok(Self::new(&Url::parse(url)?))
     }
 }
 
@@ -82,14 +96,20 @@
     fn hash<H: Hasher>(&self, state: &mut H) {
         // `as_str` gives the serialisation of a url (which has a spec) and so insulates against
         // possible changes in how the URL crate does hashing.
         self.0.as_str().hash(state);
     }
 }
 
+impl From<CanonicalUrl> for Url {
+    fn from(value: CanonicalUrl) -> Self {
+        value.0
+    }
+}
+
 impl std::fmt::Display for CanonicalUrl {
     fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
         std::fmt::Display::fmt(&self.0, f)
     }
 }
 
 /// Like [`CanonicalUrl`], but attempts to represent an underlying source repository, abstracting
@@ -100,15 +120,15 @@
 /// `https://github.com/pypa/package.git#subdirectory=pkg_b` would map to different
 /// [`CanonicalUrl`] values, but the same [`RepositoryUrl`], since they map to the same
 /// resource.
 #[derive(Debug, PartialEq, Eq, PartialOrd, Ord, Clone)]
 pub struct RepositoryUrl(Url);
 
 impl RepositoryUrl {
-    pub fn new(url: &Url) -> RepositoryUrl {
+    pub fn new(url: &Url) -> Self {
         let mut url = CanonicalUrl::new(url).0;
 
         // If a Git URL ends in a reference (like a branch, tag, or commit), remove it.
         if url.scheme().starts_with("git+") {
             if let Some(prefix) = url
                 .path()
                 .rsplit_once('@')
@@ -118,15 +138,15 @@
             }
         }
 
         // Drop any fragments and query parameters.
         url.set_fragment(None);
         url.set_query(None);
 
-        RepositoryUrl(url)
+        Self(url)
     }
 
     pub fn parse(url: &str) -> Result<Self, url::ParseError> {
         Ok(Self::new(&Url::parse(url)?))
     }
 }
 
@@ -150,19 +170,75 @@
     type Target = Url;
 
     fn deref(&self) -> &Self::Target {
         &self.0
     }
 }
 
+impl std::fmt::Display for RepositoryUrl {
+    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
+        std::fmt::Display::fmt(&self.0, f)
+    }
+}
+
 #[cfg(test)]
 mod tests {
     use super::*;
 
     #[test]
+    fn user_credential_does_not_affect_cache_key() -> Result<(), url::ParseError> {
+        let mut hasher = CacheKeyHasher::new();
+        CanonicalUrl::parse("https://example.com/pypa/sample-namespace-packages.git@2.0.0")?
+            .cache_key(&mut hasher);
+        let hash_without_creds = hasher.finish();
+
+        let mut hasher = CacheKeyHasher::new();
+        CanonicalUrl::parse(
+            "https://user:foo@example.com/pypa/sample-namespace-packages.git@2.0.0",
+        )?
+        .cache_key(&mut hasher);
+        let hash_with_creds = hasher.finish();
+        assert_eq!(
+            hash_without_creds, hash_with_creds,
+            "URLs with no user credentials should hash the same as URLs with different user credentials",
+        );
+
+        let mut hasher = CacheKeyHasher::new();
+        CanonicalUrl::parse(
+            "https://user:bar@example.com/pypa/sample-namespace-packages.git@2.0.0",
+        )?
+        .cache_key(&mut hasher);
+        let hash_with_creds = hasher.finish();
+        assert_eq!(
+            hash_without_creds, hash_with_creds,
+            "URLs with different user credentials should hash the same",
+        );
+
+        let mut hasher = CacheKeyHasher::new();
+        CanonicalUrl::parse("https://:bar@example.com/pypa/sample-namespace-packages.git@2.0.0")?
+            .cache_key(&mut hasher);
+        let hash_with_creds = hasher.finish();
+        assert_eq!(
+            hash_without_creds, hash_with_creds,
+            "URLs with no username, though with a password, should hash the same as URLs with different user credentials",
+        );
+
+        let mut hasher = CacheKeyHasher::new();
+        CanonicalUrl::parse("https://user:@example.com/pypa/sample-namespace-packages.git@2.0.0")?
+            .cache_key(&mut hasher);
+        let hash_with_creds = hasher.finish();
+        assert_eq!(
+            hash_without_creds, hash_with_creds,
+            "URLs with no password, though with a username, should hash the same as URLs with different user credentials",
+        );
+
+        Ok(())
+    }
+
+    #[test]
     fn canonical_url() -> Result<(), url::ParseError> {
         // Two URLs should be considered equal regardless of the `.git` suffix.
         assert_eq!(
             CanonicalUrl::parse("git+https://github.com/pypa/sample-namespace-packages.git")?,
             CanonicalUrl::parse("git+https://github.com/pypa/sample-namespace-packages")?,
         );
 
@@ -190,14 +266,20 @@
                 "git+https://github.com/pypa/sample-namespace-packages.git@v1.0.0"
             )?,
             CanonicalUrl::parse(
                 "git+https://github.com/pypa/sample-namespace-packages.git@v2.0.0"
             )?,
         );
 
+        // Two URLs that cannot be a base should be considered equal.
+        assert_eq!(
+            CanonicalUrl::parse("git+https:://github.com/pypa/sample-namespace-packages.git")?,
+            CanonicalUrl::parse("git+https:://github.com/pypa/sample-namespace-packages.git")?,
+        );
+
         Ok(())
     }
 
     #[test]
     fn repository_url() -> Result<(), url::ParseError> {
         // Two URLs should be considered equal regardless of the `.git` suffix.
         assert_eq!(
```

### Comparing `uv-0.1.9/crates/cache-key/src/digest.rs` & `uv-0.2.0/crates/cache-key/src/digest.rs`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/cache-key/src/stable_hash.rs` & `uv-0.2.0/crates/cache-key/src/stable_hash.rs`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/distribution-types/Cargo.toml` & `uv-0.2.0/crates/uv-distribution/Cargo.toml`

 * *Files 27% similar despite different names*

```diff
@@ -1,40 +1,49 @@
 [package]
-name = "distribution-types"
+name = "uv-distribution"
 version = "0.0.1"
 edition = { workspace = true }
 rust-version = { workspace = true }
 homepage = { workspace = true }
 documentation = { workspace = true }
 repository = { workspace = true }
 authors = { workspace = true }
 license = { workspace = true }
 
 [lints]
 workspace = true
 
 [dependencies]
-cache-key = { path = "../cache-key" }
-distribution-filename = { path = "../distribution-filename", features = ["serde"] }
-pep440_rs = { path = "../pep440-rs" }
-pep508_rs = { path = "../pep508-rs" }
-platform-tags = { path = "../platform-tags" }
-uv-auth = { path = "../uv-auth" }
-uv-fs = { path = "../uv-fs" }
-uv-git = { path = "../uv-git", features = ["vendored-openssl"] }
-uv-normalize = { path = "../uv-normalize" }
-pypi-types = { path = "../pypi-types" }
+cache-key = { workspace = true }
+distribution-filename = { workspace = true }
+distribution-types = { workspace = true }
+install-wheel-rs = { workspace = true }
+pep440_rs = { workspace = true }
+pep508_rs = { workspace = true }
+platform-tags = { workspace = true }
+pypi-types = { workspace = true }
+uv-cache = { workspace = true }
+uv-client = { workspace = true }
+uv-extract = { workspace = true }
+uv-fs = { workspace = true, features = ["tokio"] }
+uv-git = { workspace = true, features = ["vendored-openssl"] }
+uv-normalize = { workspace = true }
+uv-types = { workspace = true }
+uv-configuration = { workspace = true }
 
 anyhow = { workspace = true }
-data-encoding = { workspace = true }
 fs-err = { workspace = true }
-itertools = { workspace = true }
+futures = { workspace = true }
+nanoid = { workspace = true }
 once_cell = { workspace = true }
-rkyv = { workspace = true, features = ["strict", "validation"] }
+reqwest = { workspace = true }
+reqwest-middleware = { workspace = true }
+rmp-serde = { workspace = true }
 rustc-hash = { workspace = true }
 serde = { workspace = true, features = ["derive"] }
-serde_json = { workspace = true }
-sha2 = { workspace = true }
+tempfile = { workspace = true }
 thiserror = { workspace = true }
+tokio = { workspace = true }
+tokio-util = { workspace = true, features = ["compat"] }
 tracing = { workspace = true }
 url = { workspace = true }
-urlencoding = { workspace = true }
+zip = { workspace = true }
```

### Comparing `uv-0.1.9/crates/distribution-types/src/any.rs` & `uv-0.2.0/crates/distribution-types/src/any.rs`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/distribution-types/src/cached.rs` & `uv-0.2.0/crates/distribution-types/src/cached.rs`

 * *Files 18% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 use std::path::{Path, PathBuf};
 
-use anyhow::Result;
+use anyhow::{anyhow, Result};
 
 use distribution_filename::WheelFilename;
 use pep508_rs::VerbatimUrl;
+use pypi_types::HashDigest;
 use uv_normalize::PackageName;
 
-use crate::direct_url::{DirectUrl, LocalFileUrl};
 use crate::{
-    BuiltDist, Dist, DistributionMetadata, InstalledMetadata, InstalledVersion, Name, SourceDist,
-    VersionOrUrl,
+    BuiltDist, Dist, DistributionMetadata, Hashed, InstalledMetadata, InstalledVersion, Name,
+    ParsedPathUrl, ParsedUrl, SourceDist, VersionOrUrlRef,
 };
 
 /// A built distribution (wheel) that exists in the local cache.
 #[derive(Debug, Clone)]
 pub enum CachedDist {
     /// The distribution exists in a registry, like `PyPI`.
     Registry(CachedRegistryDist),
@@ -21,114 +21,155 @@
     Url(CachedDirectUrlDist),
 }
 
 #[derive(Debug, Clone)]
 pub struct CachedRegistryDist {
     pub filename: WheelFilename,
     pub path: PathBuf,
+    pub hashes: Vec<HashDigest>,
 }
 
 #[derive(Debug, Clone)]
 pub struct CachedDirectUrlDist {
     pub filename: WheelFilename,
     pub url: VerbatimUrl,
     pub path: PathBuf,
     pub editable: bool,
+    pub hashes: Vec<HashDigest>,
 }
 
 impl CachedDist {
     /// Initialize a [`CachedDist`] from a [`Dist`].
-    pub fn from_remote(remote: Dist, filename: WheelFilename, path: PathBuf) -> Self {
+    pub fn from_remote(
+        remote: Dist,
+        filename: WheelFilename,
+        hashes: Vec<HashDigest>,
+        path: PathBuf,
+    ) -> Self {
         match remote {
-            Dist::Built(BuiltDist::Registry(_dist)) => {
-                Self::Registry(CachedRegistryDist { filename, path })
-            }
+            Dist::Built(BuiltDist::Registry(_dist)) => Self::Registry(CachedRegistryDist {
+                filename,
+                path,
+                hashes,
+            }),
             Dist::Built(BuiltDist::DirectUrl(dist)) => Self::Url(CachedDirectUrlDist {
                 filename,
                 url: dist.url,
+                hashes,
                 path,
                 editable: false,
             }),
             Dist::Built(BuiltDist::Path(dist)) => Self::Url(CachedDirectUrlDist {
                 filename,
                 url: dist.url,
+                hashes,
                 path,
                 editable: false,
             }),
-            Dist::Source(SourceDist::Registry(_dist)) => {
-                Self::Registry(CachedRegistryDist { filename, path })
-            }
+            Dist::Source(SourceDist::Registry(_dist)) => Self::Registry(CachedRegistryDist {
+                filename,
+                path,
+                hashes,
+            }),
             Dist::Source(SourceDist::DirectUrl(dist)) => Self::Url(CachedDirectUrlDist {
                 filename,
                 url: dist.url,
+                hashes,
                 path,
                 editable: false,
             }),
             Dist::Source(SourceDist::Git(dist)) => Self::Url(CachedDirectUrlDist {
                 filename,
                 url: dist.url,
+                hashes,
                 path,
                 editable: false,
             }),
             Dist::Source(SourceDist::Path(dist)) => Self::Url(CachedDirectUrlDist {
                 filename,
                 url: dist.url,
+                hashes,
+                path,
+                editable: false,
+            }),
+            Dist::Source(SourceDist::Directory(dist)) => Self::Url(CachedDirectUrlDist {
+                filename,
+                url: dist.url,
+                hashes,
                 path,
                 editable: dist.editable,
             }),
         }
     }
 
     /// Return the [`Path`] at which the distribution is stored on-disk.
     pub fn path(&self) -> &Path {
         match self {
             Self::Registry(dist) => &dist.path,
             Self::Url(dist) => &dist.path,
         }
     }
 
-    /// Return the [`DirectUrl`] of the distribution, if it exists.
-    pub fn direct_url(&self) -> Result<Option<DirectUrl>> {
+    /// Return the [`ParsedUrl`] of the distribution, if it exists.
+    pub fn parsed_url(&self) -> Result<Option<ParsedUrl>> {
         match self {
-            CachedDist::Registry(_) => Ok(None),
-            CachedDist::Url(dist) => {
+            Self::Registry(_) => Ok(None),
+            Self::Url(dist) => {
                 if dist.editable {
                     assert_eq!(dist.url.scheme(), "file", "{}", dist.url);
-                    Ok(Some(DirectUrl::LocalFile(LocalFileUrl {
+                    Ok(Some(ParsedUrl::Path(ParsedPathUrl {
                         url: dist.url.raw().clone(),
+                        path: dist
+                            .url
+                            .to_file_path()
+                            .map_err(|()| anyhow!("Invalid path in file URL"))?,
                         editable: dist.editable,
                     })))
                 } else {
-                    DirectUrl::try_from(dist.url.raw()).map(Some)
+                    Ok(Some(ParsedUrl::try_from(dist.url.to_url())?))
                 }
             }
         }
     }
 
+    /// Returns `true` if the distribution is editable.
     pub fn editable(&self) -> bool {
         match self {
-            CachedDist::Registry(_) => false,
-            CachedDist::Url(dist) => dist.editable,
+            Self::Registry(_) => false,
+            Self::Url(dist) => dist.editable,
         }
     }
 
+    /// Returns the [`WheelFilename`] of the distribution.
     pub fn filename(&self) -> &WheelFilename {
         match self {
-            CachedDist::Registry(dist) => &dist.filename,
-            CachedDist::Url(dist) => &dist.filename,
+            Self::Registry(dist) => &dist.filename,
+            Self::Url(dist) => &dist.filename,
         }
     }
 }
 
+impl Hashed for CachedRegistryDist {
+    fn hashes(&self) -> &[HashDigest] {
+        &self.hashes
+    }
+}
+
 impl CachedDirectUrlDist {
     /// Initialize a [`CachedDirectUrlDist`] from a [`WheelFilename`], [`url::Url`], and [`Path`].
-    pub fn from_url(filename: WheelFilename, url: VerbatimUrl, path: PathBuf) -> Self {
+    pub fn from_url(
+        filename: WheelFilename,
+        url: VerbatimUrl,
+        hashes: Vec<HashDigest>,
+        path: PathBuf,
+    ) -> Self {
         Self {
             filename,
             url,
+            hashes,
             path,
             editable: false,
         }
     }
 }
 
 impl Name for CachedRegistryDist {
@@ -149,27 +190,27 @@
             Self::Registry(dist) => dist.name(),
             Self::Url(dist) => dist.name(),
         }
     }
 }
 
 impl DistributionMetadata for CachedRegistryDist {
-    fn version_or_url(&self) -> VersionOrUrl {
-        VersionOrUrl::Version(&self.filename.version)
+    fn version_or_url(&self) -> VersionOrUrlRef {
+        VersionOrUrlRef::Version(&self.filename.version)
     }
 }
 
 impl DistributionMetadata for CachedDirectUrlDist {
-    fn version_or_url(&self) -> VersionOrUrl {
-        VersionOrUrl::Url(&self.url)
+    fn version_or_url(&self) -> VersionOrUrlRef {
+        VersionOrUrlRef::Url(&self.url)
     }
 }
 
 impl DistributionMetadata for CachedDist {
-    fn version_or_url(&self) -> VersionOrUrl {
+    fn version_or_url(&self) -> VersionOrUrlRef {
         match self {
             Self::Registry(dist) => dist.version_or_url(),
             Self::Url(dist) => dist.version_or_url(),
         }
     }
 }
```

### Comparing `uv-0.1.9/crates/distribution-types/src/editable.rs` & `uv-0.2.0/crates/distribution-types/src/editable.rs`

 * *Files 25% similar despite different names*

```diff
@@ -1,8 +1,10 @@
 use std::borrow::Cow;
+use std::collections::btree_map::Entry;
+use std::collections::BTreeMap;
 use std::path::PathBuf;
 
 use url::Url;
 
 use pep508_rs::VerbatimUrl;
 use uv_normalize::ExtraName;
 
@@ -37,7 +39,57 @@
 }
 
 impl std::fmt::Display for LocalEditable {
     fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
         std::fmt::Display::fmt(&self.url, f)
     }
 }
+
+/// A collection of [`LocalEditable`]s.
+#[derive(Debug, Clone)]
+pub struct LocalEditables(Vec<LocalEditable>);
+
+impl LocalEditables {
+    /// Merge and dedupe a list of [`LocalEditable`]s.
+    ///
+    /// This function will deduplicate any editables that point to identical paths, merging their
+    /// extras.
+    pub fn from_editables(editables: impl Iterator<Item = LocalEditable>) -> Self {
+        let mut map = BTreeMap::new();
+        for editable in editables {
+            match map.entry(editable.path.clone()) {
+                Entry::Vacant(entry) => {
+                    entry.insert(editable);
+                }
+                Entry::Occupied(mut entry) => {
+                    let existing = entry.get_mut();
+                    existing.extras.extend(editable.extras);
+                }
+            }
+        }
+        Self(map.into_values().collect())
+    }
+
+    /// Return the number of editables.
+    pub fn len(&self) -> usize {
+        self.0.len()
+    }
+
+    /// Return whether the editables are empty.
+    pub fn is_empty(&self) -> bool {
+        self.0.is_empty()
+    }
+
+    /// Return the editables as a vector.
+    pub fn into_vec(self) -> Vec<LocalEditable> {
+        self.0
+    }
+}
+
+impl IntoIterator for LocalEditables {
+    type Item = LocalEditable;
+    type IntoIter = std::vec::IntoIter<LocalEditable>;
+
+    fn into_iter(self) -> Self::IntoIter {
+        self.0.into_iter()
+    }
+}
```

### Comparing `uv-0.1.9/crates/distribution-types/src/index_url.rs` & `uv-0.2.0/crates/uv-client/src/flat_index.rs`

 * *Files 24% similar despite different names*

```diff
@@ -1,343 +1,256 @@
-use std::fmt::{Display, Formatter};
-use std::ops::Deref;
 use std::path::PathBuf;
-use std::str::FromStr;
 
-use itertools::Either;
-use once_cell::sync::Lazy;
-use serde::{Deserialize, Serialize};
+use futures::{FutureExt, StreamExt};
+use reqwest::Response;
+use tracing::{debug, info_span, warn, Instrument};
 use url::Url;
 
-use pep508_rs::{split_scheme, Scheme};
-use uv_fs::normalize_url_path;
-
-static PYPI_URL: Lazy<Url> = Lazy::new(|| Url::parse("https://pypi.org/simple").unwrap());
-
-/// The url of an index, newtype'd to avoid mixing it with file urls.
-#[derive(Debug, Clone, Hash, Eq, PartialEq, Serialize, Deserialize)]
-pub enum IndexUrl {
-    Pypi,
-    Url(Url),
-}
-
-impl Display for IndexUrl {
-    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
-        match self {
-            IndexUrl::Pypi => Display::fmt(&*PYPI_URL, f),
-            IndexUrl::Url(url) => Display::fmt(url, f),
-        }
-    }
-}
-
-impl FromStr for IndexUrl {
-    type Err = url::ParseError;
-
-    fn from_str(url: &str) -> Result<Self, Self::Err> {
-        Ok(Self::from(Url::parse(url)?))
-    }
-}
-
-impl From<Url> for IndexUrl {
-    fn from(url: Url) -> Self {
-        if url == *PYPI_URL {
-            Self::Pypi
-        } else {
-            Self::Url(url)
-        }
-    }
-}
-
-impl From<IndexUrl> for Url {
-    fn from(index: IndexUrl) -> Self {
-        match index {
-            IndexUrl::Pypi => PYPI_URL.clone(),
-            IndexUrl::Url(url) => url,
-        }
-    }
-}
-
-impl Deref for IndexUrl {
-    type Target = Url;
-
-    fn deref(&self) -> &Self::Target {
-        match &self {
-            IndexUrl::Pypi => &PYPI_URL,
-            IndexUrl::Url(url) => url,
-        }
-    }
-}
-
-/// A directory with distributions or a URL to an HTML file with a flat listing of distributions.
-///
-/// Also known as `--find-links`.
-#[derive(Debug, Clone, Hash, Eq, PartialEq, Serialize, Deserialize)]
-pub enum FlatIndexLocation {
-    Path(PathBuf),
-    Url(Url),
-}
-
-impl FromStr for FlatIndexLocation {
-    type Err = url::ParseError;
-
-    /// Parse a raw string for a `--find-links` entry, which could be a URL or a local path.
-    ///
-    /// For example:
-    /// - `file:///home/ferris/project/scripts/...`
-    /// - `file:../ferris/`
-    /// - `../ferris/`
-    /// - `https://download.pytorch.org/whl/torch_stable.html`
-    fn from_str(s: &str) -> Result<Self, Self::Err> {
-        if let Some((scheme, path)) = split_scheme(s) {
-            match Scheme::parse(scheme) {
-                // Ex) `file:///home/ferris/project/scripts/...` or `file:../ferris/`
-                Some(Scheme::File) => {
-                    let path = path.strip_prefix("//").unwrap_or(path);
-
-                    // Transform, e.g., `/C:/Users/ferris/wheel-0.42.0.tar.gz` to `C:\Users\ferris\wheel-0.42.0.tar.gz`.
-                    let path = normalize_url_path(path);
-
-                    let path = PathBuf::from(path.as_ref());
-                    Ok(Self::Path(path))
-                }
-
-                // Ex) `https://download.pytorch.org/whl/torch_stable.html`
-                Some(_) => {
-                    let url = Url::parse(s)?;
-                    Ok(Self::Url(url))
-                }
-
-                // Ex) `C:\Users\ferris\wheel-0.42.0.tar.gz`
-                None => {
-                    let path = PathBuf::from(s);
-                    Ok(Self::Path(path))
-                }
-            }
-        } else {
-            // Ex) `../ferris/`
-            let path = PathBuf::from(s);
-            Ok(Self::Path(path))
-        }
-    }
-}
-
-impl Display for FlatIndexLocation {
-    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
-        match self {
-            FlatIndexLocation::Path(path) => Display::fmt(&path.display(), f),
-            FlatIndexLocation::Url(url) => Display::fmt(url, f),
-        }
-    }
-}
-
-/// The index locations to use for fetching packages.
-///
-/// By default, uses the PyPI index.
-///
-/// "pip treats all package sources equally" (<https://github.com/pypa/pip/issues/8606#issuecomment-788754817>),
-/// and so do we, i.e., you can't rely that on any particular order of querying indices.
-///
-/// If the fields are none and empty, ignore the package index, instead rely on local archives and
-/// caches.
-///
-/// From a pip perspective, this type merges `--index-url`, `--extra-index-url`, and `--find-links`.
-#[derive(Debug, Clone)]
-pub struct IndexLocations {
-    index: Option<IndexUrl>,
-    extra_index: Vec<IndexUrl>,
-    flat_index: Vec<FlatIndexLocation>,
-    no_index: bool,
-}
-
-impl Default for IndexLocations {
-    /// By default, use the `PyPI` index.
-    fn default() -> Self {
-        Self {
-            index: Some(IndexUrl::Pypi),
-            extra_index: Vec::new(),
-            flat_index: Vec::new(),
-            no_index: false,
-        }
-    }
-}
-
-impl IndexLocations {
-    /// Determine the index URLs to use for fetching packages.
-    pub fn new(
-        index: Option<IndexUrl>,
-        extra_index: Vec<IndexUrl>,
-        flat_index: Vec<FlatIndexLocation>,
-        no_index: bool,
-    ) -> Self {
+use distribution_filename::DistFilename;
+use distribution_types::{File, FileLocation, FlatIndexLocation, IndexUrl};
+use pep508_rs::VerbatimUrl;
+use uv_cache::{Cache, CacheBucket};
+
+use crate::cached_client::{CacheControl, CachedClientError};
+use crate::html::SimpleHtml;
+use crate::{Connectivity, Error, ErrorKind, RegistryClient};
+
+#[derive(Debug, thiserror::Error)]
+pub enum FlatIndexError {
+    #[error("Failed to read `--find-links` directory: {0}")]
+    FindLinksDirectory(PathBuf, #[source] FindLinksDirectoryError),
+
+    #[error("Failed to read `--find-links` URL: {0}")]
+    FindLinksUrl(Url, #[source] Error),
+}
+
+#[derive(Debug, thiserror::Error)]
+pub enum FindLinksDirectoryError {
+    #[error(transparent)]
+    Io(#[from] std::io::Error),
+    #[error(transparent)]
+    VerbatimUrl(#[from] pep508_rs::VerbatimUrlError),
+}
+
+#[derive(Debug, Default, Clone)]
+pub struct FlatIndexEntries {
+    /// The list of `--find-links` entries.
+    pub entries: Vec<(DistFilename, File, IndexUrl)>,
+    /// Whether any `--find-links` entries could not be resolved due to a lack of network
+    /// connectivity.
+    pub offline: bool,
+}
+
+impl FlatIndexEntries {
+    /// Create a [`FlatIndexEntries`] from a list of `--find-links` entries.
+    fn from_entries(entries: Vec<(DistFilename, File, IndexUrl)>) -> Self {
         Self {
-            index,
-            extra_index,
-            flat_index,
-            no_index,
+            entries,
+            offline: false,
         }
     }
 
-    /// Combine a set of index locations.
-    ///
-    /// If either the current or the other index locations have `no_index` set, the result will
-    /// have `no_index` set.
-    ///
-    /// If the current index location has an `index` set, it will be preserved.
-    #[must_use]
-    pub fn combine(
-        self,
-        index: Option<IndexUrl>,
-        extra_index: Vec<IndexUrl>,
-        flat_index: Vec<FlatIndexLocation>,
-        no_index: bool,
-    ) -> Self {
+    /// Create a [`FlatIndexEntries`] to represent an offline `--find-links` entry.
+    fn offline() -> Self {
         Self {
-            index: self.index.or(index),
-            extra_index: self.extra_index.into_iter().chain(extra_index).collect(),
-            flat_index: self.flat_index.into_iter().chain(flat_index).collect(),
-            no_index: self.no_index || no_index,
+            entries: Vec::new(),
+            offline: true,
         }
     }
-}
 
-impl<'a> IndexLocations {
-    /// Return the primary [`IndexUrl`] entry.
-    ///
-    /// If `--no-index` is set, return `None`.
-    ///
-    /// If no index is provided, use the `PyPI` index.
-    pub fn index(&'a self) -> Option<&'a IndexUrl> {
-        if self.no_index {
-            None
-        } else {
-            match self.index.as_ref() {
-                Some(index) => Some(index),
-                None => Some(&IndexUrl::Pypi),
-            }
-        }
+    /// Extend this list of `--find-links` entries with another list.
+    fn extend(&mut self, other: Self) {
+        self.entries.extend(other.entries);
+        self.offline |= other.offline;
     }
 
-    /// Return an iterator over the extra [`IndexUrl`] entries.
-    pub fn extra_index(&'a self) -> impl Iterator<Item = &'a IndexUrl> + 'a {
-        if self.no_index {
-            Either::Left(std::iter::empty())
-        } else {
-            Either::Right(self.extra_index.iter())
-        }
+    /// Return the number of `--find-links` entries.
+    fn len(&self) -> usize {
+        self.entries.len()
     }
 
-    /// Return an iterator over all [`IndexUrl`] entries.
-    pub fn indexes(&'a self) -> impl Iterator<Item = &'a IndexUrl> + 'a {
-        self.index().into_iter().chain(self.extra_index())
-    }
-
-    /// Return an iterator over the [`FlatIndexLocation`] entries.
-    pub fn flat_index(&'a self) -> impl Iterator<Item = &'a FlatIndexLocation> + 'a {
-        self.flat_index.iter()
-    }
-
-    /// Clone the index locations into a [`IndexUrls`] instance.
-    pub fn index_urls(&'a self) -> IndexUrls {
-        IndexUrls {
-            index: self.index.clone(),
-            extra_index: self.extra_index.clone(),
-            no_index: self.no_index,
-        }
+    /// Return `true` if there are no `--find-links` entries.
+    fn is_empty(&self) -> bool {
+        self.entries.is_empty()
     }
 }
 
-/// The index URLs to use for fetching packages.
-///
-/// From a pip perspective, this type merges `--index-url` and `--extra-index-url`.
+/// A client for reading distributions from `--find-links` entries (either local directories or
+/// remote HTML indexes).
 #[derive(Debug, Clone)]
-pub struct IndexUrls {
-    index: Option<IndexUrl>,
-    extra_index: Vec<IndexUrl>,
-    no_index: bool,
-}
-
-impl Default for IndexUrls {
-    /// By default, use the `PyPI` index.
-    fn default() -> Self {
-        Self {
-            index: Some(IndexUrl::Pypi),
-            extra_index: Vec::new(),
-            no_index: false,
-        }
-    }
-}
-
-impl<'a> IndexUrls {
-    /// Return the primary [`IndexUrl`] entry.
-    ///
-    /// If `--no-index` is set, return `None`.
-    ///
-    /// If no index is provided, use the `PyPI` index.
-    pub fn index(&'a self) -> Option<&'a IndexUrl> {
-        if self.no_index {
-            None
-        } else {
-            match self.index.as_ref() {
-                Some(index) => Some(index),
-                None => Some(&IndexUrl::Pypi),
+pub struct FlatIndexClient<'a> {
+    client: &'a RegistryClient,
+    cache: &'a Cache,
+}
+
+impl<'a> FlatIndexClient<'a> {
+    /// Create a new [`FlatIndexClient`].
+    pub fn new(client: &'a RegistryClient, cache: &'a Cache) -> Self {
+        Self { client, cache }
+    }
+
+    /// Read the directories and flat remote indexes from `--find-links`.
+    #[allow(clippy::result_large_err)]
+    pub async fn fetch(
+        &self,
+        indexes: impl Iterator<Item = &FlatIndexLocation>,
+    ) -> Result<FlatIndexEntries, FlatIndexError> {
+        let mut fetches = futures::stream::iter(indexes)
+            .map(|index| async move {
+                let entries = match index {
+                    FlatIndexLocation::Path(path) => Self::read_from_directory(path)
+                        .map_err(|err| FlatIndexError::FindLinksDirectory(path.clone(), err))?,
+                    FlatIndexLocation::Url(url) => self
+                        .read_from_url(url)
+                        .await
+                        .map_err(|err| FlatIndexError::FindLinksUrl(url.clone(), err))?,
+                };
+                if entries.is_empty() {
+                    warn!("No packages found in `--find-links` entry: {}", index);
+                } else {
+                    debug!(
+                        "Found {} package{} in `--find-links` entry: {}",
+                        entries.len(),
+                        if entries.len() == 1 { "" } else { "s" },
+                        index
+                    );
+                }
+                Ok::<FlatIndexEntries, FlatIndexError>(entries)
+            })
+            .buffered(16);
+
+        let mut results = FlatIndexEntries::default();
+        while let Some(entries) = fetches.next().await.transpose()? {
+            results.extend(entries);
+        }
+        Ok(results)
+    }
+
+    /// Read a flat remote index from a `--find-links` URL.
+    async fn read_from_url(&self, url: &Url) -> Result<FlatIndexEntries, Error> {
+        let cache_entry = self.cache.entry(
+            CacheBucket::FlatIndex,
+            "html",
+            format!("{}.msgpack", cache_key::digest(&url.to_string())),
+        );
+        let cache_control = match self.client.connectivity() {
+            Connectivity::Online => CacheControl::from(
+                self.cache
+                    .freshness(&cache_entry, None)
+                    .map_err(ErrorKind::Io)?,
+            ),
+            Connectivity::Offline => CacheControl::AllowStale,
+        };
+
+        let flat_index_request = self
+            .client
+            .uncached_client()
+            .get(url.clone())
+            .header("Accept-Encoding", "gzip")
+            .header("Accept", "text/html")
+            .build()
+            .map_err(ErrorKind::from)?;
+        let parse_simple_response = |response: Response| {
+            async {
+                // Use the response URL, rather than the request URL, as the base for relative URLs.
+                // This ensures that we handle redirects and other URL transformations correctly.
+                let url = response.url().clone();
+
+                let text = response.text().await.map_err(ErrorKind::from)?;
+                let SimpleHtml { base, files } = SimpleHtml::parse(&text, &url)
+                    .map_err(|err| Error::from_html_err(err, url.clone()))?;
+
+                let files: Vec<File> = files
+                    .into_iter()
+                    .filter_map(|file| {
+                        match File::try_from(file, base.as_url()) {
+                            Ok(file) => Some(file),
+                            Err(err) => {
+                                // Ignore files with unparsable version specifiers.
+                                warn!("Skipping file in {url}: {err}");
+                                None
+                            }
+                        }
+                    })
+                    .collect();
+                Ok::<Vec<File>, CachedClientError<Error>>(files)
             }
+            .boxed_local()
+            .instrument(info_span!("parse_flat_index_html", url = % url))
+        };
+        let response = self
+            .client
+            .cached_client()
+            .get_serde(
+                flat_index_request,
+                &cache_entry,
+                cache_control,
+                parse_simple_response,
+            )
+            .await;
+        match response {
+            Ok(files) => {
+                let index_url = IndexUrl::Url(VerbatimUrl::from_url(url.clone()));
+                let files = files
+                    .into_iter()
+                    .filter_map(|file| {
+                        Some((
+                            DistFilename::try_from_normalized_filename(&file.filename)?,
+                            file,
+                            index_url.clone(),
+                        ))
+                    })
+                    .collect();
+                Ok(FlatIndexEntries::from_entries(files))
+            }
+            Err(CachedClientError::Client(err)) if err.is_offline() => {
+                Ok(FlatIndexEntries::offline())
+            }
+            Err(err) => Err(err.into()),
         }
     }
 
-    /// Return an iterator over the extra [`IndexUrl`] entries.
-    pub fn extra_index(&'a self) -> impl Iterator<Item = &'a IndexUrl> + 'a {
-        if self.no_index {
-            Either::Left(std::iter::empty())
-        } else {
-            Either::Right(self.extra_index.iter())
-        }
-    }
-
-    /// Return an iterator over all [`IndexUrl`] entries.
-    pub fn indexes(&'a self) -> impl Iterator<Item = &'a IndexUrl> + 'a {
-        self.index().into_iter().chain(self.extra_index())
-    }
-
-    /// Return `true` if no index is configured.
-    pub fn no_index(&self) -> bool {
-        self.no_index
-    }
-}
+    /// Read a flat remote index from a `--find-links` directory.
+    fn read_from_directory(path: &PathBuf) -> Result<FlatIndexEntries, FindLinksDirectoryError> {
+        // Absolute paths are required for the URL conversion.
+        let path = fs_err::canonicalize(path)?;
+        let index_url = IndexUrl::Path(VerbatimUrl::from_path(&path)?);
+
+        let mut dists = Vec::new();
+        for entry in fs_err::read_dir(path)? {
+            let entry = entry?;
+            let metadata = entry.metadata()?;
+            if !metadata.is_file() {
+                continue;
+            }
 
-impl From<IndexLocations> for IndexUrls {
-    fn from(locations: IndexLocations) -> Self {
-        Self {
-            index: locations.index,
-            extra_index: locations.extra_index,
-            no_index: locations.no_index,
+            let Ok(filename) = entry.file_name().into_string() else {
+                warn!(
+                    "Skipping non-UTF-8 filename in `--find-links` directory: {}",
+                    entry.file_name().to_string_lossy()
+                );
+                continue;
+            };
+
+            let file = File {
+                dist_info_metadata: false,
+                filename: filename.to_string(),
+                hashes: Vec::new(),
+                requires_python: None,
+                size: None,
+                upload_time_utc_ms: None,
+                url: FileLocation::Path(entry.path().clone()),
+                yanked: None,
+            };
+
+            let Some(filename) = DistFilename::try_from_normalized_filename(&filename) else {
+                debug!(
+                    "Ignoring `--find-links` entry (expected a wheel or source distribution filename): {}",
+                    entry.path().display()
+                );
+                continue;
+            };
+            dists.push((filename, file, index_url.clone()));
         }
-    }
-}
-
-#[cfg(test)]
-#[cfg(unix)]
-mod test {
-    use super::*;
-
-    #[test]
-    fn parse_find_links() {
-        assert_eq!(
-            FlatIndexLocation::from_str("file:///home/ferris/project/scripts/...").unwrap(),
-            FlatIndexLocation::Path(PathBuf::from("/home/ferris/project/scripts/..."))
-        );
-        assert_eq!(
-            FlatIndexLocation::from_str("file:../ferris/").unwrap(),
-            FlatIndexLocation::Path(PathBuf::from("../ferris/"))
-        );
-        assert_eq!(
-            FlatIndexLocation::from_str("../ferris/").unwrap(),
-            FlatIndexLocation::Path(PathBuf::from("../ferris/"))
-        );
-        assert_eq!(
-            FlatIndexLocation::from_str("https://download.pytorch.org/whl/torch_stable.html")
-                .unwrap(),
-            FlatIndexLocation::Url(
-                Url::parse("https://download.pytorch.org/whl/torch_stable.html").unwrap()
-            )
-        );
+        Ok(FlatIndexEntries::from_entries(dists))
     }
 }
```

### Comparing `uv-0.1.9/crates/distribution-types/src/traits.rs` & `uv-0.2.0/crates/distribution-types/src/traits.rs`

 * *Files 15% similar despite different names*

```diff
@@ -6,40 +6,54 @@
 use uv_normalize::PackageName;
 
 use crate::error::Error;
 use crate::{
     BuiltDist, CachedDirectUrlDist, CachedDist, CachedRegistryDist, DirectUrlBuiltDist,
     DirectUrlSourceDist, Dist, DistributionId, GitSourceDist, InstalledDirectUrlDist,
     InstalledDist, InstalledRegistryDist, InstalledVersion, LocalDist, PackageId, PathBuiltDist,
-    PathSourceDist, RegistryBuiltDist, RegistrySourceDist, ResourceId, SourceDist, VersionOrUrl,
+    PathSourceDist, RegistryBuiltWheel, RegistrySourceDist, ResourceId, SourceDist, VersionId,
+    VersionOrUrlRef,
 };
 
 pub trait Name {
     /// Return the normalized [`PackageName`] of the distribution.
     fn name(&self) -> &PackageName;
 }
 
 /// Metadata that can be resolved from a requirements specification alone (i.e., prior to building
 /// or installing the distribution).
 pub trait DistributionMetadata: Name {
     /// Return a [`pep440_rs::Version`], for registry-based distributions, or a [`url::Url`],
     /// for URL-based distributions.
-    fn version_or_url(&self) -> VersionOrUrl;
+    fn version_or_url(&self) -> VersionOrUrlRef;
 
-    /// Returns a unique identifier for the package.
+    /// Returns a unique identifier for the package at the given version (e.g., `black==23.10.0`).
     ///
     /// Note that this is not equivalent to a unique identifier for the _distribution_, as multiple
     /// registry-based distributions (e.g., different wheels for the same package and version)
-    /// will return the same package ID, but different distribution IDs.
-    fn package_id(&self) -> PackageId {
+    /// will return the same version ID, but different distribution IDs.
+    fn version_id(&self) -> VersionId {
         match self.version_or_url() {
-            VersionOrUrl::Version(version) => {
-                PackageId::from_registry(self.name().clone(), version.clone())
+            VersionOrUrlRef::Version(version) => {
+                VersionId::from_registry(self.name().clone(), version.clone())
             }
-            VersionOrUrl::Url(url) => PackageId::from_url(url),
+            VersionOrUrlRef::Url(url) => VersionId::from_url(url),
+        }
+    }
+
+    /// Returns a unique identifier for a package. A package can either be identified by a name
+    /// (e.g., `black`) or a URL (e.g., `git+https://github.com/psf/black`).
+    ///
+    /// Note that this is not equivalent to a unique identifier for the _distribution_, as multiple
+    /// registry-based distributions (e.g., different wheels for the same package and version)
+    /// will return the same version ID, but different distribution IDs.
+    fn package_id(&self) -> PackageId {
+        match self.version_or_url() {
+            VersionOrUrlRef::Version(_) => PackageId::from_registry(self.name().clone()),
+            VersionOrUrlRef::Url(url) => PackageId::from_url(url),
         }
     }
 }
 
 /// Metadata that can be resolved from a built distribution.
 pub trait InstalledMetadata: Name {
     /// Return the resolved version of the installed distribution.
@@ -53,14 +67,25 @@
     /// Return the size of the distribution, if known.
     fn size(&self) -> Option<u64>;
 }
 
 pub trait Identifier {
     /// Return a unique resource identifier for the distribution, like a SHA-256 hash of the
     /// distribution's contents.
+    ///
+    /// A distribution is a specific archive of a package at a specific version. For a given package
+    /// version, there may be multiple distributions, e.g., source distribution, along with
+    /// multiple binary distributions (wheels) for different platforms. As a concrete example,
+    /// `black-23.10.0-py3-none-any.whl` would represent a (binary) distribution of the `black` package
+    /// at version `23.10.0`.
+    ///
+    /// The distribution ID is used to uniquely identify a distribution. Ideally, the distribution
+    /// ID should be a hash of the distribution's contents, though in practice, it's only required
+    /// that the ID is unique within a single invocation of the resolver (and so, e.g., a hash of
+    /// the URL would also be sufficient).
     fn distribution_id(&self) -> DistributionId;
 
     /// Return a unique resource identifier for the underlying resource backing the distribution.
     ///
     /// This is often equivalent to the distribution ID, but may differ in some cases. For example,
     /// if the same Git repository is used for two different distributions, at two different
     /// subdirectories or two different commits, then those distributions would share a resource ID,
@@ -174,15 +199,15 @@
 
 impl std::fmt::Display for PathSourceDist {
     fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
         write!(f, "{}{}", self.name(), self.version_or_url())
     }
 }
 
-impl std::fmt::Display for RegistryBuiltDist {
+impl std::fmt::Display for RegistryBuiltWheel {
     fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
         write!(f, "{}{}", self.name(), self.version_or_url())
     }
 }
 
 impl std::fmt::Display for RegistrySourceDist {
     fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
```

### Comparing `uv-0.1.9/crates/uv-installer/Cargo.toml` & `uv-0.2.0/crates/uv-build/Cargo.toml`

 * *Files 18% similar despite different names*

```diff
@@ -1,44 +1,42 @@
 [package]
-name = "uv-installer"
+name = "uv-build"
 version = "0.0.1"
+description = "Build wheels from source distributions"
 edition = { workspace = true }
 rust-version = { workspace = true }
 homepage = { workspace = true }
 documentation = { workspace = true }
 repository = { workspace = true }
 authors = { workspace = true }
 license = { workspace = true }
 
 [lints]
 workspace = true
 
 [dependencies]
-distribution-filename = { path = "../distribution-filename" }
-distribution-types = { path = "../distribution-types" }
-install-wheel-rs = { path = "../install-wheel-rs", default-features = false }
-once-map = { path = "../once-map" }
-pep440_rs = { path = "../pep440-rs" }
-pep508_rs = { path = "../pep508-rs" }
-platform-tags = { path = "../platform-tags" }
-uv-cache = { path = "../uv-cache" }
-uv-client = { path = "../uv-client" }
-uv-distribution = { path = "../uv-distribution" }
-uv-extract = { path = "../uv-extract" }
-uv-fs = { path = "../uv-fs" }
-uv-git = { path = "../uv-git", features = ["vendored-openssl"] }
-uv-interpreter = { path = "../uv-interpreter" }
-uv-normalize = { path = "../uv-normalize" }
-uv-traits = { path = "../uv-traits" }
-pypi-types = { path = "../pypi-types" }
-requirements-txt = { path = "../requirements-txt" }
+distribution-types = { workspace = true }
+pep440_rs = { workspace = true }
+pep508_rs = { workspace = true }
+uv-fs = { workspace = true }
+uv-interpreter = { workspace = true }
+uv-types = { workspace = true }
+uv-configuration = { workspace = true }
+uv-virtualenv = { workspace = true }
 
 anyhow = { workspace = true }
 fs-err = { workspace = true }
-futures = { workspace = true }
-rayon = { workspace = true }
-rustc-hash = { workspace = true }
+indoc = { workspace = true }
+itertools = { workspace = true }
+once_cell = { workspace = true }
+regex = { workspace = true }
+serde = { workspace = true }
+serde_json = { workspace = true }
 tempfile = { workspace = true }
 thiserror = { workspace = true }
 tokio = { workspace = true }
+toml = { workspace = true }
 tracing = { workspace = true }
-url = { workspace = true }
+rustc-hash = { workspace = true }
+
+[dev-dependencies]
+insta = { version = "1.36.1" }
```

### Comparing `uv-0.1.9/crates/uv-installer/src/downloader.rs` & `uv-0.2.0/crates/uv-installer/src/downloader.rs`

 * *Files 20% similar despite different names*

```diff
@@ -1,89 +1,98 @@
 use std::cmp::Reverse;
-use std::path::{Path, PathBuf};
+use std::path::Path;
 use std::sync::Arc;
 
-use futures::{FutureExt, Stream, StreamExt, TryFutureExt, TryStreamExt};
+use futures::{stream::FuturesUnordered, FutureExt, Stream, StreamExt, TryFutureExt, TryStreamExt};
 use tokio::task::JoinError;
 use tracing::instrument;
 use url::Url;
 
-use distribution_types::{CachedDist, Dist, Identifier, LocalEditable, RemoteSource, SourceDist};
+use distribution_types::{
+    BuildableSource, CachedDist, Dist, Hashed, Identifier, LocalEditable, LocalEditables,
+    RemoteSource,
+};
 use platform_tags::Tags;
 use uv_cache::Cache;
-use uv_client::RegistryClient;
-use uv_distribution::{DistributionDatabase, LocalWheel, Unzip};
-use uv_traits::{BuildContext, InFlight};
+use uv_distribution::{DistributionDatabase, LocalWheel};
+use uv_types::{BuildContext, HashStrategy, InFlight};
 
 use crate::editable::BuiltEditable;
 
 #[derive(thiserror::Error, Debug)]
 pub enum Error {
     #[error("Failed to unzip wheel: {0}")]
     Unzip(Dist, #[source] uv_extract::Error),
     #[error("Failed to fetch wheel: {0}")]
     Fetch(Dist, #[source] uv_distribution::Error),
     /// Should not occur; only seen when another task panicked.
     #[error("The task executor is broken, did some other task panic?")]
     Join(#[from] JoinError),
     #[error(transparent)]
     Editable(#[from] uv_distribution::Error),
+    #[error("Failed to write to the client cache")]
+    CacheWrite(#[source] std::io::Error),
     #[error("Unzip failed in another thread: {0}")]
     Thread(String),
 }
 
 /// Download, build, and unzip a set of distributions.
-pub struct Downloader<'a, Context: BuildContext + Send + Sync> {
-    database: DistributionDatabase<'a, Context>,
+pub struct Downloader<'a, Context: BuildContext> {
+    tags: &'a Tags,
     cache: &'a Cache,
+    hashes: &'a HashStrategy,
+    database: DistributionDatabase<'a, Context>,
     reporter: Option<Arc<dyn Reporter>>,
 }
 
-impl<'a, Context: BuildContext + Send + Sync> Downloader<'a, Context> {
+impl<'a, Context: BuildContext> Downloader<'a, Context> {
     pub fn new(
         cache: &'a Cache,
         tags: &'a Tags,
-        client: &'a RegistryClient,
-        build_context: &'a Context,
+        hashes: &'a HashStrategy,
+        database: DistributionDatabase<'a, Context>,
     ) -> Self {
         Self {
-            database: DistributionDatabase::new(cache, tags, client, build_context),
-            reporter: None,
+            tags,
             cache,
+            hashes,
+            database,
+            reporter: None,
         }
     }
 
-    /// Set the [`Reporter`] to use for this unzipper.
+    /// Set the [`Reporter`] to use for this downloader.
     #[must_use]
     pub fn with_reporter(self, reporter: impl Reporter + 'static) -> Self {
         let reporter: Arc<dyn Reporter> = Arc::new(reporter);
         Self {
-            reporter: Some(reporter.clone()),
-            database: self.database.with_reporter(Facade::from(reporter.clone())),
+            tags: self.tags,
             cache: self.cache,
+            hashes: self.hashes,
+            database: self.database.with_reporter(Facade::from(reporter.clone())),
+            reporter: Some(reporter.clone()),
         }
     }
 
     /// Fetch, build, and unzip the distributions in parallel.
     pub fn download_stream<'stream>(
         &'stream self,
         distributions: Vec<Dist>,
         in_flight: &'stream InFlight,
     ) -> impl Stream<Item = Result<CachedDist, Error>> + 'stream {
-        futures::stream::iter(distributions)
+        distributions
+            .into_iter()
             .map(|dist| async {
-                let wheel = self.get_wheel(dist, in_flight).boxed().await?;
+                let wheel = self.get_wheel(dist, in_flight).boxed_local().await?;
                 if let Some(reporter) = self.reporter.as_ref() {
                     reporter.on_progress(&wheel);
                 }
                 Ok::<CachedDist, Error>(wheel)
             })
-            // TODO(charlie): The number of concurrent fetches, such that we limit the number of
-            // concurrent builds to the number of cores, while allowing more concurrent downloads.
-            .buffer_unordered(50)
+            .collect::<FuturesUnordered<_>>()
     }
 
     /// Download, build, and unzip a set of downloaded wheels.
     #[instrument(skip_all, fields(total = distributions.len()))]
     pub async fn download(
         &self,
         mut distributions: Vec<Dist>,
@@ -105,39 +114,40 @@
         Ok(wheels)
     }
 
     /// Build a set of editables
     #[instrument(skip_all)]
     pub async fn build_editables(
         &self,
-        editables: Vec<LocalEditable>,
+        editables: LocalEditables,
         editable_wheel_dir: &Path,
     ) -> Result<Vec<BuiltEditable>, Error> {
         // Build editables in parallel
         let mut results = Vec::with_capacity(editables.len());
-        let mut fetches = futures::stream::iter(editables)
+        let mut fetches = editables
+            .into_iter()
             .map(|editable| async move {
                 let task_id = self
                     .reporter
                     .as_ref()
                     .map(|reporter| reporter.on_editable_build_start(&editable));
                 let (local_wheel, metadata) = self
                     .database
                     .build_wheel_editable(&editable, editable_wheel_dir)
                     .await
                     .map_err(Error::Editable)?;
-                let cached_dist = self.unzip_wheel(local_wheel).await?;
+                let cached_dist = CachedDist::from(local_wheel);
                 if let Some(task_id) = task_id {
                     if let Some(reporter) = &self.reporter {
                         reporter.on_editable_build_complete(&editable, task_id);
                     }
                 }
                 Ok::<_, Error>((editable, cached_dist, metadata))
             })
-            .buffer_unordered(50);
+            .collect::<FuturesUnordered<_>>();
 
         while let Some((editable, wheel, metadata)) = fetches.next().await.transpose()? {
             if let Some(reporter) = self.reporter.as_ref() {
                 reporter.on_progress(&wheel);
             }
             results.push(BuiltEditable {
                 editable,
@@ -154,21 +164,36 @@
     }
 
     /// Download, build, and unzip a single wheel.
     #[instrument(skip_all, fields(name = % dist, size = ? dist.size(), url = dist.file().map(| file | file.url.to_string()).unwrap_or_default()))]
     pub async fn get_wheel(&self, dist: Dist, in_flight: &InFlight) -> Result<CachedDist, Error> {
         let id = dist.distribution_id();
         if in_flight.downloads.register(id.clone()) {
-            let download: LocalWheel = self
+            let policy = self.hashes.get(&dist);
+            let result = self
                 .database
-                .get_or_build_wheel(dist.clone())
-                .boxed()
+                .get_or_build_wheel(&dist, self.tags, policy)
+                .boxed_local()
                 .map_err(|err| Error::Fetch(dist.clone(), err))
-                .await?;
-            let result = self.unzip_wheel(download).await;
+                .await
+                .and_then(|wheel: LocalWheel| {
+                    if wheel.satisfies(policy) {
+                        Ok(wheel)
+                    } else {
+                        Err(Error::Fetch(
+                            dist.clone(),
+                            uv_distribution::Error::hash_mismatch(
+                                dist.to_string(),
+                                policy.digests(),
+                                wheel.hashes(),
+                            ),
+                        ))
+                    }
+                })
+                .map(CachedDist::from);
             match result {
                 Ok(cached) => {
                     in_flight.downloads.done(id, Ok(cached.clone()));
                     Ok(cached)
                 }
                 Err(err) => {
                     in_flight.downloads.done(id, Err(err.to_string()));
@@ -184,55 +209,29 @@
 
             match result.as_ref() {
                 Ok(cached) => Ok(cached.clone()),
                 Err(err) => Err(Error::Thread(err.to_string())),
             }
         }
     }
-
-    /// Unzip a locally-available wheel into the cache.
-    async fn unzip_wheel(&self, download: LocalWheel) -> Result<CachedDist, Error> {
-        // Just an optimization: Avoid spawning a blocking task if there is no work to be done.
-        if let LocalWheel::Unzipped(download) = download {
-            return Ok(download.into_cached_dist());
-        }
-
-        // Unzip the wheel.
-        let archive = tokio::task::spawn_blocking({
-            let download = download.clone();
-            let cache = self.cache.clone();
-            move || -> Result<PathBuf, uv_extract::Error> {
-                // Unzip the wheel into a temporary directory.
-                let temp_dir = tempfile::tempdir_in(cache.root())?;
-                download.unzip(temp_dir.path())?;
-
-                // Persist the temporary directory to the directory store.
-                Ok(cache.persist(temp_dir.into_path(), download.target())?)
-            }
-        })
-        .await?
-        .map_err(|err| Error::Unzip(download.remote().clone(), err))?;
-
-        Ok(download.into_cached_dist(archive))
-    }
 }
 
 pub trait Reporter: Send + Sync {
     /// Callback to invoke when a wheel is unzipped. This implies that the wheel was downloaded and,
     /// if necessary, built.
     fn on_progress(&self, dist: &CachedDist);
 
     /// Callback to invoke when the operation is complete.
     fn on_complete(&self);
 
     /// Callback to invoke when a source distribution build is kicked off.
-    fn on_build_start(&self, dist: &SourceDist) -> usize;
+    fn on_build_start(&self, source: &BuildableSource) -> usize;
 
     /// Callback to invoke when a source distribution build is complete.
-    fn on_build_complete(&self, dist: &SourceDist, id: usize);
+    fn on_build_complete(&self, source: &BuildableSource, id: usize);
 
     /// Callback to invoke when a editable build is kicked off.
     fn on_editable_build_start(&self, dist: &LocalEditable) -> usize;
 
     /// Callback to invoke when a editable build is complete.
     fn on_editable_build_complete(&self, dist: &LocalEditable, id: usize);
 
@@ -251,20 +250,20 @@
 impl From<Arc<dyn Reporter>> for Facade {
     fn from(reporter: Arc<dyn Reporter>) -> Self {
         Self { reporter }
     }
 }
 
 impl uv_distribution::Reporter for Facade {
-    fn on_build_start(&self, dist: &SourceDist) -> usize {
-        self.reporter.on_build_start(dist)
+    fn on_build_start(&self, source: &BuildableSource) -> usize {
+        self.reporter.on_build_start(source)
     }
 
-    fn on_build_complete(&self, dist: &SourceDist, id: usize) {
-        self.reporter.on_build_complete(dist, id);
+    fn on_build_complete(&self, source: &BuildableSource, id: usize) {
+        self.reporter.on_build_complete(source, id);
     }
 
     fn on_checkout_start(&self, url: &Url, rev: &str) -> usize {
         self.reporter.on_checkout_start(url, rev)
     }
 
     fn on_checkout_complete(&self, url: &Url, rev: &str, index: usize) {
```

### Comparing `uv-0.1.9/crates/uv-installer/src/plan.rs` & `uv-0.2.0/crates/uv-installer/src/plan.rs`

 * *Files 12% similar despite different names*

```diff
@@ -1,29 +1,35 @@
 use std::collections::hash_map::Entry;
 use std::hash::BuildHasherDefault;
-use std::io;
 use std::path::Path;
+use std::str::FromStr;
 
 use anyhow::{bail, Result};
 use rustc_hash::FxHashMap;
 use tracing::{debug, warn};
 
+use distribution_filename::WheelFilename;
 use distribution_types::{
-    BuiltDist, CachedDirectUrlDist, CachedDist, Dist, IndexLocations, InstalledDirectUrlDist,
-    InstalledDist, InstalledMetadata, InstalledVersion, Name, SourceDist,
+    CachedDirectUrlDist, CachedDist, DirectUrlBuiltDist, DirectUrlSourceDist, DirectorySourceDist,
+    Error, GitSourceDist, Hashed, IndexLocations, InstalledDist, InstalledMetadata,
+    InstalledVersion, Name, PathBuiltDist, PathSourceDist, RemoteSource, Requirement,
+    RequirementSource, Verbatim,
 };
-use pep508_rs::{Requirement, VersionOrUrl};
 use platform_tags::Tags;
-use uv_cache::{ArchiveTimestamp, Cache, CacheBucket, CacheEntry, Timestamp, WheelCache};
-use uv_distribution::{BuiltWheelIndex, RegistryWheelIndex};
-use uv_fs::Normalized;
-use uv_interpreter::Virtualenv;
-use uv_normalize::PackageName;
-use uv_traits::NoBinary;
+use uv_cache::{ArchiveTimestamp, Cache, CacheBucket, WheelCache};
+use uv_configuration::{NoBinary, Reinstall};
+use uv_distribution::{
+    BuiltWheelIndex, HttpArchivePointer, LocalArchivePointer, RegistryWheelIndex,
+};
+use uv_fs::Simplified;
+use uv_git::GitUrl;
+use uv_interpreter::PythonEnvironment;
+use uv_types::HashStrategy;
 
+use crate::satisfies::RequirementSatisfaction;
 use crate::{ResolvedEditable, SitePackages};
 
 /// A planner to generate an [`Plan`] based on a set of requirements.
 #[derive(Debug)]
 pub struct Planner<'a> {
     requirements: &'a [Requirement],
     editable_requirements: &'a [ResolvedEditable],
@@ -51,29 +57,35 @@
     /// Partition a set of requirements into those that should be linked from the cache, those that
     /// need to be downloaded, and those that should be removed.
     ///
     /// The install plan will respect cache [`Freshness`]. Specifically, if refresh is enabled, the
     /// plan will respect cache entries created after the current time (as per the [`Refresh`]
     /// policy). Otherwise, entries will be ignored. The downstream distribution database may still
     /// read those entries from the cache after revalidating them.
+    ///
+    /// The install plan will also respect the required hashes, such that it will never return a
+    /// cached distribution that does not match the required hash. Like pip, though, it _will_
+    /// return an _installed_ distribution that does not match the required hash.
     #[allow(clippy::too_many_arguments)]
     pub fn build(
         self,
-        mut site_packages: SitePackages<'_>,
+        mut site_packages: SitePackages,
         reinstall: &Reinstall,
         no_binary: &NoBinary,
+        hasher: &HashStrategy,
         index_locations: &IndexLocations,
         cache: &Cache,
-        venv: &Virtualenv,
+        venv: &PythonEnvironment,
         tags: &Tags,
     ) -> Result<Plan> {
         // Index all the already-downloaded wheels in the cache.
-        let mut registry_index = RegistryWheelIndex::new(cache, tags, index_locations);
+        let mut registry_index = RegistryWheelIndex::new(cache, tags, index_locations, hasher);
+        let built_index = BuiltWheelIndex::new(cache, tags, hasher);
 
-        let mut local = vec![];
+        let mut cached = vec![];
         let mut remote = vec![];
         let mut reinstalls = vec![];
         let mut extraneous = vec![];
         let mut seen = FxHashMap::with_capacity_and_hasher(
             self.requirements.len(),
             BuildHasherDefault::default(),
         );
@@ -98,46 +110,48 @@
             }
 
             match requirement {
                 ResolvedEditable::Installed(installed) => {
                     debug!("Treating editable requirement as immutable: {installed}");
 
                     // Remove from the site-packages index, to avoid marking as extraneous.
-                    let Some(editable) = installed.as_editable() else {
+                    let Some(editable) = installed.wheel.as_editable() else {
                         warn!("Editable requirement is not editable: {installed}");
                         continue;
                     };
-                    if site_packages.remove_editable(editable).is_none() {
+                    let existing = site_packages.remove_editables(editable);
+                    if existing.is_empty() {
                         warn!("Editable requirement is not installed: {installed}");
                         continue;
                     }
                 }
                 ResolvedEditable::Built(built) => {
                     debug!("Treating editable requirement as mutable: {built}");
 
-                    if let Some(dist) = site_packages.remove_editable(built.editable.raw()) {
-                        // Remove any editable installs.
-                        reinstalls.push(dist);
-                    } else if let Some(dist) = site_packages.remove(built.name()) {
-                        // Remove any non-editable installs of the same package.
-                        reinstalls.push(dist);
-                    }
-                    local.push(built.wheel.clone());
+                    // Remove any editables.
+                    let existing = site_packages.remove_editables(built.editable.raw());
+                    reinstalls.extend(existing);
+
+                    // Remove any non-editable installs of the same package.
+                    let existing = site_packages.remove_packages(built.name());
+                    reinstalls.extend(existing);
+
+                    cached.push(built.wheel.clone());
                 }
             }
         }
 
         for requirement in self.requirements {
             // Filter out incompatible requirements.
-            if !requirement.evaluate_markers(venv.interpreter().markers(), &[]) {
+            if !requirement.evaluate_markers(Some(venv.interpreter().markers()), &[]) {
                 continue;
             }
 
             // If we see the same requirement twice, then we have a conflict.
-            let specifier = Specifier::NonEditable(requirement.version_or_url.as_ref());
+            let specifier = Specifier::NonEditable(&requirement.source);
             match seen.entry(requirement.name.clone()) {
                 Entry::Occupied(value) => {
                     if value.get() == &specifier {
                         continue;
                     }
                     bail!(
                         "Detected duplicate package in requirements: {}",
@@ -161,228 +175,288 @@
             let no_binary = match no_binary {
                 NoBinary::None => false,
                 NoBinary::All => true,
                 NoBinary::Packages(packages) => packages.contains(&requirement.name),
             };
 
             if reinstall {
-                if let Some(distribution) = site_packages.remove(&requirement.name) {
-                    reinstalls.push(distribution);
-                }
+                let installed_dists = site_packages.remove_packages(&requirement.name);
+                reinstalls.extend(installed_dists);
             } else {
-                if let Some(distribution) = site_packages.remove(&requirement.name) {
-                    // Filter out already-installed packages.
-                    match requirement.version_or_url.as_ref() {
-                        // If the requirement comes from a registry, check by name.
-                        None | Some(VersionOrUrl::VersionSpecifier(_)) => {
-                            if requirement.is_satisfied_by(distribution.version()) {
-                                debug!("Requirement already satisfied: {distribution}");
+                let installed_dists = site_packages.remove_packages(&requirement.name);
+                match installed_dists.as_slice() {
+                    [] => {}
+                    [distribution] => {
+                        match RequirementSatisfaction::check(distribution, &requirement.source)? {
+                            RequirementSatisfaction::Mismatch => {}
+                            RequirementSatisfaction::Satisfied => {
+                                debug!("Requirement already installed: {distribution}");
                                 continue;
                             }
-                        }
-
-                        // If the requirement comes from a direct URL, check by URL.
-                        Some(VersionOrUrl::Url(url)) => {
-                            if let InstalledDist::Url(distribution) = &distribution {
-                                if &distribution.url == url.raw() {
-                                    // If the requirement came from a local path, check freshness.
-                                    if let Ok(archive) = url.to_file_path() {
-                                        if not_modified_install(distribution, &archive)? {
-                                            debug!("Requirement already satisfied (and up-to-date): {distribution}");
-                                            continue;
-                                        }
-                                    } else {
-                                        // Otherwise, assume the requirement is up-to-date.
-                                        debug!("Requirement already satisfied (assumed up-to-date): {distribution}");
-                                        continue;
-                                    }
-                                }
+                            RequirementSatisfaction::OutOfDate => {
+                                debug!("Requirement installed, but not fresh: {distribution}");
                             }
                         }
+                        reinstalls.push(distribution.clone());
                     }
-
-                    reinstalls.push(distribution);
+                    // We reinstall installed distributions with multiple versions because
+                    // we do not want to keep multiple incompatible versions but removing
+                    // one version is likely to break another.
+                    _ => reinstalls.extend(installed_dists),
                 }
             }
 
             if cache.must_revalidate(&requirement.name) {
-                debug!("Must revalidate requirement: {requirement}");
+                debug!("Must revalidate requirement: {}", requirement.name);
                 remote.push(requirement.clone());
                 continue;
             }
 
-            // Identify any locally-available distributions that satisfy the requirement.
-            match requirement.version_or_url.as_ref() {
-                None => {
-                    if let Some((_version, distribution)) =
-                        registry_index.get(&requirement.name).next()
+            // Identify any cached distributions that satisfy the requirement.
+            match &requirement.source {
+                RequirementSource::Registry { specifier, .. } => {
+                    if let Some((_version, distribution)) = registry_index
+                        .get(&requirement.name)
+                        .find(|(version, _)| specifier.contains(version))
                     {
                         debug!("Requirement already cached: {distribution}");
-                        local.push(CachedDist::Registry(distribution.clone()));
+                        cached.push(CachedDist::Registry(distribution.clone()));
                         continue;
                     }
                 }
-                Some(VersionOrUrl::VersionSpecifier(specifier)) => {
-                    if let Some(distribution) =
-                        registry_index
-                            .get(&requirement.name)
-                            .find_map(|(version, distribution)| {
-                                if specifier.contains(version) {
-                                    Some(distribution)
-                                } else {
-                                    None
-                                }
-                            })
+                RequirementSource::Url {
+                    subdirectory,
+                    location,
+                    url,
+                } => {
+                    // Check if we have a wheel or a source distribution.
+                    if Path::new(url.path())
+                        .extension()
+                        .is_some_and(|ext| ext.eq_ignore_ascii_case("whl"))
                     {
-                        debug!("Requirement already cached: {distribution}");
-                        local.push(CachedDist::Registry(distribution.clone()));
+                        // Validate that the name in the wheel matches that of the requirement.
+                        let filename = WheelFilename::from_str(&url.filename()?)?;
+                        if filename.name != requirement.name {
+                            return Err(Error::PackageNameMismatch(
+                                requirement.name.clone(),
+                                filename.name,
+                                url.verbatim().to_string(),
+                            )
+                            .into());
+                        }
+
+                        let wheel = DirectUrlBuiltDist {
+                            filename,
+                            location: location.clone(),
+                            url: url.clone(),
+                        };
+
+                        if !wheel.filename.is_compatible(tags) {
+                            bail!(
+                                "A URL dependency is incompatible with the current platform: {}",
+                                wheel.url
+                            );
+                        }
+
+                        if no_binary {
+                            bail!(
+                                "A URL dependency points to a wheel which conflicts with `--no-binary`: {}",
+                                wheel.url
+                            );
+                        }
+
+                        // Find the exact wheel from the cache, since we know the filename in
+                        // advance.
+                        let cache_entry = cache
+                            .shard(
+                                CacheBucket::Wheels,
+                                WheelCache::Url(&wheel.url).wheel_dir(wheel.name().as_ref()),
+                            )
+                            .entry(format!("{}.http", wheel.filename.stem()));
+
+                        // Read the HTTP pointer.
+                        if let Some(pointer) = HttpArchivePointer::read_from(&cache_entry)? {
+                            let archive = pointer.into_archive();
+                            if archive.satisfies(hasher.get(&wheel)) {
+                                let cached_dist = CachedDirectUrlDist::from_url(
+                                    wheel.filename,
+                                    wheel.url,
+                                    archive.hashes,
+                                    cache.archive(&archive.id),
+                                );
+
+                                debug!("URL wheel requirement already cached: {cached_dist}");
+                                cached.push(CachedDist::Url(cached_dist));
+                                continue;
+                            }
+                        }
+                    } else {
+                        let sdist = DirectUrlSourceDist {
+                            name: requirement.name.clone(),
+                            location: location.clone(),
+                            subdirectory: subdirectory.clone(),
+                            url: url.clone(),
+                        };
+                        // Find the most-compatible wheel from the cache, since we don't know
+                        // the filename in advance.
+                        if let Some(wheel) = built_index.url(&sdist)? {
+                            let cached_dist = wheel.into_url_dist(url.clone());
+                            debug!("URL source requirement already cached: {cached_dist}");
+                            cached.push(CachedDist::Url(cached_dist));
+                            continue;
+                        }
+                    }
+                }
+                RequirementSource::Git {
+                    repository,
+                    reference,
+                    precise,
+                    subdirectory,
+                    url,
+                } => {
+                    let mut git = GitUrl::new(repository.clone(), reference.clone());
+                    if let Some(precise) = precise {
+                        git = git.with_precise(*precise);
+                    }
+                    let sdist = GitSourceDist {
+                        name: requirement.name.clone(),
+                        git: Box::new(git),
+                        subdirectory: subdirectory.clone(),
+                        url: url.clone(),
+                    };
+                    // Find the most-compatible wheel from the cache, since we don't know
+                    // the filename in advance.
+                    if let Some(wheel) = built_index.git(&sdist) {
+                        let cached_dist = wheel.into_url_dist(url.clone());
+                        debug!("Git source requirement already cached: {cached_dist}");
+                        cached.push(CachedDist::Url(cached_dist));
                         continue;
                     }
                 }
-                Some(VersionOrUrl::Url(url)) => {
-                    match Dist::from_url(requirement.name.clone(), url.clone())? {
-                        Dist::Built(BuiltDist::Registry(_)) => {
-                            // Nothing to do.
-                        }
-                        Dist::Source(SourceDist::Registry(_)) => {
-                            // Nothing to do.
-                        }
-                        Dist::Built(BuiltDist::DirectUrl(wheel)) => {
-                            if !wheel.filename.is_compatible(tags) {
-                                bail!(
-                                    "A URL dependency is incompatible with the current platform: {}",
-                                    wheel.url
-                                );
-                            }
+                RequirementSource::Path { url, .. } => {
+                    // Store the canonicalized path, which also serves to validate that it exists.
+                    let path = match url
+                        .to_file_path()
+                        .map_err(|()| Error::MissingFilePath(url.to_url()))?
+                        .canonicalize()
+                    {
+                        Ok(path) => path,
+                        Err(err) if err.kind() == std::io::ErrorKind::NotFound => {
+                            return Err(Error::NotFound(url.to_url()).into());
+                        }
+                        Err(err) => return Err(err.into()),
+                    };
 
-                            if no_binary {
-                                bail!(
-                                    "A URL dependency points to a wheel which conflicts with `--no-binary`: {}",
-                                    wheel.url
-                                );
-                            }
+                    // Check if we have a wheel or a source distribution.
+                    if path.is_dir() {
+                        let sdist = DirectorySourceDist {
+                            name: requirement.name.clone(),
+                            url: url.clone(),
+                            path,
+                            editable: false,
+                        };
+
+                        // Find the most-compatible wheel from the cache, since we don't know
+                        // the filename in advance.
+                        if let Some(wheel) = built_index.directory(&sdist)? {
+                            let cached_dist = wheel.into_url_dist(url.clone());
+                            debug!("Path source requirement already cached: {cached_dist}");
+                            cached.push(CachedDist::Url(cached_dist));
+                            continue;
+                        }
+                    } else if path
+                        .extension()
+                        .is_some_and(|ext| ext.eq_ignore_ascii_case("whl"))
+                    {
+                        // Validate that the name in the wheel matches that of the requirement.
+                        let filename = WheelFilename::from_str(&url.filename()?)?;
+                        if filename.name != requirement.name {
+                            return Err(Error::PackageNameMismatch(
+                                requirement.name.clone(),
+                                filename.name,
+                                url.verbatim().to_string(),
+                            )
+                            .into());
+                        }
 
-                            // Find the exact wheel from the cache, since we know the filename in
-                            // advance.
-                            let cache_entry = cache
-                                .shard(
-                                    CacheBucket::Wheels,
-                                    WheelCache::Url(&wheel.url)
-                                        .remote_wheel_dir(wheel.name().as_ref()),
-                                )
-                                .entry(wheel.filename.stem());
+                        let wheel = PathBuiltDist {
+                            filename,
+                            url: url.clone(),
+                            path,
+                        };
+
+                        if !wheel.filename.is_compatible(tags) {
+                            bail!(
+                                "A path dependency is incompatible with the current platform: {}",
+                                wheel.path.user_display()
+                            );
+                        }
+
+                        if no_binary {
+                            bail!(
+                                "A path dependency points to a wheel which conflicts with `--no-binary`: {}",
+                                wheel.url
+                            );
+                        }
 
-                            match cache_entry.path().canonicalize() {
-                                Ok(archive) => {
+                        // Find the exact wheel from the cache, since we know the filename in
+                        // advance.
+                        let cache_entry = cache
+                            .shard(
+                                CacheBucket::Wheels,
+                                WheelCache::Url(&wheel.url).wheel_dir(wheel.name().as_ref()),
+                            )
+                            .entry(format!("{}.rev", wheel.filename.stem()));
+
+                        if let Some(pointer) = LocalArchivePointer::read_from(&cache_entry)? {
+                            let timestamp = ArchiveTimestamp::from_file(&wheel.path)?;
+                            if pointer.is_up_to_date(timestamp) {
+                                let archive = pointer.into_archive();
+                                if archive.satisfies(hasher.get(&wheel)) {
                                     let cached_dist = CachedDirectUrlDist::from_url(
                                         wheel.filename,
                                         wheel.url,
-                                        archive,
+                                        archive.hashes,
+                                        cache.archive(&archive.id),
                                     );
 
-                                    debug!("URL wheel requirement already cached: {cached_dist}");
-                                    local.push(CachedDist::Url(cached_dist));
+                                    debug!("Path wheel requirement already cached: {cached_dist}");
+                                    cached.push(CachedDist::Url(cached_dist));
                                     continue;
                                 }
-                                Err(err) if err.kind() == io::ErrorKind::NotFound => {
-                                    // The cache entry doesn't exist, so it's not fresh.
-                                }
-                                Err(err) => return Err(err.into()),
-                            }
-                        }
-                        Dist::Built(BuiltDist::Path(wheel)) => {
-                            if !wheel.filename.is_compatible(tags) {
-                                bail!(
-                                    "A path dependency is incompatible with the current platform: {}",
-                                    wheel.path.normalized_display()
-                                );
-                            }
-
-                            if no_binary {
-                                bail!(
-                                    "A path dependency points to a wheel which conflicts with `--no-binary`: {}",
-                                    wheel.url
-                                );
-                            }
-
-                            // Find the exact wheel from the cache, since we know the filename in
-                            // advance.
-                            let cache_entry = cache
-                                .shard(
-                                    CacheBucket::Wheels,
-                                    WheelCache::Url(&wheel.url)
-                                        .remote_wheel_dir(wheel.name().as_ref()),
-                                )
-                                .entry(wheel.filename.stem());
-
-                            if not_modified_cache(&cache_entry, &wheel.path)? {
-                                match cache_entry.path().canonicalize() {
-                                    Ok(archive) => {
-                                        let cached_dist = CachedDirectUrlDist::from_url(
-                                            wheel.filename,
-                                            wheel.url,
-                                            archive,
-                                        );
-
-                                        debug!(
-                                            "URL wheel requirement already cached: {cached_dist}"
-                                        );
-                                        local.push(CachedDist::Url(cached_dist));
-                                        continue;
-                                    }
-                                    Err(err) if err.kind() == io::ErrorKind::NotFound => {
-                                        // The cache entry doesn't exist, so it's not fresh.
-                                    }
-                                    Err(err) => return Err(err.into()),
-                                }
                             }
                         }
-                        Dist::Source(SourceDist::DirectUrl(sdist)) => {
-                            // Find the most-compatible wheel from the cache, since we don't know
-                            // the filename in advance.
-                            if let Some(wheel) = BuiltWheelIndex::url(&sdist, cache, tags)? {
-                                let cached_dist = wheel.into_url_dist(url.clone());
-                                debug!("URL source requirement already cached: {cached_dist}");
-                                local.push(CachedDist::Url(cached_dist));
-                                continue;
-                            }
-                        }
-                        Dist::Source(SourceDist::Path(sdist)) => {
-                            // Find the most-compatible wheel from the cache, since we don't know
-                            // the filename in advance.
-                            if let Some(wheel) = BuiltWheelIndex::path(&sdist, cache, tags)? {
-                                let cached_dist = wheel.into_url_dist(url.clone());
-                                debug!("Path source requirement already cached: {cached_dist}");
-                                local.push(CachedDist::Url(cached_dist));
-                                continue;
-                            }
-                        }
-                        Dist::Source(SourceDist::Git(sdist)) => {
-                            // Find the most-compatible wheel from the cache, since we don't know
-                            // the filename in advance.
-                            if let Some(wheel) = BuiltWheelIndex::git(&sdist, cache, tags) {
-                                let cached_dist = wheel.into_url_dist(url.clone());
-                                debug!("Git source requirement already cached: {cached_dist}");
-                                local.push(CachedDist::Url(cached_dist));
-                                continue;
-                            }
+                    } else {
+                        let sdist = PathSourceDist {
+                            name: requirement.name.clone(),
+                            url: url.clone(),
+                            path,
+                        };
+
+                        // Find the most-compatible wheel from the cache, since we don't know
+                        // the filename in advance.
+                        if let Some(wheel) = built_index.path(&sdist)? {
+                            let cached_dist = wheel.into_url_dist(url.clone());
+                            debug!("Path source requirement already cached: {cached_dist}");
+                            cached.push(CachedDist::Url(cached_dist));
+                            continue;
                         }
                     }
                 }
             }
 
             debug!("Identified uncached requirement: {requirement}");
             remote.push(requirement.clone());
         }
 
         // Remove any unnecessary packages.
-        if !site_packages.is_empty() {
+        if site_packages.any() {
             // If uv created the virtual environment, then remove all packages, regardless of
             // whether they're considered "seed" packages.
-            let seed_packages = !venv.cfg().is_ok_and(|cfg| cfg.is_gourgeist());
+            let seed_packages = !venv.cfg().is_ok_and(|cfg| cfg.is_uv());
             for dist_info in site_packages {
                 if seed_packages
                     && matches!(
                         dist_info.name().as_ref(),
                         "pip" | "setuptools" | "wheel" | "uv"
                     )
                 {
@@ -392,114 +466,41 @@
 
                 debug!("Unnecessary package: {dist_info}");
                 extraneous.push(dist_info);
             }
         }
 
         Ok(Plan {
-            local,
+            cached,
             remote,
             reinstalls,
             extraneous,
         })
     }
 }
 
 #[derive(Debug, PartialEq, Eq)]
 enum Specifier<'a> {
     /// An editable requirement, marked by the installed version of the package.
     Editable(InstalledVersion<'a>),
     /// A non-editable requirement, marked by the version or URL specifier.
-    NonEditable(Option<&'a VersionOrUrl>),
-}
-
-/// Returns `true` if the cache entry linked to the file at the given [`Path`] is not-modified.
-///
-/// A cache entry is not modified if it exists and is newer than the file at the given path.
-fn not_modified_cache(cache_entry: &CacheEntry, artifact: &Path) -> Result<bool, io::Error> {
-    match fs_err::metadata(cache_entry.path()).map(|metadata| Timestamp::from_metadata(&metadata)) {
-        Ok(cache_timestamp) => {
-            // Determine the modification time of the wheel.
-            if let Some(artifact_timestamp) = ArchiveTimestamp::from_path(artifact)? {
-                Ok(cache_timestamp >= artifact_timestamp.timestamp())
-            } else {
-                // The artifact doesn't exist, so it's not fresh.
-                Ok(false)
-            }
-        }
-        Err(err) if err.kind() == io::ErrorKind::NotFound => {
-            // The cache entry doesn't exist, so it's not fresh.
-            Ok(false)
-        }
-        Err(err) => Err(err),
-    }
-}
-
-/// Returns `true` if the installed distribution linked to the file at the given [`Path`] is
-/// not-modified based on the modification time of the installed distribution.
-fn not_modified_install(dist: &InstalledDirectUrlDist, artifact: &Path) -> Result<bool, io::Error> {
-    // Determine the modification time of the installed distribution.
-    let dist_metadata = fs_err::metadata(&dist.path)?;
-    let dist_timestamp = Timestamp::from_metadata(&dist_metadata);
-
-    // Determine the modification time of the wheel.
-    if let Some(artifact_timestamp) = ArchiveTimestamp::from_path(artifact)? {
-        Ok(dist_timestamp >= artifact_timestamp.timestamp())
-    } else {
-        // The artifact doesn't exist, so it's not fresh.
-        Ok(false)
-    }
+    NonEditable(&'a RequirementSource),
 }
 
 #[derive(Debug, Default)]
 pub struct Plan {
     /// The distributions that are not already installed in the current environment, but are
     /// available in the local cache.
-    pub local: Vec<CachedDist>,
+    pub cached: Vec<CachedDist>,
 
     /// The distributions that are not already installed in the current environment, and are
     /// not available in the local cache.
     pub remote: Vec<Requirement>,
 
     /// Any distributions that are already installed in the current environment, but will be
     /// re-installed (including upgraded) to satisfy the requirements.
     pub reinstalls: Vec<InstalledDist>,
 
     /// Any distributions that are already installed in the current environment, and are
     /// _not_ necessary to satisfy the requirements.
     pub extraneous: Vec<InstalledDist>,
 }
-
-#[derive(Debug, Clone)]
-pub enum Reinstall {
-    /// Don't reinstall any packages; respect the existing installation.
-    None,
-
-    /// Reinstall all packages in the plan.
-    All,
-
-    /// Reinstall only the specified packages.
-    Packages(Vec<PackageName>),
-}
-
-impl Reinstall {
-    /// Determine the reinstall strategy to use.
-    pub fn from_args(reinstall: bool, reinstall_package: Vec<PackageName>) -> Self {
-        if reinstall {
-            Self::All
-        } else if !reinstall_package.is_empty() {
-            Self::Packages(reinstall_package)
-        } else {
-            Self::None
-        }
-    }
-
-    /// Returns `true` if no packages should be reinstalled.
-    pub fn is_none(&self) -> bool {
-        matches!(self, Self::None)
-    }
-
-    /// Returns `true` if all packages should be reinstalled.
-    pub fn is_all(&self) -> bool {
-        matches!(self, Self::All)
-    }
-}
```

### Comparing `uv-0.1.9/crates/uv-installer/src/site_packages.rs` & `uv-0.2.0/crates/uv/src/commands/pip/install.rs`

 * *Files 25% similar despite different names*

```diff
@@ -1,422 +1,503 @@
-use std::hash::BuildHasherDefault;
-use std::path::PathBuf;
+use std::borrow::Cow;
+use std::fmt::Write;
 
-use anyhow::{Context, Result};
+use anstream::eprint;
 use fs_err as fs;
-use rustc_hash::{FxHashMap, FxHashSet};
-use url::Url;
-
-use distribution_types::{InstalledDist, InstalledMetadata, InstalledVersion, Name};
-use pep440_rs::{Version, VersionSpecifiers};
-use pep508_rs::{Requirement, VerbatimUrl};
-use requirements_txt::EditableRequirement;
-use uv_interpreter::Virtualenv;
+use itertools::Itertools;
+use owo_colors::OwoColorize;
+use tracing::{debug, enabled, Level};
+
+use distribution_types::{IndexLocations, Resolution};
+use install_wheel_rs::linker::LinkMode;
+use platform_tags::Tags;
+use uv_auth::store_credentials_from_url;
+use uv_cache::Cache;
+use uv_client::{BaseClientBuilder, Connectivity, FlatIndexClient, RegistryClientBuilder};
+use uv_configuration::{
+    Concurrency, ConfigSettings, IndexStrategy, NoBinary, NoBuild, PreviewMode, Reinstall,
+    SetupPyStrategy, Upgrade,
+};
+use uv_configuration::{KeyringProviderType, TargetTriple};
+use uv_dispatch::BuildDispatch;
+use uv_distribution::DistributionDatabase;
+use uv_fs::Simplified;
+use uv_installer::{SatisfiesResult, SitePackages};
+use uv_interpreter::{PythonEnvironment, PythonVersion, SystemPython, Target};
 use uv_normalize::PackageName;
-
-/// An index over the packages installed in an environment.
-///
-/// Packages are indexed by both name and (for editable installs) URL.
-#[derive(Debug)]
-pub struct SitePackages<'a> {
-    venv: &'a Virtualenv,
-    /// The vector of all installed distributions.
-    distributions: Vec<InstalledDist>,
-    /// The installed distributions, keyed by name.
-    by_name: FxHashMap<PackageName, usize>,
-    /// The installed editable distributions, keyed by URL.
-    by_url: FxHashMap<Url, usize>,
-}
-
-impl<'a> SitePackages<'a> {
-    /// Build an index of installed packages from the given Python executable.
-    pub fn from_executable(venv: &'a Virtualenv) -> Result<SitePackages<'a>> {
-        let mut distributions: Vec<InstalledDist> = Vec::new();
-        let mut by_name = FxHashMap::default();
-        let mut by_url = FxHashMap::default();
-
-        // Index all installed packages by name.
-        for entry in fs::read_dir(venv.site_packages())? {
-            let entry = entry?;
-            if entry.file_type()?.is_dir() {
-                let path = entry.path();
-
-                let Some(dist_info) = InstalledDist::try_from_path(&path)
-                    .with_context(|| format!("Failed to read metadata: from {}", path.display()))?
-                else {
-                    continue;
-                };
-
-                let idx = distributions.len();
-
-                // Index the distribution by name.
-                if let Some(existing) = by_name.insert(dist_info.name().clone(), idx) {
-                    let existing = &distributions[existing];
-                    anyhow::bail!(
-                        "Found duplicate package in environment: {} ({} vs. {})",
-                        existing.name(),
-                        existing.path().display(),
-                        path.display()
-                    );
-                }
-
-                // Index the distribution by URL.
-                if let Some(url) = dist_info.as_editable() {
-                    if let Some(existing) = by_url.insert(url.clone(), idx) {
-                        let existing = &distributions[existing];
-                        anyhow::bail!(
-                            "Found duplicate editable in environment: {} ({} vs. {})",
-                            existing.name(),
-                            existing.path().display(),
-                            path.display()
-                        );
-                    }
-                }
-
-                // Add the distribution to the database.
-                distributions.push(dist_info);
-            }
+use uv_requirements::{
+    ExtrasSpecification, NamedRequirementsResolver, RequirementsSource, RequirementsSpecification,
+    SourceTreeResolver,
+};
+use uv_resolver::{
+    DependencyMode, ExcludeNewer, FlatIndex, InMemoryIndex, Lock, OptionsBuilder, PreReleaseMode,
+    ResolutionMode,
+};
+use uv_types::{BuildIsolation, HashStrategy, InFlight};
+
+use crate::commands::pip::operations;
+use crate::commands::pip::operations::Modifications;
+use crate::commands::reporters::ResolverReporter;
+use crate::commands::{elapsed, ExitStatus};
+use crate::editables::ResolvedEditables;
+use crate::printer::Printer;
+
+/// Install packages into the current environment.
+#[allow(clippy::too_many_arguments, clippy::fn_params_excessive_bools)]
+pub(crate) async fn pip_install(
+    requirements: &[RequirementsSource],
+    constraints: &[RequirementsSource],
+    overrides: &[RequirementsSource],
+    extras: &ExtrasSpecification,
+    resolution_mode: ResolutionMode,
+    prerelease_mode: PreReleaseMode,
+    dependency_mode: DependencyMode,
+    upgrade: Upgrade,
+    index_locations: IndexLocations,
+    index_strategy: IndexStrategy,
+    keyring_provider: KeyringProviderType,
+    reinstall: Reinstall,
+    link_mode: LinkMode,
+    compile: bool,
+    require_hashes: bool,
+    setup_py: SetupPyStrategy,
+    connectivity: Connectivity,
+    config_settings: &ConfigSettings,
+    no_build_isolation: bool,
+    no_build: NoBuild,
+    no_binary: NoBinary,
+    python_version: Option<PythonVersion>,
+    python_platform: Option<TargetTriple>,
+    strict: bool,
+    exclude_newer: Option<ExcludeNewer>,
+    python: Option<String>,
+    system: bool,
+    break_system_packages: bool,
+    target: Option<Target>,
+    concurrency: Concurrency,
+    uv_lock: Option<String>,
+    native_tls: bool,
+    preview: PreviewMode,
+    cache: Cache,
+    dry_run: bool,
+    printer: Printer,
+) -> anyhow::Result<ExitStatus> {
+    let start = std::time::Instant::now();
+
+    let client_builder = BaseClientBuilder::new()
+        .connectivity(connectivity)
+        .native_tls(native_tls)
+        .keyring(keyring_provider);
+
+    // Read all requirements from the provided sources.
+    let RequirementsSpecification {
+        project,
+        requirements,
+        constraints,
+        overrides,
+        editables,
+        source_trees,
+        index_url,
+        extra_index_urls,
+        no_index,
+        find_links,
+        no_binary: specified_no_binary,
+        no_build: specified_no_build,
+        extras: _,
+    } = operations::read_requirements(
+        requirements,
+        constraints,
+        overrides,
+        extras,
+        &client_builder,
+        preview,
+    )
+    .await?;
+
+    // Detect the current Python interpreter.
+    let system = if system {
+        SystemPython::Required
+    } else {
+        SystemPython::Explicit
+    };
+    let venv = PythonEnvironment::find(python.as_deref(), system, &cache)?;
+
+    debug!(
+        "Using Python {} environment at {}",
+        venv.interpreter().python_version(),
+        venv.python_executable().user_display().cyan()
+    );
+
+    // Apply any `--target` directory.
+    let venv = if let Some(target) = target {
+        debug!(
+            "Using `--target` directory at {}",
+            target.root().user_display()
+        );
+        target.init()?;
+        venv.with_target(target)
+    } else {
+        venv
+    };
+
+    // If the environment is externally managed, abort.
+    if let Some(externally_managed) = venv.interpreter().is_externally_managed() {
+        if break_system_packages {
+            debug!("Ignoring externally managed environment due to `--break-system-packages`");
+        } else {
+            return if let Some(error) = externally_managed.into_error() {
+                Err(anyhow::anyhow!(
+                    "The interpreter at {} is externally managed, and indicates the following:\n\n{}\n\nConsider creating a virtual environment with `uv venv`.",
+                    venv.root().user_display().cyan(),
+                    textwrap::indent(&error, "  ").green(),
+                ))
+            } else {
+                Err(anyhow::anyhow!(
+                    "The interpreter at {} is externally managed. Instead, create a virtual environment with `uv venv`.",
+                    venv.root().user_display().cyan()
+                ))
+            };
         }
-
-        Ok(Self {
-            venv,
-            distributions,
-            by_name,
-            by_url,
-        })
-    }
-
-    /// Returns an iterator over the installed distributions.
-    pub fn iter(&self) -> impl Iterator<Item = &InstalledDist> {
-        self.distributions.iter()
-    }
-
-    /// Returns an iterator over the the installed distributions, represented as requirements.
-    pub fn requirements(&self) -> impl Iterator<Item = Requirement> + '_ {
-        self.iter().map(|dist| Requirement {
-            name: dist.name().clone(),
-            extras: vec![],
-            version_or_url: Some(match dist.installed_version() {
-                InstalledVersion::Version(version) => {
-                    pep508_rs::VersionOrUrl::VersionSpecifier(pep440_rs::VersionSpecifiers::from(
-                        pep440_rs::VersionSpecifier::equals_version(version.clone()),
-                    ))
-                }
-                InstalledVersion::Url(url, ..) => {
-                    pep508_rs::VersionOrUrl::Url(VerbatimUrl::unknown(url.clone()))
-                }
-            }),
-            marker: None,
-        })
-    }
-
-    /// Returns the version of the given package, if it is installed.
-    pub fn get(&self, name: &PackageName) -> Option<&InstalledDist> {
-        self.by_name.get(name).map(|idx| &self.distributions[*idx])
     }
 
-    /// Remove the given package from the index, returning its version if it was installed.
-    pub fn remove(&mut self, name: &PackageName) -> Option<InstalledDist> {
-        let idx = self.by_name.get(name)?;
-        Some(self.swap_remove(*idx))
-    }
+    let _lock = venv.lock()?;
 
-    /// Returns the editable distribution installed from the given URL, if any.
-    pub fn get_editable(&self, url: &Url) -> Option<&InstalledDist> {
-        self.by_url.get(url).map(|idx| &self.distributions[*idx])
-    }
+    // Determine the set of installed packages.
+    let site_packages = SitePackages::from_executable(&venv)?;
 
-    /// Remove the editable distribution installed from the given URL, if any.
-    pub fn remove_editable(&mut self, url: &Url) -> Option<InstalledDist> {
-        let idx = self.by_url.get(url)?;
-        Some(self.swap_remove(*idx))
-    }
-
-    /// Remove the distribution at the given index.
-    fn swap_remove(&mut self, idx: usize) -> InstalledDist {
-        // Remove from the existing index.
-        let dist = self.distributions.swap_remove(idx);
-
-        // If the distribution wasn't at the end, rewrite the entries for the moved distribution.
-        if idx < self.distributions.len() {
-            let moved = &self.distributions[idx];
-            if let Some(prev) = self.by_name.get_mut(moved.name()) {
-                *prev = idx;
-            }
-            if let Some(url) = moved.as_editable() {
-                if let Some(prev) = self.by_url.get_mut(url) {
-                    *prev = idx;
+    // If the requirements are already satisfied, we're done. Ideally, the resolver would be fast
+    // enough to let us remove this check. But right now, for large environments, it's an order of
+    // magnitude faster to validate the environment than to resolve the requirements.
+    if reinstall.is_none()
+        && upgrade.is_none()
+        && source_trees.is_empty()
+        && overrides.is_empty()
+        && uv_lock.is_none()
+    {
+        match site_packages.satisfies(&requirements, &editables, &constraints)? {
+            SatisfiesResult::Fresh {
+                recursive_requirements,
+            } => {
+                if enabled!(Level::DEBUG) {
+                    for requirement in recursive_requirements
+                        .iter()
+                        .map(|entry| entry.requirement.to_string())
+                        .sorted()
+                    {
+                        debug!("Requirement satisfied: {requirement}");
+                    }
                 }
-            }
-        }
-
-        dist
-    }
-
-    /// Returns `true` if there are no installed packages.
-    pub fn is_empty(&self) -> bool {
-        self.distributions.is_empty()
-    }
-
-    /// Returns the number of installed packages.
-    pub fn len(&self) -> usize {
-        self.distributions.len()
-    }
 
-    /// Validate the installed packages in the virtual environment.
-    pub fn diagnostics(&self) -> Result<Vec<Diagnostic>> {
-        let mut diagnostics = Vec::new();
-
-        for (package, index) in &self.by_name {
-            let distribution = &self.distributions[*index];
-
-            // Determine the dependencies for the given package.
-            let Ok(metadata) = distribution.metadata() else {
-                diagnostics.push(Diagnostic::IncompletePackage {
-                    package: package.clone(),
-                    path: distribution.path().to_owned(),
-                });
-                continue;
-            };
-
-            // Verify that the package is compatible with the current Python version.
-            if let Some(requires_python) = metadata.requires_python.as_ref() {
-                if !requires_python.contains(self.venv.interpreter().python_version()) {
-                    diagnostics.push(Diagnostic::IncompatiblePythonVersion {
-                        package: package.clone(),
-                        version: self.venv.interpreter().python_version().clone(),
-                        requires_python: requires_python.clone(),
-                    });
+                debug!(
+                    "All editables satisfied: {}",
+                    editables.iter().map(ToString::to_string).join(" | ")
+                );
+                let num_requirements = requirements.len() + editables.len();
+                let s = if num_requirements == 1 { "" } else { "s" };
+                writeln!(
+                    printer.stderr(),
+                    "{}",
+                    format!(
+                        "Audited {} in {}",
+                        format!("{num_requirements} package{s}").bold(),
+                        elapsed(start.elapsed())
+                    )
+                    .dimmed()
+                )?;
+                if dry_run {
+                    writeln!(printer.stderr(), "Would make no changes")?;
                 }
+                return Ok(ExitStatus::Success);
             }
-
-            // Verify that the dependencies are installed.
-            for dependency in &metadata.requires_dist {
-                if !dependency.evaluate_markers(self.venv.interpreter().markers(), &[]) {
-                    continue;
-                }
-
-                let Some(installed) = self
-                    .by_name
-                    .get(&dependency.name)
-                    .map(|idx| &self.distributions[*idx])
-                else {
-                    diagnostics.push(Diagnostic::MissingDependency {
-                        package: package.clone(),
-                        requirement: dependency.clone(),
-                    });
-                    continue;
-                };
-
-                match &dependency.version_or_url {
-                    None | Some(pep508_rs::VersionOrUrl::Url(_)) => {
-                        // Nothing to do (accept any installed version).
-                    }
-                    Some(pep508_rs::VersionOrUrl::VersionSpecifier(version_specifier)) => {
-                        if !version_specifier.contains(installed.version()) {
-                            diagnostics.push(Diagnostic::IncompatibleDependency {
-                                package: package.clone(),
-                                version: installed.version().clone(),
-                                requirement: dependency.clone(),
-                            });
-                        }
-                    }
-                }
+            SatisfiesResult::Unsatisfied(requirement) => {
+                debug!("At least one requirement is not satisfied: {requirement}");
             }
         }
-
-        Ok(diagnostics)
     }
 
-    /// Returns `true` if the installed packages satisfy the given requirements.
-    pub fn satisfies(
-        &self,
-        requirements: &[Requirement],
-        editables: &[EditableRequirement],
-        constraints: &[Requirement],
-    ) -> Result<bool> {
-        let mut stack = Vec::<Requirement>::with_capacity(requirements.len());
-        let mut seen =
-            FxHashSet::with_capacity_and_hasher(requirements.len(), BuildHasherDefault::default());
-
-        // Add the direct requirements to the queue.
-        for dependency in requirements {
-            if dependency.evaluate_markers(self.venv.interpreter().markers(), &[]) {
-                if seen.insert(dependency.clone()) {
-                    stack.push(dependency.clone());
-                }
-            }
-        }
-
-        // Verify that all editable requirements are met.
-        for requirement in editables {
-            let Some(distribution) = self
-                .by_url
-                .get(requirement.raw())
-                .map(|idx| &self.distributions[*idx])
-            else {
-                // The package isn't installed.
-                return Ok(false);
-            };
+    let interpreter = venv.interpreter().clone();
 
-            // Recurse into the dependencies.
-            let metadata = distribution
-                .metadata()
-                .with_context(|| format!("Failed to read metadata for: {distribution}"))?;
-
-            // Add the dependencies to the queue.
-            for dependency in metadata.requires_dist {
-                if dependency
-                    .evaluate_markers(self.venv.interpreter().markers(), &requirement.extras)
-                {
-                    if seen.insert(dependency.clone()) {
-                        stack.push(dependency);
-                    }
-                }
-            }
+    // Determine the tags, markers, and interpreter to use for resolution.
+    let tags = match (python_platform, python_version.as_ref()) {
+        (Some(python_platform), Some(python_version)) => Cow::Owned(Tags::from_env(
+            &python_platform.platform(),
+            (python_version.major(), python_version.minor()),
+            interpreter.implementation_name(),
+            interpreter.implementation_tuple(),
+            interpreter.gil_disabled(),
+        )?),
+        (Some(python_platform), None) => Cow::Owned(Tags::from_env(
+            &python_platform.platform(),
+            interpreter.python_tuple(),
+            interpreter.implementation_name(),
+            interpreter.implementation_tuple(),
+            interpreter.gil_disabled(),
+        )?),
+        (None, Some(python_version)) => Cow::Owned(Tags::from_env(
+            interpreter.platform(),
+            (python_version.major(), python_version.minor()),
+            interpreter.implementation_name(),
+            interpreter.implementation_tuple(),
+            interpreter.gil_disabled(),
+        )?),
+        (None, None) => Cow::Borrowed(interpreter.tags()?),
+    };
+
+    // Apply the platform tags to the markers.
+    let markers = match (python_platform, python_version) {
+        (Some(python_platform), Some(python_version)) => {
+            Cow::Owned(python_version.markers(&python_platform.markers(interpreter.markers())))
         }
-
-        // Verify that all non-editable requirements are met.
-        while let Some(requirement) = stack.pop() {
-            let Some(distribution) = self
-                .by_name
-                .get(&requirement.name)
-                .map(|idx| &self.distributions[*idx])
-            else {
-                // The package isn't installed.
-                return Ok(false);
-            };
-
-            // Validate that the installed version matches the requirement.
-            match &requirement.version_or_url {
-                None | Some(pep508_rs::VersionOrUrl::Url(_)) => {}
-                Some(pep508_rs::VersionOrUrl::VersionSpecifier(version_specifier)) => {
-                    // The installed version doesn't satisfy the requirement.
-                    if !version_specifier.contains(distribution.version()) {
-                        return Ok(false);
-                    }
-                }
-            }
-
-            // Validate that the installed version satisfies the constraints.
-            for constraint in constraints {
-                if !constraint.evaluate_markers(self.venv.interpreter().markers(), &[]) {
-                    continue;
-                }
-
-                match &constraint.version_or_url {
-                    None | Some(pep508_rs::VersionOrUrl::Url(_)) => {}
-                    Some(pep508_rs::VersionOrUrl::VersionSpecifier(version_specifier)) => {
-                        // The installed version doesn't satisfy the constraint.
-                        if !version_specifier.contains(distribution.version()) {
-                            return Ok(false);
-                        }
-                    }
-                }
-            }
-
-            // Recurse into the dependencies.
-            let metadata = distribution
-                .metadata()
-                .with_context(|| format!("Failed to read metadata for: {distribution}"))?;
-
-            // Add the dependencies to the queue.
-            for dependency in metadata.requires_dist {
-                if dependency
-                    .evaluate_markers(self.venv.interpreter().markers(), &requirement.extras)
-                {
-                    if seen.insert(dependency.clone()) {
-                        stack.push(dependency);
-                    }
-                }
+        (Some(python_platform), None) => Cow::Owned(python_platform.markers(interpreter.markers())),
+        (None, Some(python_version)) => Cow::Owned(python_version.markers(interpreter.markers())),
+        (None, None) => Cow::Borrowed(interpreter.markers()),
+    };
+
+    // Collect the set of required hashes.
+    let hasher = if require_hashes {
+        HashStrategy::from_requirements(
+            requirements
+                .iter()
+                .chain(overrides.iter())
+                .map(|entry| (&entry.requirement, entry.hashes.as_slice())),
+            Some(&markers),
+        )?
+    } else {
+        HashStrategy::None
+    };
+
+    // Incorporate any index locations from the provided sources.
+    let index_locations =
+        index_locations.combine(index_url, extra_index_urls, find_links, no_index);
+
+    // Add all authenticated sources to the cache.
+    for url in index_locations.urls() {
+        store_credentials_from_url(url);
+    }
+
+    // Initialize the registry client.
+    let client = RegistryClientBuilder::new(cache.clone())
+        .native_tls(native_tls)
+        .connectivity(connectivity)
+        .index_urls(index_locations.index_urls())
+        .index_strategy(index_strategy)
+        .keyring(keyring_provider)
+        .markers(&markers)
+        .platform(interpreter.platform())
+        .build();
+
+    // Resolve the flat indexes from `--find-links`.
+    let flat_index = {
+        let client = FlatIndexClient::new(&client, &cache);
+        let entries = client.fetch(index_locations.flat_index()).await?;
+        FlatIndex::from_entries(entries, &tags, &hasher, &no_build, &no_binary)
+    };
+
+    // Determine whether to enable build isolation.
+    let build_isolation = if no_build_isolation {
+        BuildIsolation::Shared(&venv)
+    } else {
+        BuildIsolation::Isolated
+    };
+
+    // Combine the `--no-binary` and `--no-build` flags.
+    let no_binary = no_binary.combine(specified_no_binary);
+    let no_build = no_build.combine(specified_no_build);
+
+    // Create a shared in-memory index.
+    let index = InMemoryIndex::default();
+
+    // Track in-flight downloads, builds, etc., across resolutions.
+    let in_flight = InFlight::default();
+
+    // Create a build dispatch for resolution.
+    let resolve_dispatch = BuildDispatch::new(
+        &client,
+        &cache,
+        &interpreter,
+        &index_locations,
+        &flat_index,
+        &index,
+        &in_flight,
+        setup_py,
+        config_settings,
+        build_isolation,
+        link_mode,
+        &no_build,
+        &no_binary,
+        concurrency,
+    )
+    .with_options(OptionsBuilder::new().exclude_newer(exclude_newer).build());
+
+    // Build all editable distributions. The editables are shared between resolution and
+    // installation, and should live for the duration of the command.
+    let editables = ResolvedEditables::resolve(
+        editables
+            .into_iter()
+            .map(ResolvedEditables::from_requirement),
+        &site_packages,
+        &reinstall,
+        &hasher,
+        venv.interpreter(),
+        &tags,
+        &cache,
+        &client,
+        &resolve_dispatch,
+        concurrency,
+        printer,
+    )
+    .await?;
+
+    // Resolve the requirements.
+    let resolution = if let Some(ref root) = uv_lock {
+        let root = PackageName::new(root.to_string())?;
+        let encoded = fs::tokio::read_to_string("uv.lock").await?;
+        let lock: Lock = toml::from_str(&encoded)?;
+        lock.to_resolution(&markers, &tags, &root)
+    } else {
+        // Resolve the requirements from the provided sources.
+        let requirements = {
+            // Convert from unnamed to named requirements.
+            let mut requirements = NamedRequirementsResolver::new(
+                requirements,
+                &hasher,
+                &index,
+                DistributionDatabase::new(&client, &resolve_dispatch, concurrency.downloads),
+            )
+            .with_reporter(ResolverReporter::from(printer))
+            .resolve()
+            .await?;
+
+            // Resolve any source trees into requirements.
+            if !source_trees.is_empty() {
+                requirements.extend(
+                    SourceTreeResolver::new(
+                        source_trees,
+                        extras,
+                        &hasher,
+                        &index,
+                        DistributionDatabase::new(
+                            &client,
+                            &resolve_dispatch,
+                            concurrency.downloads,
+                        ),
+                    )
+                    .with_reporter(ResolverReporter::from(printer))
+                    .resolve()
+                    .await?,
+                );
+            }
+
+            requirements
+        };
+
+        // Resolve the overrides from the provided sources.
+        let overrides = NamedRequirementsResolver::new(
+            overrides,
+            &hasher,
+            &index,
+            DistributionDatabase::new(&client, &resolve_dispatch, concurrency.downloads),
+        )
+        .with_reporter(ResolverReporter::from(printer))
+        .resolve()
+        .await?;
+
+        let options = OptionsBuilder::new()
+            .resolution_mode(resolution_mode)
+            .prerelease_mode(prerelease_mode)
+            .dependency_mode(dependency_mode)
+            .exclude_newer(exclude_newer)
+            .index_strategy(index_strategy)
+            .build();
+
+        match operations::resolve(
+            requirements,
+            constraints,
+            overrides,
+            project,
+            &editables,
+            &hasher,
+            site_packages.clone(),
+            &reinstall,
+            &upgrade,
+            &interpreter,
+            &tags,
+            &markers,
+            &client,
+            &flat_index,
+            &index,
+            &resolve_dispatch,
+            concurrency,
+            options,
+            printer,
+        )
+        .await
+        {
+            Ok(resolution) => Resolution::from(resolution),
+            Err(operations::Error::Resolve(uv_resolver::ResolveError::NoSolution(err))) => {
+                let report = miette::Report::msg(format!("{err}"))
+                    .context("No solution found when resolving dependencies:");
+                eprint!("{report:?}");
+                return Ok(ExitStatus::Failure);
             }
+            Err(err) => return Err(err.into()),
         }
+    };
 
-        Ok(true)
-    }
-}
-
-impl IntoIterator for SitePackages<'_> {
-    type Item = InstalledDist;
-    type IntoIter = std::vec::IntoIter<Self::Item>;
+    // Re-initialize the in-flight map.
+    let in_flight = InFlight::default();
 
-    fn into_iter(self) -> Self::IntoIter {
-        self.distributions.into_iter()
-    }
-}
-
-#[derive(Debug)]
-pub enum Diagnostic {
-    IncompletePackage {
-        /// The package that is missing metadata.
-        package: PackageName,
-        /// The path to the package.
-        path: PathBuf,
-    },
-    IncompatiblePythonVersion {
-        /// The package that requires a different version of Python.
-        package: PackageName,
-        /// The version of Python that is installed.
-        version: Version,
-        /// The version of Python that is required.
-        requires_python: VersionSpecifiers,
-    },
-    MissingDependency {
-        /// The package that is missing a dependency.
-        package: PackageName,
-        /// The dependency that is missing.
-        requirement: Requirement,
-    },
-    IncompatibleDependency {
-        /// The package that has an incompatible dependency.
-        package: PackageName,
-        /// The version of the package that is installed.
-        version: Version,
-        /// The dependency that is incompatible.
-        requirement: Requirement,
-    },
-}
-
-impl Diagnostic {
-    /// Convert the diagnostic into a user-facing message.
-    pub fn message(&self) -> String {
-        match self {
-            Self::IncompletePackage { package, path } => format!(
-                "The package `{package}` is broken or incomplete (unable to read `METADATA`). Consider recreating the virtualenv, or removing the package directory at: {}.", path.display(),
-            ),
-            Self::IncompatiblePythonVersion {
-                package,
-                version,
-                requires_python,
-            } => format!(
-                "The package `{package}` requires Python {requires_python}, but `{version}` is installed."
-            ),
-            Self::MissingDependency {
-                package,
-                requirement,
-            } => {
-                format!("The package `{package}` requires `{requirement}`, but it's not installed.")
-            }
-            Self::IncompatibleDependency {
-                package,
-                version,
-                requirement,
-            } => format!(
-                "The package `{package}` requires `{requirement}`, but `{version}` is installed."
-            ),
-        }
+    // If we're running with `--reinstall`, initialize a separate `BuildDispatch`, since we may
+    // end up removing some distributions from the environment.
+    let install_dispatch = if reinstall.is_none() {
+        resolve_dispatch
+    } else {
+        BuildDispatch::new(
+            &client,
+            &cache,
+            &interpreter,
+            &index_locations,
+            &flat_index,
+            &index,
+            &in_flight,
+            setup_py,
+            config_settings,
+            build_isolation,
+            link_mode,
+            &no_build,
+            &no_binary,
+            concurrency,
+        )
+        .with_options(OptionsBuilder::new().exclude_newer(exclude_newer).build())
+    };
+
+    // Sync the environment.
+    operations::install(
+        &resolution,
+        &editables,
+        site_packages,
+        Modifications::Sufficient,
+        &reinstall,
+        &no_binary,
+        link_mode,
+        compile,
+        &index_locations,
+        &hasher,
+        &tags,
+        &client,
+        &in_flight,
+        concurrency,
+        &install_dispatch,
+        &cache,
+        &venv,
+        dry_run,
+        printer,
+    )
+    .await?;
+
+    // Validate the environment.
+    if strict {
+        operations::report_diagnostics(&resolution, &venv, printer)?;
     }
 
-    /// Returns `true` if the [`PackageName`] is involved in this diagnostic.
-    pub fn includes(&self, name: &PackageName) -> bool {
-        match self {
-            Self::IncompletePackage { package, .. } => name == package,
-            Self::IncompatiblePythonVersion { package, .. } => name == package,
-            Self::MissingDependency { package, .. } => name == package,
-            Self::IncompatibleDependency {
-                package,
-                requirement,
-                ..
-            } => name == package || &requirement.name == name,
-        }
-    }
+    Ok(ExitStatus::Success)
 }
```

### Comparing `uv-0.1.9/crates/distribution-filename/Cargo.toml` & `uv-0.2.0/crates/distribution-types/Cargo.toml`

 * *Files 22% similar despite different names*

```diff
@@ -1,29 +1,40 @@
 [package]
-name = "distribution-filename"
+name = "distribution-types"
 version = "0.0.1"
 edition = { workspace = true }
 rust-version = { workspace = true }
 homepage = { workspace = true }
 documentation = { workspace = true }
 repository = { workspace = true }
 authors = { workspace = true }
 license = { workspace = true }
 
 [lints]
 workspace = true
 
-[features]
-rkyv = ["dep:rkyv", "pep440_rs/rkyv"]
-
 [dependencies]
-pep440_rs = { path = "../pep440-rs" }
-platform-tags = { path = "../platform-tags" }
-uv-normalize = { path = "../uv-normalize" }
+cache-key = { workspace = true }
+distribution-filename = { workspace = true }
+pep440_rs = { workspace = true }
+pep508_rs = { workspace = true }
+platform-tags = { workspace = true }
+pypi-types = { workspace = true }
+uv-fs = { workspace = true }
+uv-git = { workspace = true, features = ["vendored-openssl"] }
+uv-normalize = { workspace = true }
 
-rkyv = { workspace = true, features = ["strict", "validation"], optional = true }
-serde = { workspace = true, optional = true }
+anyhow = { workspace = true }
+fs-err = { workspace = true }
+git2 = { workspace = true }
+indexmap = { workspace = true }
+itertools = { workspace = true }
+once_cell = { workspace = true }
+rkyv = { workspace = true }
+rustc-hash = { workspace = true }
+schemars = { workspace = true, optional = true }
+serde = { workspace = true, features = ["derive"] }
+serde_json = { workspace = true }
 thiserror = { workspace = true }
+tracing = { workspace = true }
 url = { workspace = true }
-
-[dev-dependencies]
-insta = { version = "1.34.0" }
+urlencoding = { workspace = true }
```

### Comparing `uv-0.1.9/crates/distribution-filename/src/lib.rs` & `uv-0.2.0/crates/distribution-filename/src/lib.rs`

 * *Files 11% similar despite different names*

```diff
@@ -39,28 +39,28 @@
         } else {
             None
         }
     }
 
     pub fn name(&self) -> &PackageName {
         match self {
-            DistFilename::SourceDistFilename(filename) => &filename.name,
-            DistFilename::WheelFilename(filename) => &filename.name,
+            Self::SourceDistFilename(filename) => &filename.name,
+            Self::WheelFilename(filename) => &filename.name,
         }
     }
 
     pub fn version(&self) -> &Version {
         match self {
-            DistFilename::SourceDistFilename(filename) => &filename.version,
-            DistFilename::WheelFilename(filename) => &filename.version,
+            Self::SourceDistFilename(filename) => &filename.version,
+            Self::WheelFilename(filename) => &filename.version,
         }
     }
 }
 
 impl Display for DistFilename {
     fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
         match self {
-            DistFilename::SourceDistFilename(filename) => Display::fmt(filename, f),
-            DistFilename::WheelFilename(filename) => Display::fmt(filename, f),
+            Self::SourceDistFilename(filename) => Display::fmt(filename, f),
+            Self::WheelFilename(filename) => Display::fmt(filename, f),
         }
     }
 }
```

### Comparing `uv-0.1.9/crates/distribution-filename/src/snapshots/distribution_filename__wheel__tests__ok_multiple_tags.snap` & `uv-0.2.0/crates/distribution-filename/src/snapshots/distribution_filename__wheel__tests__ok_multiple_tags.snap`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/distribution-filename/src/source_dist.rs` & `uv-0.2.0/crates/distribution-filename/src/source_dist.rs`

 * *Files 12% similar despite different names*

```diff
@@ -1,73 +1,88 @@
 use std::fmt::{Display, Formatter};
 use std::str::FromStr;
 
-#[cfg(feature = "serde")]
 use serde::{Deserialize, Serialize};
 use thiserror::Error;
 
 use pep440_rs::{Version, VersionParseError};
 use uv_normalize::{InvalidNameError, PackageName};
 
-#[derive(Clone, Debug, PartialEq, Eq)]
-#[cfg_attr(feature = "serde", derive(Serialize, Deserialize))]
-#[cfg_attr(
-    feature = "rkyv",
-    derive(rkyv::Archive, rkyv::Deserialize, rkyv::Serialize)
+#[derive(
+    Clone,
+    Debug,
+    PartialEq,
+    Eq,
+    Serialize,
+    Deserialize,
+    rkyv::Archive,
+    rkyv::Deserialize,
+    rkyv::Serialize,
 )]
-#[cfg_attr(feature = "rkyv", archive(check_bytes))]
-#[cfg_attr(feature = "rkyv", archive_attr(derive(Debug)))]
+#[archive(check_bytes)]
+#[archive_attr(derive(Debug))]
 pub enum SourceDistExtension {
     Zip,
     TarGz,
+    TarBz2,
 }
 
 impl FromStr for SourceDistExtension {
     type Err = String;
 
     fn from_str(s: &str) -> Result<Self, Self::Err> {
         Ok(match s {
             "zip" => Self::Zip,
             "tar.gz" => Self::TarGz,
+            "tar.bz2" => Self::TarBz2,
             other => return Err(other.to_string()),
         })
     }
 }
 
 impl Display for SourceDistExtension {
     fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
         match self {
-            SourceDistExtension::Zip => f.write_str("zip"),
-            SourceDistExtension::TarGz => f.write_str("tar.gz"),
+            Self::Zip => f.write_str("zip"),
+            Self::TarGz => f.write_str("tar.gz"),
+            Self::TarBz2 => f.write_str("tar.bz2"),
         }
     }
 }
 
 impl SourceDistExtension {
     pub fn from_filename(filename: &str) -> Option<(&str, Self)> {
         if let Some(stem) = filename.strip_suffix(".zip") {
             return Some((stem, Self::Zip));
         }
         if let Some(stem) = filename.strip_suffix(".tar.gz") {
             return Some((stem, Self::TarGz));
         }
+        if let Some(stem) = filename.strip_suffix(".tar.bz2") {
+            return Some((stem, Self::TarBz2));
+        }
         None
     }
 }
 
 /// Note that this is a normalized and not an exact representation, keep the original string if you
 /// need the latter.
-#[derive(Clone, Debug, PartialEq, Eq)]
-#[cfg_attr(feature = "serde", derive(Serialize, Deserialize))]
-#[cfg_attr(
-    feature = "rkyv",
-    derive(rkyv::Archive, rkyv::Deserialize, rkyv::Serialize)
+#[derive(
+    Clone,
+    Debug,
+    PartialEq,
+    Eq,
+    Serialize,
+    Deserialize,
+    rkyv::Archive,
+    rkyv::Deserialize,
+    rkyv::Serialize,
 )]
-#[cfg_attr(feature = "rkyv", archive(check_bytes))]
-#[cfg_attr(feature = "rkyv", archive_attr(derive(Debug)))]
+#[archive(check_bytes)]
+#[archive_attr(derive(Debug))]
 pub struct SourceDistFilename {
     pub name: PackageName,
     pub version: Version,
     pub extension: SourceDistExtension,
 }
 
 impl SourceDistFilename {
@@ -178,15 +193,15 @@
     }
 }
 
 #[derive(Error, Debug, Clone)]
 enum SourceDistFilenameErrorKind {
     #[error("Name doesn't start with package name {0}")]
     Filename(PackageName),
-    #[error("Source distributions filenames must end with .zip or .tar.gz")]
+    #[error("Source distributions filenames must end with .zip, .tar.gz, or .tar.bz2")]
     Extension,
     #[error("Version section is invalid")]
     Version(#[from] VersionParseError),
     #[error(transparent)]
     PackageName(#[from] InvalidNameError),
     #[error("Missing name-version separator")]
     Minus,
@@ -203,14 +218,15 @@
     /// Only test already normalized names since the parsing is lossy
     #[test]
     fn roundtrip() {
         for normalized in [
             "foo-lib-1.2.3.zip",
             "foo-lib-1.2.3a3.zip",
             "foo-lib-1.2.3.tar.gz",
+            "foo-lib-1.2.3.tar.bz2",
         ] {
             assert_eq!(
                 SourceDistFilename::parse(normalized, &PackageName::from_str("foo_lib").unwrap())
                     .unwrap()
                     .to_string(),
                 normalized
             );
```

### Comparing `uv-0.1.9/crates/distribution-filename/src/wheel.rs` & `uv-0.2.0/crates/distribution-filename/src/wheel.rs`

 * *Files 4% similar despite different names*

```diff
@@ -1,26 +1,21 @@
 use std::fmt::{Display, Formatter};
 use std::str::FromStr;
 
-#[cfg(feature = "serde")]
 use serde::{de, Deserialize, Deserializer, Serialize, Serializer};
 use thiserror::Error;
 use url::Url;
 
 use pep440_rs::{Version, VersionParseError};
 use platform_tags::{TagCompatibility, Tags};
 use uv_normalize::{InvalidNameError, PackageName};
 
-#[derive(Debug, Clone, Eq, PartialEq, Hash)]
-#[cfg_attr(
-    feature = "rkyv",
-    derive(rkyv::Archive, rkyv::Deserialize, rkyv::Serialize)
-)]
-#[cfg_attr(feature = "rkyv", archive(check_bytes))]
-#[cfg_attr(feature = "rkyv", archive_attr(derive(Debug)))]
+#[derive(Debug, Clone, Eq, PartialEq, Hash, rkyv::Archive, rkyv::Deserialize, rkyv::Serialize)]
+#[archive(check_bytes)]
+#[archive_attr(derive(Debug))]
 pub struct WheelFilename {
     pub name: PackageName,
     pub version: Version,
     pub python_tag: Vec<String>,
     pub abi_tag: Vec<String>,
     pub platform_tag: Vec<String>,
 }
@@ -159,15 +154,15 @@
                 )
             };
 
         let name = PackageName::from_str(name)
             .map_err(|err| WheelFilenameError::InvalidPackageName(filename.to_string(), err))?;
         let version = Version::from_str(version)
             .map_err(|err| WheelFilenameError::InvalidVersion(filename.to_string(), err))?;
-        Ok(WheelFilename {
+        Ok(Self {
             name,
             version,
             python_tag: python_tag.split('.').map(String::from).collect(),
             abi_tag: abi_tag.split('.').map(String::from).collect(),
             platform_tag: platform_tag.split('.').map(String::from).collect(),
         })
     }
@@ -192,26 +187,24 @@
                     "URL must contain a filename".to_string(),
                 )
             })?;
         Self::from_str(filename)
     }
 }
 
-#[cfg(feature = "serde")]
 impl<'de> Deserialize<'de> for WheelFilename {
     fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
     where
         D: Deserializer<'de>,
     {
         let s = String::deserialize(deserializer)?;
         FromStr::from_str(&s).map_err(de::Error::custom)
     }
 }
 
-#[cfg(feature = "serde")]
 impl Serialize for WheelFilename {
     fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
     where
         S: Serializer,
     {
         serializer.serialize_str(&self.to_string())
     }
@@ -230,64 +223,64 @@
 #[cfg(test)]
 mod tests {
     use super::*;
 
     #[test]
     fn err_not_whl_extension() {
         let err = WheelFilename::from_str("foo.rs").unwrap_err();
-        insta::assert_display_snapshot!(err, @r###"The wheel filename "foo.rs" is invalid: Must end with .whl"###);
+        insta::assert_snapshot!(err, @r###"The wheel filename "foo.rs" is invalid: Must end with .whl"###);
     }
 
     #[test]
     fn err_1_part_empty() {
         let err = WheelFilename::from_str(".whl").unwrap_err();
-        insta::assert_display_snapshot!(err, @r###"The wheel filename ".whl" is invalid: Must have a version"###);
+        insta::assert_snapshot!(err, @r###"The wheel filename ".whl" is invalid: Must have a version"###);
     }
 
     #[test]
     fn err_1_part_no_version() {
         let err = WheelFilename::from_str("foo.whl").unwrap_err();
-        insta::assert_display_snapshot!(err, @r###"The wheel filename "foo.whl" is invalid: Must have a version"###);
+        insta::assert_snapshot!(err, @r###"The wheel filename "foo.whl" is invalid: Must have a version"###);
     }
 
     #[test]
     fn err_2_part_no_pythontag() {
         let err = WheelFilename::from_str("foo-version.whl").unwrap_err();
-        insta::assert_display_snapshot!(err, @r###"The wheel filename "foo-version.whl" is invalid: Must have a Python tag"###);
+        insta::assert_snapshot!(err, @r###"The wheel filename "foo-version.whl" is invalid: Must have a Python tag"###);
     }
 
     #[test]
     fn err_3_part_no_abitag() {
         let err = WheelFilename::from_str("foo-version-python.whl").unwrap_err();
-        insta::assert_display_snapshot!(err, @r###"The wheel filename "foo-version-python.whl" is invalid: Must have an ABI tag"###);
+        insta::assert_snapshot!(err, @r###"The wheel filename "foo-version-python.whl" is invalid: Must have an ABI tag"###);
     }
 
     #[test]
     fn err_4_part_no_platformtag() {
         let err = WheelFilename::from_str("foo-version-python-abi.whl").unwrap_err();
-        insta::assert_display_snapshot!(err, @r###"The wheel filename "foo-version-python-abi.whl" is invalid: Must have a platform tag"###);
+        insta::assert_snapshot!(err, @r###"The wheel filename "foo-version-python-abi.whl" is invalid: Must have a platform tag"###);
     }
 
     #[test]
     fn err_too_many_parts() {
         let err =
             WheelFilename::from_str("foo-1.2.3-build-python-abi-platform-oops.whl").unwrap_err();
-        insta::assert_display_snapshot!(err, @r###"The wheel filename "foo-1.2.3-build-python-abi-platform-oops.whl" is invalid: Must have 5 or 6 components, but has more"###);
+        insta::assert_snapshot!(err, @r###"The wheel filename "foo-1.2.3-build-python-abi-platform-oops.whl" is invalid: Must have 5 or 6 components, but has more"###);
     }
 
     #[test]
     fn err_invalid_package_name() {
         let err = WheelFilename::from_str("f!oo-1.2.3-python-abi-platform.whl").unwrap_err();
-        insta::assert_display_snapshot!(err, @r###"The wheel filename "f!oo-1.2.3-python-abi-platform.whl" has an invalid package name"###);
+        insta::assert_snapshot!(err, @r###"The wheel filename "f!oo-1.2.3-python-abi-platform.whl" has an invalid package name"###);
     }
 
     #[test]
     fn err_invalid_version() {
         let err = WheelFilename::from_str("foo-x.y.z-python-abi-platform.whl").unwrap_err();
-        insta::assert_display_snapshot!(err, @r###"The wheel filename "foo-x.y.z-python-abi-platform.whl" has an invalid version part: expected version to start with a number, but no leading ASCII digits were found"###);
+        insta::assert_snapshot!(err, @r###"The wheel filename "foo-x.y.z-python-abi-platform.whl" has an invalid version part: expected version to start with a number, but no leading ASCII digits were found"###);
     }
 
     #[test]
     fn ok_single_tags() {
         insta::assert_debug_snapshot!(WheelFilename::from_str("foo-1.2.3-foo-bar-baz.whl"));
     }
```

### Comparing `uv-0.1.9/crates/platform-host/Cargo.toml` & `uv-0.2.0/crates/pep440-rs/Cargo.toml`

 * *Files 17% similar despite different names*

```diff
@@ -1,25 +1,33 @@
 [package]
-name = "platform-host"
-version = "0.0.1"
+name = "pep440_rs"
+version = "0.5.0"
+description = "A library for python version numbers and specifiers, implementing PEP 440"
+license = "Apache-2.0 OR BSD-2-Clause"
+include = ["/src", "Changelog.md", "License-Apache", "License-BSD", "Readme.md", "pyproject.toml"]
+
 edition = { workspace = true }
 rust-version = { workspace = true }
 homepage = { workspace = true }
 documentation = { workspace = true }
 repository = { workspace = true }
 authors = { workspace = true }
-license = { workspace = true }
 
-[lints]
-workspace = true
+[lib]
+name = "pep440_rs"
+crate-type = ["rlib", "cdylib"]
 
 [dependencies]
-fs-err = { workspace = true }
-goblin = { workspace = true }
 once_cell = { workspace = true }
-platform-info = { workspace = true }
-plist = { workspace = true }
-regex = { workspace = true }
+pyo3 = { workspace = true, optional = true, features = ["extension-module", "abi3-py37"] }
 serde = { workspace = true, features = ["derive"] }
-target-lexicon = { workspace = true }
-thiserror = { workspace = true }
-tracing = { workspace = true }
+rkyv = { workspace = true }
+tracing = { workspace = true, optional = true }
+unicode-width = { workspace = true }
+unscanny = { workspace = true }
+
+[dev-dependencies]
+indoc = { version = "2.0.4" }
+
+[features]
+# Match the API of the published crate, for compatibility.
+serde = []
```

### Comparing `uv-0.1.9/crates/uv-cache/Cargo.toml` & `uv-0.2.0/crates/uv-dispatch/Cargo.toml`

 * *Files 25% similar despite different names*

```diff
@@ -1,32 +1,33 @@
 [package]
-name = "uv-cache"
+name = "uv-dispatch"
 version = "0.0.1"
-description = "Generate stable hash digests across versions and platforms."
+description = "Avoid cyclic crate dependencies between resolver, installer and builder"
 edition = { workspace = true }
 rust-version = { workspace = true }
 homepage = { workspace = true }
 documentation = { workspace = true }
 repository = { workspace = true }
 authors = { workspace = true }
 license = { workspace = true }
 
 [lints]
 workspace = true
 
 [dependencies]
-cache-key = { path = "../cache-key" }
-distribution-types = { path = "../distribution-types" }
-uv-fs = { path = "../uv-fs" }
-uv-normalize = { path = "../uv-normalize" }
-pypi-types = { path = "../pypi-types" }
+distribution-types = { workspace = true }
+install-wheel-rs = { workspace = true }
+uv-build = { workspace = true }
+uv-cache = { workspace = true }
+uv-client = { workspace = true }
+uv-configuration = { workspace = true }
+uv-installer = { workspace = true }
+uv-interpreter = { workspace = true }
+uv-distribution = { workspace = true }
+uv-resolver = { workspace = true }
+uv-types = { workspace = true }
 
-cachedir = { workspace = true }
-clap = { workspace = true, features = ["derive"], optional = true }
-directories = { workspace = true }
-fs-err = { workspace = true, features = ["tokio"] }
-nanoid = { workspace = true }
-serde = { workspace = true, features = ["derive"] }
-tempfile = { workspace = true }
+anyhow = { workspace = true }
+futures = { workspace = true }
+itertools = { workspace = true }
+rustc-hash = { workspace = true }
 tracing = { workspace = true }
-url = { workspace = true }
-walkdir = { workspace = true }
```

### Comparing `uv-0.1.9/crates/uv-cache/src/lib.rs` & `uv-0.2.0/crates/uv-cache/src/lib.rs`

 * *Files 19% similar despite different names*

```diff
@@ -1,29 +1,37 @@
+use std::cmp::max;
 use std::fmt::{Display, Formatter};
 use std::io;
 use std::io::Write;
 use std::ops::Deref;
 use std::path::{Path, PathBuf};
 use std::sync::Arc;
 
 use fs_err as fs;
+use rustc_hash::FxHashSet;
 use tempfile::{tempdir, TempDir};
+use tracing::debug;
 
-use uv_fs::directories;
+pub use archive::ArchiveId;
+use distribution_types::InstalledDist;
+use pypi_types::Metadata23;
+use uv_fs::{cachedir, directories};
 use uv_normalize::PackageName;
 
 pub use crate::by_timestamp::CachedByTimestamp;
 #[cfg(feature = "clap")]
 pub use crate::cli::CacheArgs;
 use crate::removal::{rm_rf, Removal};
 pub use crate::timestamp::Timestamp;
 pub use crate::wheel::WheelCache;
 use crate::wheel::WheelCacheKind;
 
+mod archive;
 mod by_timestamp;
+#[cfg(feature = "clap")]
 mod cli;
 mod removal;
 mod timestamp;
 mod wheel;
 
 /// A [`CacheEntry`] which may or may not exist yet.
 #[derive(Debug, Clone)]
@@ -61,14 +69,20 @@
     /// Create a new [`CacheEntry`] with the given file name.
     #[must_use]
     pub fn with_file(&self, file: impl AsRef<Path>) -> Self {
         Self(self.dir().join(file))
     }
 }
 
+impl AsRef<Path> for CacheEntry {
+    fn as_ref(&self) -> &Path {
+        &self.0
+    }
+}
+
 /// A subdirectory within the cache.
 #[derive(Debug, Clone)]
 pub struct CacheShard(PathBuf);
 
 impl CacheShard {
     /// Return a [`CacheEntry`] within this shard.
     pub fn entry(&self, file: impl AsRef<Path>) -> CacheEntry {
@@ -110,25 +124,25 @@
     _temp_dir_drop: Option<Arc<TempDir>>,
 }
 
 impl Cache {
     /// A persistent cache directory at `root`.
     pub fn from_path(root: impl Into<PathBuf>) -> Result<Self, io::Error> {
         Ok(Self {
-            root: Self::init(root)?,
+            root: root.into(),
             refresh: Refresh::None,
             _temp_dir_drop: None,
         })
     }
 
     /// Create a temporary cache directory.
     pub fn temp() -> Result<Self, io::Error> {
         let temp_dir = tempdir()?;
         Ok(Self {
-            root: Self::init(temp_dir.path())?,
+            root: temp_dir.path().to_path_buf(),
             refresh: Refresh::None,
             _temp_dir_drop: Some(Arc::new(temp_dir)),
         })
     }
 
     /// Set the [`Refresh`] policy for the cache.
     #[must_use]
@@ -157,14 +171,19 @@
         cache_bucket: CacheBucket,
         dir: impl AsRef<Path>,
         file: impl AsRef<Path>,
     ) -> CacheEntry {
         CacheEntry::new(self.bucket(cache_bucket).join(dir), file)
     }
 
+    /// Return the path to an archive in the cache.
+    pub fn archive(&self, id: &ArchiveId) -> PathBuf {
+        self.bucket(CacheBucket::Archive).join(id)
+    }
+
     /// Returns `true` if a cache entry must be revalidated given the [`Refresh`] policy.
     pub fn must_revalidate(&self, package: &PackageName) -> bool {
         match &self.refresh {
             Refresh::None => false,
             Refresh::All(_) => true,
             Refresh::Packages(packages, _) => packages.contains(package),
         }
@@ -198,99 +217,173 @@
                 }
             }
             Err(err) if err.kind() == io::ErrorKind::NotFound => Ok(Freshness::Missing),
             Err(err) => Err(err),
         }
     }
 
-    /// Persist a temporary directory to the artifact store.
-    pub fn persist(
+    /// Persist a temporary directory to the artifact store, returning its unique ID.
+    pub async fn persist(
         &self,
         temp_dir: impl AsRef<Path>,
         path: impl AsRef<Path>,
-    ) -> io::Result<PathBuf> {
+    ) -> io::Result<ArchiveId> {
         // Create a unique ID for the artifact.
         // TODO(charlie): Support content-addressed persistence via SHAs.
-        let id = nanoid::nanoid!();
+        let id = ArchiveId::new();
 
         // Move the temporary directory into the directory store.
-        let archive_entry = self.entry(CacheBucket::Archive, "", id);
+        let archive_entry = self.entry(CacheBucket::Archive, "", &id);
         fs_err::create_dir_all(archive_entry.dir())?;
-        fs_err::rename(temp_dir.as_ref(), archive_entry.path())?;
+        uv_fs::rename_with_retry(temp_dir.as_ref(), archive_entry.path()).await?;
 
         // Create a symlink to the directory store.
         fs_err::create_dir_all(path.as_ref().parent().expect("Cache entry to have parent"))?;
         uv_fs::replace_symlink(archive_entry.path(), path.as_ref())?;
 
-        Ok(archive_entry.into_path_buf())
+        Ok(id)
     }
 
-    /// Initialize a directory for use as a cache.
-    fn init(root: impl Into<PathBuf>) -> Result<PathBuf, io::Error> {
-        let root = root.into();
+    /// Initialize the cache.
+    pub fn init(self) -> Result<Self, io::Error> {
+        let root = &self.root;
 
         // Create the cache directory, if it doesn't exist.
-        fs::create_dir_all(&root)?;
+        fs::create_dir_all(root)?;
 
         // Add the CACHEDIR.TAG.
-        cachedir::ensure_tag(&root)?;
+        cachedir::ensure_tag(root)?;
 
         // Add the .gitignore.
         match fs::OpenOptions::new()
             .write(true)
             .create_new(true)
             .open(root.join(".gitignore"))
         {
             Ok(mut file) => file.write_all(b"*")?,
             Err(err) if err.kind() == io::ErrorKind::AlreadyExists => (),
             Err(err) => return Err(err),
         }
 
+        // Add an empty .gitignore to the build bucket, to ensure that the cache's own .gitignore
+        // doesn't interfere with source distribution builds. Build backends (like hatchling) will
+        // traverse upwards to look for .gitignore files.
+        fs::create_dir_all(root.join(CacheBucket::BuiltWheels.to_str()))?;
+        match fs::OpenOptions::new().write(true).create_new(true).open(
+            root.join(CacheBucket::BuiltWheels.to_str())
+                .join(".gitignore"),
+        ) {
+            Ok(_) => {}
+            Err(err) if err.kind() == io::ErrorKind::AlreadyExists => (),
+            Err(err) => return Err(err),
+        }
+
         // Add a phony .git, if it doesn't exist, to ensure that the cache isn't considered to be
         // part of a Git repository. (Some packages will include Git metadata (like a hash) in the
         // built version if they're in a Git repository, but the cache should be viewed as an
-        // isolated store.)
+        // isolated store.).
+        // We have to put this below the gitignore. Otherwise, if the build backend uses the rust
+        // ignore crate it will walk up to the top level .gitignore and ignore its python source
+        // files.
         fs::OpenOptions::new()
             .create(true)
             .write(true)
-            .open(root.join(".git"))?;
+            .open(root.join(CacheBucket::BuiltWheels.to_str()).join(".git"))?;
 
-        fs::canonicalize(root)
+        Ok(Self {
+            root: fs::canonicalize(root)?,
+            ..self
+        })
     }
 
     /// Clear the cache, removing all entries.
     pub fn clear(&self) -> Result<Removal, io::Error> {
         rm_rf(&self.root)
     }
 
     /// Remove a package from the cache.
     ///
     /// Returns the number of entries removed from the cache.
     pub fn remove(&self, name: &PackageName) -> Result<Removal, io::Error> {
         let mut summary = Removal::default();
-        for bucket in [
-            CacheBucket::Wheels,
-            CacheBucket::BuiltWheels,
-            CacheBucket::Git,
-            CacheBucket::Interpreter,
-            CacheBucket::Simple,
-        ] {
+        for bucket in CacheBucket::iter() {
             summary += bucket.remove(self, name)?;
         }
         Ok(summary)
     }
+
+    /// Run the garbage collector on the cache, removing any dangling entries.
+    pub fn prune(&self) -> Result<Removal, io::Error> {
+        let mut summary = Removal::default();
+
+        // First, remove any top-level directories that are unused. These typically represent
+        // outdated cache buckets (e.g., `wheels-v0`, when latest is `wheels-v1`).
+        for entry in fs::read_dir(&self.root)? {
+            let entry = entry?;
+            let metadata = entry.metadata()?;
+
+            if entry.file_name() == "CACHEDIR.TAG"
+                || entry.file_name() == ".gitignore"
+                || entry.file_name() == ".git"
+            {
+                continue;
+            }
+
+            if metadata.is_dir() {
+                // If the directory is not a cache bucket, remove it.
+                if CacheBucket::iter().all(|bucket| entry.file_name() != bucket.to_str()) {
+                    let path = entry.path();
+                    debug!("Removing dangling cache entry: {}", path.display());
+                    summary += rm_rf(path)?;
+                }
+            } else {
+                // If the file is not a marker file, remove it.
+                let path = entry.path();
+                debug!("Removing dangling cache entry: {}", path.display());
+                summary += rm_rf(path)?;
+            }
+        }
+
+        // Second, remove any unused archives (by searching for archives that are not symlinked).
+        // TODO(charlie): Remove any unused source distributions. This requires introspecting the
+        // cache contents, e.g., reading and deserializing the manifests.
+        let mut references = FxHashSet::default();
+
+        for bucket in CacheBucket::iter() {
+            let bucket = self.bucket(bucket);
+            if bucket.is_dir() {
+                for entry in walkdir::WalkDir::new(bucket) {
+                    let entry = entry?;
+                    if entry.file_type().is_symlink() {
+                        references.insert(entry.path().canonicalize()?);
+                    }
+                }
+            }
+        }
+
+        for entry in fs::read_dir(self.bucket(CacheBucket::Archive))? {
+            let entry = entry?;
+            let path = entry.path().canonicalize()?;
+            if !references.contains(&path) {
+                debug!("Removing dangling cache entry: {}", path.display());
+                summary += rm_rf(path)?;
+            }
+        }
+
+        Ok(summary)
+    }
 }
 
 /// The different kinds of data in the cache are stored in different bucket, which in our case
 /// are subdirectories of the cache root.
 #[derive(Debug, Clone, Copy, Eq, PartialEq, Hash)]
 pub enum CacheBucket {
     /// Wheels (excluding built wheels), alongside their metadata and cache policy.
     ///
-    /// There are three kinds from cache entries: Wheel metadata and policy as MsgPack files, the
+    /// There are three kinds from cache entries: Wheel metadata and policy as `MsgPack` files, the
     /// wheels themselves, and the unzipped wheel archives. If a wheel file is over an in-memory
     /// size threshold, we first download the zip file into the cache, then unzip it into a
     /// directory with the same name (exclusive of the `.whl` extension).
     ///
     /// Cache structure:
     ///  * `wheel-metadata-v0/pypi/foo/{foo-1.0.0-py3-none-any.msgpack, foo-1.0.0-py3-none-any.whl}`
     ///  * `wheel-metadata-v0/<digest(index-url)>/foo/{foo-1.0.0-py3-none-any.msgpack, foo-1.0.0-py3-none-any.whl}`
@@ -465,15 +558,15 @@
     /// without the shim itself changing, we only cache when the path equals `sys.executable`, i.e.
     /// the path we're running is the python executable itself and not a shim.
     ///
     /// Cache structure: `interpreter-v0/<digest(path)>.msgpack`
     ///
     /// # Example
     ///
-    /// The contents of each of the MsgPack files has a timestamp field in unix time, the [PEP 508]
+    /// The contents of each of the `MsgPack` files has a timestamp field in unix time, the [PEP 508]
     /// markers and some information from the `sys`/`sysconfig` modules.
     ///
     /// ```json
     /// {
     ///   "timestamp": 1698047994491,
     ///   "data": {
     ///     "markers": {
@@ -513,31 +606,42 @@
     /// other buckets directly would make atomic operations impossible.
     Archive,
 }
 
 impl CacheBucket {
     fn to_str(self) -> &'static str {
         match self {
-            CacheBucket::BuiltWheels => "built-wheels-v0",
-            CacheBucket::FlatIndex => "flat-index-v0",
-            CacheBucket::Git => "git-v0",
-            CacheBucket::Interpreter => "interpreter-v0",
-            CacheBucket::Simple => "simple-v2",
-            CacheBucket::Wheels => "wheels-v0",
-            CacheBucket::Archive => "archive-v0",
+            Self::BuiltWheels => "built-wheels-v3",
+            Self::FlatIndex => "flat-index-v0",
+            Self::Git => "git-v0",
+            Self::Interpreter => "interpreter-v1",
+            Self::Simple => "simple-v7",
+            Self::Wheels => "wheels-v1",
+            Self::Archive => "archive-v0",
         }
     }
 
     /// Remove a package from the cache bucket.
     ///
     /// Returns the number of entries removed from the cache.
     fn remove(self, cache: &Cache, name: &PackageName) -> Result<Removal, io::Error> {
+        /// Returns `true` if the [`Path`] represents a built wheel for the given package.
+        fn is_match(path: &Path, name: &PackageName) -> bool {
+            let Ok(metadata) = fs_err::read(path.join("metadata.msgpack")) else {
+                return false;
+            };
+            let Ok(metadata) = rmp_serde::from_slice::<Metadata23>(&metadata) else {
+                return false;
+            };
+            metadata.name == *name
+        }
+
         let mut summary = Removal::default();
         match self {
-            CacheBucket::Wheels => {
+            Self::Wheels => {
                 // For `pypi` wheels, we expect a directory per package (indexed by name).
                 let root = cache.bucket(self).join(WheelCacheKind::Pypi);
                 summary += rm_rf(root.join(name.to_string()))?;
 
                 // For alternate indices, we expect a directory for every index, followed by a
                 // directory per package (indexed by name).
                 let root = cache.bucket(self).join(WheelCacheKind::Index);
@@ -548,79 +652,103 @@
                 // For direct URLs, we expect a directory for every URL, followed by a
                 // directory per package (indexed by name).
                 let root = cache.bucket(self).join(WheelCacheKind::Url);
                 for directory in directories(root) {
                     summary += rm_rf(directory.join(name.to_string()))?;
                 }
             }
-            CacheBucket::BuiltWheels => {
+            Self::BuiltWheels => {
                 // For `pypi` wheels, we expect a directory per package (indexed by name).
                 let root = cache.bucket(self).join(WheelCacheKind::Pypi);
                 summary += rm_rf(root.join(name.to_string()))?;
 
                 // For alternate indices, we expect a directory for every index, followed by a
                 // directory per package (indexed by name).
                 let root = cache.bucket(self).join(WheelCacheKind::Index);
                 for directory in directories(root) {
                     summary += rm_rf(directory.join(name.to_string()))?;
                 }
 
-                // For direct URLs, we expect a directory for every index, followed by a
-                // directory per package (indexed by name).
+                // For direct URLs, we expect a directory for every URL, followed by a
+                // directory per version. To determine whether the URL is relevant, we need to
+                // search for a wheel matching the package name.
                 let root = cache.bucket(self).join(WheelCacheKind::Url);
-                for directory in directories(root) {
-                    summary += rm_rf(directory.join(name.to_string()))?;
+                for url in directories(root) {
+                    if directories(&url).any(|version| is_match(&version, name)) {
+                        summary += rm_rf(url)?;
+                    }
                 }
 
                 // For local dependencies, we expect a directory for every path, followed by a
-                // directory per package (indexed by name).
+                // directory per version. To determine whether the path is relevant, we need to
+                // search for a wheel matching the package name.
                 let root = cache.bucket(self).join(WheelCacheKind::Path);
-                for directory in directories(root) {
-                    summary += rm_rf(directory.join(name.to_string()))?;
+                for path in directories(root) {
+                    if directories(&path).any(|version| is_match(&version, name)) {
+                        summary += rm_rf(path)?;
+                    }
                 }
 
                 // For Git dependencies, we expect a directory for every repository, followed by a
-                // directory for every SHA, followed by a directory per package (indexed by name).
+                // directory for every SHA. To determine whether the SHA is relevant, we need to
+                // search for a wheel matching the package name.
                 let root = cache.bucket(self).join(WheelCacheKind::Git);
-                for directory in directories(root) {
-                    for directory in directories(directory) {
-                        summary += rm_rf(directory.join(name.to_string()))?;
+                for repository in directories(root) {
+                    for sha in directories(repository) {
+                        if is_match(&sha, name) {
+                            summary += rm_rf(sha)?;
+                        }
                     }
                 }
             }
-            CacheBucket::Simple => {
+            Self::Simple => {
                 // For `pypi` wheels, we expect a rkyv file per package, indexed by name.
                 let root = cache.bucket(self).join(WheelCacheKind::Pypi);
                 summary += rm_rf(root.join(format!("{name}.rkyv")))?;
 
                 // For alternate indices, we expect a directory for every index, followed by a
                 // MsgPack file per package, indexed by name.
                 let root = cache.bucket(self).join(WheelCacheKind::Url);
                 for directory in directories(root) {
                     summary += rm_rf(directory.join(format!("{name}.rkyv")))?;
                 }
             }
-            CacheBucket::FlatIndex => {
+            Self::FlatIndex => {
                 // We can't know if the flat index includes a package, so we just remove the entire
                 // cache entry.
                 let root = cache.bucket(self);
                 summary += rm_rf(root)?;
             }
-            CacheBucket::Git => {
+            Self::Git => {
                 // Nothing to do.
             }
-            CacheBucket::Interpreter => {
+            Self::Interpreter => {
                 // Nothing to do.
             }
-            CacheBucket::Archive => {
+            Self::Archive => {
                 // Nothing to do.
             }
         }
         Ok(summary)
     }
+
+    /// Return an iterator over all cache buckets.
+    pub fn iter() -> impl Iterator<Item = CacheBucket> {
+        [
+            CacheBucket::Wheels,
+            CacheBucket::BuiltWheels,
+            CacheBucket::FlatIndex,
+            CacheBucket::Git,
+            CacheBucket::Interpreter,
+            CacheBucket::Simple,
+            CacheBucket::Archive,
+        ]
+        .iter()
+        .copied()
+    }
 }
 
 impl Display for CacheBucket {
     fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
         f.write_str(self.to_str())
     }
 }
@@ -634,50 +762,122 @@
     Approximate(Timestamp),
 }
 
 impl ArchiveTimestamp {
     /// Return the modification timestamp for an archive, which could be a file (like a wheel or a zip
     /// archive) or a directory containing a Python package.
     ///
-    /// If the path is to a directory with no entrypoint (i.e., no `pyproject.toml` or `setup.py`),
-    /// returns `None`.
+    /// If the path is to a directory with no entrypoint (i.e., no `pyproject.toml`, `setup.py`, or
+    /// `setup.cfg`), returns `None`.
     pub fn from_path(path: impl AsRef<Path>) -> Result<Option<Self>, io::Error> {
         let metadata = fs_err::metadata(path.as_ref())?;
         if metadata.is_file() {
             Ok(Some(Self::Exact(Timestamp::from_metadata(&metadata))))
         } else {
-            if let Some(metadata) = path
-                .as_ref()
-                .join("pyproject.toml")
-                .metadata()
-                .ok()
-                .filter(std::fs::Metadata::is_file)
-            {
-                Ok(Some(Self::Approximate(Timestamp::from_metadata(&metadata))))
-            } else if let Some(metadata) = path
-                .as_ref()
-                .join("setup.py")
-                .metadata()
-                .ok()
-                .filter(std::fs::Metadata::is_file)
-            {
-                Ok(Some(Self::Approximate(Timestamp::from_metadata(&metadata))))
-            } else {
-                Ok(None)
-            }
+            Self::from_source_tree(path)
         }
     }
 
+    /// Return the modification timestamp for a file.
+    pub fn from_file(path: impl AsRef<Path>) -> Result<Self, io::Error> {
+        let metadata = fs_err::metadata(path.as_ref())?;
+        Ok(Self::Exact(Timestamp::from_metadata(&metadata)))
+    }
+
+    /// Return the modification timestamp for a source tree, i.e., a directory.
+    ///
+    /// If the source tree doesn't contain an entrypoint (i.e., no `pyproject.toml`, `setup.py`, or
+    /// `setup.cfg`), returns `None`.
+    pub fn from_source_tree(path: impl AsRef<Path>) -> Result<Option<Self>, io::Error> {
+        // Compute the modification timestamp for the `pyproject.toml`, `setup.py`, and
+        // `setup.cfg` files, if they exist.
+        let pyproject_toml = path
+            .as_ref()
+            .join("pyproject.toml")
+            .metadata()
+            .ok()
+            .filter(std::fs::Metadata::is_file)
+            .as_ref()
+            .map(Timestamp::from_metadata);
+
+        let setup_py = path
+            .as_ref()
+            .join("setup.py")
+            .metadata()
+            .ok()
+            .filter(std::fs::Metadata::is_file)
+            .as_ref()
+            .map(Timestamp::from_metadata);
+
+        let setup_cfg = path
+            .as_ref()
+            .join("setup.cfg")
+            .metadata()
+            .ok()
+            .filter(std::fs::Metadata::is_file)
+            .as_ref()
+            .map(Timestamp::from_metadata);
+
+        // Take the most recent timestamp of the three files.
+        let Some(timestamp) = max(pyproject_toml, max(setup_py, setup_cfg)) else {
+            return Ok(None);
+        };
+
+        Ok(Some(Self::Approximate(timestamp)))
+    }
+
     /// Return the modification timestamp for an archive.
     pub fn timestamp(&self) -> Timestamp {
         match self {
             Self::Exact(timestamp) => *timestamp,
             Self::Approximate(timestamp) => *timestamp,
         }
     }
+
+    /// Returns `true` if the `target` (an installed or cached distribution) is up-to-date with the
+    /// source archive (`source`).
+    ///
+    /// The `target` should be an installed package in a virtual environment, or an unzipped
+    /// package in the cache.
+    ///
+    /// The `source` is a source archive, i.e., a path to a built wheel or a Python package directory.
+    pub fn up_to_date_with(source: &Path, target: ArchiveTarget) -> Result<bool, io::Error> {
+        let Some(modified_at) = Self::from_path(source)? else {
+            // If there's no entrypoint, we can't determine the modification time, so we assume that the
+            // target is not up-to-date.
+            return Ok(false);
+        };
+        let created_at = match target {
+            ArchiveTarget::Install(installed) => {
+                Timestamp::from_path(installed.path().join("METADATA"))?
+            }
+            ArchiveTarget::Cache(cache) => Timestamp::from_path(cache)?,
+        };
+        Ok(modified_at.timestamp() <= created_at)
+    }
+}
+
+#[derive(Debug, Clone, Copy)]
+pub enum ArchiveTarget<'a> {
+    /// The target is an installed package in a virtual environment.
+    Install(&'a InstalledDist),
+    /// The target is an unzipped package in the cache.
+    Cache(&'a Path),
+}
+
+impl PartialOrd for ArchiveTimestamp {
+    fn partial_cmp(&self, other: &Self) -> Option<std::cmp::Ordering> {
+        Some(self.timestamp().cmp(&other.timestamp()))
+    }
+}
+
+impl Ord for ArchiveTimestamp {
+    fn cmp(&self, other: &Self) -> std::cmp::Ordering {
+        self.timestamp().cmp(&other.timestamp())
+    }
 }
 
 #[derive(Debug, Clone, Copy, PartialEq, Eq)]
 pub enum Freshness {
     /// The cache entry is fresh according to the [`Refresh`] policy.
     Fresh,
     /// The cache entry is stale according to the [`Refresh`] policy.
@@ -705,21 +905,25 @@
     Packages(Vec<PackageName>, Timestamp),
     /// Refresh all entries created before the given timestamp.
     All(Timestamp),
 }
 
 impl Refresh {
     /// Determine the refresh strategy to use based on the command-line arguments.
-    pub fn from_args(refresh: bool, refresh_package: Vec<PackageName>) -> Self {
-        if refresh {
-            Self::All(Timestamp::now())
-        } else if !refresh_package.is_empty() {
-            Self::Packages(refresh_package, Timestamp::now())
-        } else {
-            Self::None
+    pub fn from_args(refresh: Option<bool>, refresh_package: Vec<PackageName>) -> Self {
+        match refresh {
+            Some(true) => Self::All(Timestamp::now()),
+            Some(false) => Self::None,
+            None => {
+                if refresh_package.is_empty() {
+                    Self::None
+                } else {
+                    Self::Packages(refresh_package, Timestamp::now())
+                }
+            }
         }
     }
 
     /// Returns `true` if no packages should be reinstalled.
     pub fn is_none(&self) -> bool {
         matches!(self, Self::None)
     }
```

### Comparing `uv-0.1.9/crates/uv-cache/src/removal.rs` & `uv-0.2.0/crates/uv-cache/src/removal.rs`

 * *Files 23% similar despite different names*

```diff
@@ -31,44 +31,61 @@
     fn rm_rf(&mut self, path: &Path) -> io::Result<()> {
         let metadata = match fs_err::symlink_metadata(path) {
             Ok(metadata) => metadata,
             Err(err) if err.kind() == io::ErrorKind::NotFound => return Ok(()),
             Err(err) => return Err(err),
         };
 
-        let mut rm_file = |path: &Path, meta: Result<std::fs::Metadata, walkdir::Error>| {
-            if let Ok(meta) = meta {
-                self.total_bytes += meta.len();
-            }
-            remove_file(path)?;
-
-            Ok(())
-        };
-
         if !metadata.is_dir() {
             self.num_files += 1;
-            return rm_file(path, Ok(metadata));
+
+            // Remove the file.
+            self.total_bytes += metadata.len();
+            remove_file(path)?;
+
+            return Ok(());
         }
 
         for entry in walkdir::WalkDir::new(path).contents_first(true) {
+            // If we hit a directory that lacks read permissions, try to make it readable.
+            if let Err(ref err) = entry {
+                if err
+                    .io_error()
+                    .is_some_and(|err| err.kind() == io::ErrorKind::PermissionDenied)
+                {
+                    if let Some(dir) = err.path() {
+                        if set_readable(dir).unwrap_or(false) {
+                            // Retry the operation; if we _just_ `self.rm_rf(dir)` and continue,
+                            // `walkdir` may give us duplicate entries for the directory.
+                            return self.rm_rf(path);
+                        }
+                    }
+                }
+            }
+
             let entry = entry?;
             if cfg!(windows) && entry.file_type().is_symlink() {
                 // In this branch, we try to handle junction removal.
                 self.num_files += 1;
-                fs_err::remove_dir(entry.path())?;
+                remove_dir(entry.path())?;
             } else if entry.file_type().is_dir() {
                 self.num_dirs += 1;
 
                 // The contents should have been removed by now, but sometimes a race condition is
                 // hit where other files have been added by the OS. Fall back to `remove_dir_all`,
                 // which will remove the directory robustly across platforms.
-                fs_err::remove_dir_all(entry.path())?;
+                remove_dir_all(entry.path())?;
             } else {
                 self.num_files += 1;
-                rm_file(entry.path(), entry.metadata())?;
+
+                // Remove the file.
+                if let Ok(meta) = entry.metadata() {
+                    self.total_bytes += meta.len();
+                }
+                remove_file(entry.path())?;
             }
         }
 
         Ok(())
     }
 }
 
@@ -76,37 +93,83 @@
     fn add_assign(&mut self, other: Self) {
         self.num_files += other.num_files;
         self.num_dirs += other.num_dirs;
         self.total_bytes += other.total_bytes;
     }
 }
 
-/// Like [`fs_err::remove_file`], but attempts to change the permissions to force the file to be
-/// deleted (if it is readonly).
-fn remove_file(path: &Path) -> io::Result<()> {
-    /// If the file is readonly, change the permissions to make it _not_ readonly.
-    fn set_not_readonly(path: &Path) -> io::Result<bool> {
+/// If the directory isn't readable by the current user, change the permissions to make it readable.
+#[cfg_attr(windows, allow(unused_variables, clippy::unnecessary_wraps))]
+fn set_readable(path: &Path) -> io::Result<bool> {
+    #[cfg(unix)]
+    {
+        use std::os::unix::fs::PermissionsExt;
         let mut perms = path.metadata()?.permissions();
-        if !perms.readonly() {
-            return Ok(false);
+        if perms.mode() & 0o500 == 0 {
+            perms.set_mode(perms.mode() | 0o500);
+            fs_err::set_permissions(path, perms)?;
+            return Ok(true);
         }
+    }
+    Ok(false)
+}
 
-        // We're about to delete the file, so it's fine to set the permissions to world-writable.
-        #[allow(clippy::permissions_set_readonly_false)]
-        perms.set_readonly(false);
+/// If the file is readonly, change the permissions to make it _not_ readonly.
+fn set_not_readonly(path: &Path) -> io::Result<bool> {
+    let mut perms = path.metadata()?.permissions();
+    if !perms.readonly() {
+        return Ok(false);
+    }
 
-        fs_err::set_permissions(path, perms)?;
+    // We're about to delete the file, so it's fine to set the permissions to world-writable.
+    #[allow(clippy::permissions_set_readonly_false)]
+    perms.set_readonly(false);
 
-        Ok(true)
-    }
+    fs_err::set_permissions(path, perms)?;
 
+    Ok(true)
+}
+
+/// Like [`fs_err::remove_file`], but attempts to change the permissions to force the file to be
+/// deleted (if it is readonly).
+fn remove_file(path: &Path) -> io::Result<()> {
     match fs_err::remove_file(path) {
         Ok(()) => Ok(()),
         Err(err)
             if err.kind() == io::ErrorKind::PermissionDenied
                 && set_not_readonly(path).unwrap_or(false) =>
         {
             fs_err::remove_file(path)
         }
         Err(err) => Err(err),
     }
 }
+
+/// Like [`fs_err::remove_dir`], but attempts to change the permissions to force the directory to
+/// be deleted (if it is readonly).
+fn remove_dir(path: &Path) -> io::Result<()> {
+    match fs_err::remove_dir(path) {
+        Ok(()) => Ok(()),
+        Err(err)
+            if err.kind() == io::ErrorKind::PermissionDenied
+                && set_readable(path).unwrap_or(false) =>
+        {
+            fs_err::remove_dir(path)
+        }
+        Err(err) => Err(err),
+    }
+}
+
+/// Like [`fs_err::remove_dir_all`], but attempts to change the permissions to force the directory
+/// to be deleted (if it is readonly).
+fn remove_dir_all(path: &Path) -> io::Result<()> {
+    match fs_err::remove_dir_all(path) {
+        Ok(()) => Ok(()),
+        Err(err)
+            if err.kind() == io::ErrorKind::PermissionDenied
+                && set_readable(path).unwrap_or(false) =>
+        {
+            fs_err::remove_dir_all(path)
+        }
+        Err(err) => Err(err),
+    }
+}
```

### Comparing `uv-0.1.9/crates/uv-cache/src/timestamp.rs` & `uv-0.2.0/crates/uv-cache/src/timestamp.rs`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/uv-cache/src/wheel.rs` & `uv-0.2.0/crates/uv-cache/src/wheel.rs`

 * *Files 13% similar despite different names*

```diff
@@ -1,21 +1,15 @@
 use std::path::{Path, PathBuf};
 
 use url::Url;
 
 use cache_key::{digest, CanonicalUrl};
 use distribution_types::IndexUrl;
 
-#[allow(unused_imports)] // For rustdoc
-use crate::CacheBucket;
-
 /// Cache wheels and their metadata, both from remote wheels and built from source distributions.
-///
-/// Use [`WheelCache::remote_wheel_dir`] for remote wheel metadata caching and
-/// [`WheelCache::built_wheel_dir`] for built source distributions metadata caching.
 #[derive(Debug, Clone)]
 pub enum WheelCache<'a> {
     /// Either PyPI or an alternative index, which we key by index URL.
     Index(&'a IndexUrl),
     /// A direct URL dependency, which we key by URL.
     Url(&'a Url),
     /// A path dependency, which we key by URL.
@@ -24,17 +18,18 @@
     ///
     /// Note that this variant only exists for source distributions; wheels can't be delivered
     /// through Git.
     Git(&'a Url, &'a str),
 }
 
 impl<'a> WheelCache<'a> {
-    fn bucket(&self) -> PathBuf {
+    /// The root directory for a cache bucket.
+    pub fn root(&self) -> PathBuf {
         match self {
-            WheelCache::Index(IndexUrl::Pypi) => WheelCacheKind::Pypi.root(),
+            WheelCache::Index(IndexUrl::Pypi(_)) => WheelCacheKind::Pypi.root(),
             WheelCache::Index(url) => WheelCacheKind::Index
                 .root()
                 .join(digest(&CanonicalUrl::new(url))),
             WheelCache::Url(url) => WheelCacheKind::Url
                 .root()
                 .join(digest(&CanonicalUrl::new(url))),
             WheelCache::Path(url) => WheelCacheKind::Path
@@ -43,22 +38,17 @@
             WheelCache::Git(url, sha) => WheelCacheKind::Git
                 .root()
                 .join(digest(&CanonicalUrl::new(url)))
                 .join(sha),
         }
     }
 
-    /// Metadata of a remote wheel. See [`CacheBucket::Wheels`]
-    pub fn remote_wheel_dir(&self, package_name: impl AsRef<Path>) -> PathBuf {
-        self.bucket().join(package_name)
-    }
-
-    /// Metadata of a built source distribution. See [`CacheBucket::BuiltWheels`]
-    pub fn built_wheel_dir(&self, filename: impl AsRef<Path>) -> PathBuf {
-        self.bucket().join(filename)
+    /// A subdirectory in a bucket for wheels for a specific package.
+    pub fn wheel_dir(&self, package_name: impl AsRef<Path>) -> PathBuf {
+        self.root().join(package_name)
     }
 }
 
 #[derive(Debug, Clone, Copy)]
 pub(crate) enum WheelCacheKind {
     /// A cache of data from PyPI.
     Pypi,
@@ -71,19 +61,19 @@
     /// A cache of data from a Git repository.
     Git,
 }
 
 impl WheelCacheKind {
     pub(crate) fn to_str(self) -> &'static str {
         match self {
-            WheelCacheKind::Pypi => "pypi",
-            WheelCacheKind::Index => "index",
-            WheelCacheKind::Url => "url",
-            WheelCacheKind::Path => "path",
-            WheelCacheKind::Git => "git",
+            Self::Pypi => "pypi",
+            Self::Index => "index",
+            Self::Url => "url",
+            Self::Path => "path",
+            Self::Git => "git",
         }
     }
 
     pub(crate) fn root(self) -> PathBuf {
         Path::new(self.to_str()).to_path_buf()
     }
 }
```

### Comparing `uv-0.1.9/crates/uv-warnings/src/lib.rs` & `uv-0.2.0/crates/uv-warnings/src/lib.rs`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/install-wheel-rs/Cargo.toml` & `uv-0.2.0/crates/uv-requirements/Cargo.toml`

 * *Files 24% similar despite different names*

```diff
@@ -1,65 +1,57 @@
 [package]
-name = "install-wheel-rs"
-version = "0.0.1"
-publish = false
-description = "Takes a wheel and installs it, either in a venv or for monotrail"
-keywords = ["wheel", "python"]
-
-edition = { workspace = true }
-rust-version = { workspace = true }
-homepage = { workspace = true }
-documentation = { workspace = true }
-repository = { workspace = true }
-authors = { workspace = true }
-license = { workspace = true }
-
-[lints]
-workspace = true
-
-[lib]
-name = "install_wheel_rs"
+name = "uv-requirements"
+version = "0.1.0"
+edition.workspace = true
+rust-version.workspace = true
+homepage.workspace = true
+documentation.workspace = true
+repository.workspace = true
+authors.workspace = true
+license.workspace = true
 
 [dependencies]
-distribution-filename = { path = "../distribution-filename" }
-pep440_rs = { path = "../pep440-rs" }
-platform-host = { path = "../platform-host" }
-uv-normalize = { path = "../uv-normalize" }
-uv-fs = { path = "../uv-fs" }
-pypi-types = { path = "../pypi-types" }
+cache-key = { workspace = true }
+distribution-filename = { workspace = true }
+distribution-types = { workspace = true }
+pep440_rs = { workspace = true }
+pep508_rs = { workspace = true }
+pypi-types = { workspace = true }
+requirements-txt = { workspace = true, features = ["reqwest"] }
+uv-client = { workspace = true }
+uv-configuration = { workspace = true }
+uv-distribution = { workspace = true }
+uv-fs = { workspace = true }
+uv-git = { workspace = true }
+uv-normalize = { workspace = true }
+uv-resolver = { workspace = true, features = ["clap"] }
+uv-types = { workspace = true }
+uv-warnings = { workspace = true }
 
-clap = { workspace = true, optional = true, features = ["derive", "env"] }
+anyhow = { workspace = true }
 configparser = { workspace = true }
-csv = { workspace = true }
-data-encoding = { workspace = true }
-fs-err = { workspace = true }
-fs2 = { workspace = true }
-goblin = { workspace = true }
-mailparse = { workspace = true }
-once_cell = { workspace = true }
-platform-info = { workspace = true }
-plist = { workspace = true }
-pyo3 = { workspace = true, features = ["extension-module", "abi3-py37"], optional = true }
-rayon = { workspace = true, optional = true }
-reflink-copy = { workspace = true }
-regex = { workspace = true }
+console = { workspace = true }
+ctrlc = { workspace = true }
+fs-err = { workspace = true, features = ["tokio"] }
+futures = { workspace = true }
+glob = { workspace = true }
+indexmap = { workspace = true }
+itertools = { workspace = true }
+path-absolutize = { workspace = true }
 rustc-hash = { workspace = true }
-serde = { workspace = true, features = ["derive"] }
-serde_json = { workspace = true }
-sha2 = { workspace = true }
-target-lexicon = { workspace = true }
-tempfile = { workspace = true }
+schemars = { workspace = true, optional = true }
+serde = { workspace = true }
 thiserror = { workspace = true }
+toml = { workspace = true }
 tracing = { workspace = true }
-tracing-subscriber = { workspace = true, optional = true }
 url = { workspace = true }
-walkdir = { workspace = true }
-zip = { workspace = true }
 
 [features]
-default = ["cli", "parallel"]
-python_bindings = ["pyo3", "tracing-subscriber"]
-cli = ["clap"]
-parallel = ["rayon"]
+schemars = ["dep:schemars"]
 
 [dev-dependencies]
-indoc = {version = "2.0.4"}
+indoc = "2.0.5"
+insta = { version = "1.38.0", features = ["filters", "redactions", "json"] }
+regex = { workspace = true }
+
+[lints]
+workspace = true
```

### Comparing `uv-0.1.9/crates/install-wheel-rs/Readme.md` & `uv-0.2.0/crates/install-wheel-rs/Readme.md`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/install-wheel-rs/src/lib.rs` & `uv-0.2.0/crates/install-wheel-rs/src/metadata.rs`

 * *Files 22% similar despite different names*

```diff
@@ -1,105 +1,21 @@
-//! Takes a wheel and installs it into a venv.
-
-use std::io;
 use std::io::{Read, Seek};
-use std::path::PathBuf;
+use std::path::Path;
 use std::str::FromStr;
 
-use platform_info::PlatformInfoError;
-use thiserror::Error;
-use zip::result::ZipError;
+use tracing::warn;
 use zip::ZipArchive;
 
 use distribution_filename::WheelFilename;
-pub use install_location::{normalize_name, InstallLocation, LockedDir};
 use pep440_rs::Version;
-use platform_host::{Arch, Os};
-pub use record::RecordEntry;
-pub use script::Script;
-pub use uninstall::{uninstall_wheel, Uninstall};
-use uv_fs::Normalized;
 use uv_normalize::PackageName;
-pub use wheel::{
-    install_wheel, parse_key_value_file, read_record_file, relative_to, SHEBANG_PYTHON,
-};
-
-mod install_location;
-pub mod linker;
-#[cfg(feature = "python_bindings")]
-mod python_bindings;
-mod record;
-mod script;
-mod uninstall;
-mod wheel;
-
-/// Note: The caller is responsible for adding the path of the wheel we're installing.
-#[derive(Error, Debug)]
-pub enum Error {
-    #[error(transparent)]
-    Io(#[from] io::Error),
-    /// Custom error type to add a path to error reading a file from a zip
-    #[error("Failed to reflink {} to {}", from.normalized_display(), to.normalized_display())]
-    Reflink {
-        from: PathBuf,
-        to: PathBuf,
-        #[source]
-        err: io::Error,
-    },
-    /// Tags/metadata didn't match platform
-    #[error("The wheel is incompatible with the current platform {os} {arch}")]
-    IncompatibleWheel { os: Os, arch: Arch },
-    /// The wheel is broken
-    #[error("The wheel is invalid: {0}")]
-    InvalidWheel(String),
-    /// Doesn't follow file name schema
-    #[error(transparent)]
-    InvalidWheelFileName(#[from] distribution_filename::WheelFilenameError),
-    /// The caller must add the name of the zip file (See note on type).
-    #[error("Failed to read {0} from zip file")]
-    Zip(String, #[source] ZipError),
-    #[error("Failed to run Python subcommand")]
-    PythonSubcommand(#[source] io::Error),
-    #[error("Failed to move data files")]
-    WalkDir(#[from] walkdir::Error),
-    #[error("RECORD file doesn't match wheel contents: {0}")]
-    RecordFile(String),
-    #[error("RECORD file is invalid")]
-    RecordCsv(#[from] csv::Error),
-    #[error("Broken virtualenv: {0}")]
-    BrokenVenv(String),
-    #[error("Unable to create Windows launch for {0} (only x64_64 is supported)")]
-    UnsupportedWindowsArch(&'static str),
-    #[error("Unable to create Windows launcher on non-Windows platform")]
-    NotWindows,
-    #[error("Failed to detect the current platform")]
-    PlatformInfo(#[source] PlatformInfoError),
-    #[error("Invalid version specification, only none or == is supported")]
-    Pep440,
-    #[error("Invalid direct_url.json")]
-    DirectUrlJson(#[from] serde_json::Error),
-    #[error("No .dist-info directory found")]
-    MissingDistInfo,
-    #[error("Cannot uninstall package; RECORD file not found at: {}", _0.normalized_display())]
-    MissingRecord(PathBuf),
-    #[error("Multiple .dist-info directories found: {0}")]
-    MultipleDistInfo(String),
-    #[error("Invalid wheel size")]
-    InvalidSize,
-    #[error("Invalid package name")]
-    InvalidName(#[from] uv_normalize::InvalidNameError),
-    #[error("Invalid package version")]
-    InvalidVersion(#[from] pep440_rs::VersionParseError),
-    #[error("Wheel package name does not match filename: {0} != {1}")]
-    MismatchedName(PackageName, PackageName),
-    #[error("Wheel version does not match filename: {0} != {1}")]
-    MismatchedVersion(Version, Version),
-}
 
-/// Returns `true` if the file is a `METADATA` file in a `dist-info` directory that matches the
+use crate::Error;
+
+/// Returns `true` if the file is a `METADATA` file in a `.dist-info` directory that matches the
 /// wheel filename.
 pub fn is_metadata_entry(path: &str, filename: &WheelFilename) -> bool {
     let Some((dist_info_dir, file)) = path.split_once('/') else {
         return false;
     };
     if file != "METADATA" {
         return false;
@@ -121,47 +37,35 @@
     };
     if version != filename.version {
         return false;
     }
     true
 }
 
-/// Find the `dist-info` directory from a list of files.
-///
-/// The metadata name may be uppercase, while the wheel and dist info names are lowercase, or
-/// the metadata name and the dist info name are lowercase, while the wheel name is uppercase.
-/// Either way, we just search the wheel for the name.
+/// Find the `.dist-info` directory in a zipped wheel.
 ///
 /// Returns the dist info dir prefix without the `.dist-info` extension.
 ///
-/// Reference implementation: <https://github.com/pypa/packaging/blob/2f83540272e79e3fe1f5d42abae8df0c14ddf4c2/src/packaging/utils.py#L146-L172>
-pub fn find_dist_info<'a, T: Copy>(
+/// Reference implementation: <https://github.com/pypa/pip/blob/36823099a9cdd83261fdbc8c1d2a24fa2eea72ca/src/pip/_internal/utils/wheel.py#L38>
+pub fn find_archive_dist_info<'a, T: Copy>(
     filename: &WheelFilename,
     files: impl Iterator<Item = (T, &'a str)>,
 ) -> Result<(T, &'a str), Error> {
     let metadatas: Vec<_> = files
         .filter_map(|(payload, path)| {
             let (dist_info_dir, file) = path.split_once('/')?;
             if file != "METADATA" {
                 return None;
             }
-
-            let dir_stem = dist_info_dir.strip_suffix(".dist-info")?;
-            let (name, version) = dir_stem.rsplit_once('-')?;
-            if PackageName::from_str(name).ok()? != filename.name {
-                return None;
-            }
-
-            if Version::from_str(version).ok()? != filename.version {
-                return None;
-            }
-
-            Some((payload, dir_stem))
+            let dist_info_prefix = dist_info_dir.strip_suffix(".dist-info")?;
+            Some((payload, dist_info_prefix))
         })
         .collect();
+
+    // Like `pip`, assert that there is exactly one `.dist-info` directory.
     let (payload, dist_info_prefix) = match metadatas[..] {
         [] => {
             return Err(Error::MissingDistInfo);
         }
         [(payload, path)] => (payload, path),
         _ => {
             return Err(Error::MultipleDistInfo(
@@ -169,43 +73,130 @@
                     .into_iter()
                     .map(|(_, dist_info_dir)| dist_info_dir.to_string())
                     .collect::<Vec<_>>()
                     .join(", "),
             ));
         }
     };
+
+    // Like `pip`, validate that the `.dist-info` directory is prefixed with the canonical
+    // package name, but only warn if the version is not the normalized version.
+    let Some((name, version)) = dist_info_prefix.rsplit_once('-') else {
+        return Err(Error::MissingDistInfoSegments(dist_info_prefix.to_string()));
+    };
+    if PackageName::from_str(name)? != filename.name {
+        return Err(Error::MissingDistInfoPackageName(
+            dist_info_prefix.to_string(),
+            filename.name.to_string(),
+        ));
+    }
+    if !Version::from_str(version).is_ok_and(|version| version == filename.version) {
+        warn!(
+            "{}",
+            Error::MissingDistInfoVersion(
+                dist_info_prefix.to_string(),
+                filename.version.to_string(),
+            )
+        );
+    }
+
     Ok((payload, dist_info_prefix))
 }
 
-/// Given an archive, read the `dist-info` metadata into a buffer.
-pub fn read_dist_info(
+/// Given an archive, read the `METADATA` from the `.dist-info` directory.
+pub fn read_archive_metadata(
     filename: &WheelFilename,
     archive: &mut ZipArchive<impl Read + Seek + Sized>,
 ) -> Result<Vec<u8>, Error> {
     let dist_info_prefix =
-        find_dist_info(filename, archive.file_names().map(|name| (name, name)))?.1;
+        find_archive_dist_info(filename, archive.file_names().map(|name| (name, name)))?.1;
 
     let mut file = archive
         .by_name(&format!("{dist_info_prefix}.dist-info/METADATA"))
         .map_err(|err| Error::Zip(filename.to_string(), err))?;
 
     #[allow(clippy::cast_possible_truncation)]
     let mut buffer = Vec::with_capacity(file.size() as usize);
     file.read_to_end(&mut buffer)?;
 
     Ok(buffer)
 }
 
+/// Find the `.dist-info` directory in an unzipped wheel.
+///
+/// See: <https://github.com/PyO3/python-pkginfo-rs>
+pub fn find_flat_dist_info(
+    filename: &WheelFilename,
+    path: impl AsRef<Path>,
+) -> Result<String, Error> {
+    // Iterate over `path` to find the `.dist-info` directory. It should be at the top-level.
+    let Some(dist_info_prefix) = fs_err::read_dir(path.as_ref())?.find_map(|entry| {
+        let entry = entry.ok()?;
+        let file_type = entry.file_type().ok()?;
+        if file_type.is_dir() {
+            let path = entry.path();
+
+            let extension = path.extension()?;
+            if extension != "dist-info" {
+                return None;
+            }
+
+            let dist_info_prefix = path.file_stem()?.to_str()?;
+            Some(dist_info_prefix.to_string())
+        } else {
+            None
+        }
+    }) else {
+        return Err(Error::InvalidWheel(
+            "Missing .dist-info directory".to_string(),
+        ));
+    };
+
+    // Like `pip`, validate that the `.dist-info` directory is prefixed with the canonical
+    // package name, but only warn if the version is not the normalized version.
+    let Some((name, version)) = dist_info_prefix.rsplit_once('-') else {
+        return Err(Error::MissingDistInfoSegments(dist_info_prefix.to_string()));
+    };
+    if PackageName::from_str(name)? != filename.name {
+        return Err(Error::MissingDistInfoPackageName(
+            dist_info_prefix.to_string(),
+            filename.name.to_string(),
+        ));
+    }
+    if !Version::from_str(version).is_ok_and(|version| version == filename.version) {
+        warn!(
+            "{}",
+            Error::MissingDistInfoVersion(
+                dist_info_prefix.to_string(),
+                filename.version.to_string(),
+            )
+        );
+    }
+
+    Ok(dist_info_prefix)
+}
+
+/// Read the wheel `METADATA` metadata from a `.dist-info` directory.
+pub fn read_dist_info_metadata(
+    dist_info_prefix: &str,
+    wheel: impl AsRef<Path>,
+) -> Result<Vec<u8>, Error> {
+    let metadata_file = wheel
+        .as_ref()
+        .join(format!("{dist_info_prefix}.dist-info/METADATA"));
+    Ok(fs_err::read(metadata_file)?)
+}
+
 #[cfg(test)]
 mod test {
     use std::str::FromStr;
 
     use distribution_filename::WheelFilename;
 
-    use crate::find_dist_info;
+    use crate::metadata::find_archive_dist_info;
 
     #[test]
     fn test_dot_in_name() {
         let files = [
             "mastodon/Mastodon.py",
             "mastodon/__init__.py",
             "mastodon/streaming.py",
@@ -214,11 +205,11 @@
             "Mastodon.py-1.5.1.dist-info/top_level.txt",
             "Mastodon.py-1.5.1.dist-info/WHEEL",
             "Mastodon.py-1.5.1.dist-info/METADATA",
             "Mastodon.py-1.5.1.dist-info/RECORD",
         ];
         let filename = WheelFilename::from_str("Mastodon.py-1.5.1-py2.py3-none-any.whl").unwrap();
         let (_, dist_info_prefix) =
-            find_dist_info(&filename, files.into_iter().map(|file| (file, file))).unwrap();
+            find_archive_dist_info(&filename, files.into_iter().map(|file| (file, file))).unwrap();
         assert_eq!(dist_info_prefix, "Mastodon.py-1.5.1");
     }
 }
```

### Comparing `uv-0.1.9/crates/install-wheel-rs/src/linker.rs` & `uv-0.2.0/crates/install-wheel-rs/src/linker.rs`

 * *Files 6% similar despite different names*

```diff
@@ -1,144 +1,132 @@
 //! Like `wheel.rs`, but for installing wheels that have already been unzipped, rather than
 //! reading from a zip file.
 
 use std::path::Path;
 use std::str::FromStr;
+use std::time::SystemTime;
 
-use configparser::ini::Ini;
 use fs_err as fs;
 use fs_err::{DirEntry, File};
 use reflink_copy as reflink;
+use serde::{Deserialize, Serialize};
 use tempfile::tempdir_in;
 use tracing::{debug, instrument};
 
 use distribution_filename::WheelFilename;
 use pep440_rs::Version;
 use pypi_types::DirectUrl;
 use uv_normalize::PackageName;
 
-use crate::install_location::InstallLocation;
+use crate::script::{scripts_from_ini, Script};
 use crate::wheel::{
-    extra_dist_info, install_data, parse_metadata, parse_wheel_version, read_scripts_from_section,
-    write_script_entrypoints,
+    extra_dist_info, install_data, parse_metadata, parse_wheel_file, read_record_file,
+    write_script_entrypoints, LibKind,
 };
-use crate::{read_record_file, Error, Script};
+use crate::{Error, Layout};
 
 /// Install the given wheel to the given venv
 ///
 /// The caller must ensure that the wheel is compatible to the environment.
 ///
 /// <https://packaging.python.org/en/latest/specifications/binary-distribution-format/#installing-a-wheel-distribution-1-0-py32-none-any-whl>
 ///
 /// Wheel 1.0: <https://www.python.org/dev/peps/pep-0427/>
-#[instrument(skip_all, fields(wheel = % wheel.as_ref().display()))]
+#[instrument(skip_all, fields(wheel = %filename))]
 pub fn install_wheel(
-    location: &InstallLocation<impl AsRef<Path>>,
+    layout: &Layout,
     wheel: impl AsRef<Path>,
     filename: &WheelFilename,
     direct_url: Option<&DirectUrl>,
     installer: Option<&str>,
     link_mode: LinkMode,
 ) -> Result<(), Error> {
-    let root = location.venv_root();
-
-    // TODO(charlie): Pass this in.
-    let site_packages_python = format!(
-        "python{}.{}",
-        location.python_version().0,
-        location.python_version().1
-    );
-    let site_packages = if cfg!(target_os = "windows") {
-        root.as_ref().join("Lib").join("site-packages")
-    } else {
-        root.as_ref()
-            .join("lib")
-            .join(site_packages_python)
-            .join("site-packages")
-    };
-
     let dist_info_prefix = find_dist_info(&wheel)?;
     let metadata = dist_info_metadata(&dist_info_prefix, &wheel)?;
     let (name, version) = parse_metadata(&dist_info_prefix, &metadata)?;
 
     // Validate the wheel name and version.
     {
         let name = PackageName::from_str(&name)?;
         if name != filename.name {
             return Err(Error::MismatchedName(name, filename.name.clone()));
         }
 
         let version = Version::from_str(&version)?;
-        if version != filename.version {
+        if version != filename.version && version != filename.version.clone().without_local() {
             return Err(Error::MismatchedVersion(version, filename.version.clone()));
         }
     }
 
     // We're going step by step though
     // https://packaging.python.org/en/latest/specifications/binary-distribution-format/#installing-a-wheel-distribution-1-0-py32-none-any-whl
     // > 1.a Parse distribution-1.0.dist-info/WHEEL.
     // > 1.b Check that installer is compatible with Wheel-Version. Warn if minor version is greater, abort if major version is greater.
     let wheel_file_path = wheel
         .as_ref()
         .join(format!("{dist_info_prefix}.dist-info/WHEEL"));
     let wheel_text = fs::read_to_string(wheel_file_path)?;
-    parse_wheel_version(&wheel_text)?;
+    let lib_kind = parse_wheel_file(&wheel_text)?;
 
     // > 1.c If Root-Is-Purelib == ‘true’, unpack archive into purelib (site-packages).
     // > 1.d Else unpack archive into platlib (site-packages).
-    // We always install in the same virtualenv site packages
     debug!(name, "Extracting file");
-    let num_unpacked = link_mode.link_wheel_files(&site_packages, &wheel)?;
+    let site_packages = match lib_kind {
+        LibKind::Pure => &layout.scheme.purelib,
+        LibKind::Plat => &layout.scheme.platlib,
+    };
+    let num_unpacked = link_mode.link_wheel_files(site_packages, &wheel)?;
     debug!(name, "Extracted {num_unpacked} files");
 
     // Read the RECORD file.
     let mut record_file = File::open(
         wheel
             .as_ref()
             .join(format!("{dist_info_prefix}.dist-info/RECORD")),
     )?;
     let mut record = read_record_file(&mut record_file)?;
 
-    debug!(name, "Writing entrypoints");
-    let (console_scripts, gui_scripts) = parse_scripts(&wheel, &dist_info_prefix, None)?;
-    write_script_entrypoints(
-        &site_packages,
-        location,
-        &console_scripts,
-        &mut record,
-        false,
-    )?;
-    write_script_entrypoints(&site_packages, location, &gui_scripts, &mut record, true)?;
+    let (console_scripts, gui_scripts) =
+        parse_scripts(&wheel, &dist_info_prefix, None, layout.python_version.1)?;
+
+    if console_scripts.is_empty() && gui_scripts.is_empty() {
+        debug!(name, "No entrypoints");
+    } else {
+        debug!(name, "Writing entrypoints");
+
+        fs_err::create_dir_all(&layout.scheme.scripts)?;
+        write_script_entrypoints(layout, site_packages, &console_scripts, &mut record, false)?;
+        write_script_entrypoints(layout, site_packages, &gui_scripts, &mut record, true)?;
+    }
 
-    let data_dir = site_packages.join(format!("{dist_info_prefix}.data"));
     // 2.a Unpacked archive includes distribution-1.0.dist-info/ and (if there is data) distribution-1.0.data/.
     // 2.b Move each subtree of distribution-1.0.data/ onto its destination path. Each subdirectory of distribution-1.0.data/ is a key into a dict of destination directories, such as distribution-1.0.data/(purelib|platlib|headers|scripts|data). The initially supported paths are taken from distutils.command.install.
+    let data_dir = site_packages.join(format!("{dist_info_prefix}.data"));
     if data_dir.is_dir() {
         debug!(name, "Installing data");
         install_data(
-            root.as_ref(),
-            &site_packages,
+            layout,
+            site_packages,
             &data_dir,
             &name,
-            location,
             &console_scripts,
             &gui_scripts,
             &mut record,
         )?;
         // 2.c If applicable, update scripts starting with #!python to point to the correct interpreter.
         // Script are unsupported through data
         // 2.e Remove empty distribution-1.0.data directory.
         fs::remove_dir_all(data_dir)?;
     } else {
         debug!(name, "No data");
     }
 
     debug!(name, "Writing extra metadata");
     extra_dist_info(
-        &site_packages,
+        site_packages,
         &dist_info_prefix,
         true,
         direct_url,
         installer,
         &mut record,
     )?;
 
@@ -154,14 +142,16 @@
 
     Ok(())
 }
 
 /// Find the `dist-info` directory in an unzipped wheel.
 ///
 /// See: <https://github.com/PyO3/python-pkginfo-rs>
+///
+/// See: <https://github.com/pypa/pip/blob/36823099a9cdd83261fdbc8c1d2a24fa2eea72ca/src/pip/_internal/utils/wheel.py#L38>
 fn find_dist_info(path: impl AsRef<Path>) -> Result<String, Error> {
     // Iterate over `path` to find the `.dist-info` directory. It should be at the top-level.
     let Some(dist_info) = fs::read_dir(path.as_ref())?.find_map(|entry| {
         let entry = entry.ok()?;
         let file_type = entry.file_type().ok()?;
         if file_type.is_dir() {
             let path = entry.path();
@@ -196,50 +186,37 @@
     Ok(fs::read(metadata_file)?)
 }
 
 /// Parses the `entry_points.txt` entry in the wheel for console scripts
 ///
 /// Returns (`script_name`, module, function)
 ///
-/// Extras are supposed to be ignored, which happens if you pass None for extras
+/// Extras are supposed to be ignored, which happens if you pass None for extras.
 fn parse_scripts(
     wheel: impl AsRef<Path>,
     dist_info_prefix: &str,
     extras: Option<&[String]>,
+    python_minor: u8,
 ) -> Result<(Vec<Script>, Vec<Script>), Error> {
     let entry_points_path = wheel
         .as_ref()
         .join(format!("{dist_info_prefix}.dist-info/entry_points.txt"));
 
     // Read the entry points mapping. If the file doesn't exist, we just return an empty mapping.
     let Ok(ini) = fs::read_to_string(entry_points_path) else {
         return Ok((Vec::new(), Vec::new()));
     };
 
-    let entry_points_mapping = Ini::new_cs()
-        .read(ini)
-        .map_err(|err| Error::InvalidWheel(format!("entry_points.txt is invalid: {err}")))?;
-
-    // TODO: handle extras
-    let console_scripts = match entry_points_mapping.get("console_scripts") {
-        Some(console_scripts) => {
-            read_scripts_from_section(console_scripts, "console_scripts", extras)?
-        }
-        None => Vec::new(),
-    };
-    let gui_scripts = match entry_points_mapping.get("gui_scripts") {
-        Some(gui_scripts) => read_scripts_from_section(gui_scripts, "gui_scripts", extras)?,
-        None => Vec::new(),
-    };
-
-    Ok((console_scripts, gui_scripts))
+    scripts_from_ini(extras, python_minor, ini)
 }
 
-#[derive(Debug, Clone, Copy)]
+#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
+#[serde(deny_unknown_fields, rename_all = "kebab-case")]
 #[cfg_attr(feature = "clap", derive(clap::ValueEnum))]
+#[cfg_attr(feature = "schemars", derive(schemars::JsonSchema))]
 pub enum LinkMode {
     /// Clone (i.e., copy-on-write) packages from the wheel into the site packages.
     Clone,
     /// Copy packages from the wheel into the site packages.
     Copy,
     /// Hard link packages from the wheel into the site packages.
     Hardlink,
@@ -293,14 +270,39 @@
             wheel.as_ref(),
             &entry?,
             &mut attempt,
         )?;
         count += 1;
     }
 
+    // The directory mtime is not updated when cloning and the mtime is used by CPython's
+    // import mechanisms to determine if it should look for new packages in a directory.
+    // Here, we force the mtime to be updated to ensure that packages are importable without
+    // manual cache invalidation.
+    //
+    // <https://github.com/python/cpython/blob/8336cb2b6f428246803b02a4e97fce49d0bb1e09/Lib/importlib/_bootstrap_external.py#L1601>
+    let now = SystemTime::now();
+
+    // `File.set_modified` is not available in `fs_err` yet
+    #[allow(clippy::disallowed_types)]
+    match std::fs::File::open(site_packages.as_ref()) {
+        Ok(dir) => {
+            if let Err(err) = dir.set_modified(now) {
+                debug!(
+                    "Failed to update mtime for {}: {err}",
+                    site_packages.as_ref().display()
+                );
+            }
+        }
+        Err(err) => debug!(
+            "Failed to open {} to update mtime: {err}",
+            site_packages.as_ref().display()
+        ),
+    }
+
     Ok(count)
 }
 
 // Hard linking / reflinking might not be supported but we (afaik) can't detect this ahead of time,
 // so we'll try hard linking / reflinking the first file - if this succeeds we'll know later
 // errors are not due to lack of os/fs support. If it fails, we'll switch to copying for the rest of the
 // install.
@@ -321,14 +323,23 @@
 ) -> Result<(), Error> {
     // Determine the existing and destination paths.
     let from = entry.path();
     let to = site_packages.join(from.strip_prefix(wheel).unwrap());
 
     debug!("Cloning {} to {}", from.display(), to.display());
 
+    if cfg!(windows) && from.is_dir() {
+        // On Windows, reflinking directories is not supported, so we copy each file instead.
+        fs::create_dir_all(&to)?;
+        for entry in fs::read_dir(from)? {
+            clone_recursive(site_packages, wheel, &entry?, attempt)?;
+        }
+        return Ok(());
+    }
+
     match attempt {
         Attempt::Initial => {
             if let Err(err) = reflink::reflink(&from, &to) {
                 if matches!(err.kind(), std::io::ErrorKind::AlreadyExists) {
                     // If cloning/copying fails and the directory exists already, it must be merged recursively.
                     if entry.file_type()?.is_dir() {
                         for entry in fs::read_dir(from)? {
@@ -337,22 +348,25 @@
                     } else {
                         // If file already exists, overwrite it.
                         let tempdir = tempdir_in(site_packages)?;
                         let tempfile = tempdir.path().join(from.file_name().unwrap());
                         if reflink::reflink(&from, &tempfile).is_ok() {
                             fs::rename(&tempfile, to)?;
                         } else {
-                            debug!("Failed to clone {} to temporary location {} - attempting to copy files as a fallback", from.display(), tempfile.display());
+                            debug!(
+                                "Failed to clone `{}` to temporary location `{}`, attempting to copy files as a fallback",
+                                from.display(),
+                                tempfile.display());
                             *attempt = Attempt::UseCopyFallback;
                             fs::copy(&from, &to)?;
                         }
                     }
                 } else {
                     debug!(
-                        "Failed to clone {} to {} - attempting to copy files as a fallback",
+                        "Failed to clone `{}` to `{}`, attempting to copy files as a fallback",
                         from.display(),
                         to.display()
                     );
                     // switch to copy fallback
                     *attempt = Attempt::UseCopyFallback;
                     clone_recursive(site_packages, wheel, entry, attempt)?;
                 }
@@ -467,18 +481,28 @@
                         );
                         // Removing and recreating would lead to race conditions.
                         let tempdir = tempdir_in(&site_packages)?;
                         let tempfile = tempdir.path().join(entry.file_name());
                         if fs::hard_link(path, &tempfile).is_ok() {
                             fs_err::rename(&tempfile, &out_path)?;
                         } else {
+                            debug!(
+                                "Failed to hardlink `{}` to `{}`, attempting to copy files as a fallback",
+                                out_path.display(),
+                                path.display()
+                            );
                             fs::copy(path, &out_path)?;
                             attempt = Attempt::UseCopyFallback;
                         }
                     } else {
+                        debug!(
+                            "Failed to hardlink `{}` to `{}`, attempting to copy files as a fallback",
+                            out_path.display(),
+                            path.display()
+                        );
                         fs::copy(path, &out_path)?;
                         attempt = Attempt::UseCopyFallback;
                     }
                 }
             }
             Attempt::Subsequent => {
                 if let Err(err) = fs::hard_link(path, &out_path) {
```

### Comparing `uv-0.1.9/crates/install-wheel-rs/src/wheel.rs` & `uv-0.2.0/crates/install-wheel-rs/src/wheel.rs`

 * *Files 18% similar despite different names*

```diff
@@ -1,41 +1,29 @@
 use std::collections::HashMap;
-use std::io::{BufRead, BufReader, BufWriter, Cursor, Read, Seek, Write};
+use std::io::{BufRead, BufReader, Cursor, Read, Write};
 use std::path::{Path, PathBuf};
-use std::process::{Command, ExitStatus, Stdio};
-use std::str::FromStr;
 use std::{env, io, iter};
 
-use configparser::ini::Ini;
 use data_encoding::BASE64URL_NOPAD;
 use fs_err as fs;
 use fs_err::{DirEntry, File};
 use mailparse::MailHeaderMap;
-use rustc_hash::{FxHashMap, FxHashSet};
+use rustc_hash::FxHashMap;
 use sha2::{Digest, Sha256};
-use tempfile::tempdir;
-use tracing::{debug, error, instrument, warn};
+use tracing::{instrument, warn};
 use walkdir::WalkDir;
-use zip::result::ZipError;
 use zip::write::FileOptions;
-use zip::{ZipArchive, ZipWriter};
+use zip::ZipWriter;
 
-use distribution_filename::WheelFilename;
-use pep440_rs::Version;
 use pypi_types::DirectUrl;
-use uv_fs::Normalized;
-use uv_normalize::PackageName;
+use uv_fs::Simplified;
 
-use crate::install_location::{InstallLocation, LockedDir};
 use crate::record::RecordEntry;
 use crate::script::Script;
-use crate::{find_dist_info, Error};
-
-/// `#!/usr/bin/env python`
-pub const SHEBANG_PYTHON: &str = "#!/usr/bin/env python";
+use crate::{Error, Layout};
 
 const LAUNCHER_MAGIC_NUMBER: [u8; 4] = [b'U', b'V', b'U', b'V'];
 
 #[cfg(all(windows, target_arch = "x86_64"))]
 const LAUNCHER_X86_64_GUI: &[u8] =
     include_bytes!("../../uv-trampoline/trampolines/uv-trampoline-x86_64-gui.exe");
 
@@ -94,51 +82,14 @@
                 )));
             }
         }
     }
     Ok(scripts)
 }
 
-/// Parses the `entry_points.txt` entry in the wheel for console scripts
-///
-/// Returns (`script_name`, module, function)
-///
-/// Extras are supposed to be ignored, which happens if you pass None for extras
-fn parse_scripts<R: Read + Seek>(
-    archive: &mut ZipArchive<R>,
-    dist_info_dir: &str,
-    extras: Option<&[String]>,
-) -> Result<(Vec<Script>, Vec<Script>), Error> {
-    let entry_points_path = format!("{dist_info_dir}/entry_points.txt");
-    let entry_points_mapping = match archive.by_name(&entry_points_path) {
-        Ok(file) => {
-            let ini_text = std::io::read_to_string(file)?;
-            Ini::new_cs()
-                .read(ini_text)
-                .map_err(|err| Error::InvalidWheel(format!("entry_points.txt is invalid: {err}")))?
-        }
-        Err(ZipError::FileNotFound) => return Ok((Vec::new(), Vec::new())),
-        Err(err) => return Err(Error::Zip(entry_points_path, err)),
-    };
-
-    // TODO: handle extras
-    let console_scripts = match entry_points_mapping.get("console_scripts") {
-        Some(console_scripts) => {
-            read_scripts_from_section(console_scripts, "console_scripts", extras)?
-        }
-        None => Vec::new(),
-    };
-    let gui_scripts = match entry_points_mapping.get("gui_scripts") {
-        Some(gui_scripts) => read_scripts_from_section(gui_scripts, "gui_scripts", extras)?,
-        None => Vec::new(),
-    };
-
-    Ok((console_scripts, gui_scripts))
-}
-
 /// Shamelessly stolen (and updated for recent sha2)
 /// <https://github.com/richo/hashing-copy/blob/d8dd2fdb63c6faf198de0c9e5713d6249cbb5323/src/lib.rs#L10-L52>
 /// which in turn got it from std
 /// <https://doc.rust-lang.org/1.58.0/src/std/io/copy.rs.html#128-156>
 fn copy_and_hash(reader: &mut impl Read, writer: &mut impl Write) -> io::Result<(u64, String)> {
     // TODO: Do we need to support anything besides sha256?
     let mut hasher = Sha256::new();
@@ -159,147 +110,53 @@
     }
     Ok((
         written,
         format!("sha256={}", BASE64URL_NOPAD.encode(&hasher.finalize())),
     ))
 }
 
-/// Extract all files from the wheel into the site packages
+/// Format the shebang for a given Python executable.
 ///
-/// Matches with the RECORD entries
+/// Like pip, if a shebang is non-simple (too long or contains spaces), we use `/bin/sh` as the
+/// executable.
 ///
-/// Returns paths relative to site packages
-fn unpack_wheel_files<R: Read + Seek>(
-    site_packages: &Path,
-    record_path: &str,
-    archive: &mut ZipArchive<R>,
-    record: &[RecordEntry],
-    check_hashes: bool,
-) -> Result<Vec<PathBuf>, Error> {
-    let mut extracted_paths = Vec::new();
-    // Cache the created parent dirs to avoid io calls
-    // When deactivating bytecode compilation and sha2 those were 5% of total runtime, with
-    // cache it 2.3%
-    let mut created_dirs = FxHashSet::default();
-    // https://github.com/zip-rs/zip/blob/7edf2489d5cff8b80f02ee6fc5febf3efd0a9442/examples/extract.rs
-    for i in 0..archive.len() {
-        let mut file = archive.by_index(i).map_err(|err| {
-            let file1 = format!("(index {i})");
-            Error::Zip(file1, err)
-        })?;
-        // enclosed_name takes care of evil zip paths
-        let relative = match file.enclosed_name() {
-            Some(path) => path.to_owned(),
-            None => continue,
-        };
-        let out_path = site_packages.join(&relative);
+/// See: <https://github.com/pypa/pip/blob/0ad4c94be74cc24874c6feb5bb3c2152c398a18e/src/pip/_vendor/distlib/scripts.py#L136-L165>
+fn format_shebang(executable: impl AsRef<Path>, os_name: &str) -> String {
+    // Convert the executable to a simplified path.
+    let executable = executable.as_ref().simplified_display().to_string();
 
-        if file.name().ends_with('/') {
-            // pip seems to do ignore those folders, so do we
-            // fs::create_dir_all(&out_path)?;
-            continue;
-        }
+    // Validate the shebang.
+    if os_name == "posix" {
+        // The length of the full line: the shebang, plus the leading `#` and `!`, and a trailing
+        // newline.
+        let shebang_length = 2 + executable.len() + 1;
 
-        if let Some(parent) = out_path.parent() {
-            if created_dirs.insert(parent.to_path_buf()) {
-                fs::create_dir_all(parent)?;
-            }
-        }
-        let mut outfile = BufWriter::new(File::create(&out_path)?);
-        let encoded_hash = if check_hashes {
-            let (_size, encoded_hash) = copy_and_hash(&mut file, &mut outfile)?;
-            Some(encoded_hash)
-        } else {
-            io::copy(&mut file, &mut outfile)?;
-            None
-        };
-
-        extracted_paths.push(relative.clone());
-
-        // Get and Set permissions
-        #[cfg(unix)]
-        {
-            use std::fs::Permissions;
-            use std::os::unix::fs::PermissionsExt;
-
-            if let Some(mode) = file.unix_mode() {
-                fs::set_permissions(&out_path, Permissions::from_mode(mode))?;
-            }
-        }
-
-        // This is the RECORD file that contains the hashes so naturally it can't contain it's own
-        // hash and size (but it does contain an entry with two empty fields)
-        // > 6. RECORD.jws is used for digital signatures. It is not mentioned in RECORD.
-        // > 7. RECORD.p7s is allowed as a courtesy to anyone who would prefer to use S/MIME
-        // >    signatures to secure their wheel files. It is not mentioned in RECORD.
-        let record_path = PathBuf::from(&record_path);
-        if [
-            record_path.clone(),
-            record_path.with_extension("jws"),
-            record_path.with_extension("p7s"),
-        ]
-        .contains(&relative)
-        {
-            continue;
-        }
-
-        if let Some(encoded_hash) = encoded_hash {
-            // `relative == Path::new(entry.path)` was really slow
-            let relative_str = relative.display().to_string();
-            let recorded_hash = record
-                .iter()
-                .find(|entry| relative_str == entry.path)
-                .and_then(|entry| entry.hash.as_ref())
-                .ok_or_else(|| {
-                    Error::RecordFile(format!(
-                        "Missing hash for {} (expected {})",
-                        relative.normalized_display(),
-                        encoded_hash
-                    ))
-                })?;
-            if recorded_hash != &encoded_hash {
-                if relative.as_os_str().to_string_lossy().starts_with("torch-") {
-                    error!(
-                        "Hash mismatch for {}. Recorded: {}, Actual: {}",
-                        relative.normalized_display(),
-                        recorded_hash,
-                        encoded_hash,
-                    );
-                    error!(
-                        "Torch isn't capable of producing correct hashes 🙄 Ignoring. \
-                    https://github.com/pytorch/pytorch/issues/47916"
-                    );
-                    continue;
-                }
-                return Err(Error::RecordFile(format!(
-                    "Hash mismatch for {}. Recorded: {}, Actual: {}",
-                    relative.normalized_display(),
-                    recorded_hash,
-                    encoded_hash,
-                )));
-            }
+        // If the shebang is too long, or contains spaces, wrap it in `/bin/sh`.
+        if shebang_length > 127 || executable.contains(' ') {
+            // Like Python's `shlex.quote`:
+            // > Use single quotes, and put single quotes into double quotes
+            // > The string $'b is then quoted as '$'"'"'b'
+            let executable = format!("'{}'", executable.replace('\'', r#"'"'"'"#));
+            return format!("#!/bin/sh\n'''exec' {executable} \"$0\" \"$@\"\n' '''");
         }
     }
-    Ok(extracted_paths)
-}
 
-fn get_shebang(location: &InstallLocation<impl AsRef<Path>>) -> String {
-    format!("#!{}", location.python().normalized().display())
+    format!("#!{executable}")
 }
 
 /// A Windows script is a minimal .exe launcher binary with the python entrypoint script appended as
 /// stored zip file. The launcher will look for `python[w].exe` adjacent to it in the same directory
 /// to start the embedded script.
 ///
 /// <https://github.com/pypa/pip/blob/fd0ea6bc5e8cb95e518c23d901c26ca14db17f89/src/pip/_vendor/distlib/scripts.py#L248-L262>
 #[allow(unused_variables)]
 pub(crate) fn windows_script_launcher(
     launcher_python_script: &str,
     is_gui: bool,
-    installation: &InstallLocation<impl AsRef<Path>>,
+    python_executable: impl AsRef<Path>,
 ) -> Result<Vec<u8>, Error> {
     // This method should only be called on Windows, but we avoid `#[cfg(windows)]` to retain
     // compilation on all platforms.
     if cfg!(not(windows)) {
         return Err(Error::NotWindows);
     }
 
@@ -339,16 +196,16 @@
         archive.start_file("__main__.py", stored).expect(error_msg);
         archive
             .write_all(launcher_python_script.as_bytes())
             .expect(error_msg);
         archive.finish().expect(error_msg);
     }
 
-    let python = installation.python();
-    let python_path = python.normalized().to_string_lossy();
+    let python = python_executable.as_ref();
+    let python_path = python.simplified_display().to_string();
 
     let mut launcher: Vec<u8> = Vec::with_capacity(launcher_bin.len() + payload.len());
     launcher.extend_from_slice(launcher_bin);
     launcher.extend_from_slice(&payload);
     launcher.extend_from_slice(python_path.as_bytes());
     launcher.extend_from_slice(
         &u32::try_from(python_path.as_bytes().len())
@@ -356,50 +213,61 @@
             .to_le_bytes(),
     );
     launcher.extend_from_slice(&LAUNCHER_MAGIC_NUMBER);
 
     Ok(launcher)
 }
 
-/// Create the wrapper scripts in the bin folder of the venv for launching console scripts
-///
-/// We also pass `venv_base` so we can write the same path as pip does
-///
-/// TODO: Test for this launcher directly in install-wheel-rs
+/// Create the wrapper scripts in the bin folder of the venv for launching console scripts.
 pub(crate) fn write_script_entrypoints(
+    layout: &Layout,
     site_packages: &Path,
-    location: &InstallLocation<impl AsRef<Path>>,
     entrypoints: &[Script],
     record: &mut Vec<RecordEntry>,
     is_gui: bool,
 ) -> Result<(), Error> {
     for entrypoint in entrypoints {
-        let entrypoint_relative = if cfg!(windows) {
+        let entrypoint_absolute = if cfg!(windows) {
             // On windows we actually build an .exe wrapper
             let script_name = entrypoint
-                .script_name
+                .name
                 // FIXME: What are the in-reality rules here for names?
                 .strip_suffix(".py")
-                .unwrap_or(&entrypoint.script_name)
+                .unwrap_or(&entrypoint.name)
                 .to_string()
                 + ".exe";
-            bin_rel().join(script_name)
+
+            layout.scheme.scripts.join(script_name)
         } else {
-            bin_rel().join(&entrypoint.script_name)
+            layout.scheme.scripts.join(&entrypoint.name)
         };
 
+        let entrypoint_relative = pathdiff::diff_paths(&entrypoint_absolute, site_packages)
+            .ok_or_else(|| {
+                Error::Io(io::Error::new(
+                    io::ErrorKind::Other,
+                    format!(
+                        "Could not find relative path for: {}",
+                        entrypoint_absolute.simplified_display()
+                    ),
+                ))
+            })?;
+
         // Generate the launcher script.
-        let launcher_python_script = get_script_launcher(entrypoint, &get_shebang(location));
+        let launcher_python_script = get_script_launcher(
+            entrypoint,
+            &format_shebang(&layout.sys_executable, &layout.os_name),
+        );
 
         // If necessary, wrap the launcher script in a Windows launcher binary.
         if cfg!(windows) {
             write_file_recorded(
                 site_packages,
                 &entrypoint_relative,
-                &windows_script_launcher(&launcher_python_script, is_gui, location)?,
+                &windows_script_launcher(&launcher_python_script, is_gui, &layout.sys_executable)?,
                 record,
             )?;
         } else {
             write_file_recorded(
                 site_packages,
                 &entrypoint_relative,
                 &launcher_python_script,
@@ -416,32 +284,43 @@
                 )?;
             }
         }
     }
     Ok(())
 }
 
-fn bin_rel() -> PathBuf {
-    if cfg!(windows) {
-        // windows doesn't have the python part, only Lib/site-packages
-        Path::new("..").join("..").join("Scripts")
-    } else {
-        // linux/mac has lib/python/site-packages
-        Path::new("..").join("..").join("..").join("bin")
-    }
+/// Whether the wheel should be installed into the `purelib` or `platlib` directory.
+#[derive(Debug, Clone, Copy, PartialEq, Eq)]
+pub(crate) enum LibKind {
+    /// Install into the `purelib` directory.
+    Pure,
+    /// Install into the `platlib` directory.
+    Plat,
 }
 
-/// Parse WHEEL file
+/// Parse WHEEL file.
 ///
 /// > {distribution}-{version}.dist-info/WHEEL is metadata about the archive itself in the same
 /// > basic key: value format:
-pub(crate) fn parse_wheel_version(wheel_text: &str) -> Result<(), Error> {
+pub(crate) fn parse_wheel_file(wheel_text: &str) -> Result<LibKind, Error> {
     // {distribution}-{version}.dist-info/WHEEL is metadata about the archive itself in the same basic key: value format:
     let data = parse_key_value_file(&mut wheel_text.as_bytes(), "WHEEL")?;
 
+    // Determine whether Root-Is-Purelib == ‘true’.
+    // If it is, the wheel is pure, and should be installed into purelib.
+    let root_is_purelib = data
+        .get("Root-Is-Purelib")
+        .and_then(|root_is_purelib| root_is_purelib.first())
+        .is_some_and(|root_is_purelib| root_is_purelib == "true");
+    let lib_kind = if root_is_purelib {
+        LibKind::Pure
+    } else {
+        LibKind::Plat
+    };
+
     // mkl_fft-1.3.6-58-cp310-cp310-manylinux2014_x86_64.whl has multiple Wheel-Version entries, we have to ignore that
     // like pip
     let wheel_version = data
         .get("Wheel-Version")
         .and_then(|wheel_versions| wheel_versions.first());
     let wheel_version = wheel_version
         .and_then(|wheel_version| wheel_version.split_once('.'))
@@ -450,15 +329,15 @@
                 "Invalid Wheel-Version in WHEEL file: {wheel_version:?}"
             ))
         })?;
     // pip has some test wheels that use that ancient version,
     // and technically we only need to check that the version is not higher
     if wheel_version == ("0", "1") {
         warn!("Ancient wheel version 0.1 (expected is 1.0)");
-        return Ok(());
+        return Ok(lib_kind);
     }
     // Check that installer is compatible with Wheel-Version. Warn if minor version is greater, abort if major version is greater.
     // Wheel-Version: 1.0
     if wheel_version.0 != "1" {
         return Err(Error::InvalidWheel(format!(
             "Unsupported wheel major version (expected {}, got {})",
             1, wheel_version.0
@@ -466,182 +345,38 @@
     }
     if wheel_version.1 > "0" {
         warn!(
             "Warning: Unsupported wheel minor version (expected {}, got {})",
             0, wheel_version.1
         );
     }
-    Ok(())
-}
-
-/// Call `python -m compileall` to generate pyc file for the installed code
-///
-/// 2.f Compile any installed .py to .pyc. (Uninstallers should be smart enough to remove .pyc
-/// even if it is not mentioned in RECORD.)
-#[instrument(skip_all)]
-fn bytecode_compile(
-    site_packages: &Path,
-    unpacked_paths: Vec<PathBuf>,
-    python_version: (u8, u8),
-    sys_executable: &Path,
-    // Only for logging
-    name: &str,
-    record: &mut Vec<RecordEntry>,
-) -> Result<(), Error> {
-    // https://github.com/pypa/pip/blob/b5457dfee47dd9e9f6ec45159d9d410ba44e5ea1/src/pip/_internal/operations/install/wheel.py#L592-L603
-    let py_source_paths: Vec<_> = unpacked_paths
-        .into_iter()
-        .filter(|path| {
-            path.extension()
-                .is_some_and(|ext| ext.eq_ignore_ascii_case("py"))
-                && site_packages.join(path).is_file()
-        })
-        .collect();
-
-    // bytecode compiling crashes non-deterministically with various errors, from syntax errors
-    // to cpython segmentation faults, so we add a simple retry loop
-    let mut retries = 3;
-    let (status, lines) = loop {
-        let (status, lines) =
-            bytecode_compile_inner(site_packages, &py_source_paths, sys_executable)?;
-        retries -= 1;
-        if status.success() || retries == 0 {
-            break (status, lines);
-        }
-
-        warn!("Failed to compile {name} with python compileall, retrying",);
-    };
-    if !status.success() {
-        // lossy because we want the error reporting to survive c̴̞̏ü̸̜̹̈́ŕ̴͉̈ś̷̤ė̵̤͋d̷͙̄ filenames in the zip
-        return Err(Error::PythonSubcommand(io::Error::new(
-            io::ErrorKind::Other,
-            format!("Failed to run python compileall, log above: {status}"),
-        )));
-    }
-
-    // like pip, we just ignored all that failed to compile
-    // Add each that succeeded to the RECORD
-    for py_path in lines {
-        let py_path = py_path.trim();
-        if py_path.is_empty() {
-            continue;
-        }
-        let py_path = Path::new(py_path);
-        let pyc_path = py_path
-            .parent()
-            .unwrap_or_else(|| Path::new(""))
-            .join("__pycache__")
-            // Unwrap is save because we checked for an extension before
-            .join(py_path.file_name().unwrap())
-            .with_extension(format!(
-                "cpython-{}{}.pyc",
-                python_version.0, python_version.1
-            ));
-        if !site_packages.join(&pyc_path).is_file() {
-            return Err(Error::PythonSubcommand(io::Error::new(
-                io::ErrorKind::NotFound,
-                format!(
-                    "Didn't find pyc generated by compileall: {}",
-                    site_packages.join(&pyc_path).normalized_display()
-                ),
-            )));
-        }
-        // 2.d Update distribution-1.0.dist-info/RECORD with the installed paths.
-
-        // https://www.python.org/dev/peps/pep-0376/#record
-        // > [..] a hash of the file's contents. Notice that pyc and pyo generated files don't have
-        // > any hash because they are automatically produced from py files. So checking the hash of
-        // > the corresponding py file is enough to decide if the file and its associated pyc or pyo
-        // > files have changed.
-        record.push(RecordEntry {
-            path: pyc_path.display().to_string(),
-            hash: None,
-            size: None,
-        });
-    }
-
-    Ok(())
-}
-
-/// The actual command part which we repeat if it fails
-fn bytecode_compile_inner(
-    site_packages: &Path,
-    py_source_paths: &[PathBuf],
-    sys_executable: &Path,
-) -> Result<(ExitStatus, Vec<String>), Error> {
-    let temp_dir = tempdir()?;
-    // Running python with an actual file will produce better error messages
-    let pip_compileall_py = temp_dir.path().join("pip_compileall.py");
-    fs::write(&pip_compileall_py, include_str!("pip_compileall.py"))?;
-    // We input the paths through stdin and get the successful paths returned through stdout
-    let mut bytecode_compiler = Command::new(sys_executable)
-        .arg(&pip_compileall_py)
-        .stdin(Stdio::piped())
-        .stdout(Stdio::piped())
-        .stderr(Stdio::inherit())
-        .current_dir(site_packages.normalized())
-        .spawn()
-        .map_err(Error::PythonSubcommand)?;
-
-    // https://stackoverflow.com/questions/49218599/write-to-child-process-stdin-in-rust/49597789#comment120223107_49597789
-    let mut child_stdin = bytecode_compiler
-        .stdin
-        .take()
-        .expect("Child must have stdin");
-
-    // Pass paths newline terminated to compileall
-    for path in py_source_paths {
-        debug!("bytecode compiling {}", path.display());
-        // There is no OsStr -> Bytes conversion on windows :o
-        // https://stackoverflow.com/questions/43083544/how-can-i-convert-osstr-to-u8-vecu8-on-windows
-        writeln!(&mut child_stdin, "{}", path.display()).map_err(Error::PythonSubcommand)?;
-    }
-    // Close stdin to finish and avoid indefinite blocking
-    drop(child_stdin);
-
-    // Already read stdout here to avoid it running full (pipes are limited)
-    let stdout = bytecode_compiler.stdout.take().unwrap();
-    let mut lines: Vec<String> = Vec::new();
-    for line in BufReader::new(stdout).lines() {
-        let line = line.map_err(|err| {
-            Error::PythonSubcommand(io::Error::new(
-                io::ErrorKind::Other,
-                format!("Invalid utf-8 returned by python compileall: {err}"),
-            ))
-        })?;
-        lines.push(line);
-    }
-
-    let output = bytecode_compiler
-        .wait_with_output()
-        .map_err(Error::PythonSubcommand)?;
-    Ok((output.status, lines))
+    Ok(lib_kind)
 }
 
 /// Give the path relative to the base directory
 ///
 /// lib/python/site-packages/foo/__init__.py and lib/python/site-packages -> foo/__init__.py
 /// lib/marker.txt and lib/python/site-packages -> ../../marker.txt
 /// `bin/foo_launcher` and lib/python/site-packages -> ../../../`bin/foo_launcher`
-pub fn relative_to(path: &Path, base: &Path) -> Result<PathBuf, Error> {
+pub(crate) fn relative_to(path: &Path, base: &Path) -> Result<PathBuf, Error> {
     // Find the longest common prefix, and also return the path stripped from that prefix
     let (stripped, common_prefix) = base
         .ancestors()
         .find_map(|ancestor| {
             path.strip_prefix(ancestor)
                 .ok()
                 .map(|stripped| (stripped, ancestor))
         })
         .ok_or_else(|| {
             Error::Io(io::Error::new(
                 io::ErrorKind::Other,
                 format!(
                     "Trivial strip failed: {} vs. {}",
-                    path.normalized_display(),
-                    base.normalized_display()
+                    path.simplified_display(),
+                    base.simplified_display()
                 ),
             ))
         })?;
 
     // go as many levels up as required
     let levels_up = base.components().count() - common_prefix.components().count();
     let up = iter::repeat("..").take(levels_up).collect::<PathBuf>();
@@ -675,41 +410,51 @@
             fs::rename(src, &target)?;
             let entry = record
                 .iter_mut()
                 .find(|entry| Path::new(&entry.path) == relative_to_site_packages)
                 .ok_or_else(|| {
                     Error::RecordFile(format!(
                         "Could not find entry for {} ({})",
-                        relative_to_site_packages.normalized_display(),
-                        src.normalized_display()
+                        relative_to_site_packages.simplified_display(),
+                        src.simplified_display()
                     ))
                 })?;
             entry.path = relative_to(&target, site_packages)?.display().to_string();
         }
     }
     Ok(())
 }
 
 /// Installs a single script (not an entrypoint)
 ///
 /// Has to deal with both binaries files (just move) and scripts (rewrite the shebang if applicable)
 fn install_script(
+    layout: &Layout,
     site_packages: &Path,
     record: &mut [RecordEntry],
     file: &DirEntry,
-    location: &InstallLocation<impl AsRef<Path>>,
 ) -> Result<(), Error> {
     if !file.file_type()?.is_file() {
         return Err(Error::InvalidWheel(format!(
             "Wheel contains entry in scripts directory that is not a file: {}",
             file.path().display()
         )));
     }
 
-    let target_path = bin_rel().join(file.file_name());
+    let script_absolute = layout.scheme.scripts.join(file.file_name());
+    let script_relative =
+        pathdiff::diff_paths(&script_absolute, site_packages).ok_or_else(|| {
+            Error::Io(io::Error::new(
+                io::ErrorKind::Other,
+                format!(
+                    "Could not find relative path for: {}",
+                    script_absolute.simplified_display()
+                ),
+            ))
+        })?;
 
     let path = file.path();
     let mut script = File::open(&path)?;
 
     // https://sphinx-locales.github.io/peps/pep-0427/#recommended-installer-features
     // > In wheel, scripts are packaged in {distribution}-{version}.data/scripts/.
     // > If the first line of a file in scripts/ starts with exactly b'#!python',
@@ -719,119 +464,120 @@
     // > The b'#!pythonw' convention is allowed. b'#!pythonw' indicates a GUI script
     // > instead of a console script.
     let placeholder_python = b"#!python";
     // scripts might be binaries, so we read an exact number of bytes instead of the first line as string
     let mut start = vec![0; placeholder_python.len()];
     script.read_exact(&mut start)?;
     let size_and_encoded_hash = if start == placeholder_python {
-        let start = get_shebang(location).as_bytes().to_vec();
-        let mut target = File::create(site_packages.join(&target_path))?;
+        let start = format_shebang(&layout.sys_executable, &layout.os_name)
+            .as_bytes()
+            .to_vec();
+        let mut target = File::create(&script_absolute)?;
         let size_and_encoded_hash = copy_and_hash(&mut start.chain(script), &mut target)?;
         fs::remove_file(&path)?;
         Some(size_and_encoded_hash)
     } else {
         // reading and writing is slow especially for large binaries, so we move them instead
         drop(script);
-        fs::rename(&path, site_packages.join(&target_path))?;
+        fs::rename(&path, &script_absolute)?;
         None
     };
     #[cfg(unix)]
     {
         use std::fs::Permissions;
         use std::os::unix::fs::PermissionsExt;
 
-        fs::set_permissions(
-            site_packages.join(&target_path),
-            Permissions::from_mode(0o755),
-        )?;
+        fs::set_permissions(&script_absolute, Permissions::from_mode(0o755))?;
     }
 
+    // Find the existing entry in the `RECORD`.
     let relative_to_site_packages = path
         .strip_prefix(site_packages)
         .expect("Prefix must no change");
     let entry = record
         .iter_mut()
         .find(|entry| Path::new(&entry.path) == relative_to_site_packages)
         .ok_or_else(|| {
             // This should be possible to occur at this point, but filesystems and such
             Error::RecordFile(format!(
                 "Could not find entry for {} ({})",
-                relative_to_site_packages.normalized_display(),
-                path.normalized_display()
+                relative_to_site_packages.simplified_display(),
+                path.simplified_display()
             ))
         })?;
-    entry.path = target_path.display().to_string();
+
+    // Update the entry in the `RECORD`.
+    entry.path = script_relative.simplified_display().to_string();
     if let Some((size, encoded_hash)) = size_and_encoded_hash {
         entry.size = Some(size);
         entry.hash = Some(encoded_hash);
     }
     Ok(())
 }
 
 /// Move the files from the .data directory to the right location in the venv
 #[allow(clippy::too_many_arguments)]
 #[instrument(skip_all)]
 pub(crate) fn install_data(
-    venv_root: &Path,
+    layout: &Layout,
     site_packages: &Path,
     data_dir: &Path,
     dist_name: &str,
-    location: &InstallLocation<impl AsRef<Path>>,
     console_scripts: &[Script],
     gui_scripts: &[Script],
     record: &mut [RecordEntry],
 ) -> Result<(), Error> {
     for entry in fs::read_dir(data_dir)? {
         let entry = entry?;
         let path = entry.path();
 
         match path.file_name().and_then(|name| name.to_str()) {
             Some("data") => {
                 // Move the content of the folder to the root of the venv
-                move_folder_recorded(&path, venv_root, site_packages, record)?;
+                move_folder_recorded(&path, &layout.scheme.data, site_packages, record)?;
             }
             Some("scripts") => {
+                let mut initialized = false;
                 for file in fs::read_dir(path)? {
                     let file = file?;
 
                     // Couldn't find any docs for this, took it directly from
                     // https://github.com/pypa/pip/blob/b5457dfee47dd9e9f6ec45159d9d410ba44e5ea1/src/pip/_internal/operations/install/wheel.py#L565-L583
                     let name = file.file_name().to_string_lossy().to_string();
                     let match_name = name
                         .strip_suffix(".exe")
                         .or_else(|| name.strip_suffix("-script.py"))
                         .or_else(|| name.strip_suffix(".pya"))
                         .unwrap_or(&name);
                     if console_scripts
                         .iter()
                         .chain(gui_scripts)
-                        .any(|script| script.script_name == match_name)
+                        .any(|script| script.name == match_name)
                     {
                         continue;
                     }
 
-                    install_script(site_packages, record, &file, location)?;
+                    // Create the scripts directory, if it doesn't exist.
+                    if !initialized {
+                        fs::create_dir_all(&layout.scheme.scripts)?;
+                        initialized = true;
+                    }
+
+                    install_script(layout, site_packages, record, &file)?;
                 }
             }
             Some("headers") => {
-                let target_path = venv_root
-                    .join("include")
-                    .join("site")
-                    .join(format!(
-                        "python{}.{}",
-                        location.python_version().0,
-                        location.python_version().1
-                    ))
-                    .join(dist_name);
+                let target_path = layout.scheme.include.join(dist_name);
                 move_folder_recorded(&path, &target_path, site_packages, record)?;
             }
-            Some("purelib" | "platlib") => {
-                // purelib and platlib locations are not relevant when using venvs
-                // https://stackoverflow.com/a/27882460/3549270
-                move_folder_recorded(&path, site_packages, site_packages, record)?;
+            Some("purelib") => {
+                move_folder_recorded(&path, &layout.scheme.purelib, site_packages, record)?;
+            }
+            Some("platlib") => {
+                move_folder_recorded(&path, &layout.scheme.platlib, site_packages, record)?;
             }
             _ => {
                 return Err(Error::InvalidWheel(format!(
                     "Unknown wheel data type: {:?}",
                     entry.file_name()
                 )));
             }
@@ -846,14 +592,20 @@
 /// site packages because we must only record the relative path in RECORD
 pub(crate) fn write_file_recorded(
     site_packages: &Path,
     relative_path: &Path,
     content: impl AsRef<[u8]>,
     record: &mut Vec<RecordEntry>,
 ) -> Result<(), Error> {
+    debug_assert!(
+        !relative_path.is_absolute(),
+        "Path must be relative: {}",
+        relative_path.display()
+    );
+
     File::create(site_packages.join(relative_path))?.write_all(content.as_ref())?;
     let hash = Sha256::new().chain_update(content.as_ref()).finalize();
     let encoded_hash = format!("sha256={}", BASE64URL_NOPAD.encode(&hash));
     record.push(RecordEntry {
         path: relative_path.display().to_string(),
         hash: Some(encoded_hash),
         size: Some(content.as_ref().len() as u64),
@@ -867,20 +619,14 @@
     dist_info_prefix: &str,
     requested: bool,
     direct_url: Option<&DirectUrl>,
     installer: Option<&str>,
     record: &mut Vec<RecordEntry>,
 ) -> Result<(), Error> {
     let dist_info_dir = PathBuf::from(format!("{dist_info_prefix}.dist-info"));
-    write_file_recorded(
-        site_packages,
-        &dist_info_dir.join("INSTALLER"),
-        env!("CARGO_PKG_NAME"),
-        record,
-    )?;
     if requested {
         write_file_recorded(site_packages, &dist_info_dir.join("REQUESTED"), "", record)?;
     }
     if let Some(direct_url) = direct_url {
         write_file_recorded(
             site_packages,
             &dist_info_dir.join("direct_url.json"),
@@ -897,15 +643,15 @@
         )?;
     }
     Ok(())
 }
 
 /// Reads the record file
 /// <https://www.python.org/dev/peps/pep-0376/#record>
-pub fn read_record_file(record: &mut impl Read) -> Result<Vec<RecordEntry>, Error> {
+pub(crate) fn read_record_file(record: &mut impl Read) -> Result<Vec<RecordEntry>, Error> {
     csv::ReaderBuilder::new()
         .has_headers(false)
         .escape(Some(b'"'))
         .from_reader(record)
         .deserialize()
         .map(|entry| {
             let entry: RecordEntry = entry?;
@@ -915,224 +661,39 @@
                 ..entry
             })
         })
         .collect()
 }
 
 /// Parse a file with `Key: value` entries such as WHEEL and METADATA
-pub fn parse_key_value_file(
-    file: &mut impl Read,
+fn parse_key_value_file(
+    file: impl Read,
     debug_filename: &str,
 ) -> Result<FxHashMap<String, Vec<String>>, Error> {
     let mut data: FxHashMap<String, Vec<String>> = FxHashMap::default();
 
     let file = BufReader::new(file);
     for (line_no, line) in file.lines().enumerate() {
         let line = line?.trim().to_string();
         if line.is_empty() {
             continue;
         }
-        let (key, value) = line.split_once(": ").ok_or_else(|| {
+        let (key, value) = line.split_once(':').ok_or_else(|| {
             Error::InvalidWheel(format!(
-                "Line {line_no} of the {debug_filename} file is invalid"
+                "Line {} of the {debug_filename} file is invalid",
+                line_no + 1
             ))
         })?;
-        data.entry(key.to_string())
+        data.entry(key.trim().to_string())
             .or_default()
-            .push(value.to_string());
+            .push(value.trim().to_string());
     }
     Ok(data)
 }
 
-/// Install the given wheel to the given venv
-///
-/// The caller must ensure that the wheel is compatible to the environment.
-///
-/// <https://packaging.python.org/en/latest/specifications/binary-distribution-format/#installing-a-wheel-distribution-1-0-py32-none-any-whl>
-///
-/// Wheel 1.0: <https://www.python.org/dev/peps/pep-0427/>
-#[allow(clippy::too_many_arguments)]
-#[instrument(skip_all, fields(name = % filename.name))]
-pub fn install_wheel(
-    location: &InstallLocation<LockedDir>,
-    reader: impl Read + Seek,
-    filename: &WheelFilename,
-    direct_url: Option<&DirectUrl>,
-    installer: Option<&str>,
-    compile: bool,
-    check_hashes: bool,
-    // initially used to the console scripts, currently unused. Keeping it because we likely need
-    // it for validation later.
-    _extras: &[String],
-    sys_executable: impl AsRef<Path>,
-) -> Result<String, Error> {
-    let name = &filename.name;
-
-    let base_location = location.venv_root();
-
-    let site_packages_python = format!(
-        "python{}.{}",
-        location.python_version().0,
-        location.python_version().1
-    );
-    let site_packages = if cfg!(target_os = "windows") {
-        base_location.as_ref().join("Lib").join("site-packages")
-    } else {
-        base_location
-            .as_ref()
-            .join("lib")
-            .join(site_packages_python)
-            .join("site-packages")
-    };
-
-    debug!(name = name.as_ref(), "Opening zip");
-    // No BufReader: https://github.com/zip-rs/zip/issues/381
-    let mut archive = ZipArchive::new(reader).map_err(|err| {
-        let file = "(index)".to_string();
-        Error::Zip(file, err)
-    })?;
-
-    debug!(name = name.as_ref(), "Getting wheel metadata");
-    let dist_info_prefix = find_dist_info(filename, archive.file_names().map(|name| (name, name)))?
-        .1
-        .to_string();
-    let metadata = dist_info_metadata(&dist_info_prefix, &mut archive)?;
-    let (name, version) = parse_metadata(&dist_info_prefix, &metadata)?;
-
-    // Validate the wheel name and version.
-    {
-        let name = PackageName::from_str(&name)?;
-        if name != filename.name {
-            return Err(Error::MismatchedName(name, filename.name.clone()));
-        }
-
-        let version = Version::from_str(&version)?;
-        if version != filename.version {
-            return Err(Error::MismatchedVersion(version, filename.version.clone()));
-        }
-    }
-
-    let record_path = format!("{dist_info_prefix}.dist-info/RECORD");
-    let mut record = read_record_file(&mut archive.by_name(&record_path).map_err(|err| {
-        let file = record_path.clone();
-        Error::Zip(file, err)
-    })?)?;
-
-    // We're going step by step though
-    // https://packaging.python.org/en/latest/specifications/binary-distribution-format/#installing-a-wheel-distribution-1-0-py32-none-any-whl
-    // > 1.a Parse distribution-1.0.dist-info/WHEEL.
-    // > 1.b Check that installer is compatible with Wheel-Version. Warn if minor version is greater, abort if major version is greater.
-    let wheel_file_path = format!("{dist_info_prefix}.dist-info/WHEEL");
-    let wheel_file = archive
-        .by_name(&wheel_file_path)
-        .map_err(|err| Error::Zip(wheel_file_path, err))?;
-    let wheel_text = io::read_to_string(wheel_file)?;
-    parse_wheel_version(&wheel_text)?;
-    // > 1.c If Root-Is-Purelib == ‘true’, unpack archive into purelib (site-packages).
-    // > 1.d Else unpack archive into platlib (site-packages).
-    // We always install in the same virtualenv site packages
-    debug!(name = name.as_str(), "Extracting file");
-    let unpacked_paths = unpack_wheel_files(
-        &site_packages,
-        &record_path,
-        &mut archive,
-        &record,
-        check_hashes,
-    )?;
-    debug!(
-        name = name.as_str(),
-        "Extracted {} files",
-        unpacked_paths.len()
-    );
-
-    debug!(name = name.as_str(), "Writing entrypoints");
-    let (console_scripts, gui_scripts) = parse_scripts(&mut archive, &dist_info_prefix, None)?;
-    write_script_entrypoints(
-        &site_packages,
-        location,
-        &console_scripts,
-        &mut record,
-        false,
-    )?;
-    write_script_entrypoints(&site_packages, location, &gui_scripts, &mut record, true)?;
-
-    let data_dir = site_packages.join(format!("{dist_info_prefix}.data"));
-    // 2.a Unpacked archive includes distribution-1.0.dist-info/ and (if there is data) distribution-1.0.data/.
-    // 2.b Move each subtree of distribution-1.0.data/ onto its destination path. Each subdirectory of distribution-1.0.data/ is a key into a dict of destination directories, such as distribution-1.0.data/(purelib|platlib|headers|scripts|data). The initially supported paths are taken from distutils.command.install.
-    if data_dir.is_dir() {
-        debug!(name = name.as_str(), "Installing data");
-        install_data(
-            base_location.as_ref(),
-            &site_packages,
-            &data_dir,
-            &name,
-            location,
-            &console_scripts,
-            &gui_scripts,
-            &mut record,
-        )?;
-        // 2.c If applicable, update scripts starting with #!python to point to the correct interpreter.
-        // Script are unsupported through data
-        // 2.e Remove empty distribution-1.0.data directory.
-        fs::remove_dir_all(data_dir)?;
-    } else {
-        debug!(name = name.as_str(), "No data");
-    }
-
-    // 2.f Compile any installed .py to .pyc. (Uninstallers should be smart enough to remove .pyc even if it is not mentioned in RECORD.)
-    if compile {
-        debug!(name = name.as_str(), "Bytecode compiling");
-        bytecode_compile(
-            &site_packages,
-            unpacked_paths,
-            location.python_version(),
-            sys_executable.as_ref(),
-            name.as_str(),
-            &mut record,
-        )?;
-    }
-
-    debug!(name = name.as_str(), "Writing extra metadata");
-
-    extra_dist_info(
-        &site_packages,
-        &dist_info_prefix,
-        true,
-        direct_url,
-        installer,
-        &mut record,
-    )?;
-
-    debug!(name = name.as_str(), "Writing record");
-    let mut record_writer = csv::WriterBuilder::new()
-        .has_headers(false)
-        .escape(b'"')
-        .from_path(site_packages.join(record_path))?;
-    record.sort();
-    for entry in record {
-        record_writer.serialize(entry)?;
-    }
-
-    Ok(filename.get_tag())
-}
-
-/// Read the `dist-info` metadata from a wheel archive.
-fn dist_info_metadata(
-    dist_info_prefix: &str,
-    archive: &mut ZipArchive<impl Read + Seek + Sized>,
-) -> Result<Vec<u8>, Error> {
-    let mut content = Vec::new();
-    let dist_info_file = format!("{dist_info_prefix}.dist-info/METADATA");
-    archive
-        .by_name(&dist_info_file)
-        .map_err(|err| Error::Zip(dist_info_file.clone(), err))?
-        .read_to_end(&mut content)?;
-    Ok(content)
-}
-
 /// Parse the distribution name and version from a wheel's `dist-info` metadata.
 ///
 /// See: <https://github.com/PyO3/python-pkginfo-rs>
 pub(crate) fn parse_metadata(
     dist_info_prefix: &str,
     content: &[u8],
 ) -> Result<(String, String), Error> {
@@ -1170,19 +731,23 @@
             "No `Version` field in: {dist_info_prefix}.dist-info/METADATA"
         )))?;
     Ok((name, version))
 }
 
 #[cfg(test)]
 mod test {
+    use std::io::Cursor;
     use std::path::Path;
 
     use indoc::{formatdoc, indoc};
 
-    use super::{parse_key_value_file, parse_wheel_version, read_record_file, relative_to, Script};
+    use crate::wheel::format_shebang;
+    use crate::Error;
+
+    use super::{parse_key_value_file, parse_wheel_file, read_record_file, relative_to, Script};
 
     #[test]
     fn test_parse_key_value_file() {
         let text = indoc! {"
             Wheel-Version: 1.0
             Generator: bdist_wheel (0.37.1)
             Root-Is-Purelib: false
@@ -1202,16 +767,16 @@
                 Root-Is-Purelib: true
                 Tag: py2-none-any
                 Tag: py3-none-any
                 ",
                 version
             }
         }
-        parse_wheel_version(&wheel_with_version("1.0")).unwrap();
-        parse_wheel_version(&wheel_with_version("2.0")).unwrap_err();
+        parse_wheel_file(&wheel_with_version("1.0")).unwrap();
+        parse_wheel_file(&wheel_with_version("2.0")).unwrap_err();
     }
 
     #[test]
     fn record_with_absolute_paths() {
         let record: &str = indoc! {"
             /selenium/__init__.py,sha256=l8nEsTP4D2dZVula_p4ZuCe8AGnxOq7MxMeAWNvR0Qc,811
             /selenium/common/exceptions.py,sha256=oZx2PS-g1gYLqJA_oqzE4Rq4ngplqlwwRBZDofiqni0,9309
@@ -1264,28 +829,28 @@
     }
 
     #[test]
     fn test_script_from_value() {
         assert_eq!(
             Script::from_value("launcher", "foo.bar:main", None).unwrap(),
             Some(Script {
-                script_name: "launcher".to_string(),
+                name: "launcher".to_string(),
                 module: "foo.bar".to_string(),
                 function: "main".to_string(),
             })
         );
         assert_eq!(
             Script::from_value(
                 "launcher",
                 "foo.bar:main",
                 Some(&["bar".to_string(), "baz".to_string()]),
             )
             .unwrap(),
             Some(Script {
-                script_name: "launcher".to_string(),
+                name: "launcher".to_string(),
                 module: "foo.bar".to_string(),
                 function: "main".to_string(),
             })
         );
         assert_eq!(
             Script::from_value("launcher", "foomod:main_bar [bar,baz]", Some(&[])).unwrap(),
             None
@@ -1294,22 +859,87 @@
             Script::from_value(
                 "launcher",
                 "foomod:main_bar [bar,baz]",
                 Some(&["bar".to_string(), "baz".to_string()]),
             )
             .unwrap(),
             Some(Script {
-                script_name: "launcher".to_string(),
+                name: "launcher".to_string(),
                 module: "foomod".to_string(),
                 function: "main_bar".to_string(),
             })
         );
     }
 
     #[test]
+    fn test_shebang() {
+        // By default, use a simple shebang.
+        let executable = Path::new("/usr/bin/python3");
+        let os_name = "posix";
+        assert_eq!(format_shebang(executable, os_name), "#!/usr/bin/python3");
+
+        // If the path contains spaces, we should use the `exec` trick.
+        let executable = Path::new("/usr/bin/path to python3");
+        let os_name = "posix";
+        assert_eq!(
+            format_shebang(executable, os_name),
+            "#!/bin/sh\n'''exec' '/usr/bin/path to python3' \"$0\" \"$@\"\n' '''"
+        );
+
+        // Except on Windows...
+        let executable = Path::new("/usr/bin/path to python3");
+        let os_name = "nt";
+        assert_eq!(
+            format_shebang(executable, os_name),
+            "#!/usr/bin/path to python3"
+        );
+
+        // Quotes, however, are ok.
+        let executable = Path::new("/usr/bin/'python3'");
+        let os_name = "posix";
+        assert_eq!(format_shebang(executable, os_name), "#!/usr/bin/'python3'");
+
+        // If the path is too long, we should not use the `exec` trick.
+        let executable = Path::new("/usr/bin/path/to/a/very/long/executable/executable/executable/executable/executable/executable/executable/executable/name/python3");
+        let os_name = "posix";
+        assert_eq!(format_shebang(executable, os_name), "#!/bin/sh\n'''exec' '/usr/bin/path/to/a/very/long/executable/executable/executable/executable/executable/executable/executable/executable/name/python3' \"$0\" \"$@\"\n' '''");
+    }
+
+    #[test]
+    fn test_empty_value() -> Result<(), Error> {
+        let wheel = indoc! {r"
+        Wheel-Version: 1.0
+        Generator: custom
+        Root-Is-Purelib: false
+        Tag:
+        Tag: -manylinux_2_17_x86_64
+        Tag: -manylinux2014_x86_64
+        "
+        };
+        let reader = Cursor::new(wheel.to_string().into_bytes());
+        let wheel_file = parse_key_value_file(reader, "WHEEL")?;
+        assert_eq!(
+            wheel_file.get("Wheel-Version"),
+            Some(&["1.0".to_string()].to_vec())
+        );
+        assert_eq!(
+            wheel_file.get("Tag"),
+            Some(
+                &[
+                    String::new(),
+                    "-manylinux_2_17_x86_64".to_string(),
+                    "-manylinux2014_x86_64".to_string()
+                ]
+                .to_vec()
+            )
+        );
+        Ok(())
+    }
+
+    #[test]
     #[cfg(all(windows, target_arch = "x86_64"))]
     fn test_launchers_are_small() {
         // At time of writing, they are 15872 bytes.
         assert!(
             super::LAUNCHER_X86_64_GUI.len() < 20 * 1024,
             "GUI launcher: {}",
             super::LAUNCHER_X86_64_GUI.len()
```

### Comparing `uv-0.1.9/crates/uv-build/Cargo.toml` & `uv-0.2.0/crates/install-wheel-rs/Cargo.toml`

 * *Files 27% similar despite different names*

```diff
@@ -1,43 +1,53 @@
 [package]
-name = "uv-build"
+name = "install-wheel-rs"
 version = "0.0.1"
-description = "Build wheels from source distributions"
+publish = false
+description = "Takes a wheel and installs it, either in a venv or for monotrail"
+keywords = ["wheel", "python"]
+
 edition = { workspace = true }
 rust-version = { workspace = true }
 homepage = { workspace = true }
 documentation = { workspace = true }
 repository = { workspace = true }
 authors = { workspace = true }
 license = { workspace = true }
 
 [lints]
 workspace = true
 
+[lib]
+name = "install_wheel_rs"
+
 [dependencies]
-distribution-types = { path = "../distribution-types" }
-gourgeist = { path = "../gourgeist" }
-pep508_rs = { path = "../pep508-rs" }
-platform-host = { path = "../platform-host" }
-uv-extract = { path = "../uv-extract" }
-uv-fs = { path = "../uv-fs" }
-uv-interpreter = { path = "../uv-interpreter" }
-uv-traits = { path = "../uv-traits", features = ["serde"] }
-pypi-types = { path = "../pypi-types" }
+distribution-filename = { workspace = true }
+pep440_rs = { workspace = true }
+platform-tags = { workspace = true }
+uv-normalize = { workspace = true }
+uv-fs = { workspace = true }
+pypi-types = { workspace = true }
 
-anyhow = { workspace = true }
+clap = { workspace = true, optional = true, features = ["derive"] }
+configparser = { workspace = true }
+csv = { workspace = true }
+data-encoding = { workspace = true }
 fs-err = { workspace = true }
-indoc = { workspace = true }
-itertools = { workspace = true }
+mailparse = { workspace = true }
 once_cell = { workspace = true }
-pyproject-toml = { workspace = true }
+pathdiff = { workspace = true }
+platform-info = { workspace = true }
+reflink-copy = { workspace = true }
 regex = { workspace = true }
-serde = { workspace = true }
+rustc-hash = { workspace = true }
+schemars = { workspace = true, optional = true }
+serde = { workspace = true, features = ["derive"] }
 serde_json = { workspace = true }
+sha2 = { workspace = true }
 tempfile = { workspace = true }
 thiserror = { workspace = true }
-tokio = { workspace = true, features = ["sync", "process"] }
-toml = { workspace = true }
 tracing = { workspace = true }
+walkdir = { workspace = true }
+zip = { workspace = true }
 
 [dev-dependencies]
-insta = { version = "1.34.0" }
+indoc = { version = "2.0.4" }
```

### Comparing `uv-0.1.9/crates/uv-build/src/lib.rs` & `uv-0.2.0/crates/uv-build/src/lib.rs`

 * *Files 16% similar despite different names*

```diff
@@ -1,391 +1,548 @@
 //! Build wheels from source distributions
 //!
 //! <https://packaging.python.org/en/latest/specifications/source-distribution-format/>
 
 use std::ffi::OsString;
 use std::fmt::{Display, Formatter};
 use std::io;
-use std::io::BufRead;
 use std::path::{Path, PathBuf};
-use std::process::Output;
+use std::process::{ExitStatus, Output};
+use std::rc::Rc;
 use std::str::FromStr;
-use std::sync::Arc;
 use std::{env, iter};
 
 use fs_err as fs;
 use indoc::formatdoc;
 use itertools::Itertools;
 use once_cell::sync::Lazy;
-use pyproject_toml::{BuildSystem, Project};
 use regex::Regex;
-use serde::{Deserialize, Serialize};
+use rustc_hash::FxHashMap;
+use serde::de::{value, SeqAccess, Visitor};
+use serde::{de, Deserialize, Deserializer};
 use tempfile::{tempdir_in, TempDir};
 use thiserror::Error;
 use tokio::process::Command;
-use tokio::sync::Mutex;
+use tokio::sync::{Mutex, Semaphore};
 use tracing::{debug, info_span, instrument, Instrument};
 
-use distribution_types::Resolution;
-use pep508_rs::Requirement;
-use uv_fs::Normalized;
-use uv_interpreter::{Interpreter, Virtualenv};
-use uv_traits::{BuildContext, BuildKind, ConfigSettings, SetupPyStrategy, SourceBuildTrait};
+use distribution_types::{ParsedUrlError, Requirement, Resolution};
+use pep440_rs::Version;
+use pep508_rs::PackageName;
+use uv_configuration::{BuildKind, ConfigSettings, SetupPyStrategy};
+use uv_fs::{PythonExt, Simplified};
+use uv_interpreter::{Interpreter, PythonEnvironment};
+use uv_types::{BuildContext, BuildIsolation, SourceBuildTrait};
 
 /// e.g. `pygraphviz/graphviz_wrap.c:3020:10: fatal error: graphviz/cgraph.h: No such file or directory`
-static MISSING_HEADER_RE: Lazy<Regex> = Lazy::new(|| {
+static MISSING_HEADER_RE_GCC: Lazy<Regex> = Lazy::new(|| {
     Regex::new(
         r".*\.(?:c|c..|h|h..):\d+:\d+: fatal error: (.*\.(?:h|h..)): No such file or directory",
     )
     .unwrap()
 });
 
+/// e.g. `pygraphviz/graphviz_wrap.c:3023:10: fatal error: 'graphviz/cgraph.h' file not found`
+static MISSING_HEADER_RE_CLANG: Lazy<Regex> = Lazy::new(|| {
+    Regex::new(r".*\.(?:c|c..|h|h..):\d+:\d+: fatal error: '(.*\.(?:h|h..))' file not found")
+        .unwrap()
+});
+
+/// e.g. `pygraphviz/graphviz_wrap.c(3023): fatal error C1083: Cannot open include file: 'graphviz/cgraph.h': No such file or directory`
+static MISSING_HEADER_RE_MSVC: Lazy<Regex> = Lazy::new(|| {
+    Regex::new(r".*\.(?:c|c..|h|h..)\(\d+\): fatal error C1083: Cannot open include file: '(.*\.(?:h|h..))': No such file or directory")
+        .unwrap()
+});
+
 /// e.g. `/usr/bin/ld: cannot find -lncurses: No such file or directory`
 static LD_NOT_FOUND_RE: Lazy<Regex> = Lazy::new(|| {
     Regex::new(r"/usr/bin/ld: cannot find -l([a-zA-Z10-9]+): No such file or directory").unwrap()
 });
 
+/// e.g. `error: invalid command 'bdist_wheel'`
+static WHEEL_NOT_FOUND_RE: Lazy<Regex> =
+    Lazy::new(|| Regex::new(r"error: invalid command 'bdist_wheel'").unwrap());
+
 /// The default backend to use when PEP 517 is used without a `build-system` section.
 static DEFAULT_BACKEND: Lazy<Pep517Backend> = Lazy::new(|| Pep517Backend {
     backend: "setuptools.build_meta:__legacy__".to_string(),
     backend_path: None,
-    requirements: vec![
-        Requirement::from_str("wheel").unwrap(),
-        Requirement::from_str("setuptools >= 40.8.0").unwrap(),
-    ],
+    requirements: vec![Requirement::from_pep508(
+        pep508_rs::Requirement::from_str("setuptools >= 40.8.0").unwrap(),
+    )
+    .unwrap()],
+});
+
+/// The requirements for `--legacy-setup-py` builds.
+static SETUP_PY_REQUIREMENTS: Lazy<[Requirement; 2]> = Lazy::new(|| {
+    [
+        Requirement::from_pep508(pep508_rs::Requirement::from_str("setuptools >= 40.8.0").unwrap())
+            .unwrap(),
+        Requirement::from_pep508(pep508_rs::Requirement::from_str("wheel").unwrap()).unwrap(),
+    ]
 });
 
 #[derive(Error, Debug)]
 pub enum Error {
     #[error(transparent)]
     IO(#[from] io::Error),
-    #[error("Failed to extract archive: {0}")]
-    Extraction(PathBuf, #[source] uv_extract::Error),
-    #[error("Unsupported archive format (extension not recognized): {0}")]
-    UnsupportedArchiveType(String),
     #[error("Invalid source distribution: {0}")]
     InvalidSourceDist(String),
-    #[error("Invalid pyproject.toml")]
+    #[error("Invalid `pyproject.toml`")]
     InvalidPyprojectToml(#[from] toml::de::Error),
     #[error("Editable installs with setup.py legacy builds are unsupported, please specify a build backend in pyproject.toml")]
     EditableSetupPy,
     #[error("Failed to install requirements from {0}")]
     RequirementsInstall(&'static str, #[source] anyhow::Error),
-    #[error("Source distribution not found at: {0}")]
-    NotFound(PathBuf),
     #[error("Failed to create temporary virtualenv")]
-    Gourgeist(#[from] gourgeist::Error),
+    Virtualenv(#[from] uv_virtualenv::Error),
     #[error("Failed to run {0}")]
     CommandFailed(PathBuf, #[source] io::Error),
-    #[error("{message}:\n--- stdout:\n{stdout}\n--- stderr:\n{stderr}\n---")]
+    #[error("{message} with {exit_code}\n--- stdout:\n{stdout}\n--- stderr:\n{stderr}\n---")]
     BuildBackend {
         message: String,
+        exit_code: ExitStatus,
         stdout: String,
         stderr: String,
     },
     /// Nudge the user towards installing the missing dev library
-    #[error("{message}:\n--- stdout:\n{stdout}\n--- stderr:\n{stderr}\n---")]
+    #[error("{message} with {exit_code}\n--- stdout:\n{stdout}\n--- stderr:\n{stderr}\n---")]
     MissingHeader {
         message: String,
+        exit_code: ExitStatus,
         stdout: String,
         stderr: String,
         #[source]
         missing_header_cause: MissingHeaderCause,
     },
     #[error("Failed to build PATH for build script")]
     BuildScriptPath(#[source] env::JoinPathsError),
+    #[error("Failed to parse requirements from build backend")]
+    DirectUrl(#[source] Box<ParsedUrlError>),
 }
 
 #[derive(Debug)]
 pub enum MissingLibrary {
     Header(String),
     Linker(String),
+    PythonPackage(String),
 }
 
 #[derive(Debug, Error)]
 pub struct MissingHeaderCause {
     missing_library: MissingLibrary,
-    package_id: String,
+    version_id: String,
 }
 
 impl Display for MissingHeaderCause {
     fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
         match &self.missing_library {
             MissingLibrary::Header(header) => {
                 write!(
                     f,
                     "This error likely indicates that you need to install a library that provides \"{}\" for {}",
-                    header, self.package_id
+                    header, self.version_id
                 )
             }
             MissingLibrary::Linker(library) => {
                 write!(
                     f,
                     "This error likely indicates that you need to install the library that provides a shared library \
-                    for {library} for {package_id} (e.g. lib{library}-dev)",
-                    library = library, package_id = self.package_id
+                    for {library} for {version_id} (e.g. lib{library}-dev)",
+                    library = library, version_id = self.version_id
+                )
+            }
+            MissingLibrary::PythonPackage(package) => {
+                write!(
+                    f,
+                    "This error likely indicates that you need to `uv pip install {package}` into the build environment for {version_id}",
+                    package = package, version_id = self.version_id
                 )
             }
         }
     }
 }
 
 impl Error {
     fn from_command_output(
         message: String,
         output: &Output,
-        package_id: impl Into<String>,
+        version_id: impl Into<String>,
     ) -> Self {
         let stdout = String::from_utf8_lossy(&output.stdout).trim().to_string();
         let stderr = String::from_utf8_lossy(&output.stderr).trim().to_string();
 
         // In the cases i've seen it was the 5th and 3rd last line (see test case), 10 seems like a reasonable cutoff
         let missing_library = stderr.lines().rev().take(10).find_map(|line| {
-            if let Some((_, [header])) =
-                MISSING_HEADER_RE.captures(line.trim()).map(|c| c.extract())
+            if let Some((_, [header])) = MISSING_HEADER_RE_GCC
+                .captures(line.trim())
+                .or(MISSING_HEADER_RE_CLANG.captures(line.trim()))
+                .or(MISSING_HEADER_RE_MSVC.captures(line.trim()))
+                .map(|c| c.extract())
             {
                 Some(MissingLibrary::Header(header.to_string()))
             } else if let Some((_, [library])) =
                 LD_NOT_FOUND_RE.captures(line.trim()).map(|c| c.extract())
             {
                 Some(MissingLibrary::Linker(library.to_string()))
+            } else if WHEEL_NOT_FOUND_RE.is_match(line.trim()) {
+                Some(MissingLibrary::PythonPackage("wheel".to_string()))
             } else {
                 None
             }
         });
 
         if let Some(missing_library) = missing_library {
             return Self::MissingHeader {
                 message,
+                exit_code: output.status,
                 stdout,
                 stderr,
                 missing_header_cause: MissingHeaderCause {
                     missing_library,
-                    package_id: package_id.into(),
+                    version_id: version_id.into(),
                 },
             };
         }
 
         Self::BuildBackend {
             message,
+            exit_code: output.status,
             stdout,
             stderr,
         }
     }
 }
 
-/// A pyproject.toml as specified in PEP 517
-#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq)]
+/// A `pyproject.toml` as specified in PEP 517.
+#[derive(Deserialize, Debug, Clone, PartialEq, Eq)]
 #[serde(rename_all = "kebab-case")]
 pub struct PyProjectToml {
     /// Build-related data
     pub build_system: Option<BuildSystem>,
     /// Project metadata
     pub project: Option<Project>,
 }
 
+/// The `[project]` section of a pyproject.toml as specified in PEP 621.
+///
+/// This representation only includes a subset of the fields defined in PEP 621 necessary for
+/// informing wheel builds.
+#[derive(Deserialize, Debug, Clone, PartialEq, Eq)]
+#[serde(rename_all = "kebab-case")]
+pub struct Project {
+    /// The name of the project
+    pub name: PackageName,
+    /// The version of the project as supported by PEP 440
+    pub version: Option<Version>,
+    /// Specifies which fields listed by PEP 621 were intentionally unspecified so another tool
+    /// can/will provide such metadata dynamically.
+    pub dynamic: Option<Vec<String>>,
+}
+
+/// The `[build-system]` section of a pyproject.toml as specified in PEP 517.
+#[derive(Deserialize, Debug, Clone, PartialEq, Eq)]
+#[serde(rename_all = "kebab-case")]
+pub struct BuildSystem {
+    /// PEP 508 dependencies required to execute the build system.
+    pub requires: Vec<pep508_rs::Requirement>,
+    /// A string naming a Python object that will be used to perform the build.
+    pub build_backend: Option<String>,
+    /// Specify that their backend code is hosted in-tree, this key contains a list of directories.
+    pub backend_path: Option<BackendPath>,
+}
+
+impl BackendPath {
+    /// Return an iterator over the paths in the backend path.
+    fn iter(&self) -> impl Iterator<Item = &str> {
+        self.0.iter().map(String::as_str)
+    }
+}
+
+#[derive(Debug, Clone, PartialEq, Eq)]
+pub struct BackendPath(Vec<String>);
+
+impl<'de> Deserialize<'de> for BackendPath {
+    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
+    where
+        D: Deserializer<'de>,
+    {
+        struct StringOrVec;
+
+        impl<'de> Visitor<'de> for StringOrVec {
+            type Value = Vec<String>;
+
+            fn expecting(&self, formatter: &mut Formatter) -> std::fmt::Result {
+                formatter.write_str("list of strings")
+            }
+
+            fn visit_str<E>(self, s: &str) -> Result<Self::Value, E>
+            where
+                E: de::Error,
+            {
+                // Allow exactly `backend-path = "."`, as used in `flit_core==2.3.0`.
+                if s == "." {
+                    Ok(vec![".".to_string()])
+                } else {
+                    Err(de::Error::invalid_value(de::Unexpected::Str(s), &self))
+                }
+            }
+
+            fn visit_seq<S>(self, seq: S) -> Result<Self::Value, S::Error>
+            where
+                S: SeqAccess<'de>,
+            {
+                Deserialize::deserialize(value::SeqAccessDeserializer::new(seq))
+            }
+        }
+
+        deserializer.deserialize_any(StringOrVec).map(BackendPath)
+    }
+}
+
 /// `[build-backend]` from pyproject.toml
 #[derive(Debug, Clone, PartialEq, Eq)]
 struct Pep517Backend {
     /// The build backend string such as `setuptools.build_meta:__legacy__` or `maturin` from
     /// `build-backend.backend` in pyproject.toml
     ///
     /// <https://peps.python.org/pep-0517/#build-wheel>
     backend: String,
     /// `build-backend.requirements` in pyproject.toml
     requirements: Vec<Requirement>,
     /// <https://peps.python.org/pep-0517/#in-tree-build-backends>
-    backend_path: Option<Vec<String>>,
+    backend_path: Option<BackendPath>,
 }
 
 impl Pep517Backend {
     fn backend_import(&self) -> String {
         let import = if let Some((path, object)) = self.backend.split_once(':') {
             format!("from {path} import {object} as backend")
         } else {
             format!("import {} as backend", self.backend)
         };
 
         let backend_path_encoded = self
             .backend_path
-            .clone()
-            .unwrap_or_default()
             .iter()
+            .flat_map(BackendPath::iter)
             .map(|path| {
                 // Turn into properly escaped python string
                 '"'.to_string()
                     + &path.replace('\\', "\\\\").replace('"', "\\\"")
                     + &'"'.to_string()
             })
             .join(", ");
 
         // > Projects can specify that their backend code is hosted in-tree by including the
         // > backend-path key in pyproject.toml. This key contains a list of directories, which the
         // > frontend will add to the start of sys.path when loading the backend, and running the
         // > backend hooks.
         formatdoc! {r#"
             import sys
+
+            if sys.path[0] == "":
+                sys.path.pop(0)
+
             sys.path = [{backend_path}] + sys.path
 
             {import}
         "#, backend_path = backend_path_encoded}
     }
 }
 
-/// Uses an [`Arc`] internally, clone freely
+/// Uses an [`Rc`] internally, clone freely.
 #[derive(Debug, Default, Clone)]
 pub struct SourceBuildContext {
-    /// Cache the first resolution of `pip`, `setuptools` and `wheel` we made for setup.py (and
-    /// some PEP 517) builds so we can reuse it.
-    setup_py_resolution: Arc<Mutex<Option<Resolution>>>,
+    /// An in-memory resolution of the default backend's requirements for PEP 517 builds.
+    default_resolution: Rc<Mutex<Option<Resolution>>>,
+    /// An in-memory resolution of the build requirements for `--legacy-setup-py` builds.
+    setup_py_resolution: Rc<Mutex<Option<Resolution>>>,
 }
 
 /// Holds the state through a series of PEP 517 frontend to backend calls or a single setup.py
 /// invocation.
 ///
 /// This keeps both the temp dir and the result of a potential `prepare_metadata_for_build_wheel`
 /// call which changes how we call `build_wheel`.
 pub struct SourceBuild {
     temp_dir: TempDir,
     source_tree: PathBuf,
     config_settings: ConfigSettings,
     /// If performing a PEP 517 build, the backend to use.
     pep517_backend: Option<Pep517Backend>,
+    /// The PEP 621 project metadata, if any.
+    project: Option<Project>,
     /// The virtual environment in which to build the source distribution.
-    venv: Virtualenv,
+    venv: PythonEnvironment,
     /// Populated if `prepare_metadata_for_build_wheel` was called.
     ///
-    /// > If the build frontend has previously called prepare_metadata_for_build_wheel and depends
+    /// > If the build frontend has previously called `prepare_metadata_for_build_wheel` and depends
     /// > on the wheel resulting from this call to have metadata matching this earlier call, then
-    /// > it should provide the path to the created .dist-info directory as the metadata_directory
-    /// > argument. If this argument is provided, then build_wheel MUST produce a wheel with
+    /// > it should provide the path to the created .dist-info directory as the `metadata_directory`
+    /// > argument. If this argument is provided, then `build_wheel` MUST produce a wheel with
     /// > identical metadata. The directory passed in by the build frontend MUST be identical to the
-    /// > directory created by prepare_metadata_for_build_wheel, including any unrecognized files
+    /// > directory created by `prepare_metadata_for_build_wheel`, including any unrecognized files
     /// > it created.
     metadata_directory: Option<PathBuf>,
     /// Package id such as `foo-1.2.3`, for error reporting
-    package_id: String,
+    version_id: String,
     /// Whether we do a regular PEP 517 build or an PEP 660 editable build
     build_kind: BuildKind,
+    /// Modified PATH that contains the `venv_bin`, `user_path` and `system_path` variables in that order
+    modified_path: OsString,
+    /// Environment variables to be passed in during metadata or wheel building
+    environment_variables: FxHashMap<OsString, OsString>,
+    /// Runner for Python scripts.
+    runner: PythonRunner,
 }
 
 impl SourceBuild {
     /// Create a virtual environment in which to build a source distribution, extracting the
     /// contents from an archive if necessary.
     ///
     /// `source_dist` is for error reporting only.
     #[allow(clippy::too_many_arguments)]
     pub async fn setup(
         source: &Path,
         subdirectory: Option<&Path>,
         interpreter: &Interpreter,
         build_context: &impl BuildContext,
         source_build_context: SourceBuildContext,
-        package_id: String,
+        version_id: String,
         setup_py: SetupPyStrategy,
         config_settings: ConfigSettings,
+        build_isolation: BuildIsolation<'_>,
         build_kind: BuildKind,
-    ) -> Result<SourceBuild, Error> {
+        mut environment_variables: FxHashMap<OsString, OsString>,
+        concurrent_builds: usize,
+    ) -> Result<Self, Error> {
         let temp_dir = tempdir_in(build_context.cache().root())?;
 
-        let metadata = match fs::metadata(source) {
-            Ok(metadata) => metadata,
-            Err(err) if err.kind() == io::ErrorKind::NotFound => {
-                return Err(Error::NotFound(source.to_path_buf()));
-            }
-            Err(err) => return Err(err.into()),
-        };
-
-        let source_root = if metadata.is_dir() {
-            source.to_path_buf()
-        } else {
-            debug!("Unpacking for build: {}", source.display());
-
-            let extracted = temp_dir.path().join("extracted");
-
-            // Unzip the archive into the temporary directory.
-            let reader = fs_err::tokio::File::open(source).await?;
-            uv_extract::stream::archive(tokio::io::BufReader::new(reader), source, &extracted)
-                .await
-                .map_err(|err| Error::Extraction(extracted.clone(), err))?;
-
-            // Extract the top-level directory from the archive.
-            match uv_extract::strip_component(&extracted) {
-                Ok(top_level) => top_level,
-                Err(uv_extract::Error::NonSingularArchive(_)) => extracted,
-                Err(err) => return Err(Error::Extraction(extracted.clone(), err)),
-            }
-        };
         let source_tree = if let Some(subdir) = subdirectory {
-            source_root.join(subdir)
+            source.join(subdir)
         } else {
-            source_root
+            source.to_path_buf()
         };
 
         let default_backend: Pep517Backend = DEFAULT_BACKEND.clone();
 
         // Check if we have a PEP 517 build backend.
-        let pep517_backend = Self::get_pep517_backend(setup_py, &source_tree, &default_backend)
-            .map_err(|err| *err)?;
-
-        let venv = gourgeist::create_venv(
-            &temp_dir.path().join(".venv"),
-            interpreter.clone(),
-            gourgeist::Prompt::None,
-            Vec::new(),
-        )?;
-
-        // Setup the build environment.
-        let resolved_requirements = Self::get_resolved_requirements(
-            build_context,
-            source_build_context,
-            &default_backend,
-            pep517_backend.as_ref(),
-        )
-        .await?;
-
-        build_context
-            .install(&resolved_requirements, &venv)
-            .await
-            .map_err(|err| Error::RequirementsInstall("build-system.requires (install)", err))?;
+        let (pep517_backend, project) =
+            Self::extract_pep517_backend(&source_tree, setup_py, &default_backend)
+                .map_err(|err| *err)?;
+
+        // Create a virtual environment, or install into the shared environment if requested.
+        let venv = match build_isolation {
+            BuildIsolation::Isolated => uv_virtualenv::create_venv(
+                &temp_dir.path().join(".venv"),
+                interpreter.clone(),
+                uv_virtualenv::Prompt::None,
+                false,
+                false,
+            )?,
+            BuildIsolation::Shared(venv) => venv.clone(),
+        };
 
-        if let Some(pep517_backend) = &pep517_backend {
-            create_pep517_build_environment(
-                &source_tree,
-                &venv,
-                pep517_backend,
+        // Setup the build environment. If build isolation is disabled, we assume the build
+        // environment is already setup.
+        if build_isolation.is_isolated() {
+            let resolved_requirements = Self::get_resolved_requirements(
                 build_context,
-                &package_id,
-                build_kind,
-                &config_settings,
+                source_build_context,
+                &default_backend,
+                pep517_backend.as_ref(),
             )
             .await?;
+
+            build_context
+                .install(&resolved_requirements, &venv)
+                .await
+                .map_err(|err| {
+                    Error::RequirementsInstall("build-system.requires (install)", err)
+                })?;
+        }
+
+        // Figure out what the modified path should be
+        // Remove the PATH variable from the environment variables if it's there
+        let user_path = environment_variables.remove(&OsString::from("PATH"));
+        // See if there is an OS PATH variable
+        let os_path = env::var_os("PATH");
+
+        // Prepend the user supplied PATH to the existing OS PATH
+        let modified_path = if let Some(user_path) = user_path {
+            match os_path {
+                // Prepend the user supplied PATH to the existing PATH
+                Some(env_path) => {
+                    let user_path = PathBuf::from(user_path);
+                    let new_path = env::split_paths(&user_path).chain(env::split_paths(&env_path));
+                    Some(env::join_paths(new_path).map_err(Error::BuildScriptPath)?)
+                }
+                // Use the user supplied PATH
+                None => Some(user_path),
+            }
+        } else {
+            os_path
+        };
+
+        // Prepend the venv bin directory to the modified path
+        let modified_path = if let Some(path) = modified_path {
+            let venv_path = iter::once(venv.scripts().to_path_buf()).chain(env::split_paths(&path));
+            env::join_paths(venv_path).map_err(Error::BuildScriptPath)?
+        } else {
+            OsString::from(venv.scripts())
+        };
+
+        // Create the PEP 517 build environment. If build isolation is disabled, we assume the build
+        // environment is already setup.
+        let runner = PythonRunner::new(concurrent_builds);
+        if build_isolation.is_isolated() {
+            if let Some(pep517_backend) = &pep517_backend {
+                create_pep517_build_environment(
+                    &runner,
+                    &source_tree,
+                    &venv,
+                    pep517_backend,
+                    build_context,
+                    &version_id,
+                    build_kind,
+                    &config_settings,
+                    &environment_variables,
+                    &modified_path,
+                    &temp_dir,
+                )
+                .await?;
+            }
         }
 
         Ok(Self {
             temp_dir,
             source_tree,
             pep517_backend,
+            project,
             venv,
             build_kind,
             config_settings,
             metadata_directory: None,
-            package_id,
+            version_id,
+            environment_variables,
+            modified_path,
+            runner,
         })
     }
 
     async fn get_resolved_requirements(
         build_context: &impl BuildContext,
         source_build_context: SourceBuildContext,
         default_backend: &Pep517Backend,
         pep517_backend: Option<&Pep517Backend>,
     ) -> Result<Resolution, Error> {
         Ok(if let Some(pep517_backend) = pep517_backend {
             if pep517_backend.requirements == default_backend.requirements {
-                let mut resolution = source_build_context.setup_py_resolution.lock().await;
+                let mut resolution = source_build_context.default_resolution.lock().await;
                 if let Some(resolved_requirements) = &*resolution {
                     resolved_requirements.clone()
                 } else {
                     let resolved_requirements = build_context
                         .resolve(&default_backend.requirements)
                         .await
                         .map_err(|err| {
@@ -405,52 +562,59 @@
         } else {
             // Install default requirements for `setup.py`-based builds.
             let mut resolution = source_build_context.setup_py_resolution.lock().await;
             if let Some(resolved_requirements) = &*resolution {
                 resolved_requirements.clone()
             } else {
                 let resolved_requirements = build_context
-                    .resolve(&default_backend.requirements)
+                    .resolve(&*SETUP_PY_REQUIREMENTS)
                     .await
                     .map_err(|err| Error::RequirementsInstall("setup.py build (resolve)", err))?;
                 *resolution = Some(resolved_requirements.clone());
                 resolved_requirements
             }
         })
     }
 
-    fn get_pep517_backend(
-        setup_py: SetupPyStrategy,
+    /// Extract the PEP 517 backend from the `pyproject.toml` or `setup.py` file.
+    fn extract_pep517_backend(
         source_tree: &Path,
+        setup_py: SetupPyStrategy,
         default_backend: &Pep517Backend,
-    ) -> Result<Option<Pep517Backend>, Box<Error>> {
+    ) -> Result<(Option<Pep517Backend>, Option<Project>), Box<Error>> {
         match fs::read_to_string(source_tree.join("pyproject.toml")) {
             Ok(toml) => {
                 let pyproject_toml: PyProjectToml =
                     toml::from_str(&toml).map_err(Error::InvalidPyprojectToml)?;
-                if let Some(build_system) = pyproject_toml.build_system {
-                    Ok(Some(Pep517Backend {
+                let backend = if let Some(build_system) = pyproject_toml.build_system {
+                    Pep517Backend {
                         // If `build-backend` is missing, inject the legacy setuptools backend, but
                         // retain the `requires`, to match `pip` and `build`. Note that while PEP 517
                         // says that in this case we "should revert to the legacy behaviour of running
                         // `setup.py` (either directly, or by implicitly invoking the
                         // `setuptools.build_meta:__legacy__` backend)", we found that in practice, only
                         // the legacy setuptools backend is allowed. See also:
                         // https://github.com/pypa/build/blob/de5b44b0c28c598524832dff685a98d5a5148c44/src/build/__init__.py#L114-L118
                         backend: build_system
                             .build_backend
                             .unwrap_or_else(|| "setuptools.build_meta:__legacy__".to_string()),
                         backend_path: build_system.backend_path,
-                        requirements: build_system.requires,
-                    }))
+                        requirements: build_system
+                            .requires
+                            .into_iter()
+                            .map(Requirement::from_pep508)
+                            .collect::<Result<_, _>>()
+                            .map_err(|err| Box::new(Error::DirectUrl(err)))?,
+                    }
                 } else {
                     // If a `pyproject.toml` is present, but `[build-system]` is missing, proceed with
                     // a PEP 517 build using the default backend, to match `pip` and `build`.
-                    Ok(Some(default_backend.clone()))
-                }
+                    default_backend.clone()
+                };
+                Ok((Some(backend), pyproject_toml.project))
             }
             Err(err) if err.kind() == io::ErrorKind::NotFound => {
                 // We require either a `pyproject.toml` or a `setup.py` file at the top level.
                 if !source_tree.join("setup.py").is_file() {
                     return Err(Box::new(Error::InvalidSourceDist(
                         "The archive contains neither a `pyproject.toml` nor a `setup.py` file at the top level"
                             .to_string(),
@@ -459,16 +623,16 @@
 
                 // If no `pyproject.toml` is present, by default, proceed with a PEP 517 build using
                 // the default backend, to match `build`. `pip` uses `setup.py` directly in this
                 // case (which we allow via `SetupPyStrategy::Setuptools`), but plans to make PEP
                 // 517 builds the default in the future.
                 // See: https://github.com/pypa/pip/issues/9175.
                 match setup_py {
-                    SetupPyStrategy::Pep517 => Ok(Some(default_backend.clone())),
-                    SetupPyStrategy::Setuptools => Ok(None),
+                    SetupPyStrategy::Pep517 => Ok((Some(default_backend.clone()), None)),
+                    SetupPyStrategy::Setuptools => Ok((None, None)),
                 }
             }
             Err(err) => Err(Box::new(err.into())),
         }
     }
 
     /// Try calling `prepare_metadata_for_build_wheel` to get the metadata without executing the
@@ -479,82 +643,117 @@
         };
 
         // We've already called this method; return the existing result.
         if let Some(metadata_dir) = &self.metadata_directory {
             return Ok(Some(metadata_dir.clone()));
         }
 
+        // Hatch allows for highly dynamic customization of metadata via hooks. In such cases, Hatch
+        // can't uphold the PEP 517 contract, in that the metadata Hatch would return by
+        // `prepare_metadata_for_build_wheel` isn't guaranteed to match that of the built wheel.
+        //
+        // Hatch disables `prepare_metadata_for_build_wheel` entirely for pip. We'll instead disable
+        // it on our end when metadata is defined as "dynamic" in the pyproject.toml, which should
+        // allow us to leverage the hook in _most_ cases while still avoiding incorrect metadata for
+        // the remaining cases.
+        //
+        // This heuristic will have false positives (i.e., there will be some Hatch projects for
+        // which we could have safely called `prepare_metadata_for_build_wheel`, despite having
+        // dynamic metadata). However, false positives are preferable to false negatives, since
+        // this is just an optimization.
+        //
+        // See: https://github.com/astral-sh/uv/issues/2130
+        if pep517_backend.backend == "hatchling.build" {
+            if self
+                .project
+                .as_ref()
+                .and_then(|project| project.dynamic.as_ref())
+                .is_some_and(|dynamic| {
+                    dynamic
+                        .iter()
+                        .any(|field| field == "dependencies" || field == "optional-dependencies")
+                })
+            {
+                return Ok(None);
+            }
+        }
+
         let metadata_directory = self.temp_dir.path().join("metadata_directory");
         fs::create_dir(&metadata_directory)?;
 
+        // Write the hook output to a file so that we can read it back reliably.
+        let outfile = self
+            .temp_dir
+            .path()
+            .join("prepare_metadata_for_build_wheel.txt");
+
         debug!(
             "Calling `{}.prepare_metadata_for_build_wheel()`",
             pep517_backend.backend
         );
         let script = formatdoc! {
-            r#"{}
+            r#"
+            {}
             import json
 
             prepare_metadata_for_build_wheel = getattr(backend, "prepare_metadata_for_build_wheel", None)
             if prepare_metadata_for_build_wheel:
-                print(prepare_metadata_for_build_wheel("{}", config_settings={}))
+                dirname = prepare_metadata_for_build_wheel("{}", {})
             else:
-                print()
+                dirname = None
+
+            with open("{}", "w") as fp:
+                fp.write(dirname or "")
             "#,
             pep517_backend.backend_import(),
             escape_path_for_python(&metadata_directory),
             self.config_settings.escape_for_python(),
+            outfile.escape_for_python(),
         };
         let span = info_span!(
             "run_python_script",
             script="prepare_metadata_for_build_wheel",
             python_version = %self.venv.interpreter().python_version()
         );
-        let output = run_python_script(&self.venv, &script, &self.source_tree)
+        let output = self
+            .runner
+            .run_script(
+                &self.venv,
+                &script,
+                &self.source_tree,
+                &self.environment_variables,
+                &self.modified_path,
+            )
             .instrument(span)
             .await?;
         if !output.status.success() {
             return Err(Error::from_command_output(
                 "Build backend failed to determine metadata through `prepare_metadata_for_build_wheel`".to_string(),
                 &output,
-                &self.package_id,
+                &self.version_id,
             ));
         }
-        let message = output
-            .stdout
-            .lines()
-            .last()
-            .transpose()
-            .map_err(|err| err.to_string())
-            .and_then(|last_line| last_line.ok_or("Missing message".to_string()))
-            .map_err(|err| {
-                Error::from_command_output(
-                    format!(
-                        "Build backend failed to return metadata directory with `prepare_metadata_for_build_wheel`: {err}"
-                    ),
-                    &output,
-                    &self.package_id,
-                )
-            })?;
-        if message.is_empty() {
+
+        let dirname = fs::read_to_string(&outfile)?;
+        if dirname.is_empty() {
             return Ok(None);
         }
-        self.metadata_directory = Some(metadata_directory.join(message));
+        self.metadata_directory = Some(metadata_directory.join(dirname));
         Ok(self.metadata_directory.clone())
     }
 
     /// Build a source distribution from an archive (`.zip` or `.tar.gz`), return the location of the
     /// built wheel.
     ///
     /// The location will be inside `temp_dir`, i.e. you must use the wheel before dropping the temp
     /// dir.
     ///
     /// <https://packaging.python.org/en/latest/specifications/source-distribution-format/>
-    #[instrument(skip_all, fields(package_id = self.package_id))]
-    pub async fn build(&self, wheel_dir: &Path) -> Result<String, Error> {
+    #[instrument(skip_all, fields(version_id = self.version_id))]
+    pub async fn build_wheel(&self, wheel_dir: &Path) -> Result<String, Error> {
         // The build scripts run with the extracted root as cwd, so they need the absolute path.
         let wheel_dir = fs::canonicalize(wheel_dir)?;
 
         if let Some(pep517_backend) = &self.pep517_backend {
             // Prevent clashes from two uv processes building wheels in parallel.
             let tmp_dir = tempdir_in(&wheel_dir)?;
             let filename = self.pep517_build(tmp_dir.path(), pep517_backend).await?;
@@ -564,43 +763,40 @@
             fs_err::rename(from, to)?;
             Ok(filename)
         } else {
             if self.build_kind != BuildKind::Wheel {
                 return Err(Error::EditableSetupPy);
             }
             // We checked earlier that setup.py exists.
-            let python_interpreter = self.venv.python_executable();
             let span = info_span!(
                 "run_python_script",
                 script="setup.py bdist_wheel",
                 python_version = %self.venv.interpreter().python_version()
             );
-            let output = Command::new(&python_interpreter)
-                .args(["setup.py", "bdist_wheel"])
-                .current_dir(self.source_tree.normalized())
-                .output()
+            let output = self
+                .runner
+                .run_setup_py(&self.venv, "bdist_wheel", &self.source_tree)
                 .instrument(span)
-                .await
-                .map_err(|err| Error::CommandFailed(python_interpreter, err))?;
+                .await?;
             if !output.status.success() {
                 return Err(Error::from_command_output(
                     "Failed building wheel through setup.py".to_string(),
                     &output,
-                    &self.package_id,
+                    &self.version_id,
                 ));
             }
             let dist = fs::read_dir(self.source_tree.join("dist"))?;
             let dist_dir = dist.collect::<io::Result<Vec<fs_err::DirEntry>>>()?;
             let [dist_wheel] = dist_dir.as_slice() else {
                 return Err(Error::from_command_output(
                     format!(
                         "Expected exactly wheel in `dist/` after invoking setup.py, found {dist_dir:?}"
                     ),
                     &output,
-                    &self.package_id)
+                    &self.version_id)
                 );
             };
 
             let from = dist_wheel.path();
             let to = wheel_dir.join(dist_wheel.file_name());
             fs_err::copy(from, to)?;
 
@@ -613,154 +809,208 @@
         wheel_dir: &Path,
         pep517_backend: &Pep517Backend,
     ) -> Result<String, Error> {
         let metadata_directory = self
             .metadata_directory
             .as_deref()
             .map_or("None".to_string(), |path| {
-                format!(r#""{}""#, escape_path_for_python(path))
+                format!(r#""{}""#, path.escape_for_python())
             });
+
+        // Write the hook output to a file so that we can read it back reliably.
+        let outfile = self
+            .temp_dir
+            .path()
+            .join(format!("build_{}.txt", self.build_kind));
+
         debug!(
-            "Calling `{}.build_{}(metadata_directory={})`",
-            pep517_backend.backend, self.build_kind, metadata_directory
+            r#"Calling `{}.build_{}("{}", {}, {})`"#,
+            pep517_backend.backend,
+            self.build_kind,
+            wheel_dir.escape_for_python(),
+            self.config_settings.escape_for_python(),
+            metadata_directory,
         );
-        let escaped_wheel_dir = escape_path_for_python(wheel_dir);
         let script = formatdoc! {
-            r#"{}
-            print(backend.build_{}("{}", metadata_directory={}, config_settings={}))
+            r#"
+            {}
+
+            wheel_filename = backend.build_{}("{}", {}, {})
+            with open("{}", "w") as fp:
+                fp.write(wheel_filename)
             "#,
             pep517_backend.backend_import(),
             self.build_kind,
-            escaped_wheel_dir,
+            wheel_dir.escape_for_python(),
+            self.config_settings.escape_for_python(),
             metadata_directory,
-            self.config_settings.escape_for_python()
+            outfile.escape_for_python()
         };
         let span = info_span!(
             "run_python_script",
             script=format!("build_{}", self.build_kind),
             python_version = %self.venv.interpreter().python_version()
         );
-        let output = run_python_script(&self.venv, &script, &self.source_tree)
+        let output = self
+            .runner
+            .run_script(
+                &self.venv,
+                &script,
+                &self.source_tree,
+                &self.environment_variables,
+                &self.modified_path,
+            )
             .instrument(span)
             .await?;
         if !output.status.success() {
             return Err(Error::from_command_output(
                 format!(
                     "Build backend failed to build wheel through `build_{}()`",
                     self.build_kind
                 ),
                 &output,
-                &self.package_id,
+                &self.version_id,
             ));
         }
-        let stdout = String::from_utf8_lossy(&output.stdout);
-        let distribution_filename = stdout.lines().last();
-        let Some(distribution_filename) =
-            distribution_filename.filter(|wheel| wheel_dir.join(wheel).is_file())
-        else {
+
+        let distribution_filename = fs::read_to_string(&outfile)?;
+        if !wheel_dir.join(&distribution_filename).is_file() {
             return Err(Error::from_command_output(
                 format!(
-                    "Build backend failed to build wheel through `build_{}()`",
+                    "Build backend failed to produce wheel through `build_{}()`: `{distribution_filename}` not found",
                     self.build_kind
                 ),
                 &output,
-                &self.package_id,
+                &self.version_id,
             ));
-        };
-        Ok(distribution_filename.to_string())
+        }
+        Ok(distribution_filename)
     }
 }
 
 impl SourceBuildTrait for SourceBuild {
     async fn metadata(&mut self) -> anyhow::Result<Option<PathBuf>> {
         Ok(self.get_metadata_without_build().await?)
     }
 
     async fn wheel<'a>(&'a self, wheel_dir: &'a Path) -> anyhow::Result<String> {
-        Ok(self.build(wheel_dir).await?)
+        Ok(self.build_wheel(wheel_dir).await?)
     }
 }
 
 fn escape_path_for_python(path: &Path) -> String {
     path.to_string_lossy()
         .replace('\\', "\\\\")
         .replace('"', "\\\"")
 }
 
 /// Not a method because we call it before the builder is completely initialized
+#[allow(clippy::too_many_arguments)]
 async fn create_pep517_build_environment(
+    runner: &PythonRunner,
     source_tree: &Path,
-    venv: &Virtualenv,
+    venv: &PythonEnvironment,
     pep517_backend: &Pep517Backend,
     build_context: &impl BuildContext,
-    package_id: &str,
+    version_id: &str,
     build_kind: BuildKind,
     config_settings: &ConfigSettings,
+    environment_variables: &FxHashMap<OsString, OsString>,
+    modified_path: &OsString,
+    temp_dir: &TempDir,
 ) -> Result<(), Error> {
+    // Write the hook output to a file so that we can read it back reliably.
+    let outfile = temp_dir
+        .path()
+        .join(format!("get_requires_for_build_{build_kind}.txt"));
+
     debug!(
         "Calling `{}.get_requires_for_build_{}()`",
         pep517_backend.backend, build_kind
     );
+
     let script = formatdoc! {
         r#"
             {}
             import json
 
             get_requires_for_build = getattr(backend, "get_requires_for_build_{}", None)
             if get_requires_for_build:
-                requires = get_requires_for_build(config_settings={})
+                requires = get_requires_for_build({})
             else:
                 requires = []
-            print(json.dumps(requires))
-        "#, pep517_backend.backend_import(), build_kind, config_settings.escape_for_python()
+
+            with open("{}", "w") as fp:
+                json.dump(requires, fp)
+        "#,
+        pep517_backend.backend_import(),
+        build_kind,
+        config_settings.escape_for_python(),
+        outfile.escape_for_python()
     };
     let span = info_span!(
         "run_python_script",
         script=format!("get_requires_for_build_{}", build_kind),
         python_version = %venv.interpreter().python_version()
     );
-    let output = run_python_script(venv, &script, source_tree)
+    let output = runner
+        .run_script(
+            venv,
+            &script,
+            source_tree,
+            environment_variables,
+            modified_path,
+        )
         .instrument(span)
         .await?;
     if !output.status.success() {
         return Err(Error::from_command_output(
             format!("Build backend failed to determine extra requires with `build_{build_kind}()`"),
             &output,
-            package_id,
+            version_id,
         ));
     }
-    let extra_requires = output
-        .stdout
-        .lines()
-        .last()
-        .transpose()
-        .map_err(|err| err.to_string())
-        .and_then(|last_line| last_line.ok_or("Missing message".to_string()))
-        .and_then(|message| serde_json::from_str(&message).map_err(|err| err.to_string()));
 
-    let extra_requires: Vec<Requirement> = extra_requires.map_err(|err| {
+    // Read the requirements from the output file.
+    let contents = fs_err::read(&outfile).map_err(|err| {
+        Error::from_command_output(
+            format!(
+                "Build backend failed to read extra requires from `get_requires_for_build_{build_kind}`: {err}"
+            ),
+            &output,
+            version_id,
+        )
+    })?;
+
+    // Deserialize the requirements from the output file.
+    let extra_requires: Vec<pep508_rs::Requirement> = serde_json::from_slice::<Vec<pep508_rs::Requirement>>(&contents).map_err(|err| {
         Error::from_command_output(
             format!(
                 "Build backend failed to return extra requires with `get_requires_for_build_{build_kind}`: {err}"
             ),
             &output,
-            package_id,
+            version_id,
         )
     })?;
+    let extra_requires: Vec<_> = extra_requires
+        .into_iter()
+        .map(Requirement::from_pep508)
+        .collect::<Result<_, _>>()
+        .map_err(Error::DirectUrl)?;
 
     // Some packages (such as tqdm 4.66.1) list only extra requires that have already been part of
     // the pyproject.toml requires (in this case, `wheel`). We can skip doing the whole resolution
     // and installation again.
     // TODO(konstin): Do we still need this when we have a fast resolver?
     if extra_requires
         .iter()
         .any(|req| !pep517_backend.requirements.contains(req))
     {
         debug!("Installing extra requirements for build backend");
-        let requirements: Vec<Requirement> = pep517_backend
+        let requirements: Vec<_> = pep517_backend
             .requirements
             .iter()
             .cloned()
             .chain(extra_requires)
             .collect();
         let resolution = build_context
             .resolve(&requirements)
@@ -772,36 +1022,80 @@
             .await
             .map_err(|err| Error::RequirementsInstall("build-system.requires (install)", err))?;
     }
 
     Ok(())
 }
 
-/// It is the caller's responsibility to create an informative span.
-async fn run_python_script(
-    venv: &Virtualenv,
-    script: &str,
-    source_tree: &Path,
-) -> Result<Output, Error> {
-    // Prepend the venv bin dir to PATH
-    let new_path = if let Some(old_path) = env::var_os("PATH") {
-        let new_path = iter::once(venv.bin_dir()).chain(env::split_paths(&old_path));
-        env::join_paths(new_path).map_err(Error::BuildScriptPath)?
-    } else {
-        OsString::from("")
-    };
-    Command::new(venv.python_executable())
-        .args(["-c", script])
-        .current_dir(source_tree.normalized())
-        // Activate the venv
-        .env("VIRTUAL_ENV", venv.root())
-        .env("PATH", new_path)
-        .output()
-        .await
-        .map_err(|err| Error::CommandFailed(venv.python_executable(), err))
+/// A runner that manages the execution of external python processes with a
+/// concurrency limit.
+struct PythonRunner {
+    control: Semaphore,
+}
+
+impl PythonRunner {
+    /// Create a `PythonRunner` with the provided concurrency limit.
+    fn new(concurrency: usize) -> PythonRunner {
+        PythonRunner {
+            control: Semaphore::new(concurrency),
+        }
+    }
+
+    /// Spawn a process that runs a python script in the provided environment.
+    ///
+    /// If the concurrency limit has been reached this method will wait until a pending
+    /// script completes before spawning this one.
+    ///
+    /// Note: It is the caller's responsibility to create an informative span.
+    async fn run_script(
+        &self,
+        venv: &PythonEnvironment,
+        script: &str,
+        source_tree: &Path,
+        environment_variables: &FxHashMap<OsString, OsString>,
+        modified_path: &OsString,
+    ) -> Result<Output, Error> {
+        let _permit = self.control.acquire().await.unwrap();
+
+        Command::new(venv.python_executable())
+            .args(["-c", script])
+            .current_dir(source_tree.simplified())
+            // Pass in remaining environment variables
+            .envs(environment_variables)
+            // Set the modified PATH
+            .env("PATH", modified_path)
+            // Activate the venv
+            .env("VIRTUAL_ENV", venv.root())
+            .env("CLICOLOR_FORCE", "1")
+            .output()
+            .await
+            .map_err(|err| Error::CommandFailed(venv.python_executable().to_path_buf(), err))
+    }
+
+    /// Spawn a process that runs a `setup.py` script.
+    ///
+    /// If the concurrency limit has been reached this method will wait until a pending
+    /// script completes before spawning this one.
+    ///
+    /// Note: It is the caller's responsibility to create an informative span.
+    async fn run_setup_py(
+        &self,
+        venv: &PythonEnvironment,
+        script: &str,
+        source_tree: &Path,
+    ) -> Result<Output, Error> {
+        let _permit = self.control.acquire().await.unwrap();
+
+        Command::new(venv.python_executable())
+            .args(["setup.py", script])
+            .current_dir(source_tree.simplified())
+            .output()
+            .await
+            .map_err(|err| Error::CommandFailed(venv.python_executable().to_path_buf(), err))
+    }
 }
 
 #[cfg(test)]
 mod test {
     use std::process::{ExitStatus, Output};
 
     use indoc::indoc;
@@ -836,16 +1130,18 @@
 
         let err = Error::from_command_output(
             "Failed building wheel through setup.py".to_string(),
             &output,
             "pygraphviz-1.11",
         );
         assert!(matches!(err, Error::MissingHeader { .. }));
-        insta::assert_display_snapshot!(err, @r###"
-        Failed building wheel through setup.py:
+        // Unix uses exit status, Windows uses exit code.
+        let formatted = err.to_string().replace("exit status: ", "exit code: ");
+        insta::assert_snapshot!(formatted, @r###"
+        Failed building wheel through setup.py with exit code: 0
         --- stdout:
         running bdist_wheel
         running build
         [...]
         creating build/temp.linux-x86_64-cpython-39/pygraphviz
         gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -DOPENSSL_NO_SSL3 -fPIC -DSWIG_PYTHON_STRICT_BYTE_CHAR -I/tmp/.tmpy6vVes/.venv/include -I/home/konsti/.pyenv/versions/3.9.18/include/python3.9 -c pygraphviz/graphviz_wrap.c -o build/temp.linux-x86_64-cpython-39/pygraphviz/graphviz_wrap.o
         --- stderr:
@@ -856,15 +1152,15 @@
         pygraphviz/graphviz_wrap.c:3020:10: fatal error: graphviz/cgraph.h: No such file or directory
          3020 | #include "graphviz/cgraph.h"
               |          ^~~~~~~~~~~~~~~~~~~
         compilation terminated.
         error: command '/usr/bin/gcc' failed with exit code 1
         ---
         "###);
-        insta::assert_display_snapshot!(
+        insta::assert_snapshot!(
             std::error::Error::source(&err).unwrap(),
             @r###"This error likely indicates that you need to install a library that provides "graphviz/cgraph.h" for pygraphviz-1.11"###
         );
     }
 
     #[test]
     fn missing_linker_library() {
@@ -886,25 +1182,73 @@
 
         let err = Error::from_command_output(
             "Failed building wheel through setup.py".to_string(),
             &output,
             "pygraphviz-1.11",
         );
         assert!(matches!(err, Error::MissingHeader { .. }));
-        insta::assert_display_snapshot!(err, @r###"
-        Failed building wheel through setup.py:
+        // Unix uses exit status, Windows uses exit code.
+        let formatted = err.to_string().replace("exit status: ", "exit code: ");
+        insta::assert_snapshot!(formatted, @r###"
+        Failed building wheel through setup.py with exit code: 0
         --- stdout:
 
         --- stderr:
         1099 |     n = strlen(p);
               |         ^~~~~~~~~
         /usr/bin/ld: cannot find -lncurses: No such file or directory
         collect2: error: ld returned 1 exit status
         error: command '/usr/bin/x86_64-linux-gnu-gcc' failed with exit code 1
         ---
         "###);
-        insta::assert_display_snapshot!(
+        insta::assert_snapshot!(
             std::error::Error::source(&err).unwrap(),
             @"This error likely indicates that you need to install the library that provides a shared library for ncurses for pygraphviz-1.11 (e.g. libncurses-dev)"
         );
     }
+
+    #[test]
+    fn missing_wheel_package() {
+        let output = Output {
+            status: ExitStatus::default(), // This is wrong but `from_raw` is platform-gated.
+            stdout: Vec::new(),
+            stderr: indoc!(
+                r"
+            usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]
+               or: setup.py --help [cmd1 cmd2 ...]
+               or: setup.py --help-commands
+               or: setup.py cmd --help
+
+            error: invalid command 'bdist_wheel'
+                "
+            )
+            .as_bytes()
+            .to_vec(),
+        };
+
+        let err = Error::from_command_output(
+            "Failed building wheel through setup.py".to_string(),
+            &output,
+            "pygraphviz-1.11",
+        );
+        assert!(matches!(err, Error::MissingHeader { .. }));
+        // Unix uses exit status, Windows uses exit code.
+        let formatted = err.to_string().replace("exit status: ", "exit code: ");
+        insta::assert_snapshot!(formatted, @r###"
+        Failed building wheel through setup.py with exit code: 0
+        --- stdout:
+
+        --- stderr:
+        usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]
+           or: setup.py --help [cmd1 cmd2 ...]
+           or: setup.py --help-commands
+           or: setup.py cmd --help
+
+        error: invalid command 'bdist_wheel'
+        ---
+        "###);
+        insta::assert_snapshot!(
+            std::error::Error::source(&err).unwrap(),
+            @"This error likely indicates that you need to `uv pip install wheel` into the build environment for pygraphviz-1.11"
+        );
+    }
 }
```

### Comparing `uv-0.1.9/crates/uv-fs/Cargo.toml` & `uv-0.2.0/crates/distribution-filename/Cargo.toml`

 * *Files 12% similar despite different names*

```diff
@@ -1,28 +1,26 @@
 [package]
-name = "uv-fs"
+name = "distribution-filename"
 version = "0.0.1"
 edition = { workspace = true }
 rust-version = { workspace = true }
 homepage = { workspace = true }
 documentation = { workspace = true }
 repository = { workspace = true }
 authors = { workspace = true }
 license = { workspace = true }
 
 [lints]
 workspace = true
 
 [dependencies]
-uv-warnings = { path = "../uv-warnings" }
+pep440_rs = { workspace = true }
+platform-tags = { workspace = true }
+uv-normalize = { workspace = true }
 
-dunce = { workspace = true }
-fs-err = { workspace = true }
-fs2 = { workspace = true }
-junction = { workspace = true }
-tempfile = { workspace = true }
-tracing = { workspace = true }
-urlencoding = { workspace = true }
+rkyv = { workspace = true }
+serde = { workspace = true }
+thiserror = { workspace = true }
+url = { workspace = true }
 
-[features]
-default = []
-tokio = ["fs-err/tokio"]
+[dev-dependencies]
+insta = { version = "1.36.1" }
```

### Comparing `uv-0.1.9/crates/uv-traits/Cargo.toml` & `uv-0.2.0/crates/pypi-types/Cargo.toml`

 * *Files 26% similar despite different names*

```diff
@@ -1,31 +1,32 @@
 [package]
-name = "uv-traits"
+name = "pypi-types"
 version = "0.0.1"
 edition = { workspace = true }
 rust-version = { workspace = true }
 homepage = { workspace = true }
 documentation = { workspace = true }
 repository = { workspace = true }
 authors = { workspace = true }
 license = { workspace = true }
 
 [lints]
 workspace = true
 
 [dependencies]
-clap = { workspace = true, optional = true }
-distribution-types = { path = "../distribution-types" }
-once-map = { path = "../once-map" }
-pep508_rs = { path = "../pep508-rs" }
-uv-cache = { path = "../uv-cache" }
-uv-interpreter = { path = "../uv-interpreter" }
-uv-normalize = { path = "../uv-normalize" }
+pep440_rs = { workspace = true }
+pep508_rs = { workspace = true }
+uv-normalize = { workspace = true }
 
-anyhow = { workspace = true }
-serde = { workspace = true, optional = true }
-serde_json = { workspace = true, optional = true }
-tokio = { workspace = true, features = ["sync"] }
+chrono = { workspace = true, features = ["serde"] }
+indexmap = { workspace = true, features = ["serde"] }
+mailparse = { workspace = true }
+once_cell = { workspace = true }
+regex = { workspace = true }
+rkyv = { workspace = true }
+serde = { workspace = true }
+thiserror = { workspace = true }
+toml = { workspace = true }
+tracing = { workspace = true }
+url = { workspace = true }
 
-[features]
-default = []
-serde = ["dep:serde", "dep:serde_json"]
+[dev-dependencies]
```

### Comparing `uv-0.1.9/crates/uv-extract/Cargo.toml` & `uv-0.2.0/crates/uv-workspace/Cargo.toml`

 * *Files 15% similar despite different names*

```diff
@@ -1,27 +1,31 @@
 [package]
-name = "uv-extract"
+name = "uv-workspace"
 version = "0.0.1"
 edition = { workspace = true }
 rust-version = { workspace = true }
 homepage = { workspace = true }
 documentation = { workspace = true }
 repository = { workspace = true }
 authors = { workspace = true }
 license = { workspace = true }
 
 [lints]
 workspace = true
 
 [dependencies]
-async-compression = { workspace = true, features = ["gzip"] }
-async_zip = { workspace = true, features = ["tokio"] }
-flate2 = { workspace = true }
-fs-err = { workspace = true, features = ["tokio"] }
-futures = { workspace = true }
-rayon = { workspace = true }
-rustc-hash = { workspace = true }
+distribution-types = { workspace = true, features = ["schemars"] }
+install-wheel-rs = { workspace = true, features = ["schemars"] }
+uv-configuration = { workspace = true, features = ["schemars"] }
+uv-fs = { workspace = true }
+uv-normalize = { workspace = true, features = ["schemars"] }
+uv-resolver = { workspace = true, features = ["schemars"] }
+uv-interpreter = { workspace = true, features = ["schemars"] }
+uv-warnings = { workspace = true }
+
+dirs-sys = { workspace = true }
+fs-err = { workspace = true }
+schemars = { workspace = true, optional = true }
+serde = { workspace = true }
 thiserror = { workspace = true }
-tokio = { workspace = true, features = ["io-util"] }
-tokio-tar = { workspace = true }
-tokio-util = { workspace = true, features = ["compat"] }
-zip = { workspace = true }
+toml = { workspace = true }
+tracing = { workspace = true }
```

### Comparing `uv-0.1.9/crates/uv-extract/src/stream.rs` & `uv-0.2.0/crates/uv-extract/src/stream.rs`

 * *Files 17% similar despite different names*

```diff
@@ -1,27 +1,28 @@
 use std::path::Path;
 use std::pin::Pin;
 
 use futures::StreamExt;
 use rustc_hash::FxHashSet;
 use tokio_util::compat::{FuturesAsyncReadCompatExt, TokioAsyncReadCompatExt};
+use tracing::warn;
 
 use crate::Error;
 
 /// Unzip a `.zip` archive into the target directory, without requiring `Seek`.
 ///
 /// This is useful for unzipping files as they're being downloaded. If the archive
 /// is already fully on disk, consider using `unzip_archive`, which can use multiple
 /// threads to work faster in that case.
 pub async fn unzip<R: tokio::io::AsyncRead + Unpin>(
     reader: R,
     target: impl AsRef<Path>,
 ) -> Result<(), Error> {
     let target = target.as_ref();
-    let mut reader = reader.compat();
+    let mut reader = futures::io::BufReader::with_capacity(128 * 1024, reader.compat());
     let mut zip = async_zip::base::read::stream::ZipFileReader::new(&mut reader);
 
     let mut directories = FxHashSet::default();
 
     while let Some(mut entry) = zip.next_with_entry().await? {
         // Construct the (expected) path to the file on-disk.
         let path = entry.reader().entry().filename().as_str()?;
@@ -37,15 +38,15 @@
             if let Some(parent) = path.parent() {
                 if directories.insert(parent.to_path_buf()) {
                     fs_err::tokio::create_dir_all(parent).await?;
                 }
             }
 
             // We don't know the file permissions here, because we haven't seen the central directory yet.
-            let file = fs_err::tokio::File::create(path).await?;
+            let file = fs_err::tokio::File::create(&path).await?;
             let mut writer =
                 if let Ok(size) = usize::try_from(entry.reader().entry().uncompressed_size()) {
                     tokio::io::BufWriter::with_capacity(size, file)
                 } else {
                     tokio::io::BufWriter::new(file)
                 };
             let mut reader = entry.reader_mut().compat();
@@ -61,18 +62,15 @@
     // end of the archive. The `ZipFileReader` reads until it sees a central directory signature,
     // which indicates the first entry in the central directory. So we continue reading from there.
     #[cfg(unix)]
     {
         use std::fs::Permissions;
         use std::os::unix::fs::PermissionsExt;
 
-        // To avoid lots of small reads to `reader` when parsing the central directory, wrap it in
-        // a buffer.
-        let mut buf = futures::io::BufReader::new(reader);
-        let mut directory = async_zip::base::read::cd::CentralDirectoryReader::new(&mut buf);
+        let mut directory = async_zip::base::read::cd::CentralDirectoryReader::new(&mut reader);
         while let Some(entry) = directory.next().await? {
             if entry.dir()? {
                 continue;
             }
 
             let Some(mode) = entry.unix_permissions() else {
                 continue;
@@ -107,56 +105,105 @@
     dst: P,
 ) -> std::io::Result<()> {
     let mut entries = archive.entries()?;
     let mut pinned = Pin::new(&mut entries);
     while let Some(entry) = pinned.next().await {
         // Unpack the file into the destination directory.
         let mut file = entry?;
+
+        // On Windows, skip symlink entries, as they're not supported. pip recursively copies the
+        // symlink target instead.
+        if cfg!(windows) && file.header().entry_type().is_symlink() {
+            warn!(
+                "Skipping symlink in tar archive: {}",
+                file.path()?.display()
+            );
+            continue;
+        }
+
         file.unpack_in(dst.as_ref()).await?;
 
         // Preserve the executable bit.
         #[cfg(unix)]
         {
             use std::fs::Permissions;
             use std::os::unix::fs::PermissionsExt;
 
-            let mode = file.header().mode()?;
-
-            let has_any_executable_bit = mode & 0o111;
-            if has_any_executable_bit != 0 {
-                if let Some(path) = crate::tar::unpacked_at(dst.as_ref(), &file.path()?) {
-                    let permissions = fs_err::tokio::metadata(&path).await?.permissions();
-                    fs_err::tokio::set_permissions(
-                        &path,
-                        Permissions::from_mode(permissions.mode() | 0o111),
-                    )
-                    .await?;
+            let entry_type = file.header().entry_type();
+            if entry_type.is_file() || entry_type.is_hard_link() {
+                let mode = file.header().mode()?;
+                let has_any_executable_bit = mode & 0o111;
+                if has_any_executable_bit != 0 {
+                    if let Some(path) = crate::tar::unpacked_at(dst.as_ref(), &file.path()?) {
+                        let permissions = fs_err::tokio::metadata(&path).await?.permissions();
+                        fs_err::tokio::set_permissions(
+                            &path,
+                            Permissions::from_mode(permissions.mode() | 0o111),
+                        )
+                        .await?;
+                    }
                 }
             }
         }
     }
     Ok(())
 }
 
 /// Unzip a `.tar.gz` archive into the target directory, without requiring `Seek`.
 ///
 /// This is useful for unpacking files as they're being downloaded.
-pub async fn untar<R: tokio::io::AsyncBufRead + Unpin>(
+pub async fn untar_gz<R: tokio::io::AsyncRead + Unpin>(
     reader: R,
     target: impl AsRef<Path>,
 ) -> Result<(), Error> {
+    let reader = tokio::io::BufReader::new(reader);
     let decompressed_bytes = async_compression::tokio::bufread::GzipDecoder::new(reader);
+
+    let mut archive = tokio_tar::ArchiveBuilder::new(decompressed_bytes)
+        .set_preserve_mtime(false)
+        .build();
+    untar_in(&mut archive, target.as_ref()).await?;
+    Ok(())
+}
+
+/// Unzip a `.tar.bz2` archive into the target directory, without requiring `Seek`.
+///
+/// This is useful for unpacking files as they're being downloaded.
+pub async fn untar_bz2<R: tokio::io::AsyncRead + Unpin>(
+    reader: R,
+    target: impl AsRef<Path>,
+) -> Result<(), Error> {
+    let reader = tokio::io::BufReader::new(reader);
+    let decompressed_bytes = async_compression::tokio::bufread::BzDecoder::new(reader);
+
+    let mut archive = tokio_tar::ArchiveBuilder::new(decompressed_bytes)
+        .set_preserve_mtime(false)
+        .build();
+    untar_in(&mut archive, target.as_ref()).await?;
+    Ok(())
+}
+
+/// Unzip a `.tar.zst` archive into the target directory, without requiring `Seek`.
+///
+/// This is useful for unpacking files as they're being downloaded.
+pub async fn untar_zst<R: tokio::io::AsyncRead + Unpin>(
+    reader: R,
+    target: impl AsRef<Path>,
+) -> Result<(), Error> {
+    let reader = tokio::io::BufReader::new(reader);
+    let decompressed_bytes = async_compression::tokio::bufread::ZstdDecoder::new(reader);
+
     let mut archive = tokio_tar::ArchiveBuilder::new(decompressed_bytes)
         .set_preserve_mtime(false)
         .build();
     Ok(untar_in(&mut archive, target.as_ref()).await?)
 }
 
-/// Unzip a `.zip` or `.tar.gz` archive into the target directory, without requiring `Seek`.
-pub async fn archive<R: tokio::io::AsyncBufRead + Unpin>(
+/// Unzip a `.zip`, `.tar.gz`, or `.tar.bz2` archive into the target directory, without requiring `Seek`.
+pub async fn archive<R: tokio::io::AsyncRead + Unpin>(
     reader: R,
     source: impl AsRef<Path>,
     target: impl AsRef<Path>,
 ) -> Result<(), Error> {
     // `.zip`
     if source
         .as_ref()
@@ -168,20 +215,48 @@
     }
 
     // `.tar.gz`
     if source
         .as_ref()
         .extension()
         .is_some_and(|ext| ext.eq_ignore_ascii_case("gz"))
+        && source.as_ref().file_stem().is_some_and(|stem| {
+            Path::new(stem)
+                .extension()
+                .is_some_and(|ext| ext.eq_ignore_ascii_case("tar"))
+        })
+    {
+        untar_gz(reader, target).await?;
+        return Ok(());
+    }
+
+    // `.tar.bz2`
+    if source
+        .as_ref()
+        .extension()
+        .is_some_and(|ext| ext.eq_ignore_ascii_case("bz2"))
+        && source.as_ref().file_stem().is_some_and(|stem| {
+            Path::new(stem)
+                .extension()
+                .is_some_and(|ext| ext.eq_ignore_ascii_case("tar"))
+        })
     {
-        if source.as_ref().file_stem().is_some_and(|stem| {
+        untar_bz2(reader, target).await?;
+        return Ok(());
+    }
+    // `.tar.zst`
+    if source
+        .as_ref()
+        .extension()
+        .is_some_and(|ext| ext.eq_ignore_ascii_case("zst"))
+        && source.as_ref().file_stem().is_some_and(|stem| {
             Path::new(stem)
                 .extension()
                 .is_some_and(|ext| ext.eq_ignore_ascii_case("tar"))
-        }) {
-            untar(reader, target).await?;
-            return Ok(());
-        }
+        })
+    {
+        untar_zst(reader, target).await?;
+        return Ok(());
     }
 
     Err(Error::UnsupportedArchive(source.as_ref().to_path_buf()))
 }
```

### Comparing `uv-0.1.9/crates/uv-extract/src/sync.rs` & `uv-0.2.0/crates/uv-extract/src/sync.rs`

 * *Files 5% similar despite different names*

```diff
@@ -13,15 +13,15 @@
     reader: R,
     target: &Path,
 ) -> Result<(), Error> {
     // Unzip in parallel.
     let archive = ZipArchive::new(CloneableSeekableReader::new(reader))?;
     let directories = Mutex::new(FxHashSet::default());
     (0..archive.len())
-        .par_bridge()
+        .into_par_iter()
         .map(|file_number| {
             let mut archive = archive.clone();
             let mut file = archive.by_index(file_number)?;
 
             // Determine the path of the file within the wheel.
             let Some(enclosed_name) = file.enclosed_name() else {
                 return Ok(());
```

### Comparing `uv-0.1.9/crates/uv-extract/src/tar.rs` & `uv-0.2.0/crates/uv-extract/src/tar.rs`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/uv-extract/src/vendor/cloneable_seekable_reader.rs` & `uv-0.2.0/crates/uv-extract/src/vendor/cloneable_seekable_reader.rs`

 * *Files 1% similar despite different names*

```diff
@@ -113,14 +113,15 @@
 
 impl<R: HasLength> HasLength for BufReader<R> {
     fn len(&self) -> u64 {
         self.get_ref().len()
     }
 }
 
+#[allow(clippy::disallowed_types)]
 impl HasLength for std::fs::File {
     fn len(&self) -> u64 {
         self.metadata().unwrap().len()
     }
 }
 
 impl HasLength for fs_err::File {
```

### Comparing `uv-0.1.9/crates/pypi-types/Cargo.toml` & `uv-0.2.0/crates/uv-auth/Cargo.toml`

 * *Files 20% similar despite different names*

```diff
@@ -1,35 +1,27 @@
 [package]
-name = "pypi-types"
+name = "uv-auth"
 version = "0.0.1"
-edition = { workspace = true }
-rust-version = { workspace = true }
-homepage = { workspace = true }
-documentation = { workspace = true }
-repository = { workspace = true }
-authors = { workspace = true }
-license = { workspace = true }
-
-[lints]
-workspace = true
+edition = "2021"
 
 [dependencies]
-pep440_rs = { path = "../pep440-rs", features = ["rkyv", "serde"] }
-pep508_rs = { path = "../pep508-rs", features = ["rkyv", "serde"] }
-uv-normalize = { path = "../uv-normalize" }
-
-chrono = { workspace = true, features = ["serde"] }
-mailparse = { workspace = true }
+anyhow = { workspace = true }
+async-trait = { workspace = true }
+base64 = { workspace = true }
+futures = { workspace = true }
+http = { workspace = true }
 once_cell = { workspace = true }
-regex = { workspace = true }
-rkyv = { workspace = true, features = ["strict", "validation"] }
-serde = { workspace = true }
-thiserror = { workspace = true }
+once-map = { workspace = true }
+reqwest = { workspace = true }
+reqwest-middleware = { workspace = true }
+rust-netrc = { workspace = true }
+tokio = { workspace = true }
 tracing = { workspace = true }
 url = { workspace = true }
+urlencoding = { workspace = true }
 
 [dev-dependencies]
-indoc = { version = "2.0.4" }
-insta = { version = "1.34.0" }
-serde_json = { version = "1.0.111" }
-tempfile = { version = "3.9.0" }
-test-case = { version = "3.3.1" }
+tempfile = { workspace = true }
+tokio = { workspace = true }
+wiremock = { workspace = true }
+insta = { version = "1.36.1" }
+test-log = { version = "0.2.15", features = ["trace"], default-features = false }
```

### Comparing `uv-0.1.9/crates/pypi-types/src/base_url.rs` & `uv-0.2.0/crates/pypi-types/src/base_url.rs`

 * *Files 26% similar despite different names*

```diff
@@ -1,41 +1,23 @@
 use serde::{Deserialize, Serialize};
 use url::Url;
 
-/// Join a possibly relative URL to a base URL.
-///
-/// When `maybe_relative` is not relative, then it is parsed and returned with
-/// `base` being ignored.
-///
-/// This is useful for parsing URLs that may be absolute or relative, with a
-/// known base URL, and that doesn't require having already parsed a `BaseUrl`.
-pub fn base_url_join_relative(base: &str, maybe_relative: &str) -> Result<Url, JoinRelativeError> {
-    match Url::parse(maybe_relative) {
-        Ok(absolute) => Ok(absolute),
-        Err(err) => {
-            if err == url::ParseError::RelativeUrlWithoutBase {
-                let base_url = Url::parse(base).map_err(|err| JoinRelativeError::ParseError {
-                    original: base.to_string(),
-                    source: err,
-                })?;
-
-                base_url
-                    .join(maybe_relative)
-                    .map_err(|_| JoinRelativeError::ParseError {
-                        original: format!("{base}/{maybe_relative}"),
-                        source: err,
-                    })
-            } else {
-                Err(JoinRelativeError::ParseError {
-                    original: maybe_relative.to_string(),
-                    source: err,
-                })
-            }
-        }
-    }
+/// Join a relative URL to a base URL.
+pub fn base_url_join_relative(base: &str, relative: &str) -> Result<Url, JoinRelativeError> {
+    let base_url = Url::parse(base).map_err(|err| JoinRelativeError::ParseError {
+        original: base.to_string(),
+        source: err,
+    })?;
+
+    base_url
+        .join(relative)
+        .map_err(|err| JoinRelativeError::ParseError {
+            original: format!("{base}/{relative}"),
+            source: err,
+        })
 }
 
 /// An error that occurs when `base_url_join_relative` fails.
 ///
 /// The error message includes the URL (`base` or `maybe_relative`) passed to
 /// `base_url_join_relative` that provoked the error.
 #[derive(Clone, Debug, thiserror::Error)]
@@ -53,29 +35,14 @@
         serialize_with = "Url::serialize_internal",
         deserialize_with = "Url::deserialize_internal"
     )]
     Url,
 );
 
 impl BaseUrl {
-    /// Parse the given URL. If it's relative, join it to the current [`BaseUrl`]. Allows for
-    /// parsing URLs that may be absolute or relative, with a known base URL.
-    pub fn join_relative(&self, url: &str) -> Result<Url, url::ParseError> {
-        match Url::parse(url) {
-            Ok(url) => Ok(url),
-            Err(err) => {
-                if err == url::ParseError::RelativeUrlWithoutBase {
-                    self.0.join(url)
-                } else {
-                    Err(err)
-                }
-            }
-        }
-    }
-
     /// Return the underlying [`Url`].
     pub fn as_url(&self) -> &Url {
         &self.0
     }
 
     /// Convert to the underlying [`Url`].
     #[must_use]
```

### Comparing `uv-0.1.9/crates/pypi-types/src/direct_url.rs` & `uv-0.2.0/crates/pypi-types/src/direct_url.rs`

 * *Files 18% similar despite different names*

```diff
@@ -16,14 +16,18 @@
     /// ```
     LocalDirectory { url: String, dir_info: DirInfo },
     /// The direct URL is a path to an archive. For example:
     /// ```json
     /// {"archive_info": {"hash": "sha256=75909db2664838d015e3d9139004ee16711748a52c8f336b52882266540215d8", "hashes": {"sha256": "75909db2664838d015e3d9139004ee16711748a52c8f336b52882266540215d8"}}, "url": "https://files.pythonhosted.org/packages/b8/8b/31273bf66016be6ad22bb7345c37ff350276cfd46e389a0c2ac5da9d9073/wheel-0.41.2-py3-none-any.whl"}
     /// ```
     ArchiveUrl {
+        /// The URL without parsed information (such as the Git revision or subdirectory).
+        ///
+        /// For example, for `pip install git+https://github.com/tqdm/tqdm@cc372d09dcd5a5eabdc6ed4cf365bdb0be004d44#subdirectory=.`,
+        /// the URL is `https://github.com/tqdm/tqdm`.
         url: String,
         archive_info: ArchiveInfo,
         #[serde(skip_serializing_if = "Option::is_none")]
         subdirectory: Option<PathBuf>,
     },
     /// The direct URL is path to a VCS repository. For example:
     /// ```json
@@ -71,45 +75,45 @@
     Bzr,
     Svn,
 }
 
 impl std::fmt::Display for VcsKind {
     fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
         match self {
-            VcsKind::Git => write!(f, "git"),
-            VcsKind::Hg => write!(f, "hg"),
-            VcsKind::Bzr => write!(f, "bzr"),
-            VcsKind::Svn => write!(f, "svn"),
+            Self::Git => write!(f, "git"),
+            Self::Hg => write!(f, "hg"),
+            Self::Bzr => write!(f, "bzr"),
+            Self::Svn => write!(f, "svn"),
         }
     }
 }
 
 impl TryFrom<&DirectUrl> for Url {
     type Error = url::ParseError;
 
     fn try_from(value: &DirectUrl) -> Result<Self, Self::Error> {
         match value {
-            DirectUrl::LocalDirectory { url, .. } => Url::parse(url),
+            DirectUrl::LocalDirectory { url, .. } => Self::parse(url),
             DirectUrl::ArchiveUrl {
                 url,
                 subdirectory,
                 archive_info: _,
             } => {
-                let mut url = Url::parse(url)?;
+                let mut url = Self::parse(url)?;
                 if let Some(subdirectory) = subdirectory {
                     url.set_fragment(Some(&format!("subdirectory={}", subdirectory.display())));
                 }
                 Ok(url)
             }
             DirectUrl::VcsUrl {
                 url,
                 vcs_info,
                 subdirectory,
             } => {
-                let mut url = Url::parse(&format!("{}+{}", vcs_info.vcs, url))?;
+                let mut url = Self::parse(&format!("{}+{}", vcs_info.vcs, url))?;
                 if let Some(commit_id) = &vcs_info.commit_id {
                     url.set_path(&format!("{}@{commit_id}", url.path()));
                 } else if let Some(requested_revision) = &vcs_info.requested_revision {
                     url.set_path(&format!("{}@{requested_revision}", url.path()));
                 }
                 if let Some(subdirectory) = subdirectory {
                     url.set_fragment(Some(&format!("subdirectory={}", subdirectory.display())));
```

### Comparing `uv-0.1.9/crates/pypi-types/src/lenient_requirement.rs` & `uv-0.2.0/crates/pypi-types/src/lenient_requirement.rs`

 * *Files 8% similar despite different names*

```diff
@@ -1,106 +1,135 @@
+use std::borrow::Cow;
 use std::str::FromStr;
 
 use once_cell::sync::Lazy;
 use regex::Regex;
 use serde::{de, Deserialize, Deserializer, Serialize};
 use tracing::warn;
 
 use pep440_rs::{VersionSpecifiers, VersionSpecifiersParseError};
-use pep508_rs::{Pep508Error, Requirement};
+use pep508_rs::{Pep508Error, Pep508Url, Requirement, VerbatimUrl};
 
 /// Ex) `>=7.2.0<8.0.0`
 static MISSING_COMMA: Lazy<Regex> = Lazy::new(|| Regex::new(r"(\d)([<>=~^!])").unwrap());
 /// Ex) `!=~5.0`
 static NOT_EQUAL_TILDE: Lazy<Regex> = Lazy::new(|| Regex::new(r"!=~((?:\d\.)*\d)").unwrap());
 /// Ex) `>=1.9.*`, `<3.4.*`
 static INVALID_TRAILING_DOT_STAR: Lazy<Regex> =
     Lazy::new(|| Regex::new(r"(<=|>=|<|>)(\d+(\.\d+)*)\.\*").unwrap());
 /// Ex) `!=3.0*`
 static MISSING_DOT: Lazy<Regex> = Lazy::new(|| Regex::new(r"(\d\.\d)+\*").unwrap());
 /// Ex) `>=3.6,`
 static TRAILING_COMMA: Lazy<Regex> = Lazy::new(|| Regex::new(r",\s*$").unwrap());
-/// Ex) `>= '2.7'`, `>=3.6'`
-static STRAY_QUOTES: Lazy<Regex> = Lazy::new(|| Regex::new(r#"['"]([*\d])|([*\d])['"]"#).unwrap());
 /// Ex) `>dev`
 static GREATER_THAN_DEV: Lazy<Regex> = Lazy::new(|| Regex::new(r">dev").unwrap());
 /// Ex) `>=9.0.0a1.0`
 static TRAILING_ZERO: Lazy<Regex> =
     Lazy::new(|| Regex::new(r"(\d+(\.\d)*(a|b|rc|post|dev)\d+)\.0").unwrap());
 
-/// Regex to match the invalid specifier, replacement to fix it and message about was wrong and
-/// fixed
-static FIXUPS: &[(&Lazy<Regex>, &str, &str)] = &[
+// Search and replace functions that fix invalid specifiers.
+type FixUp = for<'a> fn(&'a str) -> Cow<'a, str>;
+
+/// A list of fixups with a corresponding message about what was fixed.
+static FIXUPS: &[(FixUp, &str)] = &[
     // Given `>=7.2.0<8.0.0`, rewrite to `>=7.2.0,<8.0.0`.
-    (&MISSING_COMMA, r"$1,$2", "inserting missing comma"),
+    (
+        |input| MISSING_COMMA.replace_all(input, r"$1,$2"),
+        "inserting missing comma",
+    ),
     // Given `!=~5.0,>=4.12`, rewrite to `!=5.0.*,>=4.12`.
     (
-        &NOT_EQUAL_TILDE,
-        r"!=${1}.*",
+        |input| NOT_EQUAL_TILDE.replace_all(input, r"!=${1}.*"),
         "replacing invalid tilde with wildcard",
     ),
     // Given `>=1.9.*`, rewrite to `>=1.9`.
     (
-        &INVALID_TRAILING_DOT_STAR,
-        r"${1}${2}",
+        |input| INVALID_TRAILING_DOT_STAR.replace_all(input, r"${1}${2}"),
         "removing star after comparison operator other than equal and not equal",
     ),
     // Given `!=3.0*`, rewrite to `!=3.0.*`.
-    (&MISSING_DOT, r"${1}.*", "inserting missing dot"),
+    (
+        |input| MISSING_DOT.replace_all(input, r"${1}.*"),
+        "inserting missing dot",
+    ),
     // Given `>=3.6,`, rewrite to `>=3.6`
-    (&TRAILING_COMMA, r"${1}", "removing trailing comma"),
-    // Given `>= '2.7'`, rewrite to `>= 2.7`
-    (&STRAY_QUOTES, r"$1$2", "removing stray quotes"),
+    (
+        |input| TRAILING_COMMA.replace_all(input, r"${1}"),
+        "removing trailing comma",
+    ),
     // Given `>dev`, rewrite to `>0.0.0dev`
-    (&GREATER_THAN_DEV, r">0.0.0dev", "assuming 0.0.0dev"),
+    (
+        |input| GREATER_THAN_DEV.replace_all(input, r">0.0.0dev"),
+        "assuming 0.0.0dev",
+    ),
     // Given `>=9.0.0a1.0`, rewrite to `>=9.0.0a1`
-    (&TRAILING_ZERO, r"${1}", "removing trailing zero"),
+    (
+        |input| TRAILING_ZERO.replace_all(input, r"${1}"),
+        "removing trailing zero",
+    ),
+    (remove_stray_quotes, "removing stray quotes"),
 ];
 
+// Given `>= 2.7'`, rewrite to `>= 2.7`
+fn remove_stray_quotes(input: &str) -> Cow<'_, str> {
+    /// Ex) `'>= 2.7'`, `>=3.6'`
+    static STRAY_QUOTES: Lazy<Regex> = Lazy::new(|| Regex::new(r#"['"]"#).unwrap());
+
+    // make sure not to touch markers, which can have quotes (e.g. `python_version >= '3.7'`)
+    match input.find(';') {
+        Some(markers) => {
+            let requirement = STRAY_QUOTES.replace_all(&input[..markers], "");
+            format!("{}{}", requirement, &input[markers..]).into()
+        }
+        None => STRAY_QUOTES.replace_all(input, ""),
+    }
+}
+
 fn parse_with_fixups<Err, T: FromStr<Err = Err>>(input: &str, type_name: &str) -> Result<T, Err> {
     match T::from_str(input) {
         Ok(requirement) => Ok(requirement),
         Err(err) => {
             let mut patched_input = input.to_string();
             let mut messages = Vec::new();
-            for (matcher, replacement, message) in FIXUPS {
-                let patched = matcher.replace_all(patched_input.as_ref(), *replacement);
+            for (fixup, message) in FIXUPS {
+                let patched = fixup(patched_input.as_ref());
                 if patched != patched_input {
                     messages.push(*message);
+
+                    if let Ok(requirement) = T::from_str(&patched) {
+                        warn!(
+                            "Fixing invalid {type_name} by {} (before: `{input}`; after: `{patched}`)",
+                            messages.join(", ")
+                        );
+                        return Ok(requirement);
+                    }
+
                     patched_input = patched.to_string();
                 }
             }
 
-            if let Ok(requirement) = T::from_str(&patched_input) {
-                warn!(
-                    "Fixing invalid {type_name} by {} (before: `{input}`; after: `{patched_input}`)",
-                    messages.join(", ")
-                );
-                return Ok(requirement);
-            }
-
             Err(err)
         }
     }
 }
 
 /// Like [`Requirement`], but attempts to correct some common errors in user-provided requirements.
 #[derive(Debug, Clone, Serialize, Deserialize, Eq, PartialEq)]
-pub struct LenientRequirement(Requirement);
+pub struct LenientRequirement<T: Pep508Url = VerbatimUrl>(Requirement<T>);
 
-impl FromStr for LenientRequirement {
-    type Err = Pep508Error;
+impl<T: Pep508Url> FromStr for LenientRequirement<T> {
+    type Err = Pep508Error<T>;
 
     fn from_str(input: &str) -> Result<Self, Self::Err> {
         Ok(Self(parse_with_fixups(input, "requirement")?))
     }
 }
 
-impl From<LenientRequirement> for Requirement {
-    fn from(requirement: LenientRequirement) -> Self {
+impl<T: Pep508Url> From<LenientRequirement<T>> for Requirement<T> {
+    fn from(requirement: LenientRequirement<T>) -> Self {
         requirement.0
     }
 }
 
 /// Like [`VersionSpecifiers`], but attempts to correct some common errors in user-provided requirements.
 ///
 /// For example, we turn `>=3.x.*` into `>=3.x`.
@@ -357,8 +386,36 @@
 
         let actual: VersionSpecifiers = LenientVersionSpecifiers::from_str(">=9a1.0")
             .unwrap()
             .into();
         let expected: VersionSpecifiers = VersionSpecifiers::from_str(">=9a1").unwrap();
         assert_eq!(actual, expected);
     }
+
+    /// <https://github.com/astral-sh/uv/issues/2551>
+    #[test]
+    fn stray_quote_preserve_marker() {
+        let actual: Requirement =
+            LenientRequirement::from_str("numpy >=1.19; python_version >= \"3.7\"")
+                .unwrap()
+                .into();
+        let expected: Requirement =
+            Requirement::from_str("numpy >=1.19; python_version >= \"3.7\"").unwrap();
+        assert_eq!(actual, expected);
+
+        let actual: Requirement =
+            LenientRequirement::from_str("numpy \">=1.19\"; python_version >= \"3.7\"")
+                .unwrap()
+                .into();
+        let expected: Requirement =
+            Requirement::from_str("numpy >=1.19; python_version >= \"3.7\"").unwrap();
+        assert_eq!(actual, expected);
+
+        let actual: Requirement =
+            LenientRequirement::from_str("'numpy' >=1.19\"; python_version >= \"3.7\"")
+                .unwrap()
+                .into();
+        let expected: Requirement =
+            Requirement::from_str("numpy >=1.19; python_version >= \"3.7\"").unwrap();
+        assert_eq!(actual, expected);
+    }
 }
```

### Comparing `uv-0.1.9/crates/once-map/src/lib.rs` & `uv-0.2.0/crates/once-map/src/lib.rs`

 * *Files 16% similar despite different names*

```diff
@@ -1,25 +1,28 @@
 use std::borrow::Borrow;
-use std::hash::Hash;
+use std::hash::{BuildHasher, Hash, RandomState};
 use std::sync::Arc;
 
 use dashmap::DashMap;
 use tokio::sync::Notify;
 
 /// Run tasks only once and store the results in a parallel hash map.
 ///
 /// We often have jobs `Fn(K) -> V` that we only want to run once and memoize, e.g. network
 /// requests for metadata. When multiple tasks start the same query in parallel, e.g. through source
 /// dist builds, we want to wait until the other task is done and get a reference to the same
 /// result.
-pub struct OnceMap<K: Eq + Hash, V> {
-    items: DashMap<K, Value<V>>,
+///
+/// Note that this always clones the value out of the underlying map. Because
+/// of this, it's common to wrap the `V` in an `Arc<V>` to make cloning cheap.
+pub struct OnceMap<K, V, H = RandomState> {
+    items: DashMap<K, Value<V>, H>,
 }
 
-impl<K: Eq + Hash, V> OnceMap<K, V> {
+impl<K: Eq + Hash, V: Clone, H: BuildHasher + Clone> OnceMap<K, V, H> {
     /// Register that you want to start a job.
     ///
     /// If this method returns `true`, you need to start a job and call [`OnceMap::done`] eventually
     /// or other tasks will hang. If it returns `false`, this job is already in progress and you
     /// can [`OnceMap::wait`] for the result.
     pub fn register(&self, key: K) -> bool {
         let entry = self.items.entry(key);
@@ -30,24 +33,23 @@
                 true
             }
         }
     }
 
     /// Submit the result of a job you registered.
     pub fn done(&self, key: K, value: V) {
-        if let Some(Value::Waiting(notify)) = self.items.insert(key, Value::Filled(Arc::new(value)))
-        {
+        if let Some(Value::Waiting(notify)) = self.items.insert(key, Value::Filled(value)) {
             notify.notify_waiters();
         }
     }
 
     /// Wait for the result of a job that is running.
     ///
     /// Will hang if [`OnceMap::done`] isn't called for this key.
-    pub async fn wait(&self, key: &K) -> Option<Arc<V>> {
+    pub async fn wait(&self, key: &K) -> Option<V> {
         let entry = self.items.get(key)?;
         match entry.value() {
             Value::Filled(value) => Some(value.clone()),
             Value::Waiting(notify) => {
                 let notify = notify.clone();
                 drop(entry);
                 notify.notified().await;
@@ -57,32 +59,53 @@
                     Value::Filled(value) => Some(value.clone()),
                     Value::Waiting(_) => unreachable!("notify was called"),
                 }
             }
         }
     }
 
+    /// Wait for the result of a job that is running, in a blocking context.
+    ///
+    /// Will hang if [`OnceMap::done`] isn't called for this key.
+    pub fn wait_blocking(&self, key: &K) -> Option<V> {
+        let entry = self.items.get(key)?;
+        match entry.value() {
+            Value::Filled(value) => Some(value.clone()),
+            Value::Waiting(notify) => {
+                let notify = notify.clone();
+                drop(entry);
+                futures::executor::block_on(notify.notified());
+
+                let entry = self.items.get(key).expect("map is append-only");
+                match entry.value() {
+                    Value::Filled(value) => Some(value.clone()),
+                    Value::Waiting(_) => unreachable!("notify was called"),
+                }
+            }
+        }
+    }
+
     /// Return the result of a previous job, if any.
-    pub fn get<Q: ?Sized + Hash + Eq>(&self, key: &Q) -> Option<Arc<V>>
+    pub fn get<Q: ?Sized + Hash + Eq>(&self, key: &Q) -> Option<V>
     where
         K: Borrow<Q>,
     {
         let entry = self.items.get(key)?;
         match entry.value() {
             Value::Filled(value) => Some(value.clone()),
             Value::Waiting(_) => None,
         }
     }
 }
 
-impl<K: Eq + Hash + Clone, V> Default for OnceMap<K, V> {
+impl<K: Eq + Hash + Clone, V, H: Default + BuildHasher + Clone> Default for OnceMap<K, V, H> {
     fn default() -> Self {
         Self {
-            items: DashMap::new(),
+            items: DashMap::with_hasher(H::default()),
         }
     }
 }
 
 enum Value<V> {
     Waiting(Arc<Notify>),
-    Filled(Arc<V>),
+    Filled(V),
 }
```

### Comparing `uv-0.1.9/crates/uv-client/Cargo.toml` & `uv-0.2.0/crates/uv-extract/Cargo.toml`

 * *Files 20% similar despite different names*

```diff
@@ -1,51 +1,31 @@
 [package]
-name = "uv-client"
+name = "uv-extract"
 version = "0.0.1"
-edition = "2021"
+edition = { workspace = true }
+rust-version = { workspace = true }
+homepage = { workspace = true }
+documentation = { workspace = true }
+repository = { workspace = true }
+authors = { workspace = true }
+license = { workspace = true }
+
+[lints]
+workspace = true
 
 [dependencies]
-cache-key = { path = "../cache-key" }
-distribution-filename = { path = "../distribution-filename", features = ["rkyv", "serde"] }
-distribution-types = { path = "../distribution-types" }
-install-wheel-rs = { path = "../install-wheel-rs" }
-pep440_rs = { path = "../pep440-rs" }
-pep508_rs = { path = "../pep508-rs" }
-platform-tags = { path = "../platform-tags" }
-uv-auth = { path = "../uv-auth" }
-uv-cache = { path = "../uv-cache" }
-uv-fs = { path = "../uv-fs", features = ["tokio"] }
-uv-normalize = { path = "../uv-normalize" }
-uv-warnings = { path = "../uv-warnings" }
-pypi-types = { path = "../pypi-types" }
+pypi-types = { workspace = true }
 
-async-trait = { workspace = true }
-async_http_range_reader = { workspace = true }
+async-compression = { workspace = true, features = ["bzip2", "gzip", "zstd"] }
 async_zip = { workspace = true, features = ["tokio"] }
-chrono = { workspace = true }
 fs-err = { workspace = true, features = ["tokio"] }
 futures = { workspace = true }
-html-escape = { workspace = true }
-http = { workspace = true }
-reqwest = { workspace = true }
-reqwest-middleware = { workspace = true }
-reqwest-retry = { workspace = true }
-rkyv = { workspace = true, features = ["strict", "validation"] }
-rmp-serde = { workspace = true }
+md-5.workspace = true
+rayon = { workspace = true }
 rustc-hash = { workspace = true }
-serde = { workspace = true }
-serde_json = { workspace = true }
 sha2 = { workspace = true }
-task-local-extensions = { workspace = true }
-tempfile = { workspace = true }
 thiserror = { workspace = true }
-tl = { workspace = true }
-tokio = { workspace = true, features = ["fs"] }
-tokio-util = { workspace = true }
+tokio = { workspace = true }
+tokio-tar = { workspace = true }
+tokio-util = { workspace = true, features = ["compat"] }
 tracing = { workspace = true }
-url = { workspace = true }
-urlencoding = { workspace = true }
-
-[dev-dependencies]
-anyhow = { workspace = true }
-insta = { version = "1.34.0" }
-tokio = { workspace = true, features = ["fs", "macros"] }
+zip = { workspace = true }
```

### Comparing `uv-0.1.9/crates/uv-client/src/cached_client.rs` & `uv-0.2.0/crates/uv-client/src/cached_client.rs`

 * *Files 3% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 use std::{borrow::Cow, future::Future, path::Path};
 
 use futures::FutureExt;
 use reqwest::{Request, Response};
-use reqwest_middleware::ClientWithMiddleware;
 use rkyv::util::AlignedVec;
 use serde::de::DeserializeOwned;
 use serde::{Deserialize, Serialize};
 use tracing::{debug, info_span, instrument, trace, warn, Instrument};
 
 use uv_cache::{CacheEntry, Freshness};
 use uv_fs::write_atomic;
 
+use crate::BaseClient;
 use crate::{
     httpcache::{AfterResponse, BeforeRequest, CachePolicy, CachePolicyBuilder},
     rkyvutil::OwnedArchive,
     Error, ErrorKind,
 };
 
 /// A trait the generalizes (de)serialization at a high level.
@@ -26,15 +26,15 @@
 /// should just use `CachedClient::get_serde`. This will use a default
 /// implementation of `Cacheable` internally.
 ///
 /// Alternatively, callers using `rkyv` should use
 /// `CachedClient::get_cacheable`. If your types fit into the
 /// `rkyvutil::OwnedArchive` mold, then an implementation of `Cacheable` is
 /// already provided for that type.
-pub trait Cacheable: Sized + Send {
+pub trait Cacheable: Sized {
     /// This associated type permits customizing what the "output" type of
     /// deserialization is. It can be identical to `Self`.
     ///
     /// Typical use of this is for wrapper types used to proviate blanket trait
     /// impls without hitting overlapping impl problems.
     type Target;
 
@@ -50,15 +50,15 @@
 /// implement `Cacheable`.
 #[derive(Debug, Deserialize, Serialize)]
 #[serde(transparent)]
 pub struct SerdeCacheable<T> {
     inner: T,
 }
 
-impl<T: Send + Serialize + DeserializeOwned> Cacheable for SerdeCacheable<T> {
+impl<T: Serialize + DeserializeOwned> Cacheable for SerdeCacheable<T> {
     type Target = T;
 
     fn from_aligned_bytes(bytes: AlignedVec) -> Result<T, Error> {
         Ok(rmp_serde::from_slice::<T>(&bytes).map_err(ErrorKind::Decode)?)
     }
 
     fn to_bytes(&self) -> Result<Cow<'_, [u8]>, Error> {
@@ -71,26 +71,26 @@
         self.inner
     }
 }
 
 /// All `OwnedArchive` values are cacheable.
 impl<A> Cacheable for OwnedArchive<A>
 where
-    A: rkyv::Archive + rkyv::Serialize<crate::rkyvutil::Serializer<4096>> + Send,
+    A: rkyv::Archive + rkyv::Serialize<crate::rkyvutil::Serializer<4096>>,
     A::Archived: for<'a> rkyv::CheckBytes<rkyv::validation::validators::DefaultValidator<'a>>
         + rkyv::Deserialize<A, rkyv::de::deserializers::SharedDeserializeMap>,
 {
-    type Target = OwnedArchive<A>;
+    type Target = Self;
 
-    fn from_aligned_bytes(bytes: AlignedVec) -> Result<OwnedArchive<A>, Error> {
-        OwnedArchive::new(bytes)
+    fn from_aligned_bytes(bytes: AlignedVec) -> Result<Self, Error> {
+        Self::new(bytes)
     }
 
     fn to_bytes(&self) -> Result<Cow<'_, [u8]>, Error> {
-        Ok(Cow::from(OwnedArchive::as_bytes(self)))
+        Ok(Cow::from(Self::as_bytes(self)))
     }
 
     fn into_target(self) -> Self::Target {
         self
     }
 }
 
@@ -99,26 +99,26 @@
 pub enum CachedClientError<CallbackError> {
     Client(Error),
     Callback(CallbackError),
 }
 
 impl<CallbackError> From<Error> for CachedClientError<CallbackError> {
     fn from(error: Error) -> Self {
-        CachedClientError::Client(error)
+        Self::Client(error)
     }
 }
 
 impl<CallbackError> From<ErrorKind> for CachedClientError<CallbackError> {
     fn from(error: ErrorKind) -> Self {
-        CachedClientError::Client(error.into())
+        Self::Client(error.into())
     }
 }
 
-impl<E: Into<Error>> From<CachedClientError<E>> for Error {
-    fn from(error: CachedClientError<E>) -> Error {
+impl<E: Into<Self>> From<CachedClientError<E>> for Error {
+    fn from(error: CachedClientError<E>) -> Self {
         match error {
             CachedClientError::Client(error) => error,
             CachedClientError::Callback(error) => error.into(),
         }
     }
 }
 
@@ -131,17 +131,17 @@
     /// Allow the client to return stale responses.
     AllowStale,
 }
 
 impl From<Freshness> for CacheControl {
     fn from(value: Freshness) -> Self {
         match value {
-            Freshness::Fresh => CacheControl::None,
-            Freshness::Stale => CacheControl::MustRevalidate,
-            Freshness::Missing => CacheControl::None,
+            Freshness::Fresh => Self::None,
+            Freshness::Stale => Self::MustRevalidate,
+            Freshness::Missing => Self::None,
         }
     }
 }
 
 /// Custom caching layer over [`reqwest::Client`].
 ///
 /// The implementation takes inspiration from the `http-cache` crate, but adds support for running
@@ -154,49 +154,49 @@
 ///
 /// Unlike `http-cache`, all outputs must be serializable/deserializable in some way, by
 /// implementing the `Cacheable` trait.
 ///
 /// Again unlike `http-cache`, the caller gets full control over the cache key with the assumption
 /// that it's a file.
 #[derive(Debug, Clone)]
-pub struct CachedClient(ClientWithMiddleware);
+pub struct CachedClient(BaseClient);
 
 impl CachedClient {
-    pub fn new(client: ClientWithMiddleware) -> Self {
+    pub fn new(client: BaseClient) -> Self {
         Self(client)
     }
 
-    /// The middleware is the retry strategy
-    pub fn uncached(&self) -> ClientWithMiddleware {
+    /// The base client
+    pub fn uncached(&self) -> BaseClient {
         self.0.clone()
     }
 
     /// Make a cached request with a custom response transformation
     /// while using serde to (de)serialize cached responses.
     ///
     /// If a new response was received (no prior cached response or modified
     /// on the remote), the response is passed through `response_callback` and
     /// only the result is cached and returned. The `response_callback` is
     /// allowed to make subsequent requests, e.g. through the uncached client.
     #[instrument(skip_all)]
     pub async fn get_serde<
-        Payload: Serialize + DeserializeOwned + Send + 'static,
+        Payload: Serialize + DeserializeOwned + 'static,
         CallBackError,
         Callback,
         CallbackReturn,
     >(
         &self,
         req: Request,
         cache_entry: &CacheEntry,
         cache_control: CacheControl,
         response_callback: Callback,
     ) -> Result<Payload, CachedClientError<CallBackError>>
     where
-        Callback: FnOnce(Response) -> CallbackReturn + Send,
-        CallbackReturn: Future<Output = Result<Payload, CallBackError>> + Send,
+        Callback: FnOnce(Response) -> CallbackReturn,
+        CallbackReturn: Future<Output = Result<Payload, CallBackError>>,
     {
         let payload = self
             .get_cacheable(req, cache_entry, cache_control, move |resp| async {
                 let payload = response_callback(resp).await?;
                 Ok(SerdeCacheable { inner: payload })
             })
             .await?;
@@ -221,19 +221,23 @@
         req: Request,
         cache_entry: &CacheEntry,
         cache_control: CacheControl,
         response_callback: Callback,
     ) -> Result<Payload::Target, CachedClientError<CallBackError>>
     where
         Callback: FnOnce(Response) -> CallbackReturn,
-        CallbackReturn: Future<Output = Result<Payload, CallBackError>> + Send,
+        CallbackReturn: Future<Output = Result<Payload, CallBackError>>,
     {
         let fresh_req = req.try_clone().expect("HTTP request must be cloneable");
         let cached_response = match Self::read_cache(cache_entry).await {
-            Some(cached) => self.send_cached(req, cache_control, cached).boxed().await?,
+            Some(cached) => {
+                self.send_cached(req, cache_control, cached)
+                    .boxed_local()
+                    .await?
+            }
             None => {
                 debug!("No cache entry for: {}", req.url());
                 let (response, cache_policy) = self.fresh_request(req).await?;
                 CachedResponse::ModifiedOrNew {
                     response,
                     cache_policy,
                 }
@@ -276,29 +280,70 @@
                 .instrument(refresh_cache)
                 .await
             }
             CachedResponse::ModifiedOrNew {
                 response,
                 cache_policy,
             } => {
-                self.run_response_callback(cache_entry, cache_policy, response, response_callback)
+                // If we got a modified response, but it's a 304, then a validator failed (e.g., the
+                // ETag didn't match). We need to make a fresh request.
+                if response.status() == http::StatusCode::NOT_MODIFIED {
+                    warn!("Server returned unusable 304 for: {}", fresh_req.url());
+                    self.resend_and_heal_cache(fresh_req, cache_entry, response_callback)
+                        .await
+                } else {
+                    self.run_response_callback(
+                        cache_entry,
+                        cache_policy,
+                        response,
+                        response_callback,
+                    )
                     .await
+                }
             }
         }
     }
 
+    /// Make a request without checking whether the cache is fresh.
+    pub async fn skip_cache<
+        Payload: Serialize + DeserializeOwned + 'static,
+        CallBackError,
+        Callback,
+        CallbackReturn,
+    >(
+        &self,
+        req: Request,
+        cache_entry: &CacheEntry,
+        response_callback: Callback,
+    ) -> Result<Payload, CachedClientError<CallBackError>>
+    where
+        Callback: FnOnce(Response) -> CallbackReturn,
+        CallbackReturn: Future<Output = Result<Payload, CallBackError>>,
+    {
+        let (response, cache_policy) = self.fresh_request(req).await?;
+
+        let payload = self
+            .run_response_callback(cache_entry, cache_policy, response, move |resp| async {
+                let payload = response_callback(resp).await?;
+                Ok(SerdeCacheable { inner: payload })
+            })
+            .await?;
+
+        Ok(payload)
+    }
+
     async fn resend_and_heal_cache<Payload: Cacheable, CallBackError, Callback, CallbackReturn>(
         &self,
         req: Request,
         cache_entry: &CacheEntry,
         response_callback: Callback,
     ) -> Result<Payload::Target, CachedClientError<CallBackError>>
     where
         Callback: FnOnce(Response) -> CallbackReturn,
-        CallbackReturn: Future<Output = Result<Payload, CallBackError>> + Send,
+        CallbackReturn: Future<Output = Result<Payload, CallBackError>>,
     {
         let _ = fs_err::tokio::remove_file(&cache_entry.path()).await;
         let (response, cache_policy) = self.fresh_request(req).await?;
         self.run_response_callback(cache_entry, cache_policy, response, response_callback)
             .await
     }
 
@@ -307,19 +352,19 @@
         cache_entry: &CacheEntry,
         cache_policy: Option<Box<CachePolicy>>,
         response: Response,
         response_callback: Callback,
     ) -> Result<Payload::Target, CachedClientError<CallBackError>>
     where
         Callback: FnOnce(Response) -> CallbackReturn,
-        CallbackReturn: Future<Output = Result<Payload, CallBackError>> + Send,
+        CallbackReturn: Future<Output = Result<Payload, CallBackError>>,
     {
         let new_cache = info_span!("new_cache", file = %cache_entry.path().display());
         let data = response_callback(response)
-            .boxed()
+            .boxed_local()
             .await
             .map_err(|err| CachedClientError::Callback(err))?;
         let Some(cache_policy) = cache_policy else {
             return Ok(data.into_target());
         };
         async {
             fs_err::tokio::create_dir_all(cache_entry.dir())
@@ -332,25 +377,22 @@
                 .map_err(ErrorKind::CacheWrite)?;
             Ok(data.into_target())
         }
         .instrument(new_cache)
         .await
     }
 
+    #[instrument(name="read_and_parse_cache", skip_all, fields(file = %cache_entry.path().display()))]
     async fn read_cache(cache_entry: &CacheEntry) -> Option<DataWithCachePolicy> {
-        let span = info_span!("read_and_parse_cache", file = %cache_entry.path().display());
-        match span
-            .in_scope(|| DataWithCachePolicy::from_path_async(cache_entry.path()))
-            .await
-        {
+        match DataWithCachePolicy::from_path_async(cache_entry.path()).await {
             Ok(data) => Some(data),
             Err(err) => {
                 // When we know the cache entry doesn't exist, then things are
                 // normal and we shouldn't emit a WARN.
-                if err.kind().is_file_not_exists() {
+                if err.is_file_not_exists() {
                     trace!("No cache entry exists for {}", cache_entry.path().display());
                 } else {
                     warn!(
                         "Broken cache policy entry at {}, removing: {err}",
                         cache_entry.path().display()
                     );
                     let _ = fs_err::tokio::remove_file(&cache_entry.path()).await;
@@ -420,17 +462,17 @@
         let url = req.url().clone();
         debug!("Sending revalidation request for: {url}");
         let response = self
             .0
             .execute(req)
             .instrument(info_span!("revalidation_request", url = url.as_str()))
             .await
-            .map_err(ErrorKind::from_middleware)?
+            .map_err(ErrorKind::from)?
             .error_for_status()
-            .map_err(ErrorKind::RequestError)?;
+            .map_err(ErrorKind::from)?;
         match cached
             .cache_policy
             .after_response(new_cache_policy_builder, &response)
         {
             AfterResponse::NotModified(new_policy) => {
                 debug!("Found not-modified response for: {url}");
                 Ok(CachedResponse::NotModified {
@@ -458,17 +500,17 @@
     ) -> Result<(Response, Option<Box<CachePolicy>>), Error> {
         trace!("Sending fresh {} request for {}", req.method(), req.url());
         let cache_policy_builder = CachePolicyBuilder::new(&req);
         let response = self
             .0
             .execute(req)
             .await
-            .map_err(ErrorKind::from_middleware)?
+            .map_err(ErrorKind::from)?
             .error_for_status()
-            .map_err(ErrorKind::RequestError)?;
+            .map_err(ErrorKind::from)?;
         let cache_policy = cache_policy_builder.build(&response);
         let cache_policy = if cache_policy.to_archived().is_storable() {
             Some(Box::new(cache_policy))
         } else {
             None
         };
         Ok((response, cache_policy))
@@ -558,78 +600,79 @@
     /// Loads cached data and its associated HTTP cache policy from the given
     /// file path in an asynchronous fashion (via `spawn_blocking`).
     ///
     /// # Errors
     ///
     /// If the given byte buffer is not in a valid format or if reading the
     /// file given fails, then this returns an error.
-    async fn from_path_async(path: &Path) -> Result<DataWithCachePolicy, Error> {
+    async fn from_path_async(path: &Path) -> Result<Self, Error> {
         let path = path.to_path_buf();
-        tokio::task::spawn_blocking(move || DataWithCachePolicy::from_path_sync(&path))
+        tokio::task::spawn_blocking(move || Self::from_path_sync(&path))
             .await
             // This just forwards panics from the closure.
             .unwrap()
     }
 
     /// Loads cached data and its associated HTTP cache policy from the given
     /// file path in a synchronous fashion.
     ///
     /// # Errors
     ///
     /// If the given byte buffer is not in a valid format or if reading the
     /// file given fails, then this returns an error.
-    fn from_path_sync(path: &Path) -> Result<DataWithCachePolicy, Error> {
+    #[instrument]
+    fn from_path_sync(path: &Path) -> Result<Self, Error> {
         let file = fs_err::File::open(path).map_err(ErrorKind::Io)?;
         // Note that we don't wrap our file in a buffer because it will just
         // get passed to AlignedVec::extend_from_reader, which doesn't benefit
         // from an intermediary buffer. In effect, the AlignedVec acts as the
         // buffer.
-        DataWithCachePolicy::from_reader(file)
+        Self::from_reader(file)
     }
 
     /// Loads cached data and its associated HTTP cache policy from the given
     /// reader.
     ///
     /// # Errors
     ///
     /// If the given byte buffer is not in a valid format or if the reader
     /// fails, then this returns an error.
-    pub fn from_reader(mut rdr: impl std::io::Read) -> Result<DataWithCachePolicy, Error> {
+    pub fn from_reader(mut rdr: impl std::io::Read) -> Result<Self, Error> {
         let mut aligned_bytes = rkyv::util::AlignedVec::new();
         aligned_bytes
             .extend_from_reader(&mut rdr)
             .map_err(ErrorKind::Io)?;
-        DataWithCachePolicy::from_aligned_bytes(aligned_bytes)
+        Self::from_aligned_bytes(aligned_bytes)
     }
 
     /// Loads cached data and its associated HTTP cache policy form an in
     /// memory byte buffer.
     ///
     /// # Errors
     ///
     /// If the given byte buffer is not in a valid format, then this
     /// returns an error.
-    fn from_aligned_bytes(mut bytes: AlignedVec) -> Result<DataWithCachePolicy, Error> {
-        let cache_policy = DataWithCachePolicy::deserialize_cache_policy(&mut bytes)?;
-        Ok(DataWithCachePolicy {
+    fn from_aligned_bytes(mut bytes: AlignedVec) -> Result<Self, Error> {
+        let cache_policy = Self::deserialize_cache_policy(&mut bytes)?;
+        Ok(Self {
             data: bytes,
             cache_policy,
         })
     }
 
     /// Serializes the given cache policy and arbitrary data blob to an in
     /// memory byte buffer.
     ///
     /// # Errors
     ///
     /// If there was a problem converting the given cache policy to its
     /// serialized representation, then this routine will return an error.
     fn serialize(cache_policy: &CachePolicy, data: &[u8]) -> Result<Vec<u8>, Error> {
         let mut buf = vec![];
-        DataWithCachePolicy::serialize_to_writer(cache_policy, data, &mut buf)?;
+        Self::serialize_to_writer(cache_policy, data, &mut buf)?;
         Ok(buf)
     }
 
     /// Serializes the given cache policy and arbitrary data blob to the given
     /// writer.
     ///
     /// # Errors
@@ -665,15 +708,15 @@
     /// # Errors
     ///
     /// This returns an error if the cache policy could not be deserialized
     /// from the end of the given bytes.
     fn deserialize_cache_policy(
         bytes: &mut AlignedVec,
     ) -> Result<OwnedArchive<CachePolicy>, Error> {
-        let len = DataWithCachePolicy::deserialize_cache_policy_len(bytes)?;
+        let len = Self::deserialize_cache_policy_len(bytes)?;
         let cache_policy_bytes_start = bytes.len() - (len + 8);
         let cache_policy_bytes = &bytes[cache_policy_bytes_start..][..len];
         let mut cache_policy_bytes_aligned = AlignedVec::with_capacity(len);
         cache_policy_bytes_aligned.extend_from_slice(cache_policy_bytes);
         assert!(
             cache_policy_bytes_start <= bytes.len(),
             "slicing cache policy should result in a truncation"
@@ -707,17 +750,16 @@
             return Err(ErrorKind::ArchiveRead(msg).into());
         };
         let cache_policy_len_bytes = <[u8; 8]>::try_from(&bytes[cache_policy_len_start..])
             .expect("cache policy length is 8 bytes");
         let len_u64 = u64::from_le_bytes(cache_policy_len_bytes);
         let Ok(len_usize) = usize::try_from(len_u64) else {
             let msg = format!(
-                "data-with-cache-policy has cache policy length of {}, \
+                "data-with-cache-policy has cache policy length of {len_u64}, \
                  but overflows usize",
-                len_u64,
             );
             return Err(ErrorKind::ArchiveRead(msg).into());
         };
         if bytes.len() < len_usize + 8 {
             let msg = format!(
                 "invalid cache entry: data-with-cache-policy has cache policy length of {}, \
                  but total buffer size is {}",
```

### Comparing `uv-0.1.9/crates/uv-client/src/error.rs` & `uv-0.2.0/crates/uv-distribution/src/error.rs`

 * *Files 21% similar despite different names*

```diff
@@ -1,212 +1,199 @@
-use async_http_range_reader::AsyncHttpRangeReaderError;
-use async_zip::error::ZipError;
-use url::Url;
+use std::path::PathBuf;
 
-use distribution_filename::{WheelFilename, WheelFilenameError};
-use uv_normalize::PackageName;
-
-use crate::html;
-use crate::middleware::OfflineError;
-
-#[derive(Debug, thiserror::Error)]
-#[error(transparent)]
-pub struct Error {
-    kind: Box<ErrorKind>,
-}
-
-impl Error {
-    pub fn into_kind(self) -> ErrorKind {
-        *self.kind
-    }
+use tokio::task::JoinError;
+use zip::result::ZipError;
 
-    pub fn kind(&self) -> &ErrorKind {
-        &self.kind
-    }
-
-    pub(crate) fn from_json_err(err: serde_json::Error, url: Url) -> Self {
-        ErrorKind::BadJson { source: err, url }.into()
-    }
-
-    pub(crate) fn from_html_err(err: html::Error, url: Url) -> Self {
-        ErrorKind::BadHtml { source: err, url }.into()
-    }
-}
-
-impl From<ErrorKind> for Error {
-    fn from(kind: ErrorKind) -> Error {
-        Error {
-            kind: Box::new(kind),
-        }
-    }
-}
+use distribution_filename::WheelFilenameError;
+use distribution_types::ParsedUrlError;
+use pep440_rs::Version;
+use pypi_types::HashDigest;
+use uv_client::BetterReqwestError;
+use uv_fs::Simplified;
+use uv_normalize::PackageName;
 
 #[derive(Debug, thiserror::Error)]
-pub enum ErrorKind {
-    /// An invalid URL was provided.
-    #[error(transparent)]
-    UrlParseError(#[from] url::ParseError),
-
-    /// A base URL could not be joined with a possibly relative URL.
-    #[error(transparent)]
-    JoinRelativeError(#[from] pypi_types::JoinRelativeError),
-
-    /// Dist-info error
-    #[error(transparent)]
-    InstallWheel(#[from] install_wheel_rs::Error),
-
-    #[error("{0} isn't available locally, but making network requests to registries was banned.")]
-    NoIndex(String),
-
-    /// The package was not found in the registry.
-    ///
-    /// Make sure the package name is spelled correctly and that you've
-    /// configured the right registry to fetch it from.
-    #[error("Package `{0}` was not found in the registry.")]
-    PackageNotFound(String),
-
-    /// The metadata file could not be parsed.
-    #[error("Couldn't parse metadata of {0} from {1}")]
-    MetadataParseError(WheelFilename, String, #[source] Box<pypi_types::Error>),
-
-    /// The metadata file was not found in the wheel.
-    #[error("Metadata file `{0}` was not found in {1}")]
-    MetadataNotFound(WheelFilename, String),
-
-    /// The metadata file was not found in the registry.
-    #[error("File `{0}` was not found in the registry at {1}.")]
-    FileNotFound(String, #[source] reqwest::Error),
-
-    /// A generic request error happened while making a request. Refer to the
-    /// error message for more details.
-    #[error(transparent)]
-    RequestError(#[from] reqwest::Error),
-
-    /// A generic request middleware error happened while making a request.
-    /// Refer to the error message for more details.
-    #[error(transparent)]
-    RequestMiddlewareError(#[from] reqwest_middleware::Error),
-
-    #[error("Received some unexpected JSON from {url}")]
-    BadJson { source: serde_json::Error, url: Url },
-
-    #[error("Received some unexpected HTML from {url}")]
-    BadHtml { source: html::Error, url: Url },
-
-    #[error(transparent)]
-    AsyncHttpRangeReader(#[from] AsyncHttpRangeReaderError),
-
-    #[error("Expected a single .dist-info directory in {0}, found {1}")]
-    InvalidDistInfo(WheelFilename, String),
-
-    #[error("{0} is not a valid wheel filename")]
-    WheelFilename(#[source] WheelFilenameError),
-
+pub enum Error {
+    #[error("Building source distributions is disabled")]
+    NoBuild,
+    #[error("Using pre-built wheels is disabled")]
+    NoBinary,
+
+    // Network error
+    #[error("Failed to parse URL: {0}")]
+    Url(String, #[source] url::ParseError),
+    #[error("Expected an absolute path, but received: {}", _0.user_display())]
+    RelativePath(PathBuf),
+    #[error(transparent)]
+    JoinRelativeUrl(#[from] pypi_types::JoinRelativeError),
+    #[error("Git operation failed")]
+    Git(#[source] anyhow::Error),
+    #[error(transparent)]
+    DirectUrl(#[from] Box<ParsedUrlError>),
+    #[error(transparent)]
+    Reqwest(#[from] BetterReqwestError),
+    #[error(transparent)]
+    Client(#[from] uv_client::Error),
+
+    // Cache writing error
+    #[error("Failed to read from the distribution cache")]
+    CacheRead(#[source] std::io::Error),
+    #[error("Failed to write to the distribution cache")]
+    CacheWrite(#[source] std::io::Error),
+    #[error("Failed to deserialize cache entry")]
+    CacheDecode(#[from] rmp_serde::decode::Error),
+    #[error("Failed to serialize cache entry")]
+    CacheEncode(#[from] rmp_serde::encode::Error),
+
+    // Build error
+    #[error("Failed to build: `{0}`")]
+    Build(String, #[source] anyhow::Error),
+    #[error("Failed to build editable: `{0}`")]
+    BuildEditable(String, #[source] anyhow::Error),
+    #[error("Built wheel has an invalid filename")]
+    WheelFilename(#[from] WheelFilenameError),
     #[error("Package metadata name `{metadata}` does not match given name `{given}`")]
     NameMismatch {
         given: PackageName,
         metadata: PackageName,
     },
+    #[error("Package metadata version `{metadata}` does not match given version `{given}`")]
+    VersionMismatch { given: Version, metadata: Version },
+    #[error("Failed to parse metadata from built wheel")]
+    Metadata(#[from] pypi_types::MetadataError),
+    #[error("Failed to read `dist-info` metadata from built wheel")]
+    DistInfo(#[from] install_wheel_rs::Error),
+    #[error("Failed to read zip archive from built wheel")]
+    Zip(#[from] ZipError),
+    #[error("Source distribution directory contains neither readable pyproject.toml nor setup.py: `{}`", _0.user_display())]
+    DirWithoutEntrypoint(PathBuf),
+    #[error("Failed to extract archive")]
+    Extract(#[from] uv_extract::Error),
+    #[error("Source distribution not found at: {0}")]
+    NotFound(PathBuf),
+    #[error("The source distribution is missing a `PKG-INFO` file")]
+    MissingPkgInfo,
+    #[error("The source distribution does not support static metadata in `PKG-INFO`")]
+    DynamicPkgInfo(#[source] pypi_types::MetadataError),
+    #[error("The source distribution is missing a `pyproject.toml` file")]
+    MissingPyprojectToml,
+    #[error("The source distribution does not support static metadata in `pyproject.toml`")]
+    DynamicPyprojectToml(#[source] pypi_types::MetadataError),
+    #[error("Unsupported scheme in URL: {0}")]
+    UnsupportedScheme(String),
 
-    #[error("The wheel {0} is not a valid zip file")]
-    Zip(WheelFilename, #[source] ZipError),
-
-    #[error("Failed to write to the client cache")]
-    CacheWrite(#[source] std::io::Error),
-
-    #[error(transparent)]
-    Io(#[from] std::io::Error),
-
-    #[error("Cache deserialization failed")]
-    Decode(#[source] rmp_serde::decode::Error),
-
-    #[error("Cache serialization failed")]
-    Encode(#[source] rmp_serde::encode::Error),
-
-    /// An [`io::Error`] with a filename attached
+    /// A generic request middleware error happened while making a request.
+    /// Refer to the error message for more details.
     #[error(transparent)]
-    Persist(#[from] tempfile::PersistError),
-
-    #[error("Missing `Content-Type` header for {0}")]
-    MissingContentType(Url),
+    ReqwestMiddlewareError(#[from] anyhow::Error),
 
-    #[error("Invalid `Content-Type` header for {0}")]
-    InvalidContentTypeHeader(Url, #[source] http::header::ToStrError),
+    /// Should not occur; only seen when another task panicked.
+    #[error("The task executor is broken, did some other task panic?")]
+    Join(#[from] JoinError),
+
+    /// An I/O error that occurs while exhausting a reader to compute a hash.
+    #[error("Failed to hash distribution")]
+    HashExhaustion(#[source] std::io::Error),
+
+    #[error("Hash mismatch for `{distribution}`\n\nExpected:\n{expected}\n\nComputed:\n{actual}")]
+    MismatchedHashes {
+        distribution: String,
+        expected: String,
+        actual: String,
+    },
 
-    #[error("Unsupported `Content-Type` \"{1}\" for {0}. Expected JSON or HTML.")]
-    UnsupportedMediaType(Url, String),
+    #[error(
+        "Hash-checking is enabled, but no hashes were provided or computed for: `{distribution}`"
+    )]
+    MissingHashes { distribution: String },
+
+    #[error("Hash-checking is enabled, but no hashes were computed for: `{distribution}`\n\nExpected:\n{expected}")]
+    MissingActualHashes {
+        distribution: String,
+        expected: String,
+    },
 
-    #[error("Reading from cache archive failed: {0}")]
-    ArchiveRead(String),
+    #[error("Hash-checking is enabled, but no hashes were provided for: `{distribution}`\n\nComputed:\n{actual}")]
+    MissingExpectedHashes {
+        distribution: String,
+        actual: String,
+    },
 
-    #[error("Writing to cache archive failed: {0}")]
-    ArchiveWrite(#[source] crate::rkyvutil::SerializerError),
+    #[error("Hash-checking is not supported for local directories: `{0}`")]
+    HashesNotSupportedSourceTree(String),
 
-    #[error("Network connectivity is disabled, but the requested data wasn't found in the cache for: `{0}`")]
-    Offline(String),
+    #[error("Hash-checking is not supported for Git repositories: `{0}`")]
+    HashesNotSupportedGit(String),
 }
 
-impl ErrorKind {
-    /// Returns true if this error kind corresponds to an I/O "not found"
-    /// error.
-    pub(crate) fn is_file_not_exists(&self) -> bool {
-        let ErrorKind::Io(ref err) = *self else {
-            return false;
-        };
-        matches!(err.kind(), std::io::ErrorKind::NotFound)
+impl From<reqwest::Error> for Error {
+    fn from(error: reqwest::Error) -> Self {
+        Self::Reqwest(BetterReqwestError::from(error))
     }
+}
 
-    pub(crate) fn from_middleware(err: reqwest_middleware::Error) -> Self {
-        if let reqwest_middleware::Error::Middleware(ref underlying) = err {
-            if let Some(err) = underlying.downcast_ref::<OfflineError>() {
-                return ErrorKind::Offline(err.url().to_string());
+impl From<reqwest_middleware::Error> for Error {
+    fn from(error: reqwest_middleware::Error) -> Self {
+        match error {
+            reqwest_middleware::Error::Middleware(error) => Self::ReqwestMiddlewareError(error),
+            reqwest_middleware::Error::Reqwest(error) => {
+                Self::Reqwest(BetterReqwestError::from(error))
             }
         }
-
-        if let reqwest_middleware::Error::Reqwest(err) = err {
-            return ErrorKind::RequestError(err);
-        }
-
-        ErrorKind::RequestMiddlewareError(err)
     }
+}
 
-    /// Returns `true` if the error is due to the server not supporting HTTP range requests.
-    pub(crate) fn is_http_range_requests_unsupported(&self) -> bool {
-        match self {
-            // The server doesn't support range requests (as reported by the `HEAD` check).
-            ErrorKind::AsyncHttpRangeReader(
-                AsyncHttpRangeReaderError::HttpRangeRequestUnsupported,
-            ) => {
-                return true;
+impl Error {
+    /// Construct a hash mismatch error.
+    pub fn hash_mismatch(
+        distribution: String,
+        expected: &[HashDigest],
+        actual: &[HashDigest],
+    ) -> Error {
+        match (expected.is_empty(), actual.is_empty()) {
+            (true, true) => Self::MissingHashes { distribution },
+            (true, false) => {
+                let actual = actual
+                    .iter()
+                    .map(|hash| format!("  {hash}"))
+                    .collect::<Vec<_>>()
+                    .join("\n");
+
+                Self::MissingExpectedHashes {
+                    distribution,
+                    actual,
+                }
             }
-
-            // The server returned a "Method Not Allowed" error, indicating it doesn't support
-            // HEAD requests, so we can't check for range requests.
-            ErrorKind::RequestError(err) => {
-                if let Some(status) = err.status() {
-                    if status == reqwest::StatusCode::METHOD_NOT_ALLOWED {
-                        return true;
-                    }
+            (false, true) => {
+                let expected = expected
+                    .iter()
+                    .map(|hash| format!("  {hash}"))
+                    .collect::<Vec<_>>()
+                    .join("\n");
+
+                Self::MissingActualHashes {
+                    distribution,
+                    expected,
                 }
             }
-
-            // The server doesn't support range requests, but we only discovered this while
-            // unzipping due to erroneous server behavior.
-            ErrorKind::Zip(_, ZipError::UpstreamReadError(err)) => {
-                if let Some(inner) = err.get_ref() {
-                    if let Some(inner) = inner.downcast_ref::<AsyncHttpRangeReaderError>() {
-                        if matches!(
-                            inner,
-                            AsyncHttpRangeReaderError::HttpRangeRequestUnsupported
-                        ) {
-                            return true;
-                        }
-                    }
+            (false, false) => {
+                let expected = expected
+                    .iter()
+                    .map(|hash| format!("  {hash}"))
+                    .collect::<Vec<_>>()
+                    .join("\n");
+
+                let actual = actual
+                    .iter()
+                    .map(|hash| format!("  {hash}"))
+                    .collect::<Vec<_>>()
+                    .join("\n");
+
+                Self::MismatchedHashes {
+                    distribution,
+                    expected,
+                    actual,
                 }
             }
-
-            _ => {}
         }
-
-        false
     }
 }
```

### Comparing `uv-0.1.9/crates/uv-client/src/flat_index.rs` & `uv-0.2.0/crates/distribution-types/src/installed.rs`

 * *Files 26% similar despite different names*

```diff
@@ -1,390 +1,363 @@
-use std::collections::btree_map::Entry;
-use std::collections::BTreeMap;
-use std::path::PathBuf;
-
-use futures::{FutureExt, StreamExt};
-use reqwest::Response;
-use rustc_hash::FxHashMap;
-use tracing::{debug, info_span, instrument, warn, Instrument};
+use std::path::{Path, PathBuf};
+use std::str::FromStr;
+
+use anyhow::{anyhow, Context, Result};
+use fs_err as fs;
+use tracing::warn;
 use url::Url;
 
-use distribution_filename::DistFilename;
-use distribution_types::{
-    BuiltDist, Dist, File, FileLocation, FlatIndexLocation, IndexUrl, PrioritizedDist,
-    RegistryBuiltDist, RegistrySourceDist, SourceDist,
-};
 use pep440_rs::Version;
-use platform_tags::Tags;
-use pypi_types::{Hashes, Yanked};
-use uv_auth::safe_copy_url_auth;
-use uv_cache::{Cache, CacheBucket};
+use pypi_types::DirectUrl;
+use uv_fs::Simplified;
 use uv_normalize::PackageName;
 
-use crate::cached_client::{CacheControl, CachedClientError};
-use crate::html::SimpleHtml;
-use crate::{Connectivity, Error, ErrorKind, RegistryClient};
+use crate::{DistributionMetadata, InstalledMetadata, InstalledVersion, Name, VersionOrUrlRef};
 
-#[derive(Debug, thiserror::Error)]
-pub enum FlatIndexError {
-    #[error("Failed to read `--find-links` directory: {0}")]
-    FindLinksDirectory(PathBuf, #[source] std::io::Error),
+/// A built distribution (wheel) that is installed in a virtual environment.
+#[derive(Debug, Clone)]
+pub enum InstalledDist {
+    /// The distribution was derived from a registry, like `PyPI`.
+    Registry(InstalledRegistryDist),
+    /// The distribution was derived from an arbitrary URL.
+    Url(InstalledDirectUrlDist),
+    /// The distribution was derived from pre-existing `.egg-info` directory.
+    EggInfo(InstalledEggInfo),
+    /// The distribution was derived from an `.egg-link` pointer.
+    LegacyEditable(InstalledLegacyEditable),
+}
 
-    #[error("Failed to read `--find-links` URL: {0}")]
-    FindLinksUrl(Url, #[source] Error),
+#[derive(Debug, Clone)]
+pub struct InstalledRegistryDist {
+    pub name: PackageName,
+    pub version: Version,
+    pub path: PathBuf,
 }
 
-#[derive(Debug, Default, Clone)]
-pub struct FlatIndexEntries {
-    /// The list of `--find-links` entries.
-    entries: Vec<(DistFilename, File, IndexUrl)>,
-    /// Whether any `--find-links` entries could not be resolved due to a lack of network
-    /// connectivity.
-    offline: bool,
+#[derive(Debug, Clone)]
+pub struct InstalledDirectUrlDist {
+    pub name: PackageName,
+    pub version: Version,
+    pub direct_url: Box<DirectUrl>,
+    pub url: Url,
+    pub editable: bool,
+    pub path: PathBuf,
+}
+
+#[derive(Debug, Clone)]
+pub struct InstalledEggInfo {
+    pub name: PackageName,
+    pub version: Version,
+    pub path: PathBuf,
+}
+
+#[derive(Debug, Clone)]
+pub struct InstalledLegacyEditable {
+    pub name: PackageName,
+    pub version: Version,
+    pub egg_link: PathBuf,
+    pub target: PathBuf,
+    pub target_url: Url,
+    pub egg_info: PathBuf,
 }
 
-impl FlatIndexEntries {
-    /// Create a [`FlatIndexEntries`] from a list of `--find-links` entries.
-    fn from_entries(entries: Vec<(DistFilename, File, IndexUrl)>) -> Self {
-        Self {
-            entries,
-            offline: false,
+impl InstalledDist {
+    /// Try to parse a distribution from a `.dist-info` directory name (like `django-5.0a1.dist-info`).
+    ///
+    /// See: <https://packaging.python.org/en/latest/specifications/recording-installed-packages/#recording-installed-packages>
+    pub fn try_from_path(path: &Path) -> Result<Option<Self>> {
+        // Ex) `cffi-1.16.0.dist-info`
+        if path.extension().is_some_and(|ext| ext == "dist-info") {
+            let Some(file_stem) = path.file_stem() else {
+                return Ok(None);
+            };
+            let Some(file_stem) = file_stem.to_str() else {
+                return Ok(None);
+            };
+            let Some((name, version)) = file_stem.split_once('-') else {
+                return Ok(None);
+            };
+
+            let name = PackageName::from_str(name)?;
+            let version = Version::from_str(version).map_err(|err| anyhow!(err))?;
+            return if let Some(direct_url) = Self::direct_url(path)? {
+                match Url::try_from(&direct_url) {
+                    Ok(url) => Ok(Some(Self::Url(InstalledDirectUrlDist {
+                        name,
+                        version,
+                        editable: matches!(&direct_url, DirectUrl::LocalDirectory { dir_info, .. } if dir_info.editable == Some(true)),
+                        direct_url: Box::new(direct_url),
+                        url,
+                        path: path.to_path_buf(),
+                    }))),
+                    Err(err) => {
+                        warn!("Failed to parse direct URL: {err}");
+                        Ok(Some(Self::Registry(InstalledRegistryDist {
+                            name,
+                            version,
+                            path: path.to_path_buf(),
+                        })))
+                    }
+                }
+            } else {
+                Ok(Some(Self::Registry(InstalledRegistryDist {
+                    name,
+                    version,
+                    path: path.to_path_buf(),
+                })))
+            };
         }
-    }
 
-    /// Create a [`FlatIndexEntries`] to represent an offline `--find-links` entry.
-    fn offline() -> Self {
-        Self {
-            entries: Vec::new(),
-            offline: true,
+        // Ex) `zstandard-0.22.0-py3.12.egg-info`
+        if path.extension().is_some_and(|ext| ext == "egg-info") {
+            let Some(file_stem) = path.file_stem() else {
+                return Ok(None);
+            };
+            let Some(file_stem) = file_stem.to_str() else {
+                return Ok(None);
+            };
+            let Some((name, version_python)) = file_stem.split_once('-') else {
+                return Ok(None);
+            };
+            let Some((version, _)) = version_python.split_once('-') else {
+                return Ok(None);
+            };
+            let name = PackageName::from_str(name)?;
+            let version = Version::from_str(version).map_err(|err| anyhow!(err))?;
+            return Ok(Some(Self::EggInfo(InstalledEggInfo {
+                name,
+                version,
+                path: path.to_path_buf(),
+            })));
         }
-    }
 
-    /// Extend this list of `--find-links` entries with another list.
-    fn extend(&mut self, other: Self) {
-        self.entries.extend(other.entries);
-        self.offline |= other.offline;
+        // Ex) `zstandard.egg-link`
+        if path.extension().is_some_and(|ext| ext == "egg-link") {
+            let Some(file_stem) = path.file_stem() else {
+                return Ok(None);
+            };
+            let Some(file_stem) = file_stem.to_str() else {
+                return Ok(None);
+            };
+
+            // https://setuptools.pypa.io/en/latest/deprecated/python_eggs.html#egg-links
+            // https://github.com/pypa/pip/blob/946f95d17431f645da8e2e0bf4054a72db5be766/src/pip/_internal/metadata/importlib/_envs.py#L86-L108
+            let contents = fs::read_to_string(path)?;
+            let Some(target) = contents.lines().find_map(|line| {
+                let line = line.trim();
+                if line.is_empty() {
+                    None
+                } else {
+                    Some(PathBuf::from(line))
+                }
+            }) else {
+                warn!("Invalid `.egg-link` file: {path:?}");
+                return Ok(None);
+            };
+
+            // Match pip, but note setuptools only puts absolute paths in `.egg-link` files.
+            let target = path
+                .parent()
+                .ok_or_else(|| anyhow!("Invalid `.egg-link` path: {}", path.user_display()))?
+                .join(target);
+
+            // Normalisation comes from `pkg_resources.to_filename`.
+            let egg_info = target.join(file_stem.replace('-', "_") + ".egg-info");
+            let url = Url::from_file_path(&target)
+                .map_err(|()| anyhow!("Invalid `.egg-link` target: {}", target.user_display()))?;
+
+            // Mildly unfortunate that we must read metadata to get the version.
+            let content = match fs::read(egg_info.join("PKG-INFO")) {
+                Ok(content) => content,
+                Err(err) => {
+                    warn!("Failed to read metadata for {path:?}: {err}");
+                    return Ok(None);
+                }
+            };
+            let metadata = match pypi_types::Metadata10::parse_pkg_info(&content) {
+                Ok(metadata) => metadata,
+                Err(err) => {
+                    warn!("Failed to parse metadata for {path:?}: {err}");
+                    return Ok(None);
+                }
+            };
+
+            return Ok(Some(Self::LegacyEditable(InstalledLegacyEditable {
+                name: metadata.name,
+                version: Version::from_str(&metadata.version)?,
+                egg_link: path.to_path_buf(),
+                target,
+                target_url: url,
+                egg_info,
+            })));
+        }
+
+        Ok(None)
     }
 
-    /// Return the number of `--find-links` entries.
-    fn len(&self) -> usize {
-        self.entries.len()
+    /// Return the [`Path`] at which the distribution is stored on-disk.
+    pub fn path(&self) -> &Path {
+        match self {
+            Self::Registry(dist) => &dist.path,
+            Self::Url(dist) => &dist.path,
+            Self::EggInfo(dist) => &dist.path,
+            Self::LegacyEditable(dist) => &dist.egg_info,
+        }
     }
 
-    /// Return `true` if there are no `--find-links` entries.
-    fn is_empty(&self) -> bool {
-        self.entries.is_empty()
+    /// Return the [`Version`] of the distribution.
+    pub fn version(&self) -> &Version {
+        match self {
+            Self::Registry(dist) => &dist.version,
+            Self::Url(dist) => &dist.version,
+            Self::EggInfo(dist) => &dist.version,
+            Self::LegacyEditable(dist) => &dist.version,
+        }
     }
-}
 
-/// A client for reading distributions from `--find-links` entries (either local directories or
-/// remote HTML indexes).
-#[derive(Debug, Clone)]
-pub struct FlatIndexClient<'a> {
-    client: &'a RegistryClient,
-    cache: &'a Cache,
-}
-
-impl<'a> FlatIndexClient<'a> {
-    /// Create a new [`FlatIndexClient`].
-    pub fn new(client: &'a RegistryClient, cache: &'a Cache) -> Self {
-        Self { client, cache }
-    }
-
-    /// Read the directories and flat remote indexes from `--find-links`.
-    #[allow(clippy::result_large_err)]
-    pub async fn fetch(
-        &self,
-        indexes: impl Iterator<Item = &FlatIndexLocation>,
-    ) -> Result<FlatIndexEntries, FlatIndexError> {
-        let mut fetches = futures::stream::iter(indexes)
-            .map(|index| async move {
-                let entries = match index {
-                    FlatIndexLocation::Path(path) => Self::read_from_directory(path)
-                        .map_err(|err| FlatIndexError::FindLinksDirectory(path.clone(), err))?,
-                    FlatIndexLocation::Url(url) => self
-                        .read_from_url(url)
-                        .await
-                        .map_err(|err| FlatIndexError::FindLinksUrl(url.clone(), err))?,
-                };
-                if entries.is_empty() {
-                    warn!("No packages found in `--find-links` entry: {}", index);
-                } else {
-                    debug!(
-                        "Found {} package{} in `--find-links` entry: {}",
-                        entries.len(),
-                        if entries.len() == 1 { "" } else { "s" },
-                        index
-                    );
-                }
-                Ok::<FlatIndexEntries, FlatIndexError>(entries)
-            })
-            .buffered(16);
-
-        let mut results = FlatIndexEntries::default();
-        while let Some(entries) = fetches.next().await.transpose()? {
-            results.extend(entries);
-        }
-        Ok(results)
-    }
-
-    /// Read a flat remote index from a `--find-links` URL.
-    async fn read_from_url(&self, url: &Url) -> Result<FlatIndexEntries, Error> {
-        let cache_entry = self.cache.entry(
-            CacheBucket::FlatIndex,
-            "html",
-            format!("{}.msgpack", cache_key::digest(&url.to_string())),
-        );
-        let cache_control = match self.client.connectivity() {
-            Connectivity::Online => CacheControl::from(
-                self.cache
-                    .freshness(&cache_entry, None)
-                    .map_err(ErrorKind::Io)?,
-            ),
-            Connectivity::Offline => CacheControl::AllowStale,
+    /// Read the `direct_url.json` file from a `.dist-info` directory.
+    pub fn direct_url(path: &Path) -> Result<Option<DirectUrl>> {
+        let path = path.join("direct_url.json");
+        let Ok(file) = fs_err::File::open(path) else {
+            return Ok(None);
         };
+        let direct_url = serde_json::from_reader::<fs_err::File, DirectUrl>(file)?;
+        Ok(Some(direct_url))
+    }
 
-        let cached_client = self.client.cached_client();
-
-        let flat_index_request = cached_client
-            .uncached()
-            .get(url.clone())
-            .header("Accept-Encoding", "gzip")
-            .header("Accept", "text/html")
-            .build()
-            .map_err(ErrorKind::RequestError)?;
-        let parse_simple_response = |response: Response| {
-            async {
-                // Use the response URL, rather than the request URL, as the base for relative URLs.
-                // This ensures that we handle redirects and other URL transformations correctly.
-                let url = safe_copy_url_auth(url, response.url().clone());
-
-                let text = response.text().await.map_err(ErrorKind::RequestError)?;
-                let SimpleHtml { base, files } = SimpleHtml::parse(&text, &url)
-                    .map_err(|err| Error::from_html_err(err, url.clone()))?;
-
-                let base = safe_copy_url_auth(&url, base.into_url());
-                let files: Vec<File> = files
-                    .into_iter()
-                    .filter_map(|file| {
-                        match File::try_from(file, &base) {
-                            Ok(file) => Some(file),
-                            Err(err) => {
-                                // Ignore files with unparsable version specifiers.
-                                warn!("Skipping file in {url}: {err}");
-                                None
-                            }
-                        }
-                    })
-                    .collect();
-                Ok::<Vec<File>, CachedClientError<Error>>(files)
+    /// Read the `METADATA` file from a `.dist-info` directory.
+    pub fn metadata(&self) -> Result<pypi_types::Metadata23> {
+        match self {
+            Self::Registry(_) | Self::Url(_) => {
+                let path = self.path().join("METADATA");
+                let contents = fs::read(&path)?;
+                // TODO(zanieb): Update this to use thiserror so we can unpack parse errors downstream
+                pypi_types::Metadata23::parse_metadata(&contents).with_context(|| {
+                    format!(
+                        "Failed to parse `METADATA` file at: {}",
+                        path.user_display()
+                    )
+                })
             }
-            .boxed()
-            .instrument(info_span!("parse_flat_index_html", url = % url))
-        };
-        let response = cached_client
-            .get_serde(
-                flat_index_request,
-                &cache_entry,
-                cache_control,
-                parse_simple_response,
-            )
-            .await;
-        match response {
-            Ok(files) => {
-                let files = files
-                    .into_iter()
-                    .filter_map(|file| {
-                        Some((
-                            DistFilename::try_from_normalized_filename(&file.filename)?,
-                            file,
-                            IndexUrl::Url(url.clone()),
-                        ))
-                    })
-                    .collect();
-                Ok(FlatIndexEntries::from_entries(files))
-            }
-            Err(CachedClientError::Client(err)) if matches!(err.kind(), ErrorKind::Offline(_)) => {
-                Ok(FlatIndexEntries::offline())
+            Self::EggInfo(_) | Self::LegacyEditable(_) => {
+                let path = match self {
+                    Self::EggInfo(dist) => dist.path.join("PKG-INFO"),
+                    Self::LegacyEditable(dist) => dist.egg_info.join("PKG-INFO"),
+                    _ => unreachable!(),
+                };
+                let contents = fs::read(&path)?;
+                pypi_types::Metadata23::parse_metadata(&contents).with_context(|| {
+                    format!(
+                        "Failed to parse `PKG-INFO` file at: {}",
+                        path.user_display()
+                    )
+                })
             }
+        }
+    }
+
+    /// Return the `INSTALLER` of the distribution.
+    pub fn installer(&self) -> Result<Option<String>> {
+        let path = self.path().join("INSTALLER");
+        match fs::read_to_string(path) {
+            Ok(installer) => Ok(Some(installer)),
+            Err(err) if err.kind() == std::io::ErrorKind::NotFound => Ok(None),
             Err(err) => Err(err.into()),
         }
     }
 
-    /// Read a flat remote index from a `--find-links` directory.
-    fn read_from_directory(path: &PathBuf) -> Result<FlatIndexEntries, std::io::Error> {
-        // Absolute paths are required for the URL conversion.
-        let path = fs_err::canonicalize(path)?;
-
-        let mut dists = Vec::new();
-        for entry in fs_err::read_dir(path)? {
-            let entry = entry?;
-            let metadata = entry.metadata()?;
-            if !metadata.is_file() {
-                continue;
-            }
+    /// Return true if the distribution is editable.
+    pub fn is_editable(&self) -> bool {
+        match self {
+            Self::Registry(_) => false,
+            Self::Url(dist) => dist.editable,
+            Self::EggInfo(_) => false,
+            Self::LegacyEditable(_) => true,
+        }
+    }
 
-            let Ok(filename) = entry.file_name().into_string() else {
-                warn!(
-                    "Skipping non-UTF-8 filename in `--find-links` directory: {}",
-                    entry.file_name().to_string_lossy()
-                );
-                continue;
-            };
-
-            let file = File {
-                dist_info_metadata: None,
-                filename: filename.to_string(),
-                hashes: Hashes::default(),
-                requires_python: None,
-                size: None,
-                upload_time_utc_ms: None,
-                url: FileLocation::Path(entry.path().to_path_buf()),
-                yanked: None,
-            };
-
-            let Some(filename) = DistFilename::try_from_normalized_filename(&filename) else {
-                debug!(
-                    "Ignoring `--find-links` entry (expected a wheel or source distribution filename): {}",
-                    entry.path().display()
-                );
-                continue;
-            };
-            dists.push((filename, file, IndexUrl::Pypi));
-        }
-        Ok(FlatIndexEntries::from_entries(dists))
-    }
-}
-
-/// A set of [`PrioritizedDist`] from a `--find-links` entry, indexed by [`PackageName`]
-/// and [`Version`].
-#[derive(Debug, Clone, Default)]
-pub struct FlatIndex {
-    /// The list of [`FlatDistributions`] from the `--find-links` entries, indexed by package name.
-    index: FxHashMap<PackageName, FlatDistributions>,
-    /// Whether any `--find-links` entries could not be resolved due to a lack of network
-    /// connectivity.
-    offline: bool,
-}
-
-impl FlatIndex {
-    /// Collect all files from a `--find-links` target into a [`FlatIndex`].
-    #[instrument(skip_all)]
-    pub fn from_entries(entries: FlatIndexEntries, tags: &Tags) -> Self {
-        // Collect compatible distributions.
-        let mut index = FxHashMap::default();
-        for (filename, file, url) in entries.entries {
-            let distributions = index.entry(filename.name().clone()).or_default();
-            Self::add_file(distributions, file, filename, tags, url);
-        }
-
-        // Collect offline entries.
-        let offline = entries.offline;
-
-        Self { index, offline }
-    }
-
-    fn add_file(
-        distributions: &mut FlatDistributions,
-        file: File,
-        filename: DistFilename,
-        tags: &Tags,
-        index: IndexUrl,
-    ) {
-        // No `requires-python` here: for source distributions, we don't have that information;
-        // for wheels, we read it lazily only when selected.
-        match filename {
-            DistFilename::WheelFilename(filename) => {
-                let compatibility = filename.compatibility(tags);
-                let version = filename.version.clone();
-
-                let dist = Dist::Built(BuiltDist::Registry(RegistryBuiltDist {
-                    filename,
-                    file: Box::new(file),
-                    index,
-                }));
-                match distributions.0.entry(version) {
-                    Entry::Occupied(mut entry) => {
-                        entry.get_mut().insert_built(
-                            dist,
-                            None,
-                            Yanked::default(),
-                            None,
-                            compatibility.into(),
-                        );
-                    }
-                    Entry::Vacant(entry) => {
-                        entry.insert(PrioritizedDist::from_built(
-                            dist,
-                            None,
-                            Yanked::default(),
-                            None,
-                            compatibility.into(),
-                        ));
-                    }
-                }
-            }
-            DistFilename::SourceDistFilename(filename) => {
-                let dist = Dist::Source(SourceDist::Registry(RegistrySourceDist {
-                    filename: filename.clone(),
-                    file: Box::new(file),
-                    index,
-                }));
-                match distributions.0.entry(filename.version.clone()) {
-                    Entry::Occupied(mut entry) => {
-                        entry
-                            .get_mut()
-                            .insert_source(dist, None, Yanked::default(), None);
-                    }
-                    Entry::Vacant(entry) => {
-                        entry.insert(PrioritizedDist::from_source(
-                            dist,
-                            None,
-                            Yanked::default(),
-                            None,
-                        ));
-                    }
-                }
-            }
+    /// Return the [`Url`] of the distribution, if it is editable.
+    pub fn as_editable(&self) -> Option<&Url> {
+        match self {
+            Self::Registry(_) => None,
+            Self::Url(dist) => dist.editable.then_some(&dist.url),
+            Self::EggInfo(_) => None,
+            Self::LegacyEditable(dist) => Some(&dist.target_url),
         }
     }
+}
+
+impl DistributionMetadata for InstalledDist {
+    fn version_or_url(&self) -> VersionOrUrlRef {
+        VersionOrUrlRef::Version(self.version())
+    }
+}
+
+impl Name for InstalledRegistryDist {
+    fn name(&self) -> &PackageName {
+        &self.name
+    }
+}
+
+impl Name for InstalledDirectUrlDist {
+    fn name(&self) -> &PackageName {
+        &self.name
+    }
+}
 
-    /// Get the [`FlatDistributions`] for the given package name.
-    pub fn get(&self, package_name: &PackageName) -> Option<&FlatDistributions> {
-        self.index.get(package_name)
+impl Name for InstalledEggInfo {
+    fn name(&self) -> &PackageName {
+        &self.name
     }
+}
 
-    /// Returns `true` if there are any offline `--find-links` entries.
-    pub fn offline(&self) -> bool {
-        self.offline
+impl Name for InstalledLegacyEditable {
+    fn name(&self) -> &PackageName {
+        &self.name
     }
 }
 
-/// A set of [`PrioritizedDist`] from a `--find-links` entry for a single package, indexed
-/// by [`Version`].
-#[derive(Debug, Clone, Default)]
-pub struct FlatDistributions(BTreeMap<Version, PrioritizedDist>);
+impl Name for InstalledDist {
+    fn name(&self) -> &PackageName {
+        match self {
+            Self::Registry(dist) => dist.name(),
+            Self::Url(dist) => dist.name(),
+            Self::EggInfo(dist) => dist.name(),
+            Self::LegacyEditable(dist) => dist.name(),
+        }
+    }
+}
 
-impl FlatDistributions {
-    pub fn iter(&self) -> impl Iterator<Item = (&Version, &PrioritizedDist)> {
-        self.0.iter()
+impl InstalledMetadata for InstalledRegistryDist {
+    fn installed_version(&self) -> InstalledVersion {
+        InstalledVersion::Version(&self.version)
     }
+}
 
-    pub fn remove(&mut self, version: &Version) -> Option<PrioritizedDist> {
-        self.0.remove(version)
+impl InstalledMetadata for InstalledDirectUrlDist {
+    fn installed_version(&self) -> InstalledVersion {
+        InstalledVersion::Url(&self.url, &self.version)
     }
 }
 
-impl IntoIterator for FlatDistributions {
-    type Item = (Version, PrioritizedDist);
-    type IntoIter = std::collections::btree_map::IntoIter<Version, PrioritizedDist>;
+impl InstalledMetadata for InstalledEggInfo {
+    fn installed_version(&self) -> InstalledVersion {
+        InstalledVersion::Version(&self.version)
+    }
+}
 
-    fn into_iter(self) -> Self::IntoIter {
-        self.0.into_iter()
+impl InstalledMetadata for InstalledLegacyEditable {
+    fn installed_version(&self) -> InstalledVersion {
+        InstalledVersion::Version(&self.version)
     }
 }
 
-impl From<FlatDistributions> for BTreeMap<Version, PrioritizedDist> {
-    fn from(distributions: FlatDistributions) -> Self {
-        distributions.0
+impl InstalledMetadata for InstalledDist {
+    fn installed_version(&self) -> InstalledVersion {
+        match self {
+            Self::Registry(dist) => dist.installed_version(),
+            Self::Url(dist) => dist.installed_version(),
+            Self::EggInfo(dist) => dist.installed_version(),
+            Self::LegacyEditable(dist) => dist.installed_version(),
+        }
     }
 }
```

### Comparing `uv-0.1.9/crates/uv-client/src/html.rs` & `uv-0.2.0/crates/uv-client/src/html.rs`

 * *Files 24% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 
 use tl::HTMLTag;
 use tracing::instrument;
 use url::Url;
 
 use pep440_rs::VersionSpecifiers;
 use pypi_types::LenientVersionSpecifiers;
-use pypi_types::{BaseUrl, DistInfoMetadata, File, Hashes, Yanked};
+use pypi_types::{BaseUrl, CoreMetadata, File, Hashes, Yanked};
 
 /// A parsed structure from PyPI "HTML" index format for a single package.
 #[derive(Debug, Clone)]
 pub(crate) struct SimpleHtml {
     /// The [`BaseUrl`] to which all relative URLs should be resolved.
     pub(crate) base: BaseUrl,
     /// The list of [`File`]s available for download sorted by filename.
@@ -85,26 +85,50 @@
         if parts.next().is_some() {
             return Err(Error::FragmentParse(fragment.to_string()));
         }
 
         match name {
             "md5" => {
                 let md5 = std::str::from_utf8(value.as_bytes())?;
-                let md5 = md5.to_string();
+                let md5 = md5.to_owned().into_boxed_str();
                 Ok(Hashes {
                     md5: Some(md5),
                     sha256: None,
+                    sha384: None,
+                    sha512: None,
                 })
             }
             "sha256" => {
                 let sha256 = std::str::from_utf8(value.as_bytes())?;
-                let sha256 = sha256.to_string();
+                let sha256 = sha256.to_owned().into_boxed_str();
                 Ok(Hashes {
                     md5: None,
                     sha256: Some(sha256),
+                    sha384: None,
+                    sha512: None,
+                })
+            }
+            "sha384" => {
+                let sha384 = std::str::from_utf8(value.as_bytes())?;
+                let sha384 = sha384.to_owned().into_boxed_str();
+                Ok(Hashes {
+                    md5: None,
+                    sha256: None,
+                    sha384: Some(sha384),
+                    sha512: None,
+                })
+            }
+            "sha512" => {
+                let sha512 = std::str::from_utf8(value.as_bytes())?;
+                let sha512 = sha512.to_owned().into_boxed_str();
+                Ok(Hashes {
+                    md5: None,
+                    sha256: None,
+                    sha384: None,
+                    sha512: Some(sha512),
                 })
             }
             _ => Err(Error::UnsupportedHashAlgorithm(fragment.to_string())),
         }
     }
 
     /// Parse a [`File`] from an `<a>` tag.
@@ -117,33 +141,38 @@
             .filter(|bytes| !bytes.as_bytes().is_empty())
             .ok_or(Error::MissingHref)?;
         let href = std::str::from_utf8(href.as_bytes())?;
 
         // Extract the hash, which should be in the fragment.
         let decoded = html_escape::decode_html_entities(href);
         let (path, hashes) = if let Some((path, fragment)) = decoded.split_once('#') {
+            let fragment = urlencoding::decode(fragment)
+                .map_err(|_| Error::FragmentParse(fragment.to_string()))?;
             (
                 path,
                 if fragment.trim().is_empty() {
                     Hashes::default()
                 } else {
-                    Self::parse_hash(fragment)?
+                    Self::parse_hash(&fragment)?
                 },
             )
         } else {
             (decoded.as_ref(), Hashes::default())
         };
 
         // Extract the filename from the body text, which MUST match that of
         // the final path component of the URL.
         let filename = path
             .split('/')
             .last()
             .ok_or_else(|| Error::MissingFilename(href.to_string()))?;
 
+        // Strip any query string from the filename.
+        let filename = filename.split('?').next().unwrap_or(filename);
+
         // Unquote the filename.
         let filename = urlencoding::decode(filename)
             .map_err(|_| Error::UnsupportedFilename(filename.to_string()))?;
 
         // Extract the `requires-python` field, which should be set on the
         // `data-requires-python` attribute.
         let requires_python = if let Some(requires_python) =
@@ -152,25 +181,29 @@
             let requires_python = std::str::from_utf8(requires_python.as_bytes())?;
             let requires_python = html_escape::decode_html_entities(requires_python);
             Some(LenientVersionSpecifiers::from_str(&requires_python).map(VersionSpecifiers::from))
         } else {
             None
         };
 
-        // Extract the `data-dist-info-metadata` field, which should be set on
-        // the `data-dist-info-metadata` attribute.
-        let dist_info_metadata = if let Some(dist_info_metadata) =
-            link.attributes().get("data-dist-info-metadata").flatten()
+        // Extract the `core-metadata` field, which is either set on:
+        // - `data-core-metadata`, per PEP 714.
+        // - `data-dist-info-metadata`, per PEP 658.
+        let core_metadata = if let Some(dist_info_metadata) = link
+            .attributes()
+            .get("data-core-metadata")
+            .flatten()
+            .or_else(|| link.attributes().get("data-dist-info-metadata").flatten())
         {
             let dist_info_metadata = std::str::from_utf8(dist_info_metadata.as_bytes())?;
             let dist_info_metadata = html_escape::decode_html_entities(dist_info_metadata);
             match dist_info_metadata.as_ref() {
-                "true" => Some(DistInfoMetadata::Bool(true)),
-                "false" => Some(DistInfoMetadata::Bool(false)),
-                fragment => Some(DistInfoMetadata::Hashes(Self::parse_hash(fragment)?)),
+                "true" => Some(CoreMetadata::Bool(true)),
+                "false" => Some(CoreMetadata::Bool(false)),
+                fragment => Some(CoreMetadata::Hashes(Self::parse_hash(fragment)?)),
             }
         } else {
             None
         };
 
         // Extract the `yanked` field, which should be set on the `data-yanked`
         // attribute.
@@ -179,20 +212,22 @@
             let yanked = html_escape::decode_html_entities(yanked);
             Some(Yanked::Reason(yanked.to_string()))
         } else {
             None
         };
 
         Ok(File {
-            dist_info_metadata,
+            core_metadata,
+            dist_info_metadata: None,
+            data_dist_info_metadata: None,
             yanked,
             requires_python,
             hashes,
             filename: filename.to_string(),
-            url: href.to_string(),
+            url: decoded.to_string(),
             size: None,
             upload_time: None,
         })
     }
 }
 
 #[derive(Debug, thiserror::Error)]
@@ -214,18 +249,20 @@
 
     #[error("Expected distribution filename to be UTF-8: {0}")]
     UnsupportedFilename(String),
 
     #[error("Missing hash attribute on URL: {0}")]
     MissingHash(String),
 
-    #[error("Unexpected fragment (expected `#sha256=...`) on URL: {0}")]
+    #[error("Unexpected fragment (expected `#sha256=...` or similar) on URL: {0}")]
     FragmentParse(String),
 
-    #[error("Unsupported hash algorithm (expected `sha256`) on: {0}")]
+    #[error(
+        "Unsupported hash algorithm (expected `md5`, `sha256`, `sha384`, or `sha512`) on: {0}"
+    )]
     UnsupportedHashAlgorithm(String),
 
     #[error("Invalid `requires-python` specifier: {0}")]
     Pep440(#[source] pep440_rs::VersionSpecifiersParseError),
 }
 
 #[cfg(test)]
@@ -263,21 +300,25 @@
                     path: "/whl/jinja2/",
                     query: None,
                     fragment: None,
                 },
             ),
             files: [
                 File {
+                    core_metadata: None,
                     dist_info_metadata: None,
+                    data_dist_info_metadata: None,
                     filename: "Jinja2-3.1.2-py3-none-any.whl",
                     hashes: Hashes {
                         md5: None,
                         sha256: Some(
                             "6088930bfe239f0e6710546ab9c19c9ef35e29792895fed6e6e31a023a182a61",
                         ),
+                        sha384: None,
+                        sha512: None,
                     },
                     requires_python: None,
                     size: None,
                     upload_time: None,
                     url: "/whl/Jinja2-3.1.2-py3-none-any.whl#sha256=6088930bfe239f0e6710546ab9c19c9ef35e29792895fed6e6e31a023a182a61",
                     yanked: None,
                 },
@@ -317,21 +358,25 @@
                     path: "/whl/jinja2/",
                     query: None,
                     fragment: None,
                 },
             ),
             files: [
                 File {
+                    core_metadata: None,
                     dist_info_metadata: None,
+                    data_dist_info_metadata: None,
                     filename: "Jinja2-3.1.2-py3-none-any.whl",
                     hashes: Hashes {
                         md5: Some(
                             "6088930bfe239f0e6710546ab9c19c9ef35e29792895fed6e6e31a023a182a61",
                         ),
                         sha256: None,
+                        sha384: None,
+                        sha512: None,
                     },
                     requires_python: None,
                     size: None,
                     upload_time: None,
                     url: "/whl/Jinja2-3.1.2-py3-none-any.whl#md5=6088930bfe239f0e6710546ab9c19c9ef35e29792895fed6e6e31a023a182a61",
                     yanked: None,
                 },
@@ -374,21 +419,25 @@
                     path: "/",
                     query: None,
                     fragment: None,
                 },
             ),
             files: [
                 File {
+                    core_metadata: None,
                     dist_info_metadata: None,
+                    data_dist_info_metadata: None,
                     filename: "Jinja2-3.1.2-py3-none-any.whl",
                     hashes: Hashes {
                         md5: None,
                         sha256: Some(
                             "6088930bfe239f0e6710546ab9c19c9ef35e29792895fed6e6e31a023a182a61",
                         ),
+                        sha384: None,
+                        sha512: None,
                     },
                     requires_python: None,
                     size: None,
                     upload_time: None,
                     url: "/whl/Jinja2-3.1.2-py3-none-any.whl#sha256=6088930bfe239f0e6710546ab9c19c9ef35e29792895fed6e6e31a023a182a61",
                     yanked: None,
                 },
@@ -428,26 +477,88 @@
                     path: "/whl/jinja2/",
                     query: None,
                     fragment: None,
                 },
             ),
             files: [
                 File {
+                    core_metadata: None,
                     dist_info_metadata: None,
+                    data_dist_info_metadata: None,
                     filename: "Jinja2-3.1.2+233fca715f49-py3-none-any.whl",
                     hashes: Hashes {
                         md5: None,
                         sha256: Some(
                             "6088930bfe239f0e6710546ab9c19c9ef35e29792895fed6e6e31a023a182a61",
                         ),
+                        sha384: None,
+                        sha512: None,
                     },
                     requires_python: None,
                     size: None,
                     upload_time: None,
-                    url: "/whl/Jinja2-3.1.2&#43;233fca715f49-py3-none-any.whl#sha256=6088930bfe239f0e6710546ab9c19c9ef35e29792895fed6e6e31a023a182a61",
+                    url: "/whl/Jinja2-3.1.2+233fca715f49-py3-none-any.whl#sha256=6088930bfe239f0e6710546ab9c19c9ef35e29792895fed6e6e31a023a182a61",
+                    yanked: None,
+                },
+            ],
+        }
+        "###);
+    }
+
+    #[test]
+    fn parse_encoded_fragment() {
+        let text = r#"
+<!DOCTYPE html>
+<html>
+  <body>
+    <h1>Links for jinja2</h1>
+    <a href="/whl/Jinja2-3.1.2-py3-none-any.whl#sha256%3D4095ada29e51070f7d199a0a5bdf5c8d8e238e03f0bf4dcc02571e78c9ae800d">Jinja2-3.1.2-py3-none-any.whl</a><br/>
+  </body>
+</html>
+<!--TIMESTAMP 1703347410-->
+        "#;
+        let base = Url::parse("https://download.pytorch.org/whl/jinja2/").unwrap();
+        let result = SimpleHtml::parse(text, &base).unwrap();
+        insta::assert_debug_snapshot!(result, @r###"
+        SimpleHtml {
+            base: BaseUrl(
+                Url {
+                    scheme: "https",
+                    cannot_be_a_base: false,
+                    username: "",
+                    password: None,
+                    host: Some(
+                        Domain(
+                            "download.pytorch.org",
+                        ),
+                    ),
+                    port: None,
+                    path: "/whl/jinja2/",
+                    query: None,
+                    fragment: None,
+                },
+            ),
+            files: [
+                File {
+                    core_metadata: None,
+                    dist_info_metadata: None,
+                    data_dist_info_metadata: None,
+                    filename: "Jinja2-3.1.2-py3-none-any.whl",
+                    hashes: Hashes {
+                        md5: None,
+                        sha256: Some(
+                            "4095ada29e51070f7d199a0a5bdf5c8d8e238e03f0bf4dcc02571e78c9ae800d",
+                        ),
+                        sha384: None,
+                        sha512: None,
+                    },
+                    requires_python: None,
+                    size: None,
+                    upload_time: None,
+                    url: "/whl/Jinja2-3.1.2-py3-none-any.whl#sha256%3D4095ada29e51070f7d199a0a5bdf5c8d8e238e03f0bf4dcc02571e78c9ae800d",
                     yanked: None,
                 },
             ],
         }
         "###);
     }
 
@@ -482,19 +593,23 @@
                     path: "/whl/jinja2/",
                     query: None,
                     fragment: None,
                 },
             ),
             files: [
                 File {
+                    core_metadata: None,
                     dist_info_metadata: None,
+                    data_dist_info_metadata: None,
                     filename: "torchtext-0.17.0+cpu-cp39-cp39-win_amd64.whl",
                     hashes: Hashes {
                         md5: None,
                         sha256: None,
+                        sha384: None,
+                        sha512: None,
                     },
                     requires_python: None,
                     size: None,
                     upload_time: None,
                     url: "cpu/torchtext-0.17.0%2Bcpu-cp39-cp39-win_amd64.whl",
                     yanked: None,
                 },
@@ -534,46 +649,50 @@
                     path: "/whl/jinja2/",
                     query: None,
                     fragment: None,
                 },
             ),
             files: [
                 File {
+                    core_metadata: None,
                     dist_info_metadata: None,
+                    data_dist_info_metadata: None,
                     filename: "Jinja2-3.1.2-py3-none-any.whl",
                     hashes: Hashes {
                         md5: None,
                         sha256: None,
+                        sha384: None,
+                        sha512: None,
                     },
                     requires_python: None,
                     size: None,
                     upload_time: None,
                     url: "/whl/Jinja2-3.1.2-py3-none-any.whl",
                     yanked: None,
                 },
             ],
         }
         "###);
     }
 
     #[test]
     fn parse_missing_href() {
-        let text = r#"
+        let text = r"
 <!DOCTYPE html>
 <html>
   <body>
     <h1>Links for jinja2</h1>
     <a>Jinja2-3.1.2-py3-none-any.whl</a><br/>
   </body>
 </html>
 <!--TIMESTAMP 1703347410-->
-        "#;
+        ";
         let base = Url::parse("https://download.pytorch.org/whl/jinja2/").unwrap();
         let result = SimpleHtml::parse(text, &base).unwrap_err();
-        insta::assert_display_snapshot!(result, @"Missing href attribute on anchor link");
+        insta::assert_snapshot!(result, @"Missing href attribute on anchor link");
     }
 
     #[test]
     fn parse_empty_href() {
         let text = r#"
 <!DOCTYPE html>
 <html>
@@ -582,15 +701,15 @@
     <a href="">Jinja2-3.1.2-py3-none-any.whl</a><br/>
   </body>
 </html>
 <!--TIMESTAMP 1703347410-->
         "#;
         let base = Url::parse("https://download.pytorch.org/whl/jinja2/").unwrap();
         let result = SimpleHtml::parse(text, &base).unwrap_err();
-        insta::assert_display_snapshot!(result, @"Missing href attribute on anchor link");
+        insta::assert_snapshot!(result, @"Missing href attribute on anchor link");
     }
 
     #[test]
     fn parse_empty_fragment() {
         let text = r#"
 <!DOCTYPE html>
 <html>
@@ -620,63 +739,123 @@
                     path: "/whl/jinja2/",
                     query: None,
                     fragment: None,
                 },
             ),
             files: [
                 File {
+                    core_metadata: None,
                     dist_info_metadata: None,
+                    data_dist_info_metadata: None,
                     filename: "Jinja2-3.1.2-py3-none-any.whl",
                     hashes: Hashes {
                         md5: None,
                         sha256: None,
+                        sha384: None,
+                        sha512: None,
                     },
                     requires_python: None,
                     size: None,
                     upload_time: None,
                     url: "/whl/Jinja2-3.1.2-py3-none-any.whl#",
                     yanked: None,
                 },
             ],
         }
         "###);
     }
 
     #[test]
+    fn parse_query_string() {
+        let text = r#"
+<!DOCTYPE html>
+<html>
+  <body>
+    <h1>Links for jinja2</h1>
+    <a href="/whl/Jinja2-3.1.2-py3-none-any.whl?project=legacy">Jinja2-3.1.2-py3-none-any.whl</a><br/>
+  </body>
+</html>
+<!--TIMESTAMP 1703347410-->
+        "#;
+        let base = Url::parse("https://download.pytorch.org/whl/jinja2/").unwrap();
+        let result = SimpleHtml::parse(text, &base).unwrap();
+        insta::assert_debug_snapshot!(result, @r###"
+        SimpleHtml {
+            base: BaseUrl(
+                Url {
+                    scheme: "https",
+                    cannot_be_a_base: false,
+                    username: "",
+                    password: None,
+                    host: Some(
+                        Domain(
+                            "download.pytorch.org",
+                        ),
+                    ),
+                    port: None,
+                    path: "/whl/jinja2/",
+                    query: None,
+                    fragment: None,
+                },
+            ),
+            files: [
+                File {
+                    core_metadata: None,
+                    dist_info_metadata: None,
+                    data_dist_info_metadata: None,
+                    filename: "Jinja2-3.1.2-py3-none-any.whl",
+                    hashes: Hashes {
+                        md5: None,
+                        sha256: None,
+                        sha384: None,
+                        sha512: None,
+                    },
+                    requires_python: None,
+                    size: None,
+                    upload_time: None,
+                    url: "/whl/Jinja2-3.1.2-py3-none-any.whl?project=legacy",
+                    yanked: None,
+                },
+            ],
+        }
+        "###);
+    }
+
+    #[test]
     fn parse_missing_hash_value() {
         let text = r#"
 <!DOCTYPE html>
 <html>
   <body>
     <h1>Links for jinja2</h1>
     <a href="/whl/Jinja2-3.1.2-py3-none-any.whl#sha256">Jinja2-3.1.2-py3-none-any.whl</a><br/>
   </body>
 </html>
 <!--TIMESTAMP 1703347410-->
         "#;
         let base = Url::parse("https://download.pytorch.org/whl/jinja2/").unwrap();
         let result = SimpleHtml::parse(text, &base).unwrap_err();
-        insta::assert_display_snapshot!(result, @"Unexpected fragment (expected `#sha256=...`) on URL: sha256");
+        insta::assert_snapshot!(result, @"Unexpected fragment (expected `#sha256=...` or similar) on URL: sha256");
     }
 
     #[test]
     fn parse_unknown_hash() {
         let text = r#"
 <!DOCTYPE html>
 <html>
   <body>
     <h1>Links for jinja2</h1>
-    <a href="/whl/Jinja2-3.1.2-py3-none-any.whl#sha512=6088930bfe239f0e6710546ab9c19c9ef35e29792895fed6e6e31a023a182a61">Jinja2-3.1.2-py3-none-any.whl</a><br/>
+    <a href="/whl/Jinja2-3.1.2-py3-none-any.whl#blake2=6088930bfe239f0e6710546ab9c19c9ef35e29792895fed6e6e31a023a182a61">Jinja2-3.1.2-py3-none-any.whl</a><br/>
   </body>
 </html>
 <!--TIMESTAMP 1703347410-->
         "#;
         let base = Url::parse("https://download.pytorch.org/whl/jinja2/").unwrap();
         let result = SimpleHtml::parse(text, &base).unwrap_err();
-        insta::assert_display_snapshot!(result, @"Unsupported hash algorithm (expected `sha256`) on: sha512=6088930bfe239f0e6710546ab9c19c9ef35e29792895fed6e6e31a023a182a61");
+        insta::assert_snapshot!(result, @"Unsupported hash algorithm (expected `md5`, `sha256`, `sha384`, or `sha512`) on: blake2=6088930bfe239f0e6710546ab9c19c9ef35e29792895fed6e6e31a023a182a61");
     }
 
     #[test]
     fn parse_flat_index_html() {
         let text = r#"
             <!DOCTYPE html>
             <html>
@@ -707,32 +886,40 @@
                     path: "/jax-releases/jax_cuda_releases.html",
                     query: None,
                     fragment: None,
                 },
             ),
             files: [
                 File {
+                    core_metadata: None,
                     dist_info_metadata: None,
+                    data_dist_info_metadata: None,
                     filename: "jaxlib-0.1.52+cuda100-cp36-none-manylinux2010_x86_64.whl",
                     hashes: Hashes {
                         md5: None,
                         sha256: None,
+                        sha384: None,
+                        sha512: None,
                     },
                     requires_python: None,
                     size: None,
                     upload_time: None,
                     url: "https://storage.googleapis.com/jax-releases/cuda100/jaxlib-0.1.52+cuda100-cp36-none-manylinux2010_x86_64.whl",
                     yanked: None,
                 },
                 File {
+                    core_metadata: None,
                     dist_info_metadata: None,
+                    data_dist_info_metadata: None,
                     filename: "jaxlib-0.1.52+cuda100-cp37-none-manylinux2010_x86_64.whl",
                     hashes: Hashes {
                         md5: None,
                         sha256: None,
+                        sha384: None,
+                        sha512: None,
                     },
                     requires_python: None,
                     size: None,
                     upload_time: None,
                     url: "https://storage.googleapis.com/jax-releases/cuda100/jaxlib-0.1.52+cuda100-cp37-none-manylinux2010_x86_64.whl",
                     yanked: None,
                 },
@@ -749,17 +936,17 @@
             <!DOCTYPE html>
             <html>
             <head>
                 <title>Links for flask</title>
             </head>
             <body>
                 <h1>Links for flask</h1>
-                <a href="0.1/Flask-0.1.tar.gz#sha256=9da884457e910bf0847d396cb4b778ad9f3c3d17db1c5997cb861937bd284237"  data-gpg-sig="false" >Flask-0.1.tar.gz</a>
+                <a href="0.1/Flask-0.1.tar.gz#sha256=9da884457e910bf0847d396cb4b778ad9f3c3d17db1c5997cb861937bd284237" data-gpg-sig="false" >Flask-0.1.tar.gz</a>
                 <br/>
-                <a href="0.10.1/Flask-0.10.1.tar.gz#sha256=4c83829ff83d408b5e1d4995472265411d2c414112298f2eb4b359d9e4563373"  data-gpg-sig="false" >Flask-0.10.1.tar.gz</a>
+                <a href="0.10.1/Flask-0.10.1.tar.gz#sha256=4c83829ff83d408b5e1d4995472265411d2c414112298f2eb4b359d9e4563373" data-gpg-sig="false" >Flask-0.10.1.tar.gz</a>
                 <br/>
                 <a href="3.0.1/flask-3.0.1.tar.gz#sha256=6489f51bb3666def6f314e15f19d50a1869a19ae0e8c9a3641ffe66c77d42403" data-requires-python="&gt;=3.8" data-gpg-sig="false" >flask-3.0.1.tar.gz</a>
                 <br/>
             </body>
             </html>
         "#;
         let base = Url::parse("https://account.d.codeartifact.us-west-2.amazonaws.com/pypi/shared-packages-pypi/simple/flask/")
@@ -782,51 +969,63 @@
                     path: "/pypi/shared-packages-pypi/simple/flask/",
                     query: None,
                     fragment: None,
                 },
             ),
             files: [
                 File {
+                    core_metadata: None,
                     dist_info_metadata: None,
+                    data_dist_info_metadata: None,
                     filename: "Flask-0.1.tar.gz",
                     hashes: Hashes {
                         md5: None,
                         sha256: Some(
                             "9da884457e910bf0847d396cb4b778ad9f3c3d17db1c5997cb861937bd284237",
                         ),
+                        sha384: None,
+                        sha512: None,
                     },
                     requires_python: None,
                     size: None,
                     upload_time: None,
                     url: "0.1/Flask-0.1.tar.gz#sha256=9da884457e910bf0847d396cb4b778ad9f3c3d17db1c5997cb861937bd284237",
                     yanked: None,
                 },
                 File {
+                    core_metadata: None,
                     dist_info_metadata: None,
+                    data_dist_info_metadata: None,
                     filename: "Flask-0.10.1.tar.gz",
                     hashes: Hashes {
                         md5: None,
                         sha256: Some(
                             "4c83829ff83d408b5e1d4995472265411d2c414112298f2eb4b359d9e4563373",
                         ),
+                        sha384: None,
+                        sha512: None,
                     },
                     requires_python: None,
                     size: None,
                     upload_time: None,
                     url: "0.10.1/Flask-0.10.1.tar.gz#sha256=4c83829ff83d408b5e1d4995472265411d2c414112298f2eb4b359d9e4563373",
                     yanked: None,
                 },
                 File {
+                    core_metadata: None,
                     dist_info_metadata: None,
+                    data_dist_info_metadata: None,
                     filename: "flask-3.0.1.tar.gz",
                     hashes: Hashes {
                         md5: None,
                         sha256: Some(
                             "6489f51bb3666def6f314e15f19d50a1869a19ae0e8c9a3641ffe66c77d42403",
                         ),
+                        sha384: None,
+                        sha512: None,
                     },
                     requires_python: Some(
                         Ok(
                             VersionSpecifiers(
                                 [
                                     VersionSpecifier {
                                         operator: GreaterThanEqual,
@@ -876,21 +1075,25 @@
                     path: "/whl/jinja2/",
                     query: None,
                     fragment: None,
                 },
             ),
             files: [
                 File {
+                    core_metadata: None,
                     dist_info_metadata: None,
+                    data_dist_info_metadata: None,
                     filename: "Jinja2-3.1.2-py3-none-any.whl",
                     hashes: Hashes {
                         md5: None,
                         sha256: Some(
                             "6088930bfe239f0e6710546ab9c19c9ef35e29792895fed6e6e31a023a182a61",
                         ),
+                        sha384: None,
+                        sha512: None,
                     },
                     requires_python: Some(
                         Ok(
                             VersionSpecifiers(
                                 [
                                     VersionSpecifier {
                                         operator: GreaterThanEqual,
@@ -905,8 +1108,157 @@
                     url: "/whl/Jinja2-3.1.2-py3-none-any.whl#sha256=6088930bfe239f0e6710546ab9c19c9ef35e29792895fed6e6e31a023a182a61",
                     yanked: None,
                 },
             ],
         }
         "###);
     }
+
+    /// Respect PEP 714 (see: <https://peps.python.org/pep-0714/>).
+    #[test]
+    fn parse_core_metadata() {
+        let text = r#"
+<!DOCTYPE html>
+<html>
+  <body>
+    <h1>Links for jinja2</h1>
+    <a href="/whl/Jinja2-3.1.2-py3-none-any.whl" data-dist-info-metadata="true">Jinja2-3.1.2-py3-none-any.whl</a><br/>
+    <a href="/whl/Jinja2-3.1.3-py3-none-any.whl" data-core-metadata="true">Jinja2-3.1.3-py3-none-any.whl</a><br/>
+    <a href="/whl/Jinja2-3.1.4-py3-none-any.whl" data-dist-info-metadata="false">Jinja2-3.1.4-py3-none-any.whl</a><br/>
+    <a href="/whl/Jinja2-3.1.5-py3-none-any.whl" data-core-metadata="false">Jinja2-3.1.5-py3-none-any.whl</a><br/>
+    <a href="/whl/Jinja2-3.1.6-py3-none-any.whl" data-core-metadata="true" data-dist-info-metadata="false">Jinja2-3.1.6-py3-none-any.whl</a><br/>
+  </body>
+</html>
+        "#;
+        let base = Url::parse("https://account.d.codeartifact.us-west-2.amazonaws.com/pypi/shared-packages-pypi/simple/flask/")
+            .unwrap();
+        let result = SimpleHtml::parse(text, &base).unwrap();
+        insta::assert_debug_snapshot!(result, @r###"
+        SimpleHtml {
+            base: BaseUrl(
+                Url {
+                    scheme: "https",
+                    cannot_be_a_base: false,
+                    username: "",
+                    password: None,
+                    host: Some(
+                        Domain(
+                            "account.d.codeartifact.us-west-2.amazonaws.com",
+                        ),
+                    ),
+                    port: None,
+                    path: "/pypi/shared-packages-pypi/simple/flask/",
+                    query: None,
+                    fragment: None,
+                },
+            ),
+            files: [
+                File {
+                    core_metadata: Some(
+                        Bool(
+                            true,
+                        ),
+                    ),
+                    dist_info_metadata: None,
+                    data_dist_info_metadata: None,
+                    filename: "Jinja2-3.1.2-py3-none-any.whl",
+                    hashes: Hashes {
+                        md5: None,
+                        sha256: None,
+                        sha384: None,
+                        sha512: None,
+                    },
+                    requires_python: None,
+                    size: None,
+                    upload_time: None,
+                    url: "/whl/Jinja2-3.1.2-py3-none-any.whl",
+                    yanked: None,
+                },
+                File {
+                    core_metadata: Some(
+                        Bool(
+                            true,
+                        ),
+                    ),
+                    dist_info_metadata: None,
+                    data_dist_info_metadata: None,
+                    filename: "Jinja2-3.1.3-py3-none-any.whl",
+                    hashes: Hashes {
+                        md5: None,
+                        sha256: None,
+                        sha384: None,
+                        sha512: None,
+                    },
+                    requires_python: None,
+                    size: None,
+                    upload_time: None,
+                    url: "/whl/Jinja2-3.1.3-py3-none-any.whl",
+                    yanked: None,
+                },
+                File {
+                    core_metadata: Some(
+                        Bool(
+                            false,
+                        ),
+                    ),
+                    dist_info_metadata: None,
+                    data_dist_info_metadata: None,
+                    filename: "Jinja2-3.1.4-py3-none-any.whl",
+                    hashes: Hashes {
+                        md5: None,
+                        sha256: None,
+                        sha384: None,
+                        sha512: None,
+                    },
+                    requires_python: None,
+                    size: None,
+                    upload_time: None,
+                    url: "/whl/Jinja2-3.1.4-py3-none-any.whl",
+                    yanked: None,
+                },
+                File {
+                    core_metadata: Some(
+                        Bool(
+                            false,
+                        ),
+                    ),
+                    dist_info_metadata: None,
+                    data_dist_info_metadata: None,
+                    filename: "Jinja2-3.1.5-py3-none-any.whl",
+                    hashes: Hashes {
+                        md5: None,
+                        sha256: None,
+                        sha384: None,
+                        sha512: None,
+                    },
+                    requires_python: None,
+                    size: None,
+                    upload_time: None,
+                    url: "/whl/Jinja2-3.1.5-py3-none-any.whl",
+                    yanked: None,
+                },
+                File {
+                    core_metadata: Some(
+                        Bool(
+                            true,
+                        ),
+                    ),
+                    dist_info_metadata: None,
+                    data_dist_info_metadata: None,
+                    filename: "Jinja2-3.1.6-py3-none-any.whl",
+                    hashes: Hashes {
+                        md5: None,
+                        sha256: None,
+                        sha384: None,
+                        sha512: None,
+                    },
+                    requires_python: None,
+                    size: None,
+                    upload_time: None,
+                    url: "/whl/Jinja2-3.1.6-py3-none-any.whl",
+                    yanked: None,
+                },
+            ],
+        }
+        "###);
+    }
 }
```

### Comparing `uv-0.1.9/crates/uv-client/src/httpcache/control.rs` & `uv-0.2.0/crates/uv-client/src/httpcache/control.rs`

 * *Files 2% similar despite different names*

```diff
@@ -60,36 +60,36 @@
     pub s_maxage_seconds: Option<u64>,
     /// https://httpwg.org/specs/rfc8246.html
     pub immutable: bool,
 }
 
 impl CacheControl {
     /// Convert this to an owned archive value.
-    pub fn to_archived(&self) -> OwnedArchive<CacheControl> {
+    pub fn to_archived(&self) -> OwnedArchive<Self> {
         // There's no way (other than OOM) for serializing this type to fail.
         OwnedArchive::from_unarchived(self).expect("all possible values can be archived")
     }
 }
 
 impl<'b, B: 'b + ?Sized + AsRef<[u8]>> FromIterator<&'b B> for CacheControl {
-    fn from_iter<T: IntoIterator<Item = &'b B>>(it: T) -> CacheControl {
-        CacheControl::from_iter(CacheControlParser::new(it))
+    fn from_iter<T: IntoIterator<Item = &'b B>>(it: T) -> Self {
+        Self::from_iter(CacheControlParser::new(it))
     }
 }
 
 impl FromIterator<CacheControlDirective> for CacheControl {
-    fn from_iter<T: IntoIterator<Item = CacheControlDirective>>(it: T) -> CacheControl {
+    fn from_iter<T: IntoIterator<Item = CacheControlDirective>>(it: T) -> Self {
         fn parse_int(value: &[u8]) -> Option<u64> {
             if !value.iter().all(u8::is_ascii_digit) {
                 return None;
             }
             std::str::from_utf8(value).ok()?.parse().ok()
         }
 
-        let mut cc = CacheControl::default();
+        let mut cc = Self::default();
         for ccd in it {
             // Note that when we see invalid directive values, we follow [RFC
             // 9111 S4.2.1]. It says that invalid cache-control directives
             // should result in treating the response as stale. (Which we
             // accomplished by setting `must_revalidate` to `true`.)
             //
             // [RFC 9111 S4.2.1]: https://www.rfc-editor.org/rfc/rfc9111.html#section-4.2.1
@@ -437,16 +437,16 @@
     value: Vec<u8>,
 }
 
 impl CacheControlDirective {
     /// Returns a `must-revalidate` directive. This is useful for forcing a
     /// cache decision that the response is stale, and thus the server should
     /// be consulted for whether the cached response ought to be used or not.
-    fn must_revalidate() -> CacheControlDirective {
-        CacheControlDirective {
+    fn must_revalidate() -> Self {
+        Self {
             name: "must-revalidate".to_string(),
             value: vec![],
         }
     }
 }
 
 #[cfg(test)]
@@ -600,17 +600,17 @@
             ]
         );
     }
 
     #[test]
     fn cache_control_parse_multiple_directives_across_multiple_header_values() {
         let headers = [
-            r#"max-age=60, no-cache"#,
+            r"max-age=60, no-cache",
             r#"private="cookie""#,
-            r#"no-transform"#,
+            r"no-transform",
         ];
         let directives = CacheControlParser::new(headers).collect::<Vec<_>>();
         assert_eq!(
             directives,
             vec![
                 CacheControlDirective {
                     name: "max-age".to_string(),
@@ -631,17 +631,17 @@
             ]
         );
     }
 
     #[test]
     fn cache_control_parse_one_header_invalid() {
         let headers = [
-            r#"max-age=60, no-cache"#,
+            r"max-age=60, no-cache",
             r#", private="cookie""#,
-            r#"no-transform"#,
+            r"no-transform",
         ];
         let directives = CacheControlParser::new(headers).collect::<Vec<_>>();
         assert_eq!(
             directives,
             vec![
                 CacheControlDirective {
                     name: "max-age".to_string(),
@@ -684,15 +684,15 @@
                 },
             ]
         );
     }
 
     #[test]
     fn cache_control_parse_name_normalized() {
-        let header = r#"MAX-AGE=60"#;
+        let header = r"MAX-AGE=60";
         let directives = CacheControlParser::new([header]).collect::<Vec<_>>();
         assert_eq!(
             directives,
             vec![CacheControlDirective {
                 name: "max-age".to_string(),
                 value: b"60".to_vec(),
             },]
@@ -700,15 +700,15 @@
     }
 
     // When a duplicate directive is found, we keep the first one
     // and add in a `must-revalidate` directive to indicate that
     // things are stale and the client should do a re-check.
     #[test]
     fn cache_control_parse_duplicate_directives() {
-        let header = r#"max-age=60, no-cache, max-age=30"#;
+        let header = r"max-age=60, no-cache, max-age=30";
         let directives = CacheControlParser::new([header]).collect::<Vec<_>>();
         assert_eq!(
             directives,
             vec![
                 CacheControlDirective {
                     name: "max-age".to_string(),
                     value: b"60".to_vec(),
@@ -723,15 +723,15 @@
                 },
             ]
         );
     }
 
     #[test]
     fn cache_control_parse_duplicate_directives_across_headers() {
-        let headers = [r#"max-age=60, no-cache"#, r#"max-age=30"#];
+        let headers = [r"max-age=60, no-cache", r"max-age=30"];
         let directives = CacheControlParser::new(headers).collect::<Vec<_>>();
         assert_eq!(
             directives,
             vec![
                 CacheControlDirective {
                     name: "max-age".to_string(),
                     value: b"60".to_vec(),
@@ -748,15 +748,15 @@
         );
     }
 
     // Tests that we don't emit must-revalidate multiple times
     // even when something is duplicated multiple times.
     #[test]
     fn cache_control_parse_duplicate_redux() {
-        let header = r#"max-age=60, no-cache, no-cache, max-age=30"#;
+        let header = r"max-age=60, no-cache, no-cache, max-age=30";
         let directives = CacheControlParser::new([header]).collect::<Vec<_>>();
         assert_eq!(
             directives,
             vec![
                 CacheControlDirective {
                     name: "max-age".to_string(),
                     value: b"60".to_vec(),
```

### Comparing `uv-0.1.9/crates/uv-client/src/httpcache/mod.rs` & `uv-0.2.0/crates/uv-client/src/httpcache/mod.rs`

 * *Files 2% similar despite different names*

```diff
@@ -113,15 +113,15 @@
 achieve this `CachePolicy::to_archived` is provided, which will serialize the
 `CachePolicy` to its archived representation in bytes, and then turn that
 into an `OwnedArchive<CachePolicy>` which derefs to `ArchivedCachePolicy`.
 This is a little extra cost, but the idea is that a `CachePolicy` (not an
 `ArchivedCachePolicy`) should only be used in the slower path (i.e., when you
 actually need to make an HTTP request).
 
-[`rkyv::vec::ArchivedVec`]: hhttps://docs.rs/rkyv/0.7.43/rkyv/vec/struct.ArchivedVec.html
+[`rkyv::vec::ArchivedVec`]: https://docs.rs/rkyv/0.7.43/rkyv/vec/struct.ArchivedVec.html
 [`rkyv::string::ArchivedString`]: https://docs.rs/rkyv/0.7.43/rkyv/string/struct.ArchivedString.html
 
 # Additional reading
 
 * Short introduction to `Cache-Control`: https://csswizardry.com/2019/03/cache-control-for-civilians/
 * Caching best practcies: https://jakearchibald.com/2016/caching-best-practices/
 * Overview of HTTP caching: https://developer.mozilla.org/en-US/docs/Web/HTTP/Caching
@@ -158,16 +158,16 @@
 #[repr(C)]
 struct CacheConfig {
     shared: bool,
     heuristic_percent: u8,
 }
 
 impl Default for CacheConfig {
-    fn default() -> CacheConfig {
-        CacheConfig {
+    fn default() -> Self {
+        Self {
             // The caching uv does ought to be considered
             // private.
             shared: false,
             // This is only used to heuristically guess at a freshness lifetime
             // when other indicators (such as `max-age` and `Expires` are
             // absent.
             heuristic_percent: 10,
@@ -205,19 +205,19 @@
     ///
     /// [RFC 9111 S4.1]: https://www.rfc-editor.org/rfc/rfc9111.html#section-4.1
     request_headers: http::HeaderMap,
 }
 
 impl CachePolicyBuilder {
     /// Create a new builder of a cache policy, starting with the request.
-    pub fn new(request: &reqwest::Request) -> CachePolicyBuilder {
+    pub fn new(request: &reqwest::Request) -> Self {
         let config = CacheConfig::default();
         let request_headers = request.headers().clone();
         let request = Request::from(request);
-        CachePolicyBuilder {
+        Self {
             config,
             request,
             request_headers,
         }
     }
 
     /// Return a new policy given the response to the request that this builder
@@ -272,15 +272,15 @@
     /// archived types.
     ///
     /// These do incur an extra cost, but this should only be needed when you
     /// don't have an `ArchivedCachePolicy`. And that should only occur when
     /// you're actually performing an HTTP request. In that case, the extra
     /// cost that is done here to convert a `CachePolicy` to its archived form
     /// should be marginal.
-    pub fn to_archived(&self) -> OwnedArchive<CachePolicy> {
+    pub fn to_archived(&self) -> OwnedArchive<Self> {
         // There's no way (other than OOM) for serializing this type to fail.
         OwnedArchive::from_unarchived(self).expect("all possible values can be archived")
     }
 }
 
 impl ArchivedCachePolicy {
     /// Determines what caching behavior is correct given an existing
@@ -418,25 +418,33 @@
         // response can be updated and reused"
         //
         // So if we don't get a 304, then we know our cached response is seen
         // as stale by the origin server.
         //
         // [RFC 9111 S4.3.3]: https://www.rfc-editor.org/rfc/rfc9111.html#section-4.3.3
         if new_policy.response.status != 304 {
+            tracing::trace!(
+                "is modified because status is {:?} and not 304",
+                new_policy.response.status
+            );
             return true;
         }
         // As per [RFC 9111 S4.3.4], we need to confirm that our validators match. Here,
         // we check `ETag`.
         //
         // [RFC 9111 S4.3.4]: https://www.rfc-editor.org/rfc/rfc9111.html#section-4.3.4
         if let Some(old_etag) = self.response.headers.etag.as_ref() {
             if let Some(new_etag) = new_policy.response.headers.etag.as_ref() {
                 // We don't support weak validators, so only match if they're
                 // both strong.
                 if !old_etag.weak && !new_etag.weak && old_etag.value == new_etag.value {
+                    tracing::trace!(
+                        "not modified because old and new etag values ({:?}) match",
+                        new_etag.value,
+                    );
                     return false;
                 }
             }
         }
         // As per [RFC 9111 S4.3.4], we need to confirm that our validators match. Here,
         // we check `Last-Modified`.
         //
@@ -446,14 +454,17 @@
             if let Some(new_last_modified) = new_policy
                 .response
                 .headers
                 .last_modified_unix_timestamp
                 .as_ref()
             {
                 if old_last_modified == new_last_modified {
+                    tracing::trace!(
+                        "not modified because modified times ({new_last_modified:?}) match",
+                    );
                     return false;
                 }
             }
         }
         // As per [RFC 9111 S4.3.4], if we have no validators anywhere, then
         // we can just rely on the HTTP 304 status code and reuse the cached
         // response.
@@ -464,14 +475,18 @@
             && self.response.headers.last_modified_unix_timestamp.is_none()
             && new_policy
                 .response
                 .headers
                 .last_modified_unix_timestamp
                 .is_none()
         {
+            tracing::trace!(
+                "not modified because there are no etags or last modified \
+                 timestamps, so we assume the 304 status is correct",
+            );
             return false;
         }
         true
     }
 
     /// Sets the relevant headers on the given request so that it can be used
     /// as a revalidation request. As per [RFC 9111 S4.3.1], this permits the
@@ -991,16 +1006,16 @@
     uri: String,
     method: Method,
     headers: RequestHeaders,
     unix_timestamp: u64,
 }
 
 impl<'a> From<&'a reqwest::Request> for Request {
-    fn from(from: &'a reqwest::Request) -> Request {
-        Request {
+    fn from(from: &'a reqwest::Request) -> Self {
+        Self {
             uri: from.url().to_string(),
             method: Method::from(from.method()),
             headers: RequestHeaders::from(from.headers()),
             unix_timestamp: unix_timestamp(SystemTime::now()),
         }
     }
 }
@@ -1013,16 +1028,16 @@
     cc: CacheControl,
     /// This is set to `true` only when an `Authorization` header is present.
     /// We don't need to record the value.
     authorization: bool,
 }
 
 impl<'a> From<&'a http::HeaderMap> for RequestHeaders {
-    fn from(from: &'a http::HeaderMap) -> RequestHeaders {
-        RequestHeaders {
+    fn from(from: &'a http::HeaderMap) -> Self {
+        Self {
             cc: from.get_all("cache-control").iter().collect(),
             authorization: from.contains_key("authorization"),
         }
     }
 }
 
 /// The HTTP method used on a request.
@@ -1037,21 +1052,21 @@
 enum Method {
     Get,
     Head,
     Unrecognized,
 }
 
 impl<'a> From<&'a http::Method> for Method {
-    fn from(from: &'a http::Method) -> Method {
+    fn from(from: &'a http::Method) -> Self {
         if from == http::Method::GET {
-            Method::Get
+            Self::Get
         } else if from == http::Method::HEAD {
-            Method::Head
+            Self::Head
         } else {
-            Method::Unrecognized
+            Self::Unrecognized
         }
     }
 }
 
 #[derive(Debug, rkyv::Archive, rkyv::Deserialize, rkyv::Serialize)]
 #[archive(check_bytes)]
 #[archive_attr(derive(Debug))]
@@ -1090,16 +1105,16 @@
     /// [RFC 9110 S15]: https://www.rfc-editor.org/rfc/rfc9110#section-15
     fn has_final_status(&self) -> bool {
         self.status >= 200
     }
 }
 
 impl<'a> From<&'a reqwest::Response> for Response {
-    fn from(from: &'a reqwest::Response) -> Response {
-        Response {
+    fn from(from: &'a reqwest::Response) -> Self {
+        Self {
             status: from.status().as_u16(),
             headers: ResponseHeaders::from(from.headers()),
             unix_timestamp: unix_timestamp(SystemTime::now()),
         }
     }
 }
 
@@ -1145,16 +1160,16 @@
     /// used in revalidation requests.
     ///
     /// [RFC 9110 S8.8.3]: https://www.rfc-editor.org/rfc/rfc9110#section-8.8.3
     etag: Option<ETag>,
 }
 
 impl<'a> From<&'a http::HeaderMap> for ResponseHeaders {
-    fn from(from: &'a http::HeaderMap) -> ResponseHeaders {
-        ResponseHeaders {
+    fn from(from: &'a http::HeaderMap) -> Self {
+        Self {
             cc: from.get_all("cache-control").iter().collect(),
             age_seconds: from
                 .get("age")
                 .and_then(|header| parse_seconds(header.as_bytes())),
             date_unix_timestamp: from
                 .get("date")
                 .and_then(|header| header.to_str().ok())
@@ -1206,21 +1221,21 @@
 impl ETag {
     /// Parses an ETag from a header value.
     ///
     /// We are a little permissive here and allow arbitrary bytes,
     /// where as [RFC 9110 S8.8.3] is a bit more restrictive.
     ///
     /// [RFC 9110 S8.8.3]: https://www.rfc-editor.org/rfc/rfc9110#section-8.8.3
-    fn parse(header_value: &[u8]) -> ETag {
+    fn parse(header_value: &[u8]) -> Self {
         let (value, weak) = if header_value.starts_with(b"W/") {
             (&header_value[2..], true)
         } else {
             (header_value, false)
         };
-        ETag {
+        Self {
             value: value.to_vec(),
             weak,
         }
     }
 }
 
 /// Represents the `Vary` header on a cached response, as per [RFC 9110
@@ -1237,52 +1252,52 @@
 #[archive_attr(derive(Debug))]
 struct Vary {
     fields: Vec<VaryField>,
 }
 
 impl Vary {
     /// Returns a `Vary` header value that will never match any request.
-    fn always_fails_to_match() -> Vary {
-        Vary {
+    fn always_fails_to_match() -> Self {
+        Self {
             fields: vec![VaryField {
                 name: "*".to_string(),
                 value: vec![],
             }],
         }
     }
 
     fn from_request_response_headers(
         request: &http::HeaderMap,
         response: &http::HeaderMap,
-    ) -> Vary {
+    ) -> Self {
         // Parses the `Vary` header as per [RFC 9110 S12.5.5].
         //
         // [RFC 9110 S12.5.5]: https://www.rfc-editor.org/rfc/rfc9110#section-12.5.5
         let mut fields = vec![];
         for header in response.get_all("vary") {
             let Ok(csv) = header.to_str() else { continue };
             for header_name in csv.split(',') {
                 let header_name = header_name.trim().to_ascii_lowercase();
                 // When we see a `*`, that means a failed match is an
                 // inevitability, regardless of anything else. So just give up
                 // and return a `Vary` that will never match.
                 if header_name == "*" {
-                    return Vary::always_fails_to_match();
+                    return Self::always_fails_to_match();
                 }
                 let value = request
                     .get(&header_name)
                     .map(|header| header.as_bytes().to_vec())
                     .unwrap_or_default();
                 fields.push(VaryField {
                     name: header_name,
                     value,
                 });
             }
         }
-        Vary { fields }
+        Self { fields }
     }
 }
 
 impl ArchivedVary {
     /// Returns true only when the `Vary` header on a cached response satisfies
     /// the request header values given, as per [RFC 9111 S4.1].
     ///
```

### Comparing `uv-0.1.9/crates/uv-client/src/lib.rs` & `uv-0.2.0/crates/uv-client/src/lib.rs`

 * *Files 10% similar despite different names*

```diff
@@ -1,18 +1,22 @@
+pub use base_client::{BaseClient, BaseClientBuilder};
 pub use cached_client::{CacheControl, CachedClient, CachedClientError, DataWithCachePolicy};
-pub use error::{Error, ErrorKind};
-pub use flat_index::{FlatDistributions, FlatIndex, FlatIndexClient, FlatIndexError};
+pub use error::{BetterReqwestError, Error, ErrorKind};
+pub use flat_index::{FlatIndexClient, FlatIndexEntries, FlatIndexError};
+pub use linehaul::LineHaul;
 pub use registry_client::{
     Connectivity, RegistryClient, RegistryClientBuilder, SimpleMetadata, SimpleMetadatum,
     VersionFiles,
 };
 pub use rkyvutil::OwnedArchive;
 
+mod base_client;
 mod cached_client;
 mod error;
 mod flat_index;
 mod html;
 mod httpcache;
+mod linehaul;
 mod middleware;
 mod registry_client;
 mod remote_metadata;
 mod rkyvutil;
```

### Comparing `uv-0.1.9/crates/uv-client/src/middleware.rs` & `uv-0.2.0/crates/uv-client/src/middleware.rs`

 * *Files 6% similar despite different names*

```diff
@@ -1,12 +1,12 @@
+use http::Extensions;
 use std::fmt::Debug;
 
 use reqwest::{Request, Response};
 use reqwest_middleware::{Middleware, Next};
-use task_local_extensions::Extensions;
 use url::Url;
 
 /// A custom error type for the offline middleware.
 #[derive(Debug, Clone, PartialEq, Eq)]
 pub(crate) struct OfflineError {
     url: Url,
 }
```

### Comparing `uv-0.1.9/crates/uv-client/src/registry_client.rs` & `uv-0.2.0/crates/uv-client/src/registry_client.rs`

 * *Files 6% similar despite different names*

```diff
@@ -1,210 +1,271 @@
 use std::collections::BTreeMap;
-use std::env;
 use std::fmt::Debug;
 use std::path::Path;
 use std::str::FromStr;
 
 use async_http_range_reader::AsyncHttpRangeReader;
 use futures::{FutureExt, TryStreamExt};
-
-use reqwest::{Client, ClientBuilder, Response, StatusCode};
-use reqwest_retry::policies::ExponentialBackoff;
-use reqwest_retry::RetryTransientMiddleware;
+use http::HeaderMap;
+use reqwest::{Client, Response, StatusCode};
 use serde::{Deserialize, Serialize};
 use tokio::io::AsyncReadExt;
-use tokio_util::compat::FuturesAsyncReadCompatExt;
-use tracing::{debug, info_span, instrument, trace, warn, Instrument};
+use tokio_util::compat::{FuturesAsyncReadCompatExt, TokioAsyncReadCompatExt};
+use tracing::{info_span, instrument, trace, warn, Instrument};
 use url::Url;
 
 use distribution_filename::{DistFilename, SourceDistFilename, WheelFilename};
 use distribution_types::{BuiltDist, File, FileLocation, IndexUrl, IndexUrls, Name};
-use install_wheel_rs::{find_dist_info, is_metadata_entry};
+use install_wheel_rs::metadata::{find_archive_dist_info, is_metadata_entry};
 use pep440_rs::Version;
-use pypi_types::{Metadata21, SimpleJson};
-use uv_auth::safe_copy_url_auth;
+use pep508_rs::MarkerEnvironment;
+use platform_tags::Platform;
+use pypi_types::{Metadata23, SimpleJson};
 use uv_cache::{Cache, CacheBucket, WheelCache};
+use uv_configuration::IndexStrategy;
+use uv_configuration::KeyringProviderType;
 use uv_normalize::PackageName;
-use uv_warnings::warn_user_once;
 
+use crate::base_client::{BaseClient, BaseClientBuilder};
 use crate::cached_client::CacheControl;
 use crate::html::SimpleHtml;
-use crate::middleware::OfflineMiddleware;
 use crate::remote_metadata::wheel_metadata_from_remote_zip;
 use crate::rkyvutil::OwnedArchive;
 use crate::{CachedClient, CachedClientError, Error, ErrorKind};
 
 /// A builder for an [`RegistryClient`].
 #[derive(Debug, Clone)]
-pub struct RegistryClientBuilder {
+pub struct RegistryClientBuilder<'a> {
     index_urls: IndexUrls,
+    index_strategy: IndexStrategy,
+    keyring: KeyringProviderType,
+    native_tls: bool,
     retries: u32,
     connectivity: Connectivity,
     cache: Cache,
     client: Option<Client>,
+    markers: Option<&'a MarkerEnvironment>,
+    platform: Option<&'a Platform>,
 }
 
-impl RegistryClientBuilder {
+impl RegistryClientBuilder<'_> {
     pub fn new(cache: Cache) -> Self {
         Self {
             index_urls: IndexUrls::default(),
+            index_strategy: IndexStrategy::default(),
+            keyring: KeyringProviderType::default(),
+            native_tls: false,
             cache,
             connectivity: Connectivity::Online,
             retries: 3,
             client: None,
+            markers: None,
+            platform: None,
         }
     }
 }
 
-impl RegistryClientBuilder {
+impl<'a> RegistryClientBuilder<'a> {
     #[must_use]
     pub fn index_urls(mut self, index_urls: IndexUrls) -> Self {
         self.index_urls = index_urls;
         self
     }
 
     #[must_use]
+    pub fn index_strategy(mut self, index_strategy: IndexStrategy) -> Self {
+        self.index_strategy = index_strategy;
+        self
+    }
+
+    #[must_use]
+    pub fn keyring(mut self, keyring_type: KeyringProviderType) -> Self {
+        self.keyring = keyring_type;
+        self
+    }
+
+    #[must_use]
     pub fn connectivity(mut self, connectivity: Connectivity) -> Self {
         self.connectivity = connectivity;
         self
     }
 
     #[must_use]
     pub fn retries(mut self, retries: u32) -> Self {
         self.retries = retries;
         self
     }
 
     #[must_use]
-    pub fn cache<T>(mut self, cache: Cache) -> Self {
+    pub fn native_tls(mut self, native_tls: bool) -> Self {
+        self.native_tls = native_tls;
+        self
+    }
+
+    #[must_use]
+    pub fn cache(mut self, cache: Cache) -> Self {
         self.cache = cache;
         self
     }
 
     #[must_use]
     pub fn client(mut self, client: Client) -> Self {
         self.client = Some(client);
         self
     }
 
+    #[must_use]
+    pub fn markers(mut self, markers: &'a MarkerEnvironment) -> Self {
+        self.markers = Some(markers);
+        self
+    }
+
+    #[must_use]
+    pub fn platform(mut self, platform: &'a Platform) -> Self {
+        self.platform = Some(platform);
+        self
+    }
+
     pub fn build(self) -> RegistryClient {
-        let client_raw = self.client.unwrap_or_else(|| {
-            // Timeout options, matching https://doc.rust-lang.org/nightly/cargo/reference/config.html#httptimeout
-            // `UV_REQUEST_TIMEOUT` is provided for backwards compatibility with v0.1.6
-            let default_timeout = 5 * 60;
-            let timeout = env::var("UV_HTTP_TIMEOUT").or_else(|_| env::var("UV_REQUEST_TIMEOUT")).or_else(|_| env::var("HTTP_TIMEOUT")).and_then(|value| {
-                value.parse::<u64>()
-                    .or_else(|_| {
-                        // On parse error, warn and  use the default timeout
-                        warn_user_once!("Ignoring invalid value from environment for UV_REQUEST_TIMEOUT. Expected integer number of seconds, got \"{value}\".");
-                        Ok(default_timeout)
-                    })
-            }).unwrap_or(default_timeout);
-            debug!("Using registry request timeout of {}s", timeout);
-            // Disallow any connections.
-            let client_core = ClientBuilder::new()
-                .user_agent("uv")
-                .pool_max_idle_per_host(20)
-                .timeout(std::time::Duration::from_secs(timeout));
-
-            client_core.build().expect("Failed to build HTTP client.")
-        });
-
-        let uncached_client = match self.connectivity {
-            Connectivity::Online => {
-                let retry_policy =
-                    ExponentialBackoff::builder().build_with_max_retries(self.retries);
-                let retry_strategy = RetryTransientMiddleware::new_with_policy(retry_policy);
-                reqwest_middleware::ClientBuilder::new(client_raw.clone())
-                    .with(retry_strategy)
-                    .build()
-            }
-            Connectivity::Offline => reqwest_middleware::ClientBuilder::new(client_raw.clone())
-                .with(OfflineMiddleware)
-                .build(),
-        };
+        // Build a base client
+        let mut builder = BaseClientBuilder::new();
+
+        if let Some(client) = self.client {
+            builder = builder.client(client)
+        }
+
+        if let Some(markers) = self.markers {
+            builder = builder.markers(markers)
+        }
+
+        if let Some(platform) = self.platform {
+            builder = builder.platform(platform)
+        }
+
+        let client = builder
+            .retries(self.retries)
+            .connectivity(self.connectivity)
+            .native_tls(self.native_tls)
+            .keyring(self.keyring)
+            .build();
+
+        let timeout = client.timeout();
+        let connectivity = client.connectivity();
+
+        // Wrap in the cache middleware.
+        let client = CachedClient::new(client);
 
         RegistryClient {
             index_urls: self.index_urls,
+            index_strategy: self.index_strategy,
             cache: self.cache,
-            connectivity: self.connectivity,
-            client_raw: client_raw.clone(),
-            client: CachedClient::new(uncached_client.clone()),
+            connectivity,
+            client,
+            timeout,
         }
     }
 }
 
 /// A client for fetching packages from a `PyPI`-compatible index.
 #[derive(Debug, Clone)]
 pub struct RegistryClient {
     /// The index URLs to use for fetching packages.
     index_urls: IndexUrls,
+    /// The strategy to use when fetching across multiple indexes.
+    index_strategy: IndexStrategy,
     /// The underlying HTTP client.
     client: CachedClient,
-    /// Don't use this client, it only exists because `async_http_range_reader` needs
-    /// [`reqwest::Client] instead of [`reqwest_middleware::Client`]
-    client_raw: Client,
-    /// Used for the remote wheel METADATA cache
+    /// Used for the remote wheel METADATA cache.
     cache: Cache,
     /// The connectivity mode to use.
     connectivity: Connectivity,
+    /// Configured client timeout, in seconds.
+    timeout: u64,
 }
 
 impl RegistryClient {
     /// Return the [`CachedClient`] used by this client.
     pub fn cached_client(&self) -> &CachedClient {
         &self.client
     }
 
+    /// Return the [`BaseClient`] used by this client.
+    pub fn uncached_client(&self) -> BaseClient {
+        self.client.uncached()
+    }
+
     /// Return the [`Connectivity`] mode used by this client.
     pub fn connectivity(&self) -> Connectivity {
         self.connectivity
     }
 
+    /// Return the timeout this client is configured with, in seconds.
+    pub fn timeout(&self) -> u64 {
+        self.timeout
+    }
+
+    /// Set the index URLs to use for fetching packages.
+    #[must_use]
+    pub fn with_index_url(self, index_urls: IndexUrls) -> Self {
+        Self { index_urls, ..self }
+    }
+
     /// Fetch a package from the `PyPI` simple API.
     ///
     /// "simple" here refers to [PEP 503 – Simple Repository API](https://peps.python.org/pep-0503/)
     /// and [PEP 691 – JSON-based Simple API for Python Package Indexes](https://peps.python.org/pep-0691/),
     /// which the pypi json api approximately implements.
     #[instrument("simple_api", skip_all, fields(package = % package_name))]
     pub async fn simple(
         &self,
         package_name: &PackageName,
-    ) -> Result<(IndexUrl, OwnedArchive<SimpleMetadata>), Error> {
-        if self.index_urls.no_index() {
-            return Err(ErrorKind::NoIndex(package_name.as_ref().to_string()).into());
+    ) -> Result<Vec<(IndexUrl, OwnedArchive<SimpleMetadata>)>, Error> {
+        let mut it = self.index_urls.indexes().peekable();
+        if it.peek().is_none() {
+            return Err(ErrorKind::NoIndex(package_name.to_string()).into());
         }
 
-        for index in self.index_urls.indexes() {
-            let result = self.simple_single_index(package_name, index).await?;
-
-            return match result {
-                Ok(metadata) => Ok((index.clone(), metadata)),
+        let mut results = Vec::new();
+        for index in it {
+            match self.simple_single_index(package_name, index).await? {
+                Ok(metadata) => {
+                    results.push((index.clone(), metadata));
+
+                    // If we're only using the first match, we can stop here.
+                    if self.index_strategy == IndexStrategy::FirstIndex {
+                        break;
+                    }
+                }
                 Err(CachedClientError::Client(err)) => match err.into_kind() {
                     ErrorKind::Offline(_) => continue,
-                    ErrorKind::RequestError(err) => {
+                    ErrorKind::ReqwestError(err) => {
                         if err.status() == Some(StatusCode::NOT_FOUND)
+                            || err.status() == Some(StatusCode::UNAUTHORIZED)
                             || err.status() == Some(StatusCode::FORBIDDEN)
                         {
                             continue;
                         }
-                        Err(ErrorKind::RequestError(err).into())
+                        return Err(ErrorKind::from(err).into());
                     }
-                    other => Err(other.into()),
+                    other => return Err(other.into()),
                 },
-                Err(CachedClientError::Callback(err)) => Err(err),
+                Err(CachedClientError::Callback(err)) => return Err(err),
             };
         }
 
-        match self.connectivity {
-            Connectivity::Online => {
-                Err(ErrorKind::PackageNotFound(package_name.to_string()).into())
-            }
-            Connectivity::Offline => Err(ErrorKind::Offline(package_name.to_string()).into()),
+        if results.is_empty() {
+            return match self.connectivity {
+                Connectivity::Online => {
+                    Err(ErrorKind::PackageNotFound(package_name.to_string()).into())
+                }
+                Connectivity::Offline => Err(ErrorKind::Offline(package_name.to_string()).into()),
+            };
         }
+
+        Ok(results)
     }
 
     async fn simple_single_index(
         &self,
         package_name: &PackageName,
         index: &IndexUrl,
     ) -> Result<Result<OwnedArchive<SimpleMetadata>, CachedClientError<Error>>, Error> {
@@ -219,41 +280,41 @@
             .push("");
 
         trace!("Fetching metadata for {package_name} from {url}");
 
         let cache_entry = self.cache.entry(
             CacheBucket::Simple,
             Path::new(&match index {
-                IndexUrl::Pypi => "pypi".to_string(),
+                IndexUrl::Pypi(_) => "pypi".to_string(),
                 IndexUrl::Url(url) => cache_key::digest(&cache_key::CanonicalUrl::new(url)),
+                IndexUrl::Path(url) => cache_key::digest(&cache_key::CanonicalUrl::new(url)),
             }),
             format!("{package_name}.rkyv"),
         );
         let cache_control = match self.connectivity {
             Connectivity::Online => CacheControl::from(
                 self.cache
                     .freshness(&cache_entry, Some(package_name))
                     .map_err(ErrorKind::Io)?,
             ),
             Connectivity::Offline => CacheControl::AllowStale,
         };
 
         let simple_request = self
-            .client
-            .uncached()
+            .uncached_client()
             .get(url.clone())
             .header("Accept-Encoding", "gzip")
             .header("Accept", MediaType::accepts())
             .build()
-            .map_err(ErrorKind::RequestError)?;
+            .map_err(ErrorKind::from)?;
         let parse_simple_response = |response: Response| {
             async {
                 // Use the response URL, rather than the request URL, as the base for relative URLs.
                 // This ensures that we handle redirects and other URL transformations correctly.
-                let url = safe_copy_url_auth(&url, response.url().clone());
+                let url = response.url().clone();
 
                 let content_type = response
                     .headers()
                     .get("content-type")
                     .ok_or_else(|| Error::from(ErrorKind::MissingContentType(url.clone())))?;
                 let content_type = content_type.to_str().map_err(|err| {
                     Error::from(ErrorKind::InvalidContentTypeHeader(url.clone(), err))
@@ -264,36 +325,35 @@
                         url.clone(),
                         media_type.to_string(),
                     ))
                 })?;
 
                 let unarchived = match media_type {
                     MediaType::Json => {
-                        let bytes = response.bytes().await.map_err(ErrorKind::RequestError)?;
+                        let bytes = response.bytes().await.map_err(ErrorKind::from)?;
                         let data: SimpleJson = serde_json::from_slice(bytes.as_ref())
                             .map_err(|err| Error::from_json_err(err, url.clone()))?;
 
                         SimpleMetadata::from_files(data.files, package_name, &url)
                     }
                     MediaType::Html => {
-                        let text = response.text().await.map_err(ErrorKind::RequestError)?;
+                        let text = response.text().await.map_err(ErrorKind::from)?;
                         let SimpleHtml { base, files } = SimpleHtml::parse(&text, &url)
                             .map_err(|err| Error::from_html_err(err, url.clone()))?;
-                        let base = safe_copy_url_auth(&url, base.into_url());
 
-                        SimpleMetadata::from_files(files, package_name, &base)
+                        SimpleMetadata::from_files(files, package_name, base.as_url())
                     }
                 };
                 OwnedArchive::from_unarchived(&unarchived)
             }
-            .boxed()
+            .boxed_local()
             .instrument(info_span!("parse_simple_api", package = %package_name))
         };
         let result = self
-            .client
+            .cached_client()
             .get_cacheable(
                 simple_request,
                 &cache_entry,
                 cache_control,
                 parse_simple_response,
             )
             .await;
@@ -303,37 +363,40 @@
     /// Fetch the metadata for a remote wheel file.
     ///
     /// For a remote wheel, we try the following ways to fetch the metadata:
     /// 1. From a [PEP 658](https://peps.python.org/pep-0658/) data-dist-info-metadata url
     /// 2. From a remote wheel by partial zip reading
     /// 3. From a (temp) download of a remote wheel (this is a fallback, the webserver should support range requests)
     #[instrument(skip_all, fields(% built_dist))]
-    pub async fn wheel_metadata(&self, built_dist: &BuiltDist) -> Result<Metadata21, Error> {
+    pub async fn wheel_metadata(&self, built_dist: &BuiltDist) -> Result<Metadata23, Error> {
         let metadata = match &built_dist {
-            BuiltDist::Registry(wheel) => match &wheel.file.url {
-                FileLocation::RelativeUrl(base, url) => {
-                    let url = pypi_types::base_url_join_relative(base, url)
-                        .map_err(ErrorKind::JoinRelativeError)?;
-                    self.wheel_metadata_registry(&wheel.index, &wheel.file, &url)
-                        .await?
-                }
-                FileLocation::AbsoluteUrl(url) => {
-                    let url = Url::parse(url).map_err(ErrorKind::UrlParseError)?;
-                    self.wheel_metadata_registry(&wheel.index, &wheel.file, &url)
-                        .await?
-                }
-                FileLocation::Path(path) => {
-                    let file = fs_err::tokio::File::open(&path)
-                        .await
-                        .map_err(ErrorKind::Io)?;
-                    let reader = tokio::io::BufReader::new(file);
-                    read_metadata_async_seek(&wheel.filename, built_dist.to_string(), reader)
-                        .await?
+            BuiltDist::Registry(wheels) => {
+                let wheel = wheels.best_wheel();
+                match &wheel.file.url {
+                    FileLocation::RelativeUrl(base, url) => {
+                        let url = pypi_types::base_url_join_relative(base, url)
+                            .map_err(ErrorKind::JoinRelativeError)?;
+                        self.wheel_metadata_registry(&wheel.index, &wheel.file, &url)
+                            .await?
+                    }
+                    FileLocation::AbsoluteUrl(url) => {
+                        let url = Url::parse(url).map_err(ErrorKind::UrlParseError)?;
+                        self.wheel_metadata_registry(&wheel.index, &wheel.file, &url)
+                            .await?
+                    }
+                    FileLocation::Path(path) => {
+                        let file = fs_err::tokio::File::open(&path)
+                            .await
+                            .map_err(ErrorKind::Io)?;
+                        let reader = tokio::io::BufReader::new(file);
+                        read_metadata_async_seek(&wheel.filename, built_dist.to_string(), reader)
+                            .await?
+                    }
                 }
-            },
+            }
             BuiltDist::DirectUrl(wheel) => {
                 self.wheel_metadata_no_pep658(
                     &wheel.filename,
                     &wheel.url,
                     WheelCache::Url(&wheel.url),
                 )
                 .await?
@@ -359,59 +422,55 @@
 
     /// Fetch the metadata from a wheel file.
     async fn wheel_metadata_registry(
         &self,
         index: &IndexUrl,
         file: &File,
         url: &Url,
-    ) -> Result<Metadata21, Error> {
+    ) -> Result<Metadata23, Error> {
         // If the metadata file is available at its own url (PEP 658), download it from there.
         let filename = WheelFilename::from_str(&file.filename).map_err(ErrorKind::WheelFilename)?;
-        if file
-            .dist_info_metadata
-            .as_ref()
-            .is_some_and(pypi_types::DistInfoMetadata::is_available)
-        {
-            let url = Url::parse(&format!("{}.metadata", url)).map_err(ErrorKind::UrlParseError)?;
+        if file.dist_info_metadata {
+            let mut url = url.clone();
+            url.set_path(&format!("{}.metadata", url.path()));
 
             let cache_entry = self.cache.entry(
                 CacheBucket::Wheels,
-                WheelCache::Index(index).remote_wheel_dir(filename.name.as_ref()),
+                WheelCache::Index(index).wheel_dir(filename.name.as_ref()),
                 format!("{}.msgpack", filename.stem()),
             );
             let cache_control = match self.connectivity {
                 Connectivity::Online => CacheControl::from(
                     self.cache
                         .freshness(&cache_entry, Some(&filename.name))
                         .map_err(ErrorKind::Io)?,
                 ),
                 Connectivity::Offline => CacheControl::AllowStale,
             };
 
             let response_callback = |response: Response| async {
-                let bytes = response.bytes().await.map_err(ErrorKind::RequestError)?;
+                let bytes = response.bytes().await.map_err(ErrorKind::from)?;
 
                 info_span!("parse_metadata21")
-                    .in_scope(|| Metadata21::parse(bytes.as_ref()))
+                    .in_scope(|| Metadata23::parse_metadata(bytes.as_ref()))
                     .map_err(|err| {
                         Error::from(ErrorKind::MetadataParseError(
                             filename,
                             url.to_string(),
                             Box::new(err),
                         ))
                     })
             };
             let req = self
-                .client
-                .uncached()
+                .uncached_client()
                 .get(url.clone())
                 .build()
-                .map_err(ErrorKind::RequestError)?;
+                .map_err(ErrorKind::from)?;
             Ok(self
-                .client
+                .cached_client()
                 .get_serde(req, &cache_entry, cache_control, response_callback)
                 .await?)
         } else {
             // If we lack PEP 658 support, try using HTTP range requests to read only the
             // `.dist-info/METADATA` file from the zip, and if that also fails, download the whole wheel
             // into the cache and read from there
             self.wheel_metadata_no_pep658(&filename, url, WheelCache::Index(index))
@@ -421,151 +480,189 @@
 
     /// Get the wheel metadata if it isn't available in an index through PEP 658
     async fn wheel_metadata_no_pep658<'data>(
         &self,
         filename: &'data WheelFilename,
         url: &'data Url,
         cache_shard: WheelCache<'data>,
-    ) -> Result<Metadata21, Error> {
+    ) -> Result<Metadata23, Error> {
         let cache_entry = self.cache.entry(
             CacheBucket::Wheels,
-            cache_shard.remote_wheel_dir(filename.name.as_ref()),
+            cache_shard.wheel_dir(filename.name.as_ref()),
             format!("{}.msgpack", filename.stem()),
         );
         let cache_control = match self.connectivity {
             Connectivity::Online => CacheControl::from(
                 self.cache
                     .freshness(&cache_entry, Some(&filename.name))
                     .map_err(ErrorKind::Io)?,
             ),
             Connectivity::Offline => CacheControl::AllowStale,
         };
 
+        let req = self
+            .uncached_client()
+            .head(url.clone())
+            .header(
+                "accept-encoding",
+                http::HeaderValue::from_static("identity"),
+            )
+            .build()
+            .map_err(ErrorKind::from)?;
+
+        // Copy authorization headers from the HEAD request to subsequent requests
+        let mut headers = HeaderMap::default();
+        if let Some(authorization) = req.headers().get("authorization") {
+            headers.append("authorization", authorization.clone());
+        }
+
         // This response callback is special, we actually make a number of subsequent requests to
         // fetch the file from the remote zip.
-        let client = self.client_raw.clone();
         let read_metadata_range_request = |response: Response| {
             async {
-                let mut reader = AsyncHttpRangeReader::from_head_response(client, response)
-                    .await
-                    .map_err(ErrorKind::AsyncHttpRangeReader)?;
+                let mut reader = AsyncHttpRangeReader::from_head_response(
+                    self.uncached_client().client(),
+                    response,
+                    url.clone(),
+                    headers,
+                )
+                .await
+                .map_err(ErrorKind::AsyncHttpRangeReader)?;
                 trace!("Getting metadata for {filename} by range request");
                 let text = wheel_metadata_from_remote_zip(filename, &mut reader).await?;
-                let metadata = Metadata21::parse(text.as_bytes()).map_err(|err| {
+                let metadata = Metadata23::parse_metadata(text.as_bytes()).map_err(|err| {
                     Error::from(ErrorKind::MetadataParseError(
                         filename.clone(),
                         url.to_string(),
                         Box::new(err),
                     ))
                 })?;
-                Ok::<Metadata21, CachedClientError<Error>>(metadata)
+                Ok::<Metadata23, CachedClientError<Error>>(metadata)
             }
-            .boxed()
+            .boxed_local()
             .instrument(info_span!("read_metadata_range_request", wheel = %filename))
         };
 
-        let req = self
-            .client
-            .uncached()
-            .head(url.clone())
-            .build()
-            .map_err(ErrorKind::RequestError)?;
         let result = self
-            .client
+            .cached_client()
             .get_serde(
                 req,
                 &cache_entry,
                 cache_control,
                 read_metadata_range_request,
             )
             .await
             .map_err(crate::Error::from);
 
         match result {
             Ok(metadata) => return Ok(metadata),
             Err(err) => {
-                if err.kind().is_http_range_requests_unsupported() {
-                    // The range request version failed. Fall back to downloading the entire file
-                    // and the reading the file from the zip the regular way.
-                    warn!("Range requests not supported for {filename}; downloading wheel");
+                if err.is_http_range_requests_unsupported() {
+                    // The range request version failed. Fall back to streaming the file to search
+                    // for the METADATA file.
+                    warn!("Range requests not supported for {filename}; streaming wheel");
                 } else {
                     return Err(err);
                 }
             }
-        }
+        };
+
+        // Create a request to stream the file.
+        let req = self
+            .uncached_client()
+            .get(url.clone())
+            .header(
+                // `reqwest` defaults to accepting compressed responses.
+                // Specify identity encoding to get consistent .whl downloading
+                // behavior from servers. ref: https://github.com/pypa/pip/pull/1688
+                "accept-encoding",
+                reqwest::header::HeaderValue::from_static("identity"),
+            )
+            .build()
+            .map_err(ErrorKind::from)?;
 
         // Stream the file, searching for the METADATA.
-        let reader = self.stream_external(url).await?;
-        read_metadata_async_stream(filename, url.to_string(), reader).await
+        let read_metadata_stream = |response: Response| {
+            async {
+                let reader = response
+                    .bytes_stream()
+                    .map_err(|err| self.handle_response_errors(err))
+                    .into_async_read();
+
+                read_metadata_async_stream(filename, url.to_string(), reader).await
+            }
+            .instrument(info_span!("read_metadata_stream", wheel = %filename))
+        };
+
+        self.cached_client()
+            .get_serde(req, &cache_entry, cache_control, read_metadata_stream)
+            .await
+            .map_err(crate::Error::from)
     }
 
-    /// Stream a file from an external URL.
-    pub async fn stream_external(
-        &self,
-        url: &Url,
-    ) -> Result<Box<dyn futures::AsyncRead + Unpin + Send + Sync>, Error> {
-        Ok(Box::new(
-            self.client
-                .uncached()
-                .get(url.to_string())
-                .send()
-                .await
-                .map_err(ErrorKind::RequestMiddlewareError)?
-                .error_for_status()
-                .map_err(ErrorKind::RequestError)?
-                .bytes_stream()
-                .map_err(|err| std::io::Error::new(std::io::ErrorKind::Other, err))
-                .into_async_read(),
-        ))
+    /// Handle a specific `reqwest` error, and convert it to [`io::Error`].
+    fn handle_response_errors(&self, err: reqwest::Error) -> std::io::Error {
+        if err.is_timeout() {
+            std::io::Error::new(
+                std::io::ErrorKind::TimedOut,
+                format!(
+                    "Failed to download distribution due to network timeout. Try increasing UV_HTTP_TIMEOUT (current value: {}s).",  self.timeout()
+                ),
+            )
+        } else {
+            std::io::Error::new(std::io::ErrorKind::Other, err)
+        }
     }
 }
 
 /// Read a wheel's `METADATA` file from a zip file.
 async fn read_metadata_async_seek(
     filename: &WheelFilename,
     debug_source: String,
     reader: impl tokio::io::AsyncRead + tokio::io::AsyncSeek + Unpin,
-) -> Result<Metadata21, Error> {
-    let mut zip_reader = async_zip::tokio::read::seek::ZipFileReader::with_tokio(reader)
+) -> Result<Metadata23, Error> {
+    let reader = futures::io::BufReader::new(reader.compat());
+    let mut zip_reader = async_zip::base::read::seek::ZipFileReader::new(reader)
         .await
         .map_err(|err| ErrorKind::Zip(filename.clone(), err))?;
 
-    let (metadata_idx, _dist_info_prefix) = find_dist_info(
+    let (metadata_idx, _dist_info_prefix) = find_archive_dist_info(
         filename,
         zip_reader
             .file()
             .entries()
             .iter()
             .enumerate()
             .filter_map(|(index, entry)| Some((index, entry.filename().as_str().ok()?))),
     )
-    .map_err(ErrorKind::InstallWheel)?;
+    .map_err(ErrorKind::DistInfo)?;
 
     // Read the contents of the `METADATA` file.
     let mut contents = Vec::new();
     zip_reader
         .reader_with_entry(metadata_idx)
         .await
         .map_err(|err| ErrorKind::Zip(filename.clone(), err))?
         .read_to_end_checked(&mut contents)
         .await
         .map_err(|err| ErrorKind::Zip(filename.clone(), err))?;
 
-    let metadata = Metadata21::parse(&contents).map_err(|err| {
+    let metadata = Metadata23::parse_metadata(&contents).map_err(|err| {
         ErrorKind::MetadataParseError(filename.clone(), debug_source, Box::new(err))
     })?;
     Ok(metadata)
 }
 
 /// Like [`read_metadata_async_seek`], but doesn't use seek.
 async fn read_metadata_async_stream<R: futures::AsyncRead + Unpin>(
     filename: &WheelFilename,
     debug_source: String,
     reader: R,
-) -> Result<Metadata21, Error> {
+) -> Result<Metadata23, Error> {
+    let reader = futures::io::BufReader::with_capacity(128 * 1024, reader);
     let mut zip = async_zip::base::read::stream::ZipFileReader::new(reader);
 
     while let Some(mut entry) = zip
         .next_with_entry()
         .await
         .map_err(|err| ErrorKind::Zip(filename.clone(), err))?
     {
@@ -578,15 +675,15 @@
             .map_err(|err| ErrorKind::Zip(filename.clone(), err))?;
 
         if is_metadata_entry(path, filename) {
             let mut reader = entry.reader_mut().compat();
             let mut contents = Vec::new();
             reader.read_to_end(&mut contents).await.unwrap();
 
-            let metadata = Metadata21::parse(&contents).map_err(|err| {
+            let metadata = Metadata23::parse_metadata(&contents).map_err(|err| {
                 ErrorKind::MetadataParseError(filename.clone(), debug_source, Box::new(err))
             })?;
             return Ok(metadata);
         }
 
         // Close current file to get access to the next one. See docs:
         // https://docs.rs/async_zip/0.0.16/async_zip/base/read/stream/
@@ -698,15 +795,15 @@
                         let mut files = VersionFiles::default();
                         files.push(filename, file);
                         entry.insert(files);
                     }
                 }
             }
         }
-        SimpleMetadata(
+        Self(
             map.into_iter()
                 .map(|(version, files)| SimpleMetadatum { version, files })
                 .collect(),
         )
     }
 }
 
@@ -749,17 +846,18 @@
     #[inline]
     const fn accepts() -> &'static str {
         // See: https://peps.python.org/pep-0691/#version-format-selection
         "application/vnd.pypi.simple.v1+json, application/vnd.pypi.simple.v1+html;q=0.2, text/html;q=0.01"
     }
 }
 
-#[derive(Debug, Copy, Clone, Eq, PartialEq)]
+#[derive(Debug, Copy, Clone, Eq, PartialEq, Default)]
 pub enum Connectivity {
     /// Allow access to the network.
+    #[default]
     Online,
 
     /// Do not allow access to the network.
     Offline,
 }
 
 #[cfg(test)]
```

### Comparing `uv-0.1.9/crates/uv-client/src/remote_metadata.rs` & `uv-0.2.0/crates/uv-client/src/remote_metadata.rs`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 use async_http_range_reader::AsyncHttpRangeReader;
-use async_zip::tokio::read::seek::ZipFileReader;
+use futures::io::BufReader;
 use tokio_util::compat::TokioAsyncReadCompatExt;
 
 use distribution_filename::WheelFilename;
-use install_wheel_rs::find_dist_info;
+use install_wheel_rs::metadata::find_archive_dist_info;
 
 use crate::{Error, ErrorKind};
 
 /// Read the `.dist-info/METADATA` file from a async remote zip reader, so we avoid downloading the
 /// entire wheel just for the one file.
 ///
 /// This method is derived from `prefix-dev/rip`, which is available under the following BSD-3
@@ -57,28 +57,29 @@
     const CENTRAL_DIRECTORY_SIZE: u64 = 16384;
     // Because the zip index is at the back
     reader
         .prefetch(reader.len().saturating_sub(CENTRAL_DIRECTORY_SIZE)..reader.len())
         .await;
 
     // Construct a zip reader to uses the stream.
-    let mut reader = ZipFileReader::new(reader.compat())
+    let buf = BufReader::new(reader.compat());
+    let mut reader = async_zip::base::read::seek::ZipFileReader::new(buf)
         .await
         .map_err(|err| ErrorKind::Zip(filename.clone(), err))?;
 
-    let ((metadata_idx, metadata_entry), _dist_info_prefix) = find_dist_info(
+    let ((metadata_idx, metadata_entry), _dist_info_prefix) = find_archive_dist_info(
         filename,
         reader
             .file()
             .entries()
             .iter()
             .enumerate()
             .filter_map(|(idx, e)| Some(((idx, e), e.filename().as_str().ok()?))),
     )
-    .map_err(ErrorKind::InstallWheel)?;
+    .map_err(ErrorKind::DistInfo)?;
 
     let offset = metadata_entry.header_offset();
     let size = metadata_entry.compressed_size()
         + 30 // Header size in bytes
         + metadata_entry.filename().as_bytes().len() as u64;
 
     // The zip archive uses as BufReader which reads in chunks of 8192. To ensure we prefetch
@@ -86,14 +87,15 @@
     let buffer_size = 8192;
     let size = ((size + buffer_size - 1) / buffer_size) * buffer_size;
 
     // Fetch the bytes from the zip archive that contain the requested file.
     reader
         .inner_mut()
         .get_mut()
+        .get_mut()
         .prefetch(offset..offset + size)
         .await;
 
     // Read the contents of the METADATA file
     let mut contents = String::new();
     reader
         .reader_with_entry(metadata_idx)
```

### Comparing `uv-0.1.9/crates/uv-client/src/rkyvutil.rs` & `uv-0.2.0/crates/uv-client/src/rkyvutil.rs`

 * *Files 2% similar despite different names*

```diff
@@ -72,92 +72,92 @@
     /// Create a new owned archived value from the raw aligned bytes of the
     /// serialized representation of an `A`.
     ///
     /// # Errors
     ///
     /// If the bytes fail validation (e.g., contains unaligned pointers or
     /// strings aren't valid UTF-8), then this returns an error.
-    pub fn new(raw: rkyv::util::AlignedVec) -> Result<OwnedArchive<A>, Error> {
+    pub fn new(raw: rkyv::util::AlignedVec) -> Result<Self, Error> {
         // We convert the error to a simple string because... the error type
         // does not implement Send. And I don't think we really need to keep
         // the error type around anyway.
         let _ = rkyv::validation::validators::check_archived_root::<A>(&raw)
             .map_err(|e| ErrorKind::ArchiveRead(e.to_string()))?;
-        Ok(OwnedArchive {
+        Ok(Self {
             raw,
             archive: std::marker::PhantomData,
         })
     }
 
     /// Like `OwnedArchive::new`, but reads the value from the given reader.
     ///
     /// Note that this consumes the entirety of the given reader.
     ///
     /// # Errors
     ///
     /// If the bytes fail validation (e.g., contains unaligned pointers or
     /// strings aren't valid UTF-8), then this returns an error.
-    pub fn from_reader<R: std::io::Read>(mut rdr: R) -> Result<OwnedArchive<A>, Error> {
+    pub fn from_reader<R: std::io::Read>(mut rdr: R) -> Result<Self, Error> {
         let mut buf = rkyv::util::AlignedVec::with_capacity(1024);
         buf.extend_from_reader(&mut rdr).map_err(ErrorKind::Io)?;
-        OwnedArchive::new(buf)
+        Self::new(buf)
     }
 
     /// Creates an owned archive value from the unarchived value.
     ///
     /// # Errors
     ///
     /// This can fail if creating an archive for the given type fails.
     /// Currently, this, at minimum, includes cases where an `A` contains a
     /// `PathBuf` that is not valid UTF-8.
-    pub fn from_unarchived(unarchived: &A) -> Result<OwnedArchive<A>, Error> {
+    pub fn from_unarchived(unarchived: &A) -> Result<Self, Error> {
         use rkyv::ser::Serializer;
 
         let mut serializer = crate::rkyvutil::Serializer::<4096>::default();
         serializer
             .serialize_value(unarchived)
             .map_err(ErrorKind::ArchiveWrite)?;
         let raw = serializer.into_serializer().into_inner();
-        Ok(OwnedArchive {
+        Ok(Self {
             raw,
             archive: std::marker::PhantomData,
         })
     }
 
     /// Write the underlying bytes of this archived value to the given writer.
     ///
     /// Note that because this type has a `Deref` impl, this method requires
     /// fully-qualified syntax. So, if `o` is an `OwnedValue`, then use
     /// `OwnedValue::write(&o, wtr)`.
     ///
     /// # Errors
     ///
     /// Any failures from writing are returned to the caller.
-    pub fn write<W: std::io::Write>(this: &OwnedArchive<A>, mut wtr: W) -> Result<(), Error> {
+    pub fn write<W: std::io::Write>(this: &Self, mut wtr: W) -> Result<(), Error> {
         Ok(wtr.write_all(&this.raw).map_err(ErrorKind::Io)?)
     }
 
     /// Returns the raw underlying bytes of this owned archive value.
     ///
     /// They are guaranteed to be a valid serialization of `Archived<A>`.
     ///
     /// Note that because this type has a `Deref` impl, this method requires
     /// fully-qualified syntax. So, if `o` is an `OwnedValue`, then use
     /// `OwnedValue::as_bytes(&o)`.
-    pub fn as_bytes(this: &OwnedArchive<A>) -> &[u8] {
+    pub fn as_bytes(this: &Self) -> &[u8] {
         &this.raw
     }
 
     /// Deserialize this owned archived value into the original
     /// `SimpleMetadata`.
     ///
     /// Note that because this type has a `Deref` impl, this method requires
     /// fully-qualified syntax. So, if `o` is an `OwnedValue`, then use
     /// `OwnedValue::deserialize(&o)`.
-    pub fn deserialize(this: &OwnedArchive<A>) -> A {
+    pub fn deserialize(this: &Self) -> A {
         (**this)
             .deserialize(&mut SharedDeserializeMap::new())
             .expect("valid archive must deserialize correctly")
     }
 }
 
 impl<A> std::ops::Deref for OwnedArchive<A>
@@ -295,25 +295,25 @@
     Composite(CompositeSerializerError<Infallible, AllocScratchError, SharedSerializeMapError>),
     AsString(rkyv::with::AsStringError),
 }
 
 impl std::fmt::Display for SerializerError {
     fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
         match *self {
-            SerializerError::Composite(ref e) => e.fmt(f),
-            SerializerError::AsString(ref e) => e.fmt(f),
+            Self::Composite(ref e) => e.fmt(f),
+            Self::AsString(ref e) => e.fmt(f),
         }
     }
 }
 
 impl std::error::Error for SerializerError {
     fn source(&self) -> Option<&(dyn std::error::Error + 'static)> {
         match *self {
-            SerializerError::Composite(ref e) => Some(e),
-            SerializerError::AsString(ref e) => Some(e),
+            Self::Composite(ref e) => Some(e),
+            Self::AsString(ref e) => Some(e),
         }
     }
 }
 
 /// Provides a way to build a serializer error if converting an
 /// `OsString`/`PathBuf` to a `String` fails. i.e., It's invalid UTF-8.
 ///
@@ -331,11 +331,11 @@
 /// If we didn't need to use `rkyv::with::AsString` (which we do for
 /// serializing `PathBuf` at time of writing), then we could just
 /// use an `AllocSerializer` directly (which is a type alias for
 /// `CompositeSerializer<...>`.
 ///
 /// [AsString]: https://docs.rs/rkyv/0.7.43/rkyv/with/struct.AsString.html
 impl From<rkyv::with::AsStringError> for SerializerError {
-    fn from(e: rkyv::with::AsStringError) -> SerializerError {
-        SerializerError::AsString(e)
+    fn from(e: rkyv::with::AsStringError) -> Self {
+        Self::AsString(e)
     }
 }
```

### Comparing `uv-0.1.9/crates/uv-client/tests/remote_metadata.rs` & `uv-0.2.0/crates/uv-client/tests/remote_metadata.rs`

 * *Files 12% similar despite different names*

```diff
@@ -1,29 +1,31 @@
 use std::str::FromStr;
 
 use anyhow::Result;
+use url::Url;
 
 use distribution_filename::WheelFilename;
 use distribution_types::{BuiltDist, DirectUrlBuiltDist};
 use pep508_rs::VerbatimUrl;
 use uv_cache::Cache;
 use uv_client::RegistryClientBuilder;
 
 #[tokio::test]
 async fn remote_metadata_with_and_without_cache() -> Result<()> {
-    let cache = Cache::temp()?;
+    let cache = Cache::temp()?.init()?;
     let client = RegistryClientBuilder::new(cache).build();
 
     // The first run is without cache (the tempdir is empty), the second has the cache from the
     // first run.
     for _ in 0..2 {
         let url = "https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl";
         let filename = WheelFilename::from_str(url.rsplit_once('/').unwrap().1)?;
         let dist = BuiltDist::DirectUrl(DirectUrlBuiltDist {
             filename,
+            location: Url::parse(url).unwrap(),
             url: VerbatimUrl::from_str(url).unwrap(),
         });
         let metadata = client.wheel_metadata(&dist).await.unwrap();
         assert_eq!(metadata.version.to_string(), "4.66.1");
     }
 
     Ok(())
```

### Comparing `uv-0.1.9/crates/uv-distribution/Cargo.toml` & `uv-0.2.0/crates/requirements-txt/Cargo.toml`

 * *Files 24% similar despite different names*

```diff
@@ -1,46 +1,44 @@
 [package]
-name = "uv-distribution"
+name = "requirements-txt"
 version = "0.0.1"
 edition = { workspace = true }
 rust-version = { workspace = true }
 homepage = { workspace = true }
 documentation = { workspace = true }
 repository = { workspace = true }
 authors = { workspace = true }
 license = { workspace = true }
 
 [lints]
 workspace = true
 
 [dependencies]
-cache-key = { path = "../cache-key" }
-distribution-filename = { path = "../distribution-filename", features = ["serde"] }
-distribution-types = { path = "../distribution-types" }
-install-wheel-rs = { path = "../install-wheel-rs" }
-pep440_rs = { path = "../pep440-rs" }
-pep508_rs = { path =     "../pep508-rs" }
-platform-tags = { path = "../platform-tags" }
-uv-cache = { path = "../uv-cache" }
-uv-client = { path = "../uv-client" }
-uv-extract = { path = "../uv-extract" }
-uv-fs = { path = "../uv-fs", features = ["tokio"] }
-uv-git = { path = "../uv-git", features = ["vendored-openssl"] }
-uv-normalize = { path = "../uv-normalize" }
-uv-traits = { path = "../uv-traits" }
-pypi-types = { path = "../pypi-types" }
+distribution-types = { workspace = true }
+pep508_rs = { workspace = true }
+uv-client = { workspace = true }
+uv-fs = { workspace = true }
+uv-normalize = { workspace = true }
+uv-configuration = { workspace = true }
+uv-warnings = { workspace = true }
 
-anyhow = { workspace = true }
 fs-err = { workspace = true }
-futures = { workspace = true }
-nanoid = { workspace = true }
-reqwest = { workspace = true }
-rmp-serde = { workspace = true }
-rustc-hash = { workspace = true }
-serde = { workspace = true , features = ["derive"] }
-tempfile = { workspace = true }
+regex = { workspace = true }
+reqwest = { workspace = true, optional = true }
+reqwest-middleware = { workspace = true, optional = true }
 thiserror = { workspace = true }
-tokio = { workspace = true }
-tokio-util = { workspace = true, features = ["compat"] }
 tracing = { workspace = true }
+unscanny = { workspace = true }
 url = { workspace = true }
-zip = { workspace = true }
+
+[features]
+http = ["reqwest", "reqwest-middleware"]
+
+[dev-dependencies]
+anyhow = { version = "1.0.80" }
+assert_fs = { version = "1.1.1" }
+indoc = { version = "2.0.4" }
+insta = { version = "1.36.1", features = ["filters"] }
+itertools = { version = "0.13.0" }
+tempfile = { version = "3.9.0" }
+test-case = { version = "3.3.1" }
+tokio = { version = "1.35.1" }
```

### Comparing `uv-0.1.9/crates/uv-distribution/src/locks.rs` & `uv-0.2.0/crates/uv-distribution/src/locks.rs`

 * *Files 4% similar despite different names*

```diff
@@ -1,20 +1,20 @@
-use std::sync::Arc;
+use std::rc::Rc;
 
 use rustc_hash::FxHashMap;
 use tokio::sync::Mutex;
 
 use distribution_types::{Identifier, ResourceId};
 
 /// A set of locks used to prevent concurrent access to the same resource.
 #[derive(Debug, Default)]
-pub(crate) struct Locks(Mutex<FxHashMap<ResourceId, Arc<Mutex<()>>>>);
+pub(crate) struct Locks(Mutex<FxHashMap<ResourceId, Rc<Mutex<()>>>>);
 
 impl Locks {
     /// Acquire a lock on the given resource.
-    pub(crate) async fn acquire(&self, dist: &impl Identifier) -> Arc<Mutex<()>> {
+    pub(crate) async fn acquire(&self, dist: &impl Identifier) -> Rc<Mutex<()>> {
         let mut map = self.0.lock().await;
         map.entry(dist.resource_id())
-            .or_insert_with(|| Arc::new(Mutex::new(())))
+            .or_insert_with(|| Rc::new(Mutex::new(())))
             .clone()
     }
 }
```

### Comparing `uv-0.1.9/crates/uv-distribution/src/reporter.rs` & `uv-0.2.0/crates/uv-distribution/src/reporter.rs`

 * *Files 8% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 use std::sync::Arc;
 
 use url::Url;
 
-use distribution_types::SourceDist;
+use distribution_types::BuildableSource;
 
 pub trait Reporter: Send + Sync {
     /// Callback to invoke when a source distribution build is kicked off.
-    fn on_build_start(&self, dist: &SourceDist) -> usize;
+    fn on_build_start(&self, source: &BuildableSource) -> usize;
 
     /// Callback to invoke when a source distribution build is complete.
-    fn on_build_complete(&self, dist: &SourceDist, id: usize);
+    fn on_build_complete(&self, source: &BuildableSource, id: usize);
 
     /// Callback to invoke when a repository checkout begins.
     fn on_checkout_start(&self, url: &Url, rev: &str) -> usize;
 
     /// Callback to invoke when a repository checkout completes.
     fn on_checkout_complete(&self, url: &Url, rev: &str, index: usize);
 }
```

### Comparing `uv-0.1.9/crates/uv-distribution/src/source/built_wheel_metadata.rs` & `uv-0.2.0/crates/uv-distribution/src/source/built_wheel_metadata.rs`

 * *Files 16% similar despite different names*

```diff
@@ -1,28 +1,32 @@
 use std::path::PathBuf;
 use std::str::FromStr;
 
 use distribution_filename::WheelFilename;
+use distribution_types::Hashed;
 use platform_tags::Tags;
+use pypi_types::HashDigest;
 use uv_cache::CacheShard;
 use uv_fs::files;
 
 /// The information about the wheel we either just built or got from the cache.
 #[derive(Debug, Clone)]
-pub struct BuiltWheelMetadata {
+pub(crate) struct BuiltWheelMetadata {
     /// The path to the built wheel.
     pub(crate) path: PathBuf,
     /// The expected path to the downloaded wheel's entry in the cache.
     pub(crate) target: PathBuf,
     /// The parsed filename.
     pub(crate) filename: WheelFilename,
+    /// The computed hashes of the source distribution from which the wheel was built.
+    pub(crate) hashes: Vec<HashDigest>,
 }
 
 impl BuiltWheelMetadata {
-    /// Find a compatible wheel in the cache based on the given manifest.
+    /// Find a compatible wheel in the cache.
     pub(crate) fn find_in_cache(tags: &Tags, cache_shard: &CacheShard) -> Option<Self> {
         for directory in files(cache_shard) {
             if let Some(metadata) = Self::from_path(directory, cache_shard) {
                 // Validate that the wheel is compatible with the target platform.
                 if metadata.filename.is_compatible(tags) {
                     return Some(metadata);
                 }
@@ -35,10 +39,24 @@
     fn from_path(path: PathBuf, cache_shard: &CacheShard) -> Option<Self> {
         let filename = path.file_name()?.to_str()?;
         let filename = WheelFilename::from_str(filename).ok()?;
         Some(Self {
             target: cache_shard.join(filename.stem()),
             path,
             filename,
+            hashes: vec![],
         })
     }
+
+    /// Set the computed hashes of the wheel.
+    #[must_use]
+    pub(crate) fn with_hashes(mut self, hashes: Vec<HashDigest>) -> Self {
+        self.hashes = hashes;
+        self
+    }
+}
+
+impl Hashed for BuiltWheelMetadata {
+    fn hashes(&self) -> &[HashDigest] {
+        &self.hashes
+    }
 }
```

### Comparing `uv-0.1.9/crates/uv-distribution/src/source/mod.rs` & `uv-0.2.0/crates/uv-distribution/src/source/mod.rs`

 * *Files 26% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 //! Fetch and build source distributions from remote sources.
 
+use std::borrow::Cow;
 use std::path::{Path, PathBuf};
 use std::str::FromStr;
 use std::sync::Arc;
 
 use anyhow::Result;
 use fs_err::tokio as fs;
 use futures::{FutureExt, TryStreamExt};
@@ -11,948 +12,1427 @@
 use tokio_util::compat::FuturesAsyncReadCompatExt;
 use tracing::{debug, info_span, instrument, Instrument};
 use url::Url;
 use zip::ZipArchive;
 
 use distribution_filename::WheelFilename;
 use distribution_types::{
-    DirectArchiveUrl, DirectGitUrl, Dist, FileLocation, GitSourceDist, LocalEditable, Name,
-    PathSourceDist, RemoteSource, SourceDist,
+    BuildableSource, DirectorySourceDist, DirectorySourceUrl, Dist, FileLocation, GitSourceUrl,
+    HashPolicy, Hashed, LocalEditable, ParsedArchiveUrl, PathSourceUrl, RemoteSource, SourceDist,
+    SourceUrl,
 };
-use install_wheel_rs::read_dist_info;
-use pep508_rs::VerbatimUrl;
+use install_wheel_rs::metadata::read_archive_metadata;
 use platform_tags::Tags;
-use pypi_types::Metadata21;
+use pypi_types::{HashDigest, Metadata23};
 use uv_cache::{
-    ArchiveTimestamp, CacheBucket, CacheEntry, CacheShard, CachedByTimestamp, Freshness, WheelCache,
+    ArchiveTimestamp, CacheBucket, CacheEntry, CacheShard, CachedByTimestamp, Freshness, Timestamp,
+    WheelCache,
 };
 use uv_client::{
     CacheControl, CachedClientError, Connectivity, DataWithCachePolicy, RegistryClient,
 };
+use uv_configuration::{BuildKind, NoBuild};
+use uv_extract::hash::Hasher;
 use uv_fs::{write_atomic, LockedFile};
-use uv_git::{Fetch, GitSource};
-use uv_traits::{BuildContext, BuildKind, NoBuild, SourceBuildTrait};
+use uv_types::{BuildContext, SourceBuildTrait};
 
+use crate::distribution_database::ManagedClient;
 use crate::error::Error;
-use crate::reporter::Facade;
+use crate::git::{fetch_git_archive, resolve_precise};
 use crate::source::built_wheel_metadata::BuiltWheelMetadata;
-use crate::source::manifest::Manifest;
-use crate::Reporter;
+use crate::source::revision::Revision;
+use crate::{ArchiveMetadata, Reporter};
 
 mod built_wheel_metadata;
-mod manifest;
+mod revision;
 
 /// Fetch and build a source distribution from a remote source, or from a local cache.
-pub struct SourceDistCachedBuilder<'a, T: BuildContext> {
+pub struct SourceDistributionBuilder<'a, T: BuildContext> {
     build_context: &'a T,
-    client: &'a RegistryClient,
     reporter: Option<Arc<dyn Reporter>>,
-    tags: &'a Tags,
 }
 
-/// The name of the file that contains the cached manifest, encoded via `MsgPack`.
-pub(crate) const MANIFEST: &str = "manifest.msgpack";
+/// The name of the file that contains the revision ID for a remote distribution, encoded via `MsgPack`.
+pub(crate) const HTTP_REVISION: &str = "revision.http";
+
+/// The name of the file that contains the revision ID for a local distribution, encoded via `MsgPack`.
+pub(crate) const LOCAL_REVISION: &str = "revision.rev";
 
 /// The name of the file that contains the cached distribution metadata, encoded via `MsgPack`.
 pub(crate) const METADATA: &str = "metadata.msgpack";
 
-impl<'a, T: BuildContext> SourceDistCachedBuilder<'a, T> {
-    /// Initialize a [`SourceDistCachedBuilder`] from a [`BuildContext`].
-    pub fn new(build_context: &'a T, client: &'a RegistryClient, tags: &'a Tags) -> Self {
+impl<'a, T: BuildContext> SourceDistributionBuilder<'a, T> {
+    /// Initialize a [`SourceDistributionBuilder`] from a [`BuildContext`].
+    pub fn new(build_context: &'a T) -> Self {
         Self {
             build_context,
             reporter: None,
-            client,
-            tags,
         }
     }
 
     /// Set the [`Reporter`] to use for this source distribution fetcher.
     #[must_use]
     pub fn with_reporter(self, reporter: Arc<dyn Reporter>) -> Self {
         Self {
             reporter: Some(reporter),
             ..self
         }
     }
 
     /// Download and build a [`SourceDist`].
-    pub async fn download_and_build(
+    pub(super) async fn download_and_build(
         &self,
-        source_dist: &SourceDist,
+        source: &BuildableSource<'_>,
+        tags: &Tags,
+        hashes: HashPolicy<'_>,
+        client: &ManagedClient<'_>,
     ) -> Result<BuiltWheelMetadata, Error> {
-        let built_wheel_metadata = match &source_dist {
-            SourceDist::DirectUrl(direct_url_source_dist) => {
-                let filename = direct_url_source_dist
-                    .filename()
-                    .expect("Distribution must have a filename");
-                let DirectArchiveUrl { url, subdirectory } =
-                    DirectArchiveUrl::from(direct_url_source_dist.url.raw());
-
-                // For direct URLs, cache directly under the hash of the URL itself.
+        let built_wheel_metadata = match &source {
+            BuildableSource::Dist(SourceDist::Registry(dist)) => {
+                // For registry source distributions, shard by package, then version, for
+                // convenience in debugging.
                 let cache_shard = self.build_context.cache().shard(
                     CacheBucket::BuiltWheels,
-                    WheelCache::Url(&url).remote_wheel_dir(direct_url_source_dist.name().as_ref()),
+                    WheelCache::Index(&dist.index)
+                        .wheel_dir(dist.name.as_ref())
+                        .join(dist.version.to_string()),
                 );
 
-                self.url(
-                    source_dist,
-                    &filename,
-                    &url,
-                    &cache_shard,
-                    subdirectory.as_deref(),
-                )
-                .boxed()
-                .await?
-            }
-            SourceDist::Registry(registry_source_dist) => {
-                let url = match &registry_source_dist.file.url {
+                let url = match &dist.file.url {
                     FileLocation::RelativeUrl(base, url) => {
                         pypi_types::base_url_join_relative(base, url)?
                     }
                     FileLocation::AbsoluteUrl(url) => {
                         Url::parse(url).map_err(|err| Error::Url(url.clone(), err))?
                     }
                     FileLocation::Path(path) => {
-                        let path_source_dist = PathSourceDist {
-                            name: registry_source_dist.filename.name.clone(),
-                            url: VerbatimUrl::unknown(
-                                Url::from_file_path(path).expect("path is absolute"),
-                            ),
-                            path: path.clone(),
-                            editable: false,
-                        };
-                        return self.path(source_dist, &path_source_dist).boxed().await;
+                        let url = Url::from_file_path(path)
+                            .map_err(|()| Error::RelativePath(path.clone()))?;
+                        return self
+                            .archive(
+                                source,
+                                &PathSourceUrl {
+                                    url: &url,
+                                    path: Cow::Borrowed(path),
+                                },
+                                &cache_shard,
+                                tags,
+                                hashes,
+                            )
+                            .boxed_local()
+                            .await;
                     }
                 };
 
-                // For registry source distributions, shard by package, then version.
-                let cache_shard = self.build_context.cache().shard(
-                    CacheBucket::BuiltWheels,
-                    WheelCache::Index(&registry_source_dist.index)
-                        .remote_wheel_dir(registry_source_dist.filename.name.as_ref())
-                        .join(registry_source_dist.filename.version.to_string()),
-                );
-
                 self.url(
-                    source_dist,
-                    &registry_source_dist.file.filename,
+                    source,
+                    &dist.file.filename,
                     &url,
                     &cache_shard,
                     None,
+                    tags,
+                    hashes,
+                    client,
                 )
-                .boxed()
+                .boxed_local()
                 .await?
             }
-            SourceDist::Git(git_source_dist) => {
-                self.git(source_dist, git_source_dist).boxed().await?
+            BuildableSource::Dist(SourceDist::DirectUrl(dist)) => {
+                let filename = dist.filename().expect("Distribution must have a filename");
+                let ParsedArchiveUrl { url, subdirectory } =
+                    ParsedArchiveUrl::from(dist.url.to_url());
+
+                // For direct URLs, cache directly under the hash of the URL itself.
+                let cache_shard = self
+                    .build_context
+                    .cache()
+                    .shard(CacheBucket::BuiltWheels, WheelCache::Url(&url).root());
+
+                self.url(
+                    source,
+                    &filename,
+                    &url,
+                    &cache_shard,
+                    subdirectory.as_deref(),
+                    tags,
+                    hashes,
+                    client,
+                )
+                .boxed_local()
+                .await?
+            }
+            BuildableSource::Dist(SourceDist::Git(dist)) => {
+                self.git(source, &GitSourceUrl::from(dist), tags, hashes)
+                    .boxed_local()
+                    .await?
             }
-            SourceDist::Path(path_source_dist) => {
-                self.path(source_dist, path_source_dist).boxed().await?
+            BuildableSource::Dist(SourceDist::Directory(dist)) => {
+                self.source_tree(source, &DirectorySourceUrl::from(dist), tags, hashes)
+                    .boxed_local()
+                    .await?
+            }
+            BuildableSource::Dist(SourceDist::Path(dist)) => {
+                let cache_shard = self
+                    .build_context
+                    .cache()
+                    .shard(CacheBucket::BuiltWheels, WheelCache::Path(&dist.url).root());
+                self.archive(
+                    source,
+                    &PathSourceUrl::from(dist),
+                    &cache_shard,
+                    tags,
+                    hashes,
+                )
+                .boxed_local()
+                .await?
+            }
+            BuildableSource::Url(SourceUrl::Direct(resource)) => {
+                let filename = resource
+                    .url
+                    .filename()
+                    .expect("Distribution must have a filename");
+                let ParsedArchiveUrl { url, subdirectory } =
+                    ParsedArchiveUrl::from(resource.url.clone());
+
+                // For direct URLs, cache directly under the hash of the URL itself.
+                let cache_shard = self
+                    .build_context
+                    .cache()
+                    .shard(CacheBucket::BuiltWheels, WheelCache::Url(&url).root());
+
+                self.url(
+                    source,
+                    &filename,
+                    &url,
+                    &cache_shard,
+                    subdirectory.as_deref(),
+                    tags,
+                    hashes,
+                    client,
+                )
+                .boxed_local()
+                .await?
+            }
+            BuildableSource::Url(SourceUrl::Git(resource)) => {
+                self.git(source, resource, tags, hashes)
+                    .boxed_local()
+                    .await?
+            }
+            BuildableSource::Url(SourceUrl::Directory(resource)) => {
+                self.source_tree(source, resource, tags, hashes)
+                    .boxed_local()
+                    .await?
+            }
+            BuildableSource::Url(SourceUrl::Path(resource)) => {
+                let cache_shard = self.build_context.cache().shard(
+                    CacheBucket::BuiltWheels,
+                    WheelCache::Path(resource.url).root(),
+                );
+                self.archive(source, resource, &cache_shard, tags, hashes)
+                    .boxed_local()
+                    .await?
             }
         };
 
         Ok(built_wheel_metadata)
     }
 
     /// Download a [`SourceDist`] and determine its metadata. This typically involves building the
     /// source distribution into a wheel; however, some build backends support determining the
     /// metadata without building the source distribution.
-    pub async fn download_and_build_metadata(
+    pub(super) async fn download_and_build_metadata(
         &self,
-        source_dist: &SourceDist,
-    ) -> Result<Metadata21, Error> {
-        let metadata = match &source_dist {
-            SourceDist::DirectUrl(direct_url_source_dist) => {
-                let filename = direct_url_source_dist
-                    .filename()
-                    .expect("Distribution must have a filename");
-                let DirectArchiveUrl { url, subdirectory } =
-                    DirectArchiveUrl::from(direct_url_source_dist.url.raw());
-
-                // For direct URLs, cache directly under the hash of the URL itself.
+        source: &BuildableSource<'_>,
+        hashes: HashPolicy<'_>,
+        client: &ManagedClient<'_>,
+    ) -> Result<ArchiveMetadata, Error> {
+        let metadata = match &source {
+            BuildableSource::Dist(SourceDist::Registry(dist)) => {
+                // For registry source distributions, shard by package, then version.
                 let cache_shard = self.build_context.cache().shard(
                     CacheBucket::BuiltWheels,
-                    WheelCache::Url(&url).remote_wheel_dir(direct_url_source_dist.name().as_ref()),
+                    WheelCache::Index(&dist.index)
+                        .wheel_dir(dist.name.as_ref())
+                        .join(dist.version.to_string()),
                 );
 
-                self.url_metadata(
-                    source_dist,
-                    &filename,
-                    &url,
-                    &cache_shard,
-                    subdirectory.as_deref(),
-                )
-                .boxed()
-                .await?
-            }
-            SourceDist::Registry(registry_source_dist) => {
-                let url = match &registry_source_dist.file.url {
+                let url = match &dist.file.url {
                     FileLocation::RelativeUrl(base, url) => {
                         pypi_types::base_url_join_relative(base, url)?
                     }
                     FileLocation::AbsoluteUrl(url) => {
                         Url::parse(url).map_err(|err| Error::Url(url.clone(), err))?
                     }
                     FileLocation::Path(path) => {
-                        let path_source_dist = PathSourceDist {
-                            name: registry_source_dist.filename.name.clone(),
-                            url: VerbatimUrl::unknown(
-                                Url::from_file_path(path).expect("path is absolute"),
-                            ),
-                            path: path.clone(),
-                            editable: false,
-                        };
+                        let url = Url::from_file_path(path)
+                            .map_err(|()| Error::RelativePath(path.clone()))?;
                         return self
-                            .path_metadata(source_dist, &path_source_dist)
-                            .boxed()
+                            .archive_metadata(
+                                source,
+                                &PathSourceUrl {
+                                    url: &url,
+                                    path: Cow::Borrowed(path),
+                                },
+                                &cache_shard,
+                                hashes,
+                            )
+                            .boxed_local()
                             .await;
                     }
                 };
 
-                // For registry source distributions, shard by package, then version.
-                let cache_shard = self.build_context.cache().shard(
-                    CacheBucket::BuiltWheels,
-                    WheelCache::Index(&registry_source_dist.index)
-                        .remote_wheel_dir(registry_source_dist.filename.name.as_ref())
-                        .join(registry_source_dist.filename.version.to_string()),
-                );
-
                 self.url_metadata(
-                    source_dist,
-                    &registry_source_dist.file.filename,
+                    source,
+                    &dist.file.filename,
                     &url,
                     &cache_shard,
                     None,
+                    hashes,
+                    client,
+                )
+                .boxed_local()
+                .await?
+            }
+            BuildableSource::Dist(SourceDist::DirectUrl(dist)) => {
+                let filename = dist.filename().expect("Distribution must have a filename");
+                let ParsedArchiveUrl { url, subdirectory } =
+                    ParsedArchiveUrl::from(dist.url.to_url());
+
+                // For direct URLs, cache directly under the hash of the URL itself.
+                let cache_shard = self
+                    .build_context
+                    .cache()
+                    .shard(CacheBucket::BuiltWheels, WheelCache::Url(&url).root());
+
+                self.url_metadata(
+                    source,
+                    &filename,
+                    &url,
+                    &cache_shard,
+                    subdirectory.as_deref(),
+                    hashes,
+                    client,
                 )
-                .boxed()
+                .boxed_local()
                 .await?
             }
-            SourceDist::Git(git_source_dist) => {
-                self.git_metadata(source_dist, git_source_dist)
-                    .boxed()
+            BuildableSource::Dist(SourceDist::Git(dist)) => {
+                self.git_metadata(source, &GitSourceUrl::from(dist), hashes)
+                    .boxed_local()
                     .await?
             }
-            SourceDist::Path(path_source_dist) => {
-                self.path_metadata(source_dist, path_source_dist)
-                    .boxed()
+            BuildableSource::Dist(SourceDist::Directory(dist)) => {
+                self.source_tree_metadata(source, &DirectorySourceUrl::from(dist), hashes)
+                    .boxed_local()
+                    .await?
+            }
+            BuildableSource::Dist(SourceDist::Path(dist)) => {
+                let cache_shard = self
+                    .build_context
+                    .cache()
+                    .shard(CacheBucket::BuiltWheels, WheelCache::Path(&dist.url).root());
+                self.archive_metadata(source, &PathSourceUrl::from(dist), &cache_shard, hashes)
+                    .boxed_local()
+                    .await?
+            }
+            BuildableSource::Url(SourceUrl::Direct(resource)) => {
+                let filename = resource
+                    .url
+                    .filename()
+                    .expect("Distribution must have a filename");
+                let ParsedArchiveUrl { url, subdirectory } =
+                    ParsedArchiveUrl::from(resource.url.clone());
+
+                // For direct URLs, cache directly under the hash of the URL itself.
+                let cache_shard = self
+                    .build_context
+                    .cache()
+                    .shard(CacheBucket::BuiltWheels, WheelCache::Url(&url).root());
+
+                self.url_metadata(
+                    source,
+                    &filename,
+                    &url,
+                    &cache_shard,
+                    subdirectory.as_deref(),
+                    hashes,
+                    client,
+                )
+                .boxed_local()
+                .await?
+            }
+            BuildableSource::Url(SourceUrl::Git(resource)) => {
+                self.git_metadata(source, resource, hashes)
+                    .boxed_local()
+                    .await?
+            }
+            BuildableSource::Url(SourceUrl::Directory(resource)) => {
+                self.source_tree_metadata(source, resource, hashes)
+                    .boxed_local()
+                    .await?
+            }
+
+            BuildableSource::Url(SourceUrl::Path(resource)) => {
+                let cache_shard = self.build_context.cache().shard(
+                    CacheBucket::BuiltWheels,
+                    WheelCache::Path(resource.url).root(),
+                );
+                self.archive_metadata(source, resource, &cache_shard, hashes)
+                    .boxed_local()
                     .await?
             }
         };
 
         Ok(metadata)
     }
 
     /// Build a source distribution from a remote URL.
     #[allow(clippy::too_many_arguments)]
     async fn url<'data>(
         &self,
-        source_dist: &'data SourceDist,
+        source: &BuildableSource<'data>,
         filename: &'data str,
         url: &'data Url,
         cache_shard: &CacheShard,
         subdirectory: Option<&'data Path>,
+        tags: &Tags,
+        hashes: HashPolicy<'_>,
+        client: &ManagedClient<'_>,
     ) -> Result<BuiltWheelMetadata, Error> {
-        let cache_entry = cache_shard.entry(MANIFEST);
-        let cache_control = match self.client.connectivity() {
-            Connectivity::Online => CacheControl::from(
-                self.build_context
-                    .cache()
-                    .freshness(&cache_entry, Some(source_dist.name()))
-                    .map_err(Error::CacheRead)?,
-            ),
-            Connectivity::Offline => CacheControl::AllowStale,
-        };
+        let _lock = lock_shard(cache_shard).await?;
 
-        let download = |response| {
-            async {
-                // At this point, we're seeing a new or updated source distribution. Initialize a
-                // new manifest, to collect the source and built artifacts.
-                let manifest = Manifest::new();
-
-                // Download the source distribution.
-                debug!("Downloading source distribution: {source_dist}");
-                let source_dist_entry = cache_shard.shard(manifest.id()).entry(filename);
-                self.persist_source_dist_url(response, source_dist, filename, &source_dist_entry)
-                    .await?;
+        // Fetch the revision for the source distribution.
+        let revision = self
+            .url_revision(source, filename, url, cache_shard, hashes, client)
+            .await?;
 
-                Ok(manifest)
-            }
-            .boxed()
-            .instrument(info_span!("download", source_dist = %source_dist))
-        };
-        let req = self
-            .client
-            .cached_client()
-            .uncached()
-            .get(url.clone())
-            .build()?;
-        let manifest = self
-            .client
-            .cached_client()
-            .get_serde(req, &cache_entry, cache_control, download)
-            .await
-            .map_err(|err| match err {
-                CachedClientError::Callback(err) => err,
-                CachedClientError::Client(err) => Error::Client(err),
-            })?;
+        // Before running the build, check that the hashes match.
+        if !revision.satisfies(hashes) {
+            return Err(Error::hash_mismatch(
+                source.to_string(),
+                hashes.digests(),
+                revision.hashes(),
+            ));
+        }
 
-        // From here on, scope all operations to the current build. Within the manifest shard,
-        // there's no need to check for freshness, since entries have to be fresher than the
-        // manifest itself. There's also no need to lock, since we never replace entries within the
-        // shard.
-        let cache_shard = cache_shard.shard(manifest.id());
+        // Scope all operations to the revision. Within the revision, there's no need to check for
+        // freshness, since entries have to be fresher than the revision itself.
+        let cache_shard = cache_shard.shard(revision.id());
 
         // If the cache contains a compatible wheel, return it.
-        if let Some(built_wheel) = BuiltWheelMetadata::find_in_cache(self.tags, &cache_shard) {
-            return Ok(built_wheel);
+        if let Some(built_wheel) = BuiltWheelMetadata::find_in_cache(tags, &cache_shard) {
+            return Ok(built_wheel.with_hashes(revision.into_hashes()));
         }
 
         let task = self
             .reporter
             .as_ref()
-            .map(|reporter| reporter.on_build_start(source_dist));
+            .map(|reporter| reporter.on_build_start(source));
 
         // Build the source distribution.
         let source_dist_entry = cache_shard.entry(filename);
         let (disk_filename, wheel_filename, metadata) = self
-            .build_source_dist(
-                source_dist,
-                source_dist_entry.path(),
-                subdirectory,
-                &cache_shard,
-            )
+            .build_distribution(source, source_dist_entry.path(), subdirectory, &cache_shard)
             .await?;
 
         if let Some(task) = task {
             if let Some(reporter) = self.reporter.as_ref() {
-                reporter.on_build_complete(source_dist, task);
+                reporter.on_build_complete(source, task);
             }
         }
 
         // Store the metadata.
         let metadata_entry = cache_shard.entry(METADATA);
         write_atomic(metadata_entry.path(), rmp_serde::to_vec(&metadata)?)
             .await
             .map_err(Error::CacheWrite)?;
 
         Ok(BuiltWheelMetadata {
             path: cache_shard.join(&disk_filename),
             target: cache_shard.join(wheel_filename.stem()),
             filename: wheel_filename,
+            hashes: revision.into_hashes(),
         })
     }
 
     /// Build the source distribution's metadata from a local path.
     ///
     /// If the build backend supports `prepare_metadata_for_build_wheel`, this method will avoid
     /// building the wheel.
     #[allow(clippy::too_many_arguments)]
     async fn url_metadata<'data>(
         &self,
-        source_dist: &'data SourceDist,
+        source: &BuildableSource<'data>,
         filename: &'data str,
         url: &'data Url,
         cache_shard: &CacheShard,
         subdirectory: Option<&'data Path>,
-    ) -> Result<Metadata21, Error> {
-        let cache_entry = cache_shard.entry(MANIFEST);
-        let cache_control = match self.client.connectivity() {
+        hashes: HashPolicy<'_>,
+        client: &ManagedClient<'_>,
+    ) -> Result<ArchiveMetadata, Error> {
+        let _lock = lock_shard(cache_shard).await?;
+
+        // Fetch the revision for the source distribution.
+        let revision = self
+            .url_revision(source, filename, url, cache_shard, hashes, client)
+            .await?;
+
+        // Before running the build, check that the hashes match.
+        if !revision.satisfies(hashes) {
+            return Err(Error::hash_mismatch(
+                source.to_string(),
+                hashes.digests(),
+                revision.hashes(),
+            ));
+        }
+
+        // Scope all operations to the revision. Within the revision, there's no need to check for
+        // freshness, since entries have to be fresher than the revision itself.
+        let cache_shard = cache_shard.shard(revision.id());
+
+        // If the cache contains compatible metadata, return it.
+        let metadata_entry = cache_shard.entry(METADATA);
+        if let Some(metadata) = read_cached_metadata(&metadata_entry).await? {
+            debug!("Using cached metadata for: {source}");
+            return Ok(ArchiveMetadata {
+                metadata,
+                hashes: revision.into_hashes(),
+            });
+        }
+
+        // Otherwise, we either need to build the metadata or the wheel.
+        let source_dist_entry = cache_shard.entry(filename);
+
+        // If the backend supports `prepare_metadata_for_build_wheel`, use it.
+        if let Some(metadata) = self
+            .build_metadata(source, source_dist_entry.path(), subdirectory)
+            .boxed_local()
+            .await?
+        {
+            // Store the metadata.
+            fs::create_dir_all(metadata_entry.dir())
+                .await
+                .map_err(Error::CacheWrite)?;
+            write_atomic(metadata_entry.path(), rmp_serde::to_vec(&metadata)?)
+                .await
+                .map_err(Error::CacheWrite)?;
+
+            return Ok(ArchiveMetadata {
+                metadata,
+                hashes: revision.into_hashes(),
+            });
+        }
+
+        let task = self
+            .reporter
+            .as_ref()
+            .map(|reporter| reporter.on_build_start(source));
+
+        // Build the source distribution.
+        let (_disk_filename, _wheel_filename, metadata) = self
+            .build_distribution(source, source_dist_entry.path(), subdirectory, &cache_shard)
+            .await?;
+
+        // Store the metadata.
+        write_atomic(metadata_entry.path(), rmp_serde::to_vec(&metadata)?)
+            .await
+            .map_err(Error::CacheWrite)?;
+
+        if let Some(task) = task {
+            if let Some(reporter) = self.reporter.as_ref() {
+                reporter.on_build_complete(source, task);
+            }
+        }
+
+        Ok(ArchiveMetadata {
+            metadata,
+            hashes: revision.into_hashes(),
+        })
+    }
+
+    /// Return the [`Revision`] for a remote URL, refreshing it if necessary.
+    async fn url_revision(
+        &self,
+        source: &BuildableSource<'_>,
+        filename: &str,
+        url: &Url,
+        cache_shard: &CacheShard,
+        hashes: HashPolicy<'_>,
+        client: &ManagedClient<'_>,
+    ) -> Result<Revision, Error> {
+        let cache_entry = cache_shard.entry(HTTP_REVISION);
+        let cache_control = match client.unmanaged.connectivity() {
             Connectivity::Online => CacheControl::from(
                 self.build_context
                     .cache()
-                    .freshness(&cache_entry, Some(source_dist.name()))
+                    .freshness(&cache_entry, source.name())
                     .map_err(Error::CacheRead)?,
             ),
             Connectivity::Offline => CacheControl::AllowStale,
         };
 
         let download = |response| {
             async {
                 // At this point, we're seeing a new or updated source distribution. Initialize a
-                // new manifest, to collect the source and built artifacts.
-                let manifest = Manifest::new();
+                // new revision, to collect the source and built artifacts.
+                let revision = Revision::new();
 
                 // Download the source distribution.
-                debug!("Downloading source distribution: {source_dist}");
-                let source_dist_entry = cache_shard.shard(manifest.id()).entry(filename);
-                self.persist_source_dist_url(response, source_dist, filename, &source_dist_entry)
+                debug!("Downloading source distribution: {source}");
+                let entry = cache_shard.shard(revision.id()).entry(filename);
+                let hashes = self
+                    .download_archive(response, source, filename, entry.path(), hashes)
                     .await?;
 
-                Ok(manifest)
+                Ok(revision.with_hashes(hashes))
             }
-            .boxed()
-            .instrument(info_span!("download", source_dist = %source_dist))
+            .boxed_local()
+            .instrument(info_span!("download", source_dist = %source))
         };
-        let req = self
-            .client
-            .cached_client()
-            .uncached()
-            .get(url.clone())
-            .build()?;
-        let manifest = self
-            .client
-            .cached_client()
-            .get_serde(req, &cache_entry, cache_control, download)
+        let req = Self::request(url.clone(), client.unmanaged)?;
+        let revision = client
+            .managed(|client| {
+                client
+                    .cached_client()
+                    .get_serde(req, &cache_entry, cache_control, download)
+            })
             .await
             .map_err(|err| match err {
                 CachedClientError::Callback(err) => err,
                 CachedClientError::Client(err) => Error::Client(err),
             })?;
 
-        // From here on, scope all operations to the current build. Within the manifest shard,
-        // there's no need to check for freshness, since entries have to be fresher than the
-        // manifest itself. There's also no need to lock, since we never replace entries within the
-        // shard.
-        let cache_shard = cache_shard.shard(manifest.id());
+        // If the archive is missing the required hashes, force a refresh.
+        if revision.has_digests(hashes) {
+            Ok(revision)
+        } else {
+            client
+                .managed(|client| async move {
+                    client
+                        .cached_client()
+                        .skip_cache(Self::request(url.clone(), client)?, &cache_entry, download)
+                        .await
+                        .map_err(|err| match err {
+                            CachedClientError::Callback(err) => err,
+                            CachedClientError::Client(err) => Error::Client(err),
+                        })
+                })
+                .await
+        }
+    }
+
+    /// Build a source distribution from a local archive (e.g., `.tar.gz` or `.zip`).
+    async fn archive(
+        &self,
+        source: &BuildableSource<'_>,
+        resource: &PathSourceUrl<'_>,
+        cache_shard: &CacheShard,
+        tags: &Tags,
+        hashes: HashPolicy<'_>,
+    ) -> Result<BuiltWheelMetadata, Error> {
+        let _lock = lock_shard(cache_shard).await?;
+
+        // Fetch the revision for the source distribution.
+        let revision = self
+            .archive_revision(source, resource, cache_shard, hashes)
+            .await?;
+
+        // Before running the build, check that the hashes match.
+        if !revision.satisfies(hashes) {
+            return Err(Error::hash_mismatch(
+                source.to_string(),
+                hashes.digests(),
+                revision.hashes(),
+            ));
+        }
+
+        // Scope all operations to the revision. Within the revision, there's no need to check for
+        // freshness, since entries have to be fresher than the revision itself.
+        let cache_shard = cache_shard.shard(revision.id());
+
+        // If the cache contains a compatible wheel, return it.
+        if let Some(built_wheel) = BuiltWheelMetadata::find_in_cache(tags, &cache_shard) {
+            return Ok(built_wheel);
+        }
+
+        let source_entry = cache_shard.entry("source");
+
+        // Otherwise, we need to build a wheel.
+        let task = self
+            .reporter
+            .as_ref()
+            .map(|reporter| reporter.on_build_start(source));
+
+        let (disk_filename, filename, metadata) = self
+            .build_distribution(source, source_entry.path(), None, &cache_shard)
+            .await?;
+
+        if let Some(task) = task {
+            if let Some(reporter) = self.reporter.as_ref() {
+                reporter.on_build_complete(source, task);
+            }
+        }
+
+        // Store the metadata.
+        let metadata_entry = cache_shard.entry(METADATA);
+        write_atomic(metadata_entry.path(), rmp_serde::to_vec(&metadata)?)
+            .await
+            .map_err(Error::CacheWrite)?;
+
+        Ok(BuiltWheelMetadata {
+            path: cache_shard.join(&disk_filename),
+            target: cache_shard.join(filename.stem()),
+            filename,
+            hashes: revision.into_hashes(),
+        })
+    }
+
+    /// Build the source distribution's metadata from a local archive (e.g., `.tar.gz` or `.zip`).
+    ///
+    /// If the build backend supports `prepare_metadata_for_build_wheel`, this method will avoid
+    /// building the wheel.
+    async fn archive_metadata(
+        &self,
+        source: &BuildableSource<'_>,
+        resource: &PathSourceUrl<'_>,
+        cache_shard: &CacheShard,
+        hashes: HashPolicy<'_>,
+    ) -> Result<ArchiveMetadata, Error> {
+        let _lock = lock_shard(cache_shard).await?;
+
+        // Fetch the revision for the source distribution.
+        let revision = self
+            .archive_revision(source, resource, cache_shard, hashes)
+            .await?;
+
+        // Before running the build, check that the hashes match.
+        if !revision.satisfies(hashes) {
+            return Err(Error::hash_mismatch(
+                source.to_string(),
+                hashes.digests(),
+                revision.hashes(),
+            ));
+        }
+
+        // Scope all operations to the revision. Within the revision, there's no need to check for
+        // freshness, since entries have to be fresher than the revision itself.
+        let cache_shard = cache_shard.shard(revision.id());
 
         // If the cache contains compatible metadata, return it.
         let metadata_entry = cache_shard.entry(METADATA);
         if let Some(metadata) = read_cached_metadata(&metadata_entry).await? {
-            debug!("Using cached metadata for {source_dist}");
-            return Ok(metadata.clone());
+            debug!("Using cached metadata for: {source}");
+            return Ok(ArchiveMetadata {
+                metadata,
+                hashes: revision.into_hashes(),
+            });
         }
 
-        // Otherwise, we either need to build the metadata or the wheel.
-        let source_dist_entry = cache_shard.entry(filename);
+        let source_entry = cache_shard.entry("source");
 
         // If the backend supports `prepare_metadata_for_build_wheel`, use it.
         if let Some(metadata) = self
-            .build_source_dist_metadata(source_dist, source_dist_entry.path(), subdirectory)
-            .boxed()
+            .build_metadata(source, source_entry.path(), None)
+            .boxed_local()
             .await?
         {
             // Store the metadata.
-            let cache_entry = cache_shard.entry(METADATA);
-            fs::create_dir_all(cache_entry.dir())
+            fs::create_dir_all(metadata_entry.dir())
                 .await
                 .map_err(Error::CacheWrite)?;
-            write_atomic(cache_entry.path(), rmp_serde::to_vec(&metadata)?)
+            write_atomic(metadata_entry.path(), rmp_serde::to_vec(&metadata)?)
                 .await
                 .map_err(Error::CacheWrite)?;
 
-            return Ok(metadata);
+            return Ok(ArchiveMetadata {
+                metadata,
+                hashes: revision.into_hashes(),
+            });
         }
 
+        // Otherwise, we need to build a wheel.
         let task = self
             .reporter
             .as_ref()
-            .map(|reporter| reporter.on_build_start(source_dist));
+            .map(|reporter| reporter.on_build_start(source));
 
-        // Build the source distribution.
-        let (_disk_filename, _wheel_filename, metadata) = self
-            .build_source_dist(
-                source_dist,
-                source_dist_entry.path(),
-                subdirectory,
-                &cache_shard,
-            )
+        let (_disk_filename, _filename, metadata) = self
+            .build_distribution(source, source_entry.path(), None, &cache_shard)
             .await?;
 
+        if let Some(task) = task {
+            if let Some(reporter) = self.reporter.as_ref() {
+                reporter.on_build_complete(source, task);
+            }
+        }
+
         // Store the metadata.
-        let cache_entry = cache_shard.entry(METADATA);
-        write_atomic(cache_entry.path(), rmp_serde::to_vec(&metadata)?)
+        write_atomic(metadata_entry.path(), rmp_serde::to_vec(&metadata)?)
             .await
             .map_err(Error::CacheWrite)?;
 
-        if let Some(task) = task {
-            if let Some(reporter) = self.reporter.as_ref() {
-                reporter.on_build_complete(source_dist, task);
+        Ok(ArchiveMetadata {
+            metadata,
+            hashes: revision.into_hashes(),
+        })
+    }
+
+    /// Return the [`Revision`] for a local archive, refreshing it if necessary.
+    async fn archive_revision(
+        &self,
+        source: &BuildableSource<'_>,
+        resource: &PathSourceUrl<'_>,
+        cache_shard: &CacheShard,
+        hashes: HashPolicy<'_>,
+    ) -> Result<Revision, Error> {
+        // Determine the last-modified time of the source distribution.
+        let modified = ArchiveTimestamp::from_file(&resource.path).map_err(Error::CacheRead)?;
+
+        // Read the existing metadata from the cache.
+        let revision_entry = cache_shard.entry(LOCAL_REVISION);
+
+        // If the revision already exists, return it. There's no need to check for freshness, since
+        // we use an exact timestamp.
+        if let Some(pointer) = LocalRevisionPointer::read_from(&revision_entry)? {
+            if pointer.is_up_to_date(modified) {
+                let revision = pointer.into_revision();
+                if revision.has_digests(hashes) {
+                    return Ok(revision);
+                }
             }
         }
 
-        Ok(metadata)
+        // Otherwise, we need to create a new revision.
+        let revision = Revision::new();
+
+        // Unzip the archive to a temporary directory.
+        debug!("Unpacking source distribution: {source}");
+        let entry = cache_shard.shard(revision.id()).entry("source");
+        let hashes = self
+            .persist_archive(&resource.path, entry.path(), hashes)
+            .await?;
+        let revision = revision.with_hashes(hashes);
+
+        // Persist the revision.
+        write_atomic(
+            revision_entry.path(),
+            rmp_serde::to_vec(&CachedByTimestamp {
+                timestamp: modified.timestamp(),
+                data: revision.clone(),
+            })?,
+        )
+        .await
+        .map_err(Error::CacheWrite)?;
+
+        Ok(revision)
     }
 
-    /// Build a source distribution from a local path.
-    async fn path(
+    /// Build a source distribution from a local source tree (i.e., directory).
+    async fn source_tree(
         &self,
-        source_dist: &SourceDist,
-        path_source_dist: &PathSourceDist,
+        source: &BuildableSource<'_>,
+        resource: &DirectorySourceUrl<'_>,
+        tags: &Tags,
+        hashes: HashPolicy<'_>,
     ) -> Result<BuiltWheelMetadata, Error> {
+        // Before running the build, check that the hashes match.
+        if hashes.is_validate() {
+            return Err(Error::HashesNotSupportedSourceTree(source.to_string()));
+        }
+
         let cache_shard = self.build_context.cache().shard(
             CacheBucket::BuiltWheels,
-            WheelCache::Path(&path_source_dist.url)
-                .remote_wheel_dir(path_source_dist.name().as_ref()),
+            WheelCache::Path(resource.url).root(),
         );
 
-        // Determine the last-modified time of the source distribution.
-        let Some(modified) =
-            ArchiveTimestamp::from_path(&path_source_dist.path).map_err(Error::CacheRead)?
-        else {
-            return Err(Error::DirWithoutEntrypoint);
-        };
+        let _lock = lock_shard(&cache_shard).await?;
 
-        // Read the existing metadata from the cache.
-        let manifest_entry = cache_shard.entry(MANIFEST);
-        let manifest_freshness = self
-            .build_context
-            .cache()
-            .freshness(&manifest_entry, Some(source_dist.name()))
-            .map_err(Error::CacheRead)?;
-        let manifest =
-            refresh_timestamp_manifest(&manifest_entry, manifest_freshness, modified).await?;
+        // Fetch the revision for the source distribution.
+        let revision = self
+            .source_tree_revision(source, resource, &cache_shard)
+            .await?;
 
-        // From here on, scope all operations to the current build. Within the manifest shard,
-        // there's no need to check for freshness, since entries have to be fresher than the
-        // manifest itself. There's also no need to lock, since we never replace entries within the
-        // shard.
-        let cache_shard = cache_shard.shard(manifest.id());
+        // Scope all operations to the revision. Within the revision, there's no need to check for
+        // freshness, since entries have to be fresher than the revision itself.
+        let cache_shard = cache_shard.shard(revision.id());
 
         // If the cache contains a compatible wheel, return it.
-        if let Some(built_wheel) = BuiltWheelMetadata::find_in_cache(self.tags, &cache_shard) {
+        if let Some(built_wheel) = BuiltWheelMetadata::find_in_cache(tags, &cache_shard) {
             return Ok(built_wheel);
         }
 
         // Otherwise, we need to build a wheel.
         let task = self
             .reporter
             .as_ref()
-            .map(|reporter| reporter.on_build_start(source_dist));
+            .map(|reporter| reporter.on_build_start(source));
 
         let (disk_filename, filename, metadata) = self
-            .build_source_dist(source_dist, &path_source_dist.path, None, &cache_shard)
+            .build_distribution(source, &resource.path, None, &cache_shard)
             .await?;
 
         if let Some(task) = task {
             if let Some(reporter) = self.reporter.as_ref() {
-                reporter.on_build_complete(source_dist, task);
+                reporter.on_build_complete(source, task);
             }
         }
 
         // Store the metadata.
-        let cache_entry = cache_shard.entry(METADATA);
-        write_atomic(cache_entry.path(), rmp_serde::to_vec(&metadata)?)
+        let metadata_entry = cache_shard.entry(METADATA);
+        write_atomic(metadata_entry.path(), rmp_serde::to_vec(&metadata)?)
             .await
             .map_err(Error::CacheWrite)?;
 
         Ok(BuiltWheelMetadata {
             path: cache_shard.join(&disk_filename),
             target: cache_shard.join(filename.stem()),
             filename,
+            hashes: vec![],
         })
     }
 
-    /// Build the source distribution's metadata from a local path.
+    /// Build the source distribution's metadata from a local source tree (i.e., a directory).
     ///
     /// If the build backend supports `prepare_metadata_for_build_wheel`, this method will avoid
     /// building the wheel.
-    async fn path_metadata(
+    async fn source_tree_metadata(
         &self,
-        source_dist: &SourceDist,
-        path_source_dist: &PathSourceDist,
-    ) -> Result<Metadata21, Error> {
+        source: &BuildableSource<'_>,
+        resource: &DirectorySourceUrl<'_>,
+        hashes: HashPolicy<'_>,
+    ) -> Result<ArchiveMetadata, Error> {
+        // Before running the build, check that the hashes match.
+        if hashes.is_validate() {
+            return Err(Error::HashesNotSupportedSourceTree(source.to_string()));
+        }
+
         let cache_shard = self.build_context.cache().shard(
             CacheBucket::BuiltWheels,
-            WheelCache::Path(&path_source_dist.url)
-                .remote_wheel_dir(path_source_dist.name().as_ref()),
+            WheelCache::Path(resource.url).root(),
         );
 
-        // Determine the last-modified time of the source distribution.
-        let Some(modified) =
-            ArchiveTimestamp::from_path(&path_source_dist.path).map_err(Error::CacheRead)?
-        else {
-            return Err(Error::DirWithoutEntrypoint);
-        };
+        let _lock = lock_shard(&cache_shard).await?;
 
-        // Read the existing metadata from the cache, to clear stale entries.
-        let manifest_entry = cache_shard.entry(MANIFEST);
-        let manifest_freshness = self
-            .build_context
-            .cache()
-            .freshness(&manifest_entry, Some(source_dist.name()))
-            .map_err(Error::CacheRead)?;
-        let manifest =
-            refresh_timestamp_manifest(&manifest_entry, manifest_freshness, modified).await?;
+        // Fetch the revision for the source distribution.
+        let revision = self
+            .source_tree_revision(source, resource, &cache_shard)
+            .await?;
 
-        // From here on, scope all operations to the current build. Within the manifest shard,
-        // there's no need to check for freshness, since entries have to be fresher than the
-        // manifest itself. There's also no need to lock, since we never replace entries within the
-        // shard.
-        let cache_shard = cache_shard.shard(manifest.id());
+        // Scope all operations to the revision. Within the revision, there's no need to check for
+        // freshness, since entries have to be fresher than the revision itself.
+        let cache_shard = cache_shard.shard(revision.id());
 
         // If the cache contains compatible metadata, return it.
         let metadata_entry = cache_shard.entry(METADATA);
-        if self
-            .build_context
-            .cache()
-            .freshness(&metadata_entry, Some(source_dist.name()))
-            .is_ok_and(Freshness::is_fresh)
-        {
-            if let Some(metadata) = read_cached_metadata(&metadata_entry).await? {
-                debug!("Using cached metadata for {source_dist}");
-                return Ok(metadata.clone());
-            }
+        if let Some(metadata) = read_cached_metadata(&metadata_entry).await? {
+            debug!("Using cached metadata for: {source}");
+            return Ok(ArchiveMetadata::from(metadata));
         }
 
         // If the backend supports `prepare_metadata_for_build_wheel`, use it.
         if let Some(metadata) = self
-            .build_source_dist_metadata(source_dist, &path_source_dist.path, None)
-            .boxed()
+            .build_metadata(source, &resource.path, None)
+            .boxed_local()
             .await?
         {
             // Store the metadata.
-            let cache_entry = cache_shard.entry(METADATA);
-            fs::create_dir_all(cache_entry.dir())
+            fs::create_dir_all(metadata_entry.dir())
                 .await
                 .map_err(Error::CacheWrite)?;
-            write_atomic(cache_entry.path(), rmp_serde::to_vec(&metadata)?)
+            write_atomic(metadata_entry.path(), rmp_serde::to_vec(&metadata)?)
                 .await
                 .map_err(Error::CacheWrite)?;
 
-            return Ok(metadata);
+            return Ok(ArchiveMetadata::from(metadata));
         }
 
         // Otherwise, we need to build a wheel.
         let task = self
             .reporter
             .as_ref()
-            .map(|reporter| reporter.on_build_start(source_dist));
+            .map(|reporter| reporter.on_build_start(source));
 
         let (_disk_filename, _filename, metadata) = self
-            .build_source_dist(source_dist, &path_source_dist.path, None, &cache_shard)
+            .build_distribution(source, &resource.path, None, &cache_shard)
             .await?;
 
         if let Some(task) = task {
             if let Some(reporter) = self.reporter.as_ref() {
-                reporter.on_build_complete(source_dist, task);
+                reporter.on_build_complete(source, task);
             }
         }
 
         // Store the metadata.
-        let cache_entry = cache_shard.entry(METADATA);
-        write_atomic(cache_entry.path(), rmp_serde::to_vec(&metadata)?)
+        write_atomic(metadata_entry.path(), rmp_serde::to_vec(&metadata)?)
             .await
             .map_err(Error::CacheWrite)?;
 
-        Ok(metadata)
+        Ok(ArchiveMetadata::from(metadata))
+    }
+
+    /// Return the [`Revision`] for a local source tree, refreshing it if necessary.
+    async fn source_tree_revision(
+        &self,
+        source: &BuildableSource<'_>,
+        resource: &DirectorySourceUrl<'_>,
+        cache_shard: &CacheShard,
+    ) -> Result<Revision, Error> {
+        // Determine the last-modified time of the source distribution.
+        let Some(modified) =
+            ArchiveTimestamp::from_source_tree(&resource.path).map_err(Error::CacheRead)?
+        else {
+            return Err(Error::DirWithoutEntrypoint(resource.path.to_path_buf()));
+        };
+
+        // Read the existing metadata from the cache.
+        let entry = cache_shard.entry(LOCAL_REVISION);
+        let freshness = self
+            .build_context
+            .cache()
+            .freshness(&entry, source.name())
+            .map_err(Error::CacheRead)?;
+
+        // If the revision is fresh, return it.
+        if freshness.is_fresh() {
+            if let Some(pointer) = LocalRevisionPointer::read_from(&entry)? {
+                if pointer.timestamp == modified.timestamp() {
+                    return Ok(pointer.into_revision());
+                }
+            }
+        }
+
+        // Otherwise, we need to create a new revision.
+        let revision = Revision::new();
+        let pointer = LocalRevisionPointer {
+            timestamp: modified.timestamp(),
+            revision: revision.clone(),
+        };
+        pointer.write_to(&entry).await?;
+
+        Ok(revision)
     }
 
     /// Build a source distribution from a Git repository.
     async fn git(
         &self,
-        source_dist: &SourceDist,
-        git_source_dist: &GitSourceDist,
+        source: &BuildableSource<'_>,
+        resource: &GitSourceUrl<'_>,
+        tags: &Tags,
+        hashes: HashPolicy<'_>,
     ) -> Result<BuiltWheelMetadata, Error> {
-        let (fetch, subdirectory) = self.download_source_dist_git(&git_source_dist.url).await?;
+        // Before running the build, check that the hashes match.
+        if hashes.is_validate() {
+            return Err(Error::HashesNotSupportedGit(source.to_string()));
+        }
+
+        // Resolve to a precise Git SHA.
+        let url = if let Some(url) = resolve_precise(
+            &resource.git,
+            self.build_context.cache(),
+            self.reporter.as_ref(),
+        )
+        .await?
+        {
+            Cow::Owned(url)
+        } else {
+            Cow::Borrowed(resource.git.as_ref())
+        };
+
+        let subdirectory = resource.subdirectory.as_deref();
+
+        // Fetch the Git repository.
+        let fetch =
+            fetch_git_archive(&url, self.build_context.cache(), self.reporter.as_ref()).await?;
 
         let git_sha = fetch.git().precise().expect("Exact commit after checkout");
         let cache_shard = self.build_context.cache().shard(
             CacheBucket::BuiltWheels,
-            WheelCache::Git(&git_source_dist.url, &git_sha.to_short_string())
-                .remote_wheel_dir(git_source_dist.name().as_ref()),
+            WheelCache::Git(resource.url, &git_sha.to_short_string()).root(),
         );
 
+        let _lock = lock_shard(&cache_shard).await?;
+
         // If the cache contains a compatible wheel, return it.
-        if let Some(built_wheel) = BuiltWheelMetadata::find_in_cache(self.tags, &cache_shard) {
+        if let Some(built_wheel) = BuiltWheelMetadata::find_in_cache(tags, &cache_shard) {
             return Ok(built_wheel);
         }
 
         let task = self
             .reporter
             .as_ref()
-            .map(|reporter| reporter.on_build_start(source_dist));
+            .map(|reporter| reporter.on_build_start(source));
 
         let (disk_filename, filename, metadata) = self
-            .build_source_dist(
-                source_dist,
-                fetch.path(),
-                subdirectory.as_deref(),
-                &cache_shard,
-            )
+            .build_distribution(source, fetch.path(), subdirectory, &cache_shard)
             .await?;
 
         if let Some(task) = task {
             if let Some(reporter) = self.reporter.as_ref() {
-                reporter.on_build_complete(source_dist, task);
+                reporter.on_build_complete(source, task);
             }
         }
 
         // Store the metadata.
-        let cache_entry = cache_shard.entry(METADATA);
-        write_atomic(cache_entry.path(), rmp_serde::to_vec(&metadata)?)
+        let metadata_entry = cache_shard.entry(METADATA);
+        write_atomic(metadata_entry.path(), rmp_serde::to_vec(&metadata)?)
             .await
             .map_err(Error::CacheWrite)?;
 
         Ok(BuiltWheelMetadata {
             path: cache_shard.join(&disk_filename),
             target: cache_shard.join(filename.stem()),
             filename,
+            hashes: vec![],
         })
     }
 
     /// Build the source distribution's metadata from a Git repository.
     ///
     /// If the build backend supports `prepare_metadata_for_build_wheel`, this method will avoid
     /// building the wheel.
     async fn git_metadata(
         &self,
-        source_dist: &SourceDist,
-        git_source_dist: &GitSourceDist,
-    ) -> Result<Metadata21, Error> {
-        let (fetch, subdirectory) = self.download_source_dist_git(&git_source_dist.url).await?;
+        source: &BuildableSource<'_>,
+        resource: &GitSourceUrl<'_>,
+        hashes: HashPolicy<'_>,
+    ) -> Result<ArchiveMetadata, Error> {
+        // Before running the build, check that the hashes match.
+        if hashes.is_validate() {
+            return Err(Error::HashesNotSupportedGit(source.to_string()));
+        }
+
+        // Resolve to a precise Git SHA.
+        let url = if let Some(url) = resolve_precise(
+            &resource.git,
+            self.build_context.cache(),
+            self.reporter.as_ref(),
+        )
+        .await?
+        {
+            Cow::Owned(url)
+        } else {
+            Cow::Borrowed(resource.git.as_ref())
+        };
+
+        let subdirectory = resource.subdirectory.as_deref();
+
+        // Fetch the Git repository.
+        let fetch =
+            fetch_git_archive(&url, self.build_context.cache(), self.reporter.as_ref()).await?;
 
         let git_sha = fetch.git().precise().expect("Exact commit after checkout");
         let cache_shard = self.build_context.cache().shard(
             CacheBucket::BuiltWheels,
-            WheelCache::Git(&git_source_dist.url, &git_sha.to_short_string())
-                .remote_wheel_dir(git_source_dist.name().as_ref()),
+            WheelCache::Git(resource.url, &git_sha.to_short_string()).root(),
         );
 
+        let _lock = lock_shard(&cache_shard).await?;
+
         // If the cache contains compatible metadata, return it.
         let metadata_entry = cache_shard.entry(METADATA);
         if self
             .build_context
             .cache()
-            .freshness(&metadata_entry, Some(source_dist.name()))
+            .freshness(&metadata_entry, source.name())
             .is_ok_and(Freshness::is_fresh)
         {
             if let Some(metadata) = read_cached_metadata(&metadata_entry).await? {
-                debug!("Using cached metadata for {source_dist}");
-                return Ok(metadata.clone());
+                debug!("Using cached metadata for: {source}");
+                return Ok(ArchiveMetadata::from(metadata));
             }
         }
 
         // If the backend supports `prepare_metadata_for_build_wheel`, use it.
         if let Some(metadata) = self
-            .build_source_dist_metadata(source_dist, fetch.path(), subdirectory.as_deref())
-            .boxed()
+            .build_metadata(source, fetch.path(), subdirectory)
+            .boxed_local()
             .await?
         {
             // Store the metadata.
-            let cache_entry = cache_shard.entry(METADATA);
-            fs::create_dir_all(cache_entry.dir())
+            fs::create_dir_all(metadata_entry.dir())
                 .await
                 .map_err(Error::CacheWrite)?;
-            write_atomic(cache_entry.path(), rmp_serde::to_vec(&metadata)?)
+            write_atomic(metadata_entry.path(), rmp_serde::to_vec(&metadata)?)
                 .await
                 .map_err(Error::CacheWrite)?;
 
-            return Ok(metadata);
+            return Ok(ArchiveMetadata::from(metadata));
         }
 
         // Otherwise, we need to build a wheel.
         let task = self
             .reporter
             .as_ref()
-            .map(|reporter| reporter.on_build_start(source_dist));
+            .map(|reporter| reporter.on_build_start(source));
 
         let (_disk_filename, _filename, metadata) = self
-            .build_source_dist(
-                source_dist,
-                fetch.path(),
-                subdirectory.as_deref(),
-                &cache_shard,
-            )
+            .build_distribution(source, fetch.path(), subdirectory, &cache_shard)
             .await?;
 
         if let Some(task) = task {
             if let Some(reporter) = self.reporter.as_ref() {
-                reporter.on_build_complete(source_dist, task);
+                reporter.on_build_complete(source, task);
             }
         }
 
         // Store the metadata.
-        let cache_entry = cache_shard.entry(METADATA);
-        write_atomic(cache_entry.path(), rmp_serde::to_vec(&metadata)?)
+        write_atomic(metadata_entry.path(), rmp_serde::to_vec(&metadata)?)
             .await
             .map_err(Error::CacheWrite)?;
 
-        Ok(metadata)
+        Ok(ArchiveMetadata::from(metadata))
     }
 
     /// Download and unzip a source distribution into the cache from an HTTP response.
-    async fn persist_source_dist_url<'data>(
+    async fn download_archive(
         &self,
         response: Response,
-        source_dist: &SourceDist,
+        source: &BuildableSource<'_>,
         filename: &str,
-        cache_entry: &'data CacheEntry,
-    ) -> Result<&'data Path, Error> {
-        let cache_path = cache_entry.path();
-        if cache_path.is_dir() {
-            debug!("Distribution is already cached: {source_dist}");
-            return Ok(cache_path);
-        }
-
-        // Download and unzip the source distribution into a temporary directory.
-        let span =
-            info_span!("download_source_dist", filename = filename, source_dist = %source_dist);
+        target: &Path,
+        hashes: HashPolicy<'_>,
+    ) -> Result<Vec<HashDigest>, Error> {
         let temp_dir =
-            tempfile::tempdir_in(self.build_context.cache().root()).map_err(Error::CacheWrite)?;
+            tempfile::tempdir_in(self.build_context.cache().bucket(CacheBucket::BuiltWheels))
+                .map_err(Error::CacheWrite)?;
         let reader = response
             .bytes_stream()
             .map_err(|err| std::io::Error::new(std::io::ErrorKind::Other, err))
             .into_async_read();
-        uv_extract::stream::archive(reader.compat(), filename, temp_dir.path()).await?;
+
+        // Create a hasher for each hash algorithm.
+        let algorithms = hashes.algorithms();
+        let mut hashers = algorithms.into_iter().map(Hasher::from).collect::<Vec<_>>();
+        let mut hasher = uv_extract::hash::HashReader::new(reader.compat(), &mut hashers);
+
+        // Download and unzip the source distribution into a temporary directory.
+        let span = info_span!("download_source_dist", filename = filename, source_dist = %source);
+        uv_extract::stream::archive(&mut hasher, filename, temp_dir.path()).await?;
         drop(span);
 
+        // If necessary, exhaust the reader to compute the hash.
+        if !hashes.is_none() {
+            hasher.finish().await.map_err(Error::HashExhaustion)?;
+        }
+
+        let hashes = hashers.into_iter().map(HashDigest::from).collect();
+
         // Extract the top-level directory.
         let extracted = match uv_extract::strip_component(temp_dir.path()) {
             Ok(top_level) => top_level,
             Err(uv_extract::Error::NonSingularArchive(_)) => temp_dir.into_path(),
             Err(err) => return Err(err.into()),
         };
 
         // Persist it to the cache.
-        fs_err::tokio::create_dir_all(cache_path.parent().expect("Cache entry to have parent"))
+        fs_err::tokio::create_dir_all(target.parent().expect("Cache entry to have parent"))
             .await
             .map_err(Error::CacheWrite)?;
-        fs_err::tokio::rename(extracted, &cache_path)
+        fs_err::tokio::rename(extracted, target)
             .await
             .map_err(Error::CacheWrite)?;
 
-        Ok(cache_path)
+        Ok(hashes)
     }
 
-    /// Download a source distribution from a Git repository.
-    async fn download_source_dist_git(&self, url: &Url) -> Result<(Fetch, Option<PathBuf>), Error> {
-        debug!("Fetching source distribution from Git: {url}");
-        let git_dir = self.build_context.cache().bucket(CacheBucket::Git);
+    /// Extract a local archive, and store it at the given [`CacheEntry`].
+    async fn persist_archive(
+        &self,
+        path: &Path,
+        target: &Path,
+        hashes: HashPolicy<'_>,
+    ) -> Result<Vec<HashDigest>, Error> {
+        debug!("Unpacking for build: {}", path.display());
 
-        // Avoid races between different processes, too.
-        let lock_dir = git_dir.join("locks");
-        fs::create_dir_all(&lock_dir)
+        let temp_dir =
+            tempfile::tempdir_in(self.build_context.cache().bucket(CacheBucket::BuiltWheels))
+                .map_err(Error::CacheWrite)?;
+        let reader = fs_err::tokio::File::open(&path)
             .await
-            .map_err(Error::CacheWrite)?;
-        let canonical_url = cache_key::CanonicalUrl::new(url);
-        let _lock = LockedFile::acquire(
-            lock_dir.join(cache_key::digest(&canonical_url)),
-            &canonical_url,
-        )
-        .map_err(Error::CacheWrite)?;
+            .map_err(Error::CacheRead)?;
 
-        let DirectGitUrl { url, subdirectory } = DirectGitUrl::try_from(url).map_err(Error::Git)?;
+        // Create a hasher for each hash algorithm.
+        let algorithms = hashes.algorithms();
+        let mut hashers = algorithms.into_iter().map(Hasher::from).collect::<Vec<_>>();
+        let mut hasher = uv_extract::hash::HashReader::new(reader, &mut hashers);
 
-        let source = if let Some(reporter) = &self.reporter {
-            GitSource::new(url, git_dir).with_reporter(Facade::from(reporter.clone()))
-        } else {
-            GitSource::new(url, git_dir)
+        // Unzip the archive into a temporary directory.
+        uv_extract::stream::archive(&mut hasher, path, &temp_dir.path()).await?;
+
+        // If necessary, exhaust the reader to compute the hash.
+        if !hashes.is_none() {
+            hasher.finish().await.map_err(Error::HashExhaustion)?;
+        }
+
+        let hashes = hashers.into_iter().map(HashDigest::from).collect();
+
+        // Extract the top-level directory from the archive.
+        let extracted = match uv_extract::strip_component(temp_dir.path()) {
+            Ok(top_level) => top_level,
+            Err(uv_extract::Error::NonSingularArchive(_)) => temp_dir.path().to_path_buf(),
+            Err(err) => return Err(err.into()),
         };
-        let fetch = tokio::task::spawn_blocking(move || source.fetch())
-            .await?
-            .map_err(Error::Git)?;
-        Ok((fetch, subdirectory))
+
+        // Persist it to the cache.
+        fs_err::tokio::create_dir_all(target.parent().expect("Cache entry to have parent"))
+            .await
+            .map_err(Error::CacheWrite)?;
+        fs_err::tokio::rename(extracted, &target)
+            .await
+            .map_err(Error::CacheWrite)?;
+
+        Ok(hashes)
     }
 
     /// Build a source distribution, storing the built wheel in the cache.
     ///
     /// Returns the un-normalized disk filename, the parsed, normalized filename and the metadata
-    #[instrument(skip_all, fields(dist))]
-    async fn build_source_dist(
+    #[instrument(skip_all, fields(dist = %source))]
+    async fn build_distribution(
         &self,
-        dist: &SourceDist,
-        source_dist: &Path,
+        source: &BuildableSource<'_>,
+        source_root: &Path,
         subdirectory: Option<&Path>,
         cache_shard: &CacheShard,
-    ) -> Result<(String, WheelFilename, Metadata21), Error> {
-        debug!("Building: {dist}");
+    ) -> Result<(String, WheelFilename, Metadata23), Error> {
+        debug!("Building: {source}");
 
-        // Guard against build of source distributions when disabled
+        // Guard against build of source distributions when disabled.
         let no_build = match self.build_context.no_build() {
             NoBuild::All => true,
             NoBuild::None => false,
-            NoBuild::Packages(packages) => packages.contains(dist.name()),
+            NoBuild::Packages(packages) => {
+                source.name().is_some_and(|name| packages.contains(name))
+            }
         };
         if no_build {
             return Err(Error::NoBuild);
         }
 
         // Build the wheel.
         fs::create_dir_all(&cache_shard)
             .await
             .map_err(Error::CacheWrite)?;
         let disk_filename = self
             .build_context
             .setup_build(
-                source_dist,
+                source_root,
                 subdirectory,
-                &dist.to_string(),
-                Some(dist),
+                &source.to_string(),
+                source.as_dist(),
                 BuildKind::Wheel,
             )
             .await
-            .map_err(|err| Error::Build(dist.to_string(), err))?
+            .map_err(|err| Error::Build(source.to_string(), err))?
             .wheel(cache_shard)
             .await
-            .map_err(|err| Error::Build(dist.to_string(), err))?;
+            .map_err(|err| Error::Build(source.to_string(), err))?;
 
         // Read the metadata from the wheel.
         let filename = WheelFilename::from_str(&disk_filename)?;
         let metadata = read_wheel_metadata(&filename, cache_shard.join(&disk_filename))?;
 
         // Validate the metadata.
-        if &metadata.name != dist.name() {
-            return Err(Error::NameMismatch {
-                metadata: metadata.name,
-                given: dist.name().clone(),
-            });
-        }
+        validate(source, &metadata)?;
 
-        debug!("Finished building: {dist}");
+        debug!("Finished building: {source}");
         Ok((disk_filename, filename, metadata))
     }
 
     /// Build the metadata for a source distribution.
-    #[instrument(skip_all, fields(dist))]
-    async fn build_source_dist_metadata(
+    #[instrument(skip_all, fields(dist = %source))]
+    async fn build_metadata(
         &self,
-        dist: &SourceDist,
-        source_dist: &Path,
+        source: &BuildableSource<'_>,
+        source_root: &Path,
         subdirectory: Option<&Path>,
-    ) -> Result<Option<Metadata21>, Error> {
-        debug!("Preparing metadata for: {dist}");
+    ) -> Result<Option<Metadata23>, Error> {
+        debug!("Preparing metadata for: {source}");
+
+        // Attempt to read static metadata from the `PKG-INFO` file.
+        match read_pkg_info(source_root, subdirectory).await {
+            Ok(metadata) => {
+                debug!("Found static `PKG-INFO` for: {source}");
+
+                // Validate the metadata.
+                validate(source, &metadata)?;
+
+                return Ok(Some(metadata));
+            }
+            Err(err @ (Error::MissingPkgInfo | Error::DynamicPkgInfo(_))) => {
+                debug!("No static `PKG-INFO` available for: {source} ({err:?})");
+            }
+            Err(err) => return Err(err),
+        }
+
+        // Attempt to read static metadata from the `pyproject.toml`.
+        match read_pyproject_toml(source_root, subdirectory).await {
+            Ok(metadata) => {
+                debug!("Found static `pyproject.toml` for: {source}");
+
+                // Validate the metadata.
+                validate(source, &metadata)?;
+
+                return Ok(Some(metadata));
+            }
+            Err(err @ (Error::MissingPyprojectToml | Error::DynamicPyprojectToml(_))) => {
+                debug!("No static `pyproject.toml` available for: {source} ({err:?})");
+            }
+            Err(err) => return Err(err),
+        }
 
         // Setup the builder.
         let mut builder = self
             .build_context
             .setup_build(
-                source_dist,
+                source_root,
                 subdirectory,
-                &dist.to_string(),
-                Some(dist),
+                &source.to_string(),
+                source.as_dist(),
                 BuildKind::Wheel,
             )
             .await
-            .map_err(|err| Error::Build(dist.to_string(), err))?;
+            .map_err(|err| Error::Build(source.to_string(), err))?;
 
         // Build the metadata.
         let dist_info = builder
             .metadata()
             .await
-            .map_err(|err| Error::Build(dist.to_string(), err))?;
+            .map_err(|err| Error::Build(source.to_string(), err))?;
         let Some(dist_info) = dist_info else {
             return Ok(None);
         };
 
         // Read the metadata from disk.
-        debug!("Prepared metadata for: {dist}");
+        debug!("Prepared metadata for: {source}");
         let content = fs::read(dist_info.join("METADATA"))
             .await
             .map_err(Error::CacheRead)?;
-        let metadata = Metadata21::parse(&content)?;
+        let metadata = Metadata23::parse_metadata(&content)?;
 
         // Validate the metadata.
-        if &metadata.name != dist.name() {
-            return Err(Error::NameMismatch {
-                metadata: metadata.name,
-                given: dist.name().clone(),
-            });
-        }
+        validate(source, &metadata)?;
 
         Ok(Some(metadata))
     }
 
     /// Build a single directory into an editable wheel
     pub async fn build_editable(
         &self,
         editable: &LocalEditable,
         editable_wheel_dir: &Path,
-    ) -> Result<(Dist, String, WheelFilename, Metadata21), Error> {
+    ) -> Result<(Dist, String, WheelFilename, Metadata23), Error> {
         debug!("Building (editable) {editable}");
+
+        // Verify that the editable exists.
+        if !editable.path.exists() {
+            return Err(Error::NotFound(editable.path.clone()));
+        }
+
+        // Build the wheel.
         let disk_filename = self
             .build_context
             .setup_build(
                 &editable.path,
                 None,
                 &editable.to_string(),
                 None,
@@ -960,108 +1440,218 @@
             )
             .await
             .map_err(|err| Error::BuildEditable(editable.to_string(), err))?
             .wheel(editable_wheel_dir)
             .await
             .map_err(|err| Error::BuildEditable(editable.to_string(), err))?;
         let filename = WheelFilename::from_str(&disk_filename)?;
+
         // We finally have the name of the package and can construct the dist.
-        let dist = Dist::Source(SourceDist::Path(PathSourceDist {
+        let dist = Dist::Source(SourceDist::Directory(DirectorySourceDist {
             name: filename.name.clone(),
             url: editable.url().clone(),
             path: editable.path.clone(),
             editable: true,
         }));
         let metadata = read_wheel_metadata(&filename, editable_wheel_dir.join(&disk_filename))?;
 
         debug!("Finished building (editable): {dist}");
         Ok((dist, disk_filename, filename, metadata))
     }
+
+    /// Returns a GET [`reqwest::Request`] for the given URL.
+    fn request(url: Url, client: &RegistryClient) -> Result<reqwest::Request, reqwest::Error> {
+        client
+            .uncached_client()
+            .get(url)
+            .header(
+                // `reqwest` defaults to accepting compressed responses.
+                // Specify identity encoding to get consistent .whl downloading
+                // behavior from servers. ref: https://github.com/pypa/pip/pull/1688
+                "accept-encoding",
+                reqwest::header::HeaderValue::from_static("identity"),
+            )
+            .build()
+    }
 }
 
-/// Read an existing HTTP-cached [`Manifest`], if it exists.
-pub(crate) fn read_http_manifest(cache_entry: &CacheEntry) -> Result<Option<Manifest>, Error> {
-    match std::fs::File::open(cache_entry.path()) {
-        Ok(file) => {
-            let data = DataWithCachePolicy::from_reader(file)?.data;
-            Ok(Some(rmp_serde::from_slice::<Manifest>(&data)?))
+/// Validate that the source distribution matches the built metadata.
+fn validate(source: &BuildableSource<'_>, metadata: &Metadata23) -> Result<(), Error> {
+    if let Some(name) = source.name() {
+        if metadata.name != *name {
+            return Err(Error::NameMismatch {
+                metadata: metadata.name.clone(),
+                given: name.clone(),
+            });
+        }
+    }
+
+    if let Some(version) = source.version() {
+        if metadata.version != *version {
+            return Err(Error::VersionMismatch {
+                metadata: metadata.version.clone(),
+                given: version.clone(),
+            });
         }
-        Err(err) if err.kind() == std::io::ErrorKind::NotFound => Ok(None),
-        Err(err) => Err(Error::CacheRead(err)),
     }
+
+    Ok(())
 }
 
-/// Read an existing timestamped [`Manifest`], if it exists and is up-to-date.
+/// A pointer to a source distribution revision in the cache, fetched from an HTTP archive.
 ///
-/// If the cache entry is stale, a new entry will be created.
-pub(crate) fn read_timestamp_manifest(
-    cache_entry: &CacheEntry,
-    modified: ArchiveTimestamp,
-) -> Result<Option<Manifest>, Error> {
-    // If the cache entry is up-to-date, return it.
-    match std::fs::read(cache_entry.path()) {
-        Ok(cached) => {
-            let cached = rmp_serde::from_slice::<CachedByTimestamp<Manifest>>(&cached)?;
-            if cached.timestamp == modified.timestamp() {
-                return Ok(Some(cached.data));
+/// Encoded with `MsgPack`, and represented on disk by a `.http` file.
+#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
+pub(crate) struct HttpRevisionPointer {
+    revision: Revision,
+}
+
+impl HttpRevisionPointer {
+    /// Read an [`HttpRevisionPointer`] from the cache.
+    pub(crate) fn read_from(path: impl AsRef<Path>) -> Result<Option<Self>, Error> {
+        match fs_err::File::open(path.as_ref()) {
+            Ok(file) => {
+                let data = DataWithCachePolicy::from_reader(file)?.data;
+                let revision = rmp_serde::from_slice::<Revision>(&data)?;
+                Ok(Some(Self { revision }))
             }
+            Err(err) if err.kind() == std::io::ErrorKind::NotFound => Ok(None),
+            Err(err) => Err(Error::CacheRead(err)),
         }
-        Err(err) if err.kind() == std::io::ErrorKind::NotFound => {}
-        Err(err) => return Err(Error::CacheRead(err)),
     }
-    Ok(None)
+
+    /// Return the [`Revision`] from the pointer.
+    pub(crate) fn into_revision(self) -> Revision {
+        self.revision
+    }
 }
 
-/// Read an existing timestamped [`Manifest`], if it exists and is up-to-date.
+/// A pointer to a source distribution revision in the cache, fetched from a local path.
 ///
-/// If the cache entry is stale, a new entry will be created.
-pub(crate) async fn refresh_timestamp_manifest(
-    cache_entry: &CacheEntry,
-    freshness: Freshness,
-    modified: ArchiveTimestamp,
-) -> Result<Manifest, Error> {
-    // If we know the exact modification time, we don't need to force a revalidate.
-    if matches!(modified, ArchiveTimestamp::Exact(_)) || freshness.is_fresh() {
-        if let Some(manifest) = read_timestamp_manifest(cache_entry, modified)? {
-            return Ok(manifest);
+/// Encoded with `MsgPack`, and represented on disk by a `.rev` file.
+#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
+pub(crate) struct LocalRevisionPointer {
+    timestamp: Timestamp,
+    revision: Revision,
+}
+
+impl LocalRevisionPointer {
+    /// Read an [`LocalRevisionPointer`] from the cache.
+    pub(crate) fn read_from(path: impl AsRef<Path>) -> Result<Option<Self>, Error> {
+        match fs_err::read(path) {
+            Ok(cached) => Ok(Some(rmp_serde::from_slice::<LocalRevisionPointer>(
+                &cached,
+            )?)),
+            Err(err) if err.kind() == std::io::ErrorKind::NotFound => Ok(None),
+            Err(err) => Err(Error::CacheRead(err)),
         }
     }
 
-    // Otherwise, create a new manifest.
-    let manifest = Manifest::new();
-    fs::create_dir_all(&cache_entry.dir())
-        .await
-        .map_err(Error::CacheWrite)?;
-    write_atomic(
-        cache_entry.path(),
-        rmp_serde::to_vec(&CachedByTimestamp {
-            timestamp: modified.timestamp(),
-            data: manifest.clone(),
-        })?,
-    )
-    .await
-    .map_err(Error::CacheWrite)?;
-    Ok(manifest)
+    /// Write an [`LocalRevisionPointer`] to the cache.
+    async fn write_to(&self, entry: &CacheEntry) -> Result<(), Error> {
+        fs::create_dir_all(&entry.dir())
+            .await
+            .map_err(Error::CacheWrite)?;
+        write_atomic(entry.path(), rmp_serde::to_vec(&self)?)
+            .await
+            .map_err(Error::CacheWrite)
+    }
+
+    /// Returns `true` if the revision is up-to-date with the given modified timestamp.
+    pub(crate) fn is_up_to_date(&self, modified: ArchiveTimestamp) -> bool {
+        self.timestamp == modified.timestamp()
+    }
+
+    /// Return the [`Revision`] from the pointer.
+    pub(crate) fn into_revision(self) -> Revision {
+        self.revision
+    }
 }
 
-/// Read an existing cached [`Metadata21`], if it exists.
-pub(crate) async fn read_cached_metadata(
-    cache_entry: &CacheEntry,
-) -> Result<Option<Metadata21>, Error> {
+/// Read the [`Metadata23`] from a source distribution's `PKG-INFO` file, if it uses Metadata 2.2
+/// or later _and_ none of the required fields (`Requires-Python`, `Requires-Dist`, and
+/// `Provides-Extra`) are marked as dynamic.
+async fn read_pkg_info(
+    source_tree: &Path,
+    subdirectory: Option<&Path>,
+) -> Result<Metadata23, Error> {
+    // Read the `PKG-INFO` file.
+    let pkg_info = match subdirectory {
+        Some(subdirectory) => source_tree.join(subdirectory).join("PKG-INFO"),
+        None => source_tree.join("PKG-INFO"),
+    };
+    let content = match fs::read(pkg_info).await {
+        Ok(content) => content,
+        Err(err) if err.kind() == std::io::ErrorKind::NotFound => {
+            return Err(Error::MissingPkgInfo);
+        }
+        Err(err) => return Err(Error::CacheRead(err)),
+    };
+
+    // Parse the metadata.
+    let metadata = Metadata23::parse_pkg_info(&content).map_err(Error::DynamicPkgInfo)?;
+
+    Ok(metadata)
+}
+
+/// Read the [`Metadata23`] from a source distribution's `pyproject.toml` file, if it defines static
+/// metadata consistent with PEP 621.
+async fn read_pyproject_toml(
+    source_tree: &Path,
+    subdirectory: Option<&Path>,
+) -> Result<Metadata23, Error> {
+    // Read the `pyproject.toml` file.
+    let pyproject_toml = match subdirectory {
+        Some(subdirectory) => source_tree.join(subdirectory).join("pyproject.toml"),
+        None => source_tree.join("pyproject.toml"),
+    };
+    let content = match fs::read_to_string(pyproject_toml).await {
+        Ok(content) => content,
+        Err(err) if err.kind() == std::io::ErrorKind::NotFound => {
+            return Err(Error::MissingPyprojectToml);
+        }
+        Err(err) => return Err(Error::CacheRead(err)),
+    };
+
+    // Parse the metadata.
+    let metadata =
+        Metadata23::parse_pyproject_toml(&content).map_err(Error::DynamicPyprojectToml)?;
+
+    Ok(metadata)
+}
+
+/// Read an existing cached [`Metadata23`], if it exists.
+async fn read_cached_metadata(cache_entry: &CacheEntry) -> Result<Option<Metadata23>, Error> {
     match fs::read(&cache_entry.path()).await {
-        Ok(cached) => Ok(Some(rmp_serde::from_slice::<Metadata21>(&cached)?)),
+        Ok(cached) => Ok(Some(rmp_serde::from_slice::<Metadata23>(&cached)?)),
         Err(err) if err.kind() == std::io::ErrorKind::NotFound => Ok(None),
         Err(err) => Err(Error::CacheRead(err)),
     }
 }
 
-/// Read the [`Metadata21`] from a built wheel.
+/// Read the [`Metadata23`] from a built wheel.
 fn read_wheel_metadata(
     filename: &WheelFilename,
     wheel: impl Into<PathBuf>,
-) -> Result<Metadata21, Error> {
+) -> Result<Metadata23, Error> {
     let file = fs_err::File::open(wheel).map_err(Error::CacheRead)?;
     let reader = std::io::BufReader::new(file);
     let mut archive = ZipArchive::new(reader)?;
-    let dist_info = read_dist_info(filename, &mut archive)?;
-    Ok(Metadata21::parse(&dist_info)?)
+    let dist_info = read_archive_metadata(filename, &mut archive)?;
+    Ok(Metadata23::parse_metadata(&dist_info)?)
+}
+
+/// Apply an advisory lock to a [`CacheShard`] to prevent concurrent builds.
+async fn lock_shard(cache_shard: &CacheShard) -> Result<LockedFile, Error> {
+    let root = cache_shard.as_ref();
+
+    fs_err::create_dir_all(root).map_err(Error::CacheWrite)?;
+
+    let lock: LockedFile = tokio::task::spawn_blocking({
+        let root = root.to_path_buf();
+        move || LockedFile::acquire(root.join(".lock"), root.display())
+    })
+    .await?
+    .map_err(Error::CacheWrite)?;
+
+    Ok(lock)
 }
```

### Comparing `uv-0.1.9/crates/uv-normalize/src/extra_name.rs` & `uv-0.2.0/crates/uv-normalize/src/extra_name.rs`

 * *Files 7% similar despite different names*

```diff
@@ -1,25 +1,25 @@
-#[cfg(feature = "serde")]
-use serde::{Deserialize, Deserializer, Serialize};
 use std::fmt;
 use std::fmt::{Display, Formatter};
 use std::str::FromStr;
 
+use serde::{Deserialize, Deserializer, Serialize};
+
 use crate::{validate_and_normalize_owned, validate_and_normalize_ref, InvalidNameError};
 
 /// The normalized name of an extra dependency group.
 ///
-/// Converts the name to lowercase and collapses any run of the characters `-`, `_` and `.`
-/// down to a single `-`, e.g., `---`, `.`, and `__` all get converted to just `-`.
+/// Converts the name to lowercase and collapses runs of `-`, `_`, and `.` down to a single `-`.
+/// For example, `---`, `.`, and `__` are all converted to a single `-`.
 ///
 /// See:
 /// - <https://peps.python.org/pep-0685/#specification/>
 /// - <https://packaging.python.org/en/latest/specifications/name-normalization/>
-#[cfg_attr(feature = "serde", derive(Serialize))]
-#[derive(Debug, Clone, PartialEq, Eq, Hash, PartialOrd, Ord)]
+#[derive(Debug, Clone, PartialEq, Eq, Hash, PartialOrd, Ord, Serialize)]
+#[cfg_attr(feature = "schemars", derive(schemars::JsonSchema))]
 pub struct ExtraName(String);
 
 impl ExtraName {
     /// Create a validated, normalized extra name.
     pub fn new(name: String) -> Result<Self, InvalidNameError> {
         validate_and_normalize_owned(name).map(Self)
     }
@@ -29,15 +29,14 @@
     type Err = InvalidNameError;
 
     fn from_str(name: &str) -> Result<Self, Self::Err> {
         validate_and_normalize_ref(name).map(Self)
     }
 }
 
-#[cfg(feature = "serde")]
 impl<'de> Deserialize<'de> for ExtraName {
     fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
     where
         D: Deserializer<'de>,
     {
         let s = String::deserialize(deserializer)?;
         Self::from_str(&s).map_err(serde::de::Error::custom)
```

### Comparing `uv-0.1.9/crates/uv-normalize/src/lib.rs` & `uv-0.2.0/crates/uv-normalize/src/lib.rs`

 * *Files 4% similar despite different names*

```diff
@@ -31,25 +31,25 @@
             b'a'..=b'z' | b'0'..=b'9' => {
                 normalized.push(char as char);
             }
             b'-' | b'_' | b'.' => {
                 match last {
                     // Names can't start with punctuation.
                     None => return Err(InvalidNameError(name.as_ref().to_string())),
-                    Some(b'-') | Some(b'_') | Some(b'.') => {}
+                    Some(b'-' | b'_' | b'.') => {}
                     Some(_) => normalized.push('-'),
                 }
             }
             _ => return Err(InvalidNameError(name.as_ref().to_string())),
         }
         last = Some(char);
     }
 
     // Names can't end with punctuation.
-    if matches!(last, Some(b'-') | Some(b'_') | Some(b'.')) {
+    if matches!(last, Some(b'-' | b'_' | b'.')) {
         return Err(InvalidNameError(name.as_ref().to_string()));
     }
 
     Ok(normalized)
 }
 
 /// Returns `true` if the name is already normalized.
@@ -79,15 +79,15 @@
             }
             _ => return Err(InvalidNameError(name.as_ref().to_string())),
         }
         last = Some(char);
     }
 
     // Names can't end with punctuation.
-    if matches!(last, Some(b'-') | Some(b'_') | Some(b'.')) {
+    if matches!(last, Some(b'-' | b'_' | b'.')) {
         return Err(InvalidNameError(name.as_ref().to_string()));
     }
 
     Ok(true)
 }
 
 /// Invalid [`crate::PackageName`] or [`crate::ExtraName`].
@@ -139,27 +139,27 @@
         }
     }
 
     #[test]
     fn check() {
         let inputs = ["friendly-bard", "friendlybard"];
         for input in inputs {
-            assert!(is_normalized(input).unwrap(), "{:?}", input);
+            assert!(is_normalized(input).unwrap(), "{input:?}");
         }
 
         let inputs = [
             "friendly.bard",
             "friendly.BARD",
             "friendly_bard",
             "friendly--bard",
             "friendly-.bard",
             "FrIeNdLy-._.-bArD",
         ];
         for input in inputs {
-            assert!(!is_normalized(input).unwrap(), "{:?}", input);
+            assert!(!is_normalized(input).unwrap(), "{input:?}");
         }
     }
 
     #[test]
     fn unchanged() {
         // Unchanged
         let unchanged = ["friendly-bard", "1okay", "okay2"];
```

### Comparing `uv-0.1.9/crates/uv-normalize/src/package_name.rs` & `uv-0.2.0/crates/uv-normalize/src/package_name.rs`

 * *Files 14% similar despite different names*

```diff
@@ -1,29 +1,36 @@
 use std::borrow::Cow;
 use std::str::FromStr;
 
-#[cfg(feature = "serde")]
 use serde::{Deserialize, Deserializer, Serialize};
 
 use crate::{validate_and_normalize_owned, validate_and_normalize_ref, InvalidNameError};
 
 /// The normalized name of a package.
 ///
-/// Converts the name to lowercase and collapses any run of the characters `-`, `_` and `.`
-/// down to a single `-`, e.g., `---`, `.`, and `__` all get converted to just `-`.
+/// Converts the name to lowercase and collapses runs of `-`, `_`, and `.` down to a single `-`.
+/// For example, `---`, `.`, and `__` are all converted to a single `-`.
 ///
 /// See: <https://packaging.python.org/en/latest/specifications/name-normalization/>
-#[cfg_attr(feature = "serde", derive(Serialize))]
-#[cfg_attr(
-    feature = "rkyv",
-    derive(rkyv::Archive, rkyv::Deserialize, rkyv::Serialize),
-    archive(check_bytes),
-    archive_attr(derive(Debug))
+#[derive(
+    Debug,
+    Clone,
+    PartialEq,
+    Eq,
+    Hash,
+    PartialOrd,
+    Ord,
+    Serialize,
+    rkyv::Archive,
+    rkyv::Deserialize,
+    rkyv::Serialize,
 )]
-#[derive(Debug, Clone, PartialEq, Eq, Hash, PartialOrd, Ord)]
+#[cfg_attr(feature = "schemars", derive(schemars::JsonSchema))]
+#[archive(check_bytes)]
+#[archive_attr(derive(Debug))]
 pub struct PackageName(String);
 
 impl PackageName {
     /// Create a validated, normalized package name.
     pub fn new(name: String) -> Result<Self, InvalidNameError> {
         validate_and_normalize_owned(name).map(Self)
     }
@@ -50,30 +57,29 @@
             Cow::Owned(owned_string)
         } else {
             Cow::Borrowed(self.0.as_str())
         }
     }
 }
 
-impl From<&PackageName> for PackageName {
+impl From<&Self> for PackageName {
     /// Required for `WaitMap::wait`.
-    fn from(package_name: &PackageName) -> Self {
+    fn from(package_name: &Self) -> Self {
         package_name.clone()
     }
 }
 
 impl FromStr for PackageName {
     type Err = InvalidNameError;
 
     fn from_str(name: &str) -> Result<Self, Self::Err> {
         validate_and_normalize_ref(name).map(Self)
     }
 }
 
-#[cfg(feature = "serde")]
 impl<'de> Deserialize<'de> for PackageName {
     fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
     where
         D: Deserializer<'de>,
     {
         let s = String::deserialize(deserializer)?;
         Self::from_str(&s).map_err(serde::de::Error::custom)
```

### Comparing `uv-0.1.9/crates/uv-git/Cargo.toml` & `uv-0.2.0/crates/uv-git/Cargo.toml`

 * *Files 13% similar despite different names*

```diff
@@ -9,30 +9,28 @@
 authors = { workspace = true }
 license = { workspace = true }
 
 [lints]
 workspace = true
 
 [dependencies]
-cache-key = { path = "../cache-key" }
-uv-fs = { path = "../uv-fs" }
+cache-key = { workspace = true }
+uv-fs = { workspace = true }
 
 anyhow = { workspace = true }
 base64 = { workspace = true }
 cargo-util = { workspace = true }
 git2 = { workspace = true }
 glob = { workspace = true }
-hex = { workspace = true }
 hmac = { workspace = true }
 home = { workspace = true }
-once_cell = { workspace = true }
 rand = { workspace = true }
 reqwest = { workspace = true, features = ["blocking"] }
-serde = { workspace = true }
 sha1 = { workspace = true }
 tokio = { workspace = true }
 tracing = { workspace = true }
 url = { workspace = true }
+fs-err = { workspace = true }
 
 [features]
 vendored-libgit2 = ["git2/vendored-libgit2"]
 vendored-openssl = ["git2/vendored-openssl"]
```

### Comparing `uv-0.1.9/crates/uv-git/src/git.rs` & `uv-0.2.0/crates/uv-git/src/git.rs`

 * *Files 6% similar despite different names*

```diff
@@ -4,94 +4,113 @@
 use std::borrow::Cow;
 use std::path::{Path, PathBuf};
 use std::process::Command;
 use std::{env, str};
 
 use anyhow::{anyhow, Context, Result};
 use cargo_util::{paths, ProcessBuilder};
-use git2::{self, ErrorClass, ObjectType};
+use git2::{ErrorClass, ObjectType};
 use reqwest::Client;
 use reqwest::StatusCode;
 use tracing::{debug, warn};
 use url::Url;
-use uv_fs::Normalized;
+use uv_fs::Simplified;
 
 use crate::util::retry;
 use crate::FetchStrategy;
 
 /// A file indicates that if present, `git reset` has been done and a repo
 /// checkout is ready to go. See [`GitCheckout::reset`] for why we need this.
 const CHECKOUT_READY_LOCK: &str = ".ok";
 
 /// A reference to commit or commit-ish.
-#[derive(Debug, Clone)]
-pub(crate) enum GitReference {
-    /// From a branch.
-    #[allow(unused)]
+#[derive(Debug, Clone, PartialEq, Eq, Hash, PartialOrd, Ord)]
+pub enum GitReference {
+    /// A specific branch.
     Branch(String),
-    /// From a tag.
-    #[allow(unused)]
+    /// A specific tag.
     Tag(String),
+    /// A specific (short) commit.
+    ShortCommit(String),
     /// From a reference that's ambiguously a branch or tag.
     BranchOrTag(String),
+    /// From a reference that's ambiguously a short commit, a branch, or a tag.
+    BranchOrTagOrCommit(String),
+    /// From a named reference, like `refs/pull/493/head`.
+    NamedRef(String),
     /// From a specific revision, using a full 40-character commit hash.
     FullCommit(String),
-    /// From a truncated revision.
-    ShortCommit(String),
-    /// From a named reference, like `refs/pull/493/head`.
-    Ref(String),
     /// The default branch of the repository, the reference named `HEAD`.
     DefaultBranch,
 }
 
 /// Strategy when fetching refspecs for a [`GitReference`]
 enum RefspecStrategy {
     // All refspecs should be fetched, if any fail then the fetch will fail
     All,
-    // Stop after the first successful fetch, if none suceed then the fetch will fail
+    // Stop after the first successful fetch, if none succeed then the fetch will fail
     First,
 }
 
 impl GitReference {
-    pub(crate) fn from_rev(rev: &str) -> Self {
+    /// Creates a [`GitReference`] from an arbitrary revision string, which could represent a
+    /// branch, tag, commit, or named ref.
+    pub fn from_rev(rev: String) -> Self {
         if rev.starts_with("refs/") {
-            Self::Ref(rev.to_owned())
-        } else if looks_like_commit_hash(rev) {
+            Self::NamedRef(rev)
+        } else if looks_like_commit_hash(&rev) {
             if rev.len() == 40 {
-                Self::FullCommit(rev.to_owned())
+                Self::FullCommit(rev)
             } else {
-                Self::ShortCommit(rev.to_owned())
+                Self::BranchOrTagOrCommit(rev)
             }
         } else {
-            Self::BranchOrTag(rev.to_owned())
+            Self::BranchOrTag(rev)
         }
     }
 
-    /// Views the short ID as a `str`.
-    pub(crate) fn as_str(&self) -> &str {
+    /// Converts the [`GitReference`] to a `str`.
+    pub fn as_str(&self) -> Option<&str> {
+        match self {
+            Self::Tag(rev) => Some(rev),
+            Self::Branch(rev) => Some(rev),
+            Self::ShortCommit(rev) => Some(rev),
+            Self::BranchOrTag(rev) => Some(rev),
+            Self::BranchOrTagOrCommit(rev) => Some(rev),
+            Self::FullCommit(rev) => Some(rev),
+            Self::NamedRef(rev) => Some(rev),
+            Self::DefaultBranch => None,
+        }
+    }
+
+    /// Converts the [`GitReference`] to a `str` that can be used as a revision.
+    pub(crate) fn as_rev(&self) -> &str {
         match self {
-            GitReference::Branch(rev)
-            | GitReference::Tag(rev)
-            | GitReference::BranchOrTag(rev)
-            | GitReference::FullCommit(rev)
-            | GitReference::ShortCommit(rev)
-            | GitReference::Ref(rev) => rev,
-            GitReference::DefaultBranch => "HEAD",
+            Self::Tag(rev) => rev,
+            Self::Branch(rev) => rev,
+            Self::ShortCommit(rev) => rev,
+            Self::BranchOrTag(rev) => rev,
+            Self::BranchOrTagOrCommit(rev) => rev,
+            Self::FullCommit(rev) => rev,
+            Self::NamedRef(rev) => rev,
+            Self::DefaultBranch => "HEAD",
         }
     }
 
+    /// Returns the kind of this reference.
     pub(crate) fn kind_str(&self) -> &str {
         match self {
-            GitReference::Branch(_) => "branch",
-            GitReference::Tag(_) => "tag",
-            GitReference::BranchOrTag(_) => "branch or tag",
-            GitReference::FullCommit(_) => "commit",
-            GitReference::ShortCommit(_) => "short commit",
-            GitReference::Ref(_) => "ref",
-            GitReference::DefaultBranch => "default branch",
+            Self::Branch(_) => "branch",
+            Self::Tag(_) => "tag",
+            Self::ShortCommit(_) => "short commit",
+            Self::BranchOrTag(_) => "branch or tag",
+            Self::FullCommit(_) => "commit",
+            Self::BranchOrTagOrCommit(_) => "branch, tag, or commit",
+            Self::NamedRef(_) => "ref",
+            Self::DefaultBranch => "default branch",
         }
     }
 }
 
 /// A short abbreviated OID.
 ///
 /// Exists for avoiding extra allocations in [`GitDatabase::to_short_id`].
@@ -132,16 +151,16 @@
     revision: git2::Oid,
     /// Underlying Git repository instance for this checkout.
     repo: git2::Repository,
 }
 
 impl GitRemote {
     /// Creates an instance for a remote repository URL.
-    pub(crate) fn new(url: &Url) -> GitRemote {
-        GitRemote { url: url.clone() }
+    pub(crate) fn new(url: &Url) -> Self {
+        Self { url: url.clone() }
     }
 
     /// Gets the remote repository URL.
     pub(crate) fn url(&self) -> &Url {
         &self.url
     }
 
@@ -166,15 +185,15 @@
         strategy: FetchStrategy,
         client: &Client,
     ) -> Result<(GitDatabase, git2::Oid)> {
         let locked_ref = locked_rev.map(|oid| GitReference::FullCommit(oid.to_string()));
         let reference = locked_ref.as_ref().unwrap_or(reference);
         if let Some(mut db) = db {
             fetch(&mut db.repo, self.url.as_str(), reference, strategy, client)
-                .with_context(|| format!("failed to fetch into: {}", into.normalized_display()))?;
+                .with_context(|| format!("failed to fetch into: {}", into.user_display()))?;
 
             let resolved_commit_hash = match locked_rev {
                 Some(rev) => db.contains(rev).then_some(rev),
                 None => reference.resolve(&db.repo).ok(),
             };
             if let Some(rev) = resolved_commit_hash {
                 return Ok((db, rev));
@@ -186,15 +205,15 @@
         // resolution to figure out what we cloned.
         if into.exists() {
             paths::remove_dir_all(into)?;
         }
         paths::create_dir_all(into)?;
         let mut repo = init(into, true)?;
         fetch(&mut repo, self.url.as_str(), reference, strategy, client)
-            .with_context(|| format!("failed to clone into: {}", into.normalized_display()))?;
+            .with_context(|| format!("failed to clone into: {}", into.user_display()))?;
         let rev = match locked_rev {
             Some(rev) => rev,
             None => reference.resolve(&repo)?,
         };
 
         Ok((
             GitDatabase {
@@ -255,62 +274,92 @@
 }
 
 impl GitReference {
     /// Resolves self to an object ID with objects the `repo` currently has.
     pub(crate) fn resolve(&self, repo: &git2::Repository) -> Result<git2::Oid> {
         let refkind = self.kind_str();
         let id = match self {
-            // Note that we resolve the named tag here in sync with where it's
-            // fetched into via `fetch` below.
-            GitReference::Tag(s) => (|| -> Result<git2::Oid> {
+            Self::Tag(s) => (|| -> Result<git2::Oid> {
                 let refname = format!("refs/remotes/origin/tags/{s}");
                 let id = repo.refname_to_id(&refname)?;
                 let obj = repo.find_object(id, None)?;
                 let obj = obj.peel(ObjectType::Commit)?;
                 Ok(obj.id())
             })()
-            .with_context(|| format!("failed to find {refkind} `{s}`"))?,
+            .with_context(|| format!("failed to find tag `{s}`"))?,
 
-            // Resolve the remote name since that's all we're configuring in
-            // `fetch` below.
-            GitReference::Branch(s) => {
+            Self::Branch(s) => {
                 let name = format!("origin/{s}");
-                let b = repo
-                    .find_branch(&name, git2::BranchType::Remote)
-                    .with_context(|| format!("failed to find {refkind} `{s}`"))?;
-                b.get()
-                    .target()
-                    .ok_or_else(|| anyhow::format_err!("{refkind} `{s}` did not have a target"))?
+
+                // Resolve the remote name since that's all we're configuring in
+                // `fetch` below.
+                repo.find_branch(&name, git2::BranchType::Remote)
+                    .ok()
+                    .and_then(|b| b.get().target())
+                    .ok_or_else(|| anyhow::format_err!("failed to find {refkind} `{s}`"))?
             }
 
             // Attempt to resolve the branch, then the tag.
-            GitReference::BranchOrTag(s) => {
+            Self::BranchOrTag(s) => {
                 let name = format!("origin/{s}");
 
+                // Resolve the remote name since that's all we're configuring in
+                // `fetch` below.
                 repo.find_branch(&name, git2::BranchType::Remote)
                     .ok()
                     .and_then(|b| b.get().target())
                     .or_else(|| {
+                        // Note that we resolve the named tag here in sync with where it's
+                        // fetched into via `fetch` below.
                         let refname = format!("refs/remotes/origin/tags/{s}");
                         let id = repo.refname_to_id(&refname).ok()?;
                         let obj = repo.find_object(id, None).ok()?;
                         let obj = obj.peel(ObjectType::Commit).ok()?;
                         Some(obj.id())
                     })
                     .ok_or_else(|| anyhow::format_err!("failed to find {refkind} `{s}`"))?
             }
 
-            // We'll be using the HEAD commit
-            GitReference::DefaultBranch => {
+            // Attempt to resolve the branch, then the tag, then the commit.
+            Self::BranchOrTagOrCommit(s) => {
+                let name = format!("origin/{s}");
+
+                // Resolve the remote name since that's all we're configuring in
+                // `fetch` below.
+                repo.find_branch(&name, git2::BranchType::Remote)
+                    .ok()
+                    .and_then(|b| b.get().target())
+                    .or_else(|| {
+                        // Note that we resolve the named tag here in sync with where it's
+                        // fetched into via `fetch` below.
+                        let refname = format!("refs/remotes/origin/tags/{s}");
+                        let id = repo.refname_to_id(&refname).ok()?;
+                        let obj = repo.find_object(id, None).ok()?;
+                        let obj = obj.peel(ObjectType::Commit).ok()?;
+                        Some(obj.id())
+                    })
+                    .or_else(|| {
+                        // Resolve the commit.
+                        let obj = repo.revparse_single(s).ok()?;
+                        match obj.as_tag() {
+                            Some(tag) => Some(tag.target_id()),
+                            None => Some(obj.id()),
+                        }
+                    })
+                    .ok_or_else(|| anyhow::format_err!("failed to find {refkind} `{s}`"))?
+            }
+
+            // We'll be using the HEAD commit.
+            Self::DefaultBranch => {
                 let head_id = repo.refname_to_id("refs/remotes/origin/HEAD")?;
                 let head = repo.find_object(head_id, None)?;
                 head.peel(ObjectType::Commit)?.id()
             }
 
-            GitReference::FullCommit(s) | GitReference::ShortCommit(s) | GitReference::Ref(s) => {
+            Self::FullCommit(s) | Self::ShortCommit(s) | Self::NamedRef(s) => {
                 let obj = repo.revparse_single(s)?;
                 match obj.as_tag() {
                     Some(tag) => tag.target_id(),
                     None => obj.id(),
                 }
             }
         };
@@ -384,15 +433,15 @@
             // `git2` doesn't seem to handle shallow repos correctly when doing
             // a local clone. Fortunately all that's needed is the copy of the
             // one file that defines the shallow boundary, the commits which
             // have their parents omitted as part of the shallow clone.
             //
             // TODO(git2): remove this when git2 supports shallow clone correctly
             if database.repo.is_shallow() {
-                std::fs::copy(
+                fs_err::copy(
                     database.repo.path().join("shallow"),
                     r.path().join("shallow"),
                 )?;
             }
             repo = Some(r);
             Ok(())
         })?;
@@ -894,15 +943,15 @@
         })?;
         Ok(())
     })
 }
 
 /// Attempts to fetch the given git `reference` for a Git repository.
 ///
-/// This is the main entry for git clone/fetch. It does the followings:
+/// This is the main entry for git clone/fetch. It does the following:
 ///
 /// * Turns [`GitReference`] into refspecs accordingly.
 /// * Dispatches `git fetch` using libgit2 or git CLI.
 ///
 /// The `remote_url` argument is the git remote URL where we want to fetch from.
 pub(crate) fn fetch(
     repo: &mut git2::Repository,
@@ -950,19 +999,40 @@
             ));
             refspecs.push(format!(
                 "+refs/tags/{branch_or_tag}:refs/remotes/origin/tags/{branch_or_tag}"
             ));
             refspec_strategy = RefspecStrategy::First;
         }
 
+        // For ambiguous references, we can fetch the exact commit (if known); otherwise,
+        // we fetch all branches and tags.
+        GitReference::ShortCommit(branch_or_tag_or_commit)
+        | GitReference::BranchOrTagOrCommit(branch_or_tag_or_commit) => {
+            // The `oid_to_fetch` is the exact commit we want to fetch. But it could be the exact
+            // commit of a branch or tag. We should only fetch it directly if it's the exact commit
+            // of a short commit hash.
+            if let Some(oid_to_fetch) =
+                oid_to_fetch.filter(|oid| is_short_hash_of(branch_or_tag_or_commit, *oid))
+            {
+                refspecs.push(format!("+{oid_to_fetch}:refs/commit/{oid_to_fetch}"));
+            } else {
+                // We don't know what the rev will point to. To handle this
+                // situation we fetch all branches and tags, and then we pray
+                // it's somewhere in there.
+                refspecs.push(String::from("+refs/heads/*:refs/remotes/origin/*"));
+                refspecs.push(String::from("+HEAD:refs/remotes/origin/HEAD"));
+                tags = true;
+            }
+        }
+
         GitReference::DefaultBranch => {
             refspecs.push(String::from("+HEAD:refs/remotes/origin/HEAD"));
         }
 
-        GitReference::Ref(rev) => {
+        GitReference::NamedRef(rev) => {
             refspecs.push(format!("+{rev}:{rev}"));
         }
 
         GitReference::FullCommit(rev) => {
             if let Some(oid_to_fetch) = oid_to_fetch {
                 refspecs.push(format!("+{oid_to_fetch}:refs/commit/{oid_to_fetch}"));
             } else {
@@ -971,41 +1041,32 @@
                 // Note that with typical settings for shallowing, we will just fetch a single `rev`
                 // as single commit.
                 // The reason we write to `refs/remotes/origin/HEAD` is that it's of special significance
                 // when during `GitReference::resolve()`, but otherwise it shouldn't matter.
                 refspecs.push(format!("+{rev}:refs/remotes/origin/HEAD"));
             }
         }
-
-        GitReference::ShortCommit(_) => {
-            if let Some(oid_to_fetch) = oid_to_fetch {
-                refspecs.push(format!("+{oid_to_fetch}:refs/commit/{oid_to_fetch}"));
-            } else {
-                // We don't know what the rev will point to. To handle this
-                // situation we fetch all branches and tags, and then we pray
-                // it's somewhere in there.
-                refspecs.push(String::from("+refs/heads/*:refs/remotes/origin/*"));
-                refspecs.push(String::from("+HEAD:refs/remotes/origin/HEAD"));
-                tags = true;
-            }
-        }
     }
 
     debug!("Performing a Git fetch for: {remote_url}");
     match strategy {
         FetchStrategy::Cli => {
             let result = match refspec_strategy {
                 RefspecStrategy::All => fetch_with_cli(repo, remote_url, refspecs.as_slice(), tags),
                 RefspecStrategy::First => {
                     // Try each refspec
                     let mut errors = refspecs
                         .iter()
                         .map_while(|refspec| {
-                            let fetch_result =
-                                fetch_with_cli(repo, remote_url, &[refspec.clone()], tags);
+                            let fetch_result = fetch_with_cli(
+                                repo,
+                                remote_url,
+                                std::slice::from_ref(refspec),
+                                tags,
+                            );
 
                             // Stop after the first success and log failures
                             match fetch_result {
                                 Err(ref err) => {
                                     debug!("failed to fetch refspec `{refspec}`: {err}");
                                     Some(fetch_result)
                                 }
@@ -1030,15 +1091,15 @@
             match reference {
                 // With the default branch, adding context is confusing
                 GitReference::DefaultBranch => result,
                 _ => result.with_context(|| {
                     format!(
                         "failed to fetch {} `{}`",
                         reference.kind_str(),
-                        reference.as_str()
+                        reference.as_rev()
                     )
                 }),
             }
         }
         FetchStrategy::Libgit2 => {
             // Libgit2 does not fail if a refspec is missing, so the `refspec_strategy`
             // is not handled here
@@ -1313,44 +1374,41 @@
 
     let local_object = reference.resolve(repo).ok();
     let github_branch_name = match reference {
         GitReference::Branch(branch) => branch,
         GitReference::Tag(tag) => tag,
         GitReference::BranchOrTag(branch_or_tag) => branch_or_tag,
         GitReference::DefaultBranch => "HEAD",
-        GitReference::Ref(rev) => rev,
-        GitReference::FullCommit(rev) | GitReference::ShortCommit(rev) => {
-            if looks_like_commit_hash(rev) {
-                // `revparse_single` (used by `resolve`) is the only way to turn
-                // short hash -> long hash, but it also parses other things,
-                // like branch and tag names, which might coincidentally be
-                // valid hex.
-                //
-                // We only return early if `rev` is a prefix of the object found
-                // by `revparse_single`. Don't bother talking to GitHub in that
-                // case, since commit hashes are permanent. If a commit with the
-                // requested hash is already present in the local clone, its
-                // contents must be the same as what is on the server for that
-                // hash.
-                //
-                // If `rev` is not found locally by `revparse_single`, we'll
-                // need GitHub to resolve it and get a hash. If `rev` is found
-                // but is not a short hash of the found object, it's probably a
-                // branch and we also need to get a hash from GitHub, in case
-                // the branch has moved.
-                if let Some(local_object) = local_object {
-                    if is_short_hash_of(rev, local_object) {
-                        return Ok(FastPathRev::UpToDate);
-                    }
+        GitReference::NamedRef(rev) => rev,
+        GitReference::FullCommit(rev)
+        | GitReference::ShortCommit(rev)
+        | GitReference::BranchOrTagOrCommit(rev) => {
+            // `revparse_single` (used by `resolve`) is the only way to turn
+            // short hash -> long hash, but it also parses other things,
+            // like branch and tag names, which might coincidentally be
+            // valid hex.
+            //
+            // We only return early if `rev` is a prefix of the object found
+            // by `revparse_single`. Don't bother talking to GitHub in that
+            // case, since commit hashes are permanent. If a commit with the
+            // requested hash is already present in the local clone, its
+            // contents must be the same as what is on the server for that
+            // hash.
+            //
+            // If `rev` is not found locally by `revparse_single`, we'll
+            // need GitHub to resolve it and get a hash. If `rev` is found
+            // but is not a short hash of the found object, it's probably a
+            // branch and we also need to get a hash from GitHub, in case
+            // the branch has moved.
+            if let Some(local_object) = local_object {
+                if is_short_hash_of(rev, local_object) {
+                    return Ok(FastPathRev::UpToDate);
                 }
-                rev
-            } else {
-                debug!("can't use github fast path with `rev = \"{}\"`", rev);
-                return Ok(FastPathRev::Indeterminate);
             }
+            rev
         }
     };
 
     // This expects GitHub urls in the form `github.com/user/repo` and nothing
     // else
     let mut pieces = url
         .path_segments()
```

### Comparing `uv-0.1.9/crates/uv-git/src/known_hosts.rs` & `uv-0.2.0/crates/uv-git/src/known_hosts.rs`

 * *Files 2% similar despite different names*

```diff
@@ -97,16 +97,16 @@
     HostHasOnlyCertAuthority {
         hostname: String,
         location: KnownHostLocation,
     },
 }
 
 impl From<anyhow::Error> for KnownHostError {
-    fn from(err: anyhow::Error) -> KnownHostError {
-        KnownHostError::CheckError(err)
+    fn from(err: anyhow::Error) -> Self {
+        Self::CheckError(err)
     }
 }
 
 /// The location where a host key was located.
 #[derive(Clone)]
 enum KnownHostLocation {
     /// Loaded from a file from disk.
@@ -114,18 +114,18 @@
     /// Part of the hard-coded bundled keys in Cargo.
     Bundled,
 }
 
 impl Display for KnownHostLocation {
     fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
         let loc = match self {
-            KnownHostLocation::File { path, lineno } => {
+            Self::File { path, lineno } => {
                 format!("{} line {lineno}", path.display())
             }
-            KnownHostLocation::Bundled => "bundled with cargo".to_string(),
+            Self::Bundled => "bundled with cargo".to_string(),
         };
         f.write_str(&loc)
     }
 }
 
 /// The git2 callback used to validate a certificate (only ssh known hosts are validated).
 pub(crate) fn certificate_check(
```

### Comparing `uv-0.1.9/crates/uv-git/src/lib.rs` & `uv-0.2.0/crates/uv-git/src/lib.rs`

 * *Files 12% similar despite different names*

```diff
@@ -1,64 +1,70 @@
 use std::str::FromStr;
 use url::Url;
 
-use crate::git::GitReference;
+pub use crate::git::GitReference;
 pub use crate::sha::GitSha;
 pub use crate::source::{Fetch, GitSource, Reporter};
 
 mod git;
 mod known_hosts;
 mod sha;
 mod source;
 mod util;
 
 /// A URL reference to a Git repository.
-#[derive(Debug, Clone)]
+#[derive(Debug, Clone, PartialEq, PartialOrd, Eq, Hash, Ord)]
 pub struct GitUrl {
-    /// The URL of the Git repository, with any query parameters and fragments removed.
+    /// The URL of the Git repository, with any query parameters, fragments, and leading `git+`
+    /// removed.
     repository: Url,
     /// The reference to the commit to use, which could be a branch, tag or revision.
     reference: GitReference,
     /// The precise commit to use, if known.
     precise: Option<GitSha>,
 }
 
 impl GitUrl {
+    pub fn new(repository: Url, reference: GitReference) -> Self {
+        Self {
+            repository,
+            reference,
+            precise: None,
+        }
+    }
+
     #[must_use]
-    pub(crate) fn with_precise(mut self, precise: GitSha) -> Self {
+    pub fn with_precise(mut self, precise: GitSha) -> Self {
         self.precise = Some(precise);
         self
     }
 
     /// Return the [`Url`] of the Git repository.
     pub fn repository(&self) -> &Url {
         &self.repository
     }
 
     /// Return the reference to the commit to use, which could be a branch, tag or revision.
-    pub fn reference(&self) -> Option<&str> {
-        match &self.reference {
-            GitReference::Branch(rev)
-            | GitReference::Tag(rev)
-            | GitReference::BranchOrTag(rev)
-            | GitReference::Ref(rev)
-            | GitReference::FullCommit(rev)
-            | GitReference::ShortCommit(rev) => Some(rev),
-            GitReference::DefaultBranch => None,
-        }
+    pub fn reference(&self) -> &GitReference {
+        &self.reference
+    }
+
+    /// Returns `true` if the reference is a full commit.
+    pub fn is_full_commit(&self) -> bool {
+        matches!(self.reference, GitReference::FullCommit(_))
     }
 
     /// Return the precise commit, if known.
     pub fn precise(&self) -> Option<GitSha> {
         self.precise
     }
 }
 
 impl TryFrom<Url> for GitUrl {
-    type Error = anyhow::Error;
+    type Error = git2::Error;
 
     /// Initialize a [`GitUrl`] source from a URL.
     fn try_from(mut url: Url) -> Result<Self, Self::Error> {
         // Remove any query parameters and fragments.
         url.set_fragment(None);
         url.set_query(None);
 
@@ -66,15 +72,15 @@
         // extract it.
         let mut reference = GitReference::DefaultBranch;
         if let Some((prefix, suffix)) = url
             .path()
             .rsplit_once('@')
             .map(|(prefix, suffix)| (prefix.to_string(), suffix.to_string()))
         {
-            reference = GitReference::from_rev(&suffix);
+            reference = GitReference::from_rev(suffix);
             url.set_path(&prefix);
         }
 
         let precise = if let GitReference::FullCommit(rev) = &reference {
             Some(GitSha::from_str(rev)?)
         } else {
             None
@@ -96,18 +102,19 @@
         if let Some(precise) = git.precise {
             url.set_path(&format!("{}@{}", url.path(), precise));
         } else {
             // Otherwise, add the branch or tag name.
             match git.reference {
                 GitReference::Branch(rev)
                 | GitReference::Tag(rev)
+                | GitReference::ShortCommit(rev)
                 | GitReference::BranchOrTag(rev)
-                | GitReference::Ref(rev)
+                | GitReference::NamedRef(rev)
                 | GitReference::FullCommit(rev)
-                | GitReference::ShortCommit(rev) => {
+                | GitReference::BranchOrTagOrCommit(rev) => {
                     url.set_path(&format!("{}@{}", url.path(), rev));
                 }
                 GitReference::DefaultBranch => {}
             }
         }
 
         url
```

### Comparing `uv-0.1.9/crates/uv-git/src/sha.rs` & `uv-0.2.0/crates/uv-git/src/sha.rs`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 use std::str::FromStr;
 
 /// A complete Git SHA, i.e., a 40-character hexadecimal representation of a Git commit.
-#[derive(Debug, Copy, Clone)]
+#[derive(Debug, Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Hash)]
 pub struct GitSha(git2::Oid);
 
 impl GitSha {
     /// Convert the SHA to a truncated representation, i.e., the first 16 characters of the SHA.
     pub fn to_short_string(&self) -> String {
         self.0.to_string()[0..16].to_string()
     }
```

### Comparing `uv-0.1.9/crates/uv-git/src/source.rs` & `uv-0.2.0/crates/uv-git/src/source.rs`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 //! Git support is derived from Cargo's implementation.
 //! Cargo is dual-licensed under either Apache 2.0 or MIT, at the user's choice.
 //! Source: <https://github.com/rust-lang/cargo/blob/23eb492cf920ce051abfc56bbaf838514dc8365c/src/cargo/sources/git/source.rs>
 use std::path::{Path, PathBuf};
 
 use anyhow::Result;
 use reqwest::Client;
-use tracing::debug;
+use tracing::{debug, instrument};
 use url::Url;
 
 use cache_key::{digest, RepositoryUrl};
 
 use crate::git::GitRemote;
 use crate::{FetchStrategy, GitSha, GitUrl};
 
@@ -45,14 +45,15 @@
         Self {
             reporter: Some(Box::new(reporter)),
             ..self
         }
     }
 
     /// Fetch the underlying Git repository at the given revision.
+    #[instrument(skip(self), fields(repository = %self.git.repository, rev = ?self.git.precise))]
     pub fn fetch(self) -> Result<Fetch> {
         // The path to the repo, within the Git database.
         let ident = digest(&RepositoryUrl::new(&self.git.repository));
         let db_path = self.cache.join("db").join(&ident);
 
         let remote = GitRemote::new(&self.git.repository);
         let (db, actual_rev, task) = match (self.git.precise, remote.db_at(&db_path).ok()) {
@@ -65,15 +66,15 @@
             // situation that we have a locked revision but the database
             // doesn't have it.
             (locked_rev, db) => {
                 debug!("Updating git source `{:?}`", self.git.repository);
 
                 // Report the checkout operation to the reporter.
                 let task = self.reporter.as_ref().map(|reporter| {
-                    reporter.on_checkout_start(remote.url(), self.git.reference.as_str())
+                    reporter.on_checkout_start(remote.url(), self.git.reference.as_rev())
                 });
 
                 let (db, actual_rev) = remote.checkout(
                     &db_path,
                     db,
                     &self.git.reference,
                     locked_rev.map(git2::Oid::from),
```

### Comparing `uv-0.1.9/crates/uv-git/src/util/errors.rs` & `uv-0.2.0/crates/uv-git/src/util/errors.rs`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/uv-git/src/util/mod.rs` & `uv-0.2.0/crates/uv-git/src/util/mod.rs`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/uv-git/src/util/retry.rs` & `uv-0.2.0/crates/uv-git/src/util/retry.rs`

 * *Files 2% similar despite different names*

```diff
@@ -90,16 +90,16 @@
 /// The maximum amount of additional time the initial retry will take (milliseconds).
 ///
 /// The initial delay will be [`INITIAL_RETRY_SLEEP_BASE_MS`] plus a random range
 /// from 0 to this value.
 const INITIAL_RETRY_JITTER_MS: u64 = 1000;
 
 impl Retry {
-    pub(crate) fn new() -> Retry {
-        Retry {
+    pub(crate) fn new() -> Self {
+        Self {
             retries: 0,
             max_retries: 3,
         }
     }
 
     /// Calls the given callback, and returns a [`RetryResult`] which
     /// indicates whether or not this needs to be called again at some point
```

### Comparing `uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-basic.txt.snap` & `uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-for-poetry.txt.snap`

 * *Files 26% similar despite different names*

```diff
@@ -1,152 +1,129 @@
 ---
 source: crates/requirements-txt/src/lib.rs
 expression: actual
 ---
 RequirementsTxt {
     requirements: [
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "numpy",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "1.24.2",
-                                },
-                            ],
-                        ),
-                    ),
-                ),
-                marker: None,
-            },
-            hashes: [],
-            editable: false,
-        },
-        RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "pandas",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "2.0.0",
-                                },
-                            ],
-                        ),
-                    ),
-                ),
-                marker: None,
-            },
-            hashes: [],
-            editable: false,
-        },
-        RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "python-dateutil",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "2.8.2",
-                                },
-                            ],
-                        ),
-                    ),
-                ),
-                marker: None,
-            },
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "inflection",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: Equal,
+                                        version: "0.5.1",
+                                    },
+                                ],
+                            ),
+                        ),
+                    ),
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/for-poetry.txt",
+                        ),
+                    ),
+                },
+            ),
             hashes: [],
-            editable: false,
         },
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "pytz",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "2023.3",
-                                },
-                            ],
-                        ),
-                    ),
-                ),
-                marker: None,
-            },
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "upsidedown",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: Equal,
+                                        version: "0.4",
+                                    },
+                                ],
+                            ),
+                        ),
+                    ),
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/for-poetry.txt",
+                        ),
+                    ),
+                },
+            ),
             hashes: [],
-            editable: false,
         },
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "six",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "1.16.0",
-                                },
-                            ],
-                        ),
-                    ),
-                ),
-                marker: None,
-            },
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "numpy",
+                    ),
+                    extras: [],
+                    version_or_url: None,
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/for-poetry.txt",
+                        ),
+                    ),
+                },
+            ),
             hashes: [],
-            editable: false,
         },
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "tzdata",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "2023.3",
-                                },
-                            ],
-                        ),
-                    ),
-                ),
-                marker: None,
-            },
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "pandas",
+                    ),
+                    extras: [
+                        ExtraName(
+                            "tabulate",
+                        ),
+                    ],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: GreaterThanEqual,
+                                        version: "1",
+                                    },
+                                    VersionSpecifier {
+                                        operator: LessThan,
+                                        version: "2",
+                                    },
+                                ],
+                            ),
+                        ),
+                    ),
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/for-poetry.txt",
+                        ),
+                    ),
+                },
+            ),
             hashes: [],
-            editable: false,
         },
     ],
     constraints: [],
     editables: [],
     index_url: None,
     extra_index_urls: [],
     find_links: [],
     no_index: false,
+    no_binary: None,
+    only_binary: None,
 }
```

### Comparing `uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-constraints-a.txt.snap` & `uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-constraints-b.txt.snap`

 * *Files 8% similar despite different names*

```diff
@@ -1,76 +1,74 @@
 ---
 source: crates/requirements-txt/src/lib.rs
 expression: actual
 ---
 RequirementsTxt {
     requirements: [
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "django-debug-toolbar",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: LessThan,
-                                    version: "2.2",
-                                },
-                            ],
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "django",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: Equal,
+                                        version: "2.1.15",
+                                    },
+                                ],
+                            ),
                         ),
                     ),
-                ),
-                marker: None,
-            },
-            hashes: [],
-            editable: false,
-        },
-    ],
-    constraints: [
-        Requirement {
-            name: PackageName(
-                "django",
-            ),
-            extras: [],
-            version_or_url: Some(
-                VersionSpecifier(
-                    VersionSpecifiers(
-                        [
-                            VersionSpecifier {
-                                operator: Equal,
-                                version: "2.1.15",
-                            },
-                        ],
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/constraints-b.txt",
+                        ),
                     ),
-                ),
+                },
             ),
-            marker: None,
+            hashes: [],
         },
-        Requirement {
-            name: PackageName(
-                "pytz",
-            ),
-            extras: [],
-            version_or_url: Some(
-                VersionSpecifier(
-                    VersionSpecifiers(
-                        [
-                            VersionSpecifier {
-                                operator: Equal,
-                                version: "2023.3",
-                            },
-                        ],
+        RequirementEntry {
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "pytz",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: Equal,
+                                        version: "2023.3",
+                                    },
+                                ],
+                            ),
+                        ),
                     ),
-                ),
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/constraints-b.txt",
+                        ),
+                    ),
+                },
             ),
-            marker: None,
+            hashes: [],
         },
     ],
+    constraints: [],
     editables: [],
     index_url: None,
     extra_index_urls: [],
     find_links: [],
     no_index: false,
+    no_binary: None,
+    only_binary: None,
 }
```

### Comparing `uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-constraints-b.txt.snap` & `uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-include-a.txt.snap`

 * *Files 13% similar despite different names*

```diff
@@ -1,60 +1,63 @@
 ---
 source: crates/requirements-txt/src/lib.rs
 expression: actual
 ---
 RequirementsTxt {
     requirements: [
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "django",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "2.1.15",
-                                },
-                            ],
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "tomli",
+                    ),
+                    extras: [],
+                    version_or_url: None,
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/include-b.txt",
                         ),
                     ),
-                ),
-                marker: None,
-            },
+                },
+            ),
             hashes: [],
-            editable: false,
         },
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "pytz",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "2023.3",
-                                },
-                            ],
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "numpy",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: Equal,
+                                        version: "1.24.2",
+                                    },
+                                ],
+                            ),
+                        ),
+                    ),
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/include-a.txt",
                         ),
                     ),
-                ),
-                marker: None,
-            },
+                },
+            ),
             hashes: [],
-            editable: false,
         },
     ],
     constraints: [],
     editables: [],
     index_url: None,
     extra_index_urls: [],
     find_links: [],
     no_index: false,
+    no_binary: None,
+    only_binary: None,
 }
```

### Comparing `uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-editable.txt.snap` & `uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-whitespace.txt.snap`

 * *Files 9% similar despite different names*

```diff
@@ -1,65 +1,79 @@
 ---
 source: crates/requirements-txt/src/lib.rs
 expression: actual
 ---
 RequirementsTxt {
     requirements: [
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "numpy",
-                ),
-                extras: [],
-                version_or_url: None,
-                marker: None,
-            },
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "numpy",
+                    ),
+                    extras: [],
+                    version_or_url: None,
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/whitespace.txt",
+                        ),
+                    ),
+                },
+            ),
             hashes: [],
-            editable: false,
         },
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "pandas",
-                ),
-                extras: [
-                    ExtraName(
-                        "tabulate",
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "pandas",
                     ),
-                ],
-                version_or_url: Some(
-                    Url(
-                        VerbatimUrl {
-                            url: Url {
-                                scheme: "https",
-                                cannot_be_a_base: false,
-                                username: "",
-                                password: None,
-                                host: Some(
-                                    Domain(
-                                        "github.com",
+                    extras: [
+                        ExtraName(
+                            "tabulate",
+                        ),
+                    ],
+                    version_or_url: Some(
+                        Url(
+                            VerbatimUrl {
+                                url: Url {
+                                    scheme: "https",
+                                    cannot_be_a_base: false,
+                                    username: "",
+                                    password: None,
+                                    host: Some(
+                                        Domain(
+                                            "github.com",
+                                        ),
                                     ),
+                                    port: None,
+                                    path: "/pandas-dev/pandas",
+                                    query: None,
+                                    fragment: None,
+                                },
+                                given: Some(
+                                    "https://github.com/pandas-dev/pandas",
                                 ),
-                                port: None,
-                                path: "/pandas-dev/pandas",
-                                query: None,
-                                fragment: None,
                             },
-                            given: Some(
-                                "https://github.com/pandas-dev/pandas",
-                            ),
-                        },
+                        ),
+                    ),
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/whitespace.txt",
+                        ),
                     ),
-                ),
-                marker: None,
-            },
+                },
+            ),
             hashes: [],
-            editable: false,
         },
     ],
     constraints: [],
     editables: [],
     index_url: None,
     extra_index_urls: [],
     find_links: [],
     no_index: false,
+    no_binary: None,
+    only_binary: None,
 }
```

### Comparing `uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-for-poetry.txt.snap` & `uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-constraints-b.txt.snap`

 * *Files 10% similar despite different names*

```diff
@@ -1,103 +1,74 @@
 ---
 source: crates/requirements-txt/src/lib.rs
 expression: actual
 ---
 RequirementsTxt {
     requirements: [
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "inflection",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "0.5.1",
-                                },
-                            ],
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "django",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: Equal,
+                                        version: "2.1.15",
+                                    },
+                                ],
+                            ),
                         ),
                     ),
-                ),
-                marker: None,
-            },
-            hashes: [],
-            editable: false,
-        },
-        RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "upsidedown",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "0.4",
-                                },
-                            ],
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/constraints-b.txt",
                         ),
                     ),
-                ),
-                marker: None,
-            },
-            hashes: [],
-            editable: false,
-        },
-        RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "numpy",
-                ),
-                extras: [],
-                version_or_url: None,
-                marker: None,
-            },
+                },
+            ),
             hashes: [],
-            editable: false,
         },
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "pandas",
-                ),
-                extras: [
-                    ExtraName(
-                        "tabulate",
-                    ),
-                ],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: GreaterThanEqual,
-                                    version: "1",
-                                },
-                                VersionSpecifier {
-                                    operator: LessThan,
-                                    version: "2",
-                                },
-                            ],
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "pytz",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: Equal,
+                                        version: "2023.3",
+                                    },
+                                ],
+                            ),
+                        ),
+                    ),
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/constraints-b.txt",
                         ),
                     ),
-                ),
-                marker: None,
-            },
+                },
+            ),
             hashes: [],
-            editable: false,
         },
     ],
     constraints: [],
     editables: [],
     index_url: None,
     extra_index_urls: [],
     find_links: [],
     no_index: false,
+    no_binary: None,
+    only_binary: None,
 }
```

### Comparing `uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-include-a.txt.snap` & `uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-include-a.txt.snap`

 * *Files 12% similar despite different names*

```diff
@@ -1,49 +1,63 @@
 ---
 source: crates/requirements-txt/src/lib.rs
 expression: actual
 ---
 RequirementsTxt {
     requirements: [
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "tomli",
-                ),
-                extras: [],
-                version_or_url: None,
-                marker: None,
-            },
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "tomli",
+                    ),
+                    extras: [],
+                    version_or_url: None,
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/include-b.txt",
+                        ),
+                    ),
+                },
+            ),
             hashes: [],
-            editable: false,
         },
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "numpy",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "1.24.2",
-                                },
-                            ],
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "numpy",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: Equal,
+                                        version: "1.24.2",
+                                    },
+                                ],
+                            ),
+                        ),
+                    ),
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/include-a.txt",
                         ),
                     ),
-                ),
-                marker: None,
-            },
+                },
+            ),
             hashes: [],
-            editable: false,
         },
     ],
     constraints: [],
     editables: [],
     index_url: None,
     extra_index_urls: [],
     find_links: [],
     no_index: false,
+    no_binary: None,
+    only_binary: None,
 }
```

### Comparing `uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-include-b.txt.snap` & `uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-include-b.txt.snap`

 * *Files 24% similar despite different names*

```diff
@@ -1,26 +1,34 @@
 ---
 source: crates/requirements-txt/src/lib.rs
 expression: actual
 ---
 RequirementsTxt {
     requirements: [
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "tomli",
-                ),
-                extras: [],
-                version_or_url: None,
-                marker: None,
-            },
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "tomli",
+                    ),
+                    extras: [],
+                    version_or_url: None,
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/include-b.txt",
+                        ),
+                    ),
+                },
+            ),
             hashes: [],
-            editable: false,
         },
     ],
     constraints: [],
     editables: [],
     index_url: None,
     extra_index_urls: [],
     find_links: [],
     no_index: false,
+    no_binary: None,
+    only_binary: None,
 }
```

### Comparing `uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-poetry-with-hashes.txt.snap` & `uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-poetry-with-hashes.txt.snap`

 * *Files 16% similar despite different names*

```diff
@@ -1,289 +1,297 @@
 ---
 source: crates/requirements-txt/src/lib.rs
 expression: actual
 ---
 RequirementsTxt {
     requirements: [
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "werkzeug",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "werkzeug",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: Equal,
+                                        version: "2.2.3",
+                                    },
+                                ],
+                            ),
+                        ),
+                    ),
+                    marker: Some(
+                        And(
                             [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "2.2.3",
-                                },
+                                Expression(
+                                    Version {
+                                        key: PythonVersion,
+                                        specifier: VersionSpecifier {
+                                            operator: GreaterThanEqual,
+                                            version: "3.8",
+                                        },
+                                    },
+                                ),
+                                Expression(
+                                    Version {
+                                        key: PythonVersion,
+                                        specifier: VersionSpecifier {
+                                            operator: LessThan,
+                                            version: "4.0",
+                                        },
+                                    },
+                                ),
                             ],
                         ),
                     ),
-                ),
-                marker: Some(
-                    And(
-                        [
-                            Expression(
-                                MarkerExpression {
-                                    l_value: MarkerEnvVersion(
-                                        PythonVersion,
-                                    ),
-                                    operator: GreaterEqual,
-                                    r_value: QuotedString(
-                                        "3.8",
-                                    ),
-                                },
-                            ),
-                            Expression(
-                                MarkerExpression {
-                                    l_value: MarkerEnvVersion(
-                                        PythonVersion,
-                                    ),
-                                    operator: LessThan,
-                                    r_value: QuotedString(
-                                        "4.0",
-                                    ),
-                                },
-                            ),
-                        ],
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/poetry-with-hashes.txt",
+                        ),
                     ),
-                ),
-            },
+                },
+            ),
             hashes: [
                 "sha256:2e1ccc9417d4da358b9de6f174e3ac094391ea1d4fbef2d667865d819dfd0afe",
             ],
-            editable: false,
         },
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "urllib3",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "urllib3",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: Equal,
+                                        version: "1.26.15",
+                                    },
+                                ],
+                            ),
+                        ),
+                    ),
+                    marker: Some(
+                        And(
                             [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "1.26.15",
-                                },
+                                Expression(
+                                    Version {
+                                        key: PythonVersion,
+                                        specifier: VersionSpecifier {
+                                            operator: GreaterThanEqual,
+                                            version: "3.8",
+                                        },
+                                    },
+                                ),
+                                Expression(
+                                    Version {
+                                        key: PythonVersion,
+                                        specifier: VersionSpecifier {
+                                            operator: LessThan,
+                                            version: "4",
+                                        },
+                                    },
+                                ),
                             ],
                         ),
                     ),
-                ),
-                marker: Some(
-                    And(
-                        [
-                            Expression(
-                                MarkerExpression {
-                                    l_value: MarkerEnvVersion(
-                                        PythonVersion,
-                                    ),
-                                    operator: GreaterEqual,
-                                    r_value: QuotedString(
-                                        "3.8",
-                                    ),
-                                },
-                            ),
-                            Expression(
-                                MarkerExpression {
-                                    l_value: MarkerEnvVersion(
-                                        PythonVersion,
-                                    ),
-                                    operator: LessThan,
-                                    r_value: QuotedString(
-                                        "4",
-                                    ),
-                                },
-                            ),
-                        ],
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/poetry-with-hashes.txt",
+                        ),
                     ),
-                ),
-            },
+                },
+            ),
             hashes: [
                 "sha256:8a388717b9476f934a21484e8c8e61875ab60644d29b9b39e11e4b9dc1c6b305",
             ],
-            editable: false,
         },
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "ansicon",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "ansicon",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: Equal,
+                                        version: "1.89.0",
+                                    },
+                                ],
+                            ),
+                        ),
+                    ),
+                    marker: Some(
+                        And(
                             [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "1.89.0",
-                                },
+                                Expression(
+                                    Version {
+                                        key: PythonVersion,
+                                        specifier: VersionSpecifier {
+                                            operator: GreaterThanEqual,
+                                            version: "3.8",
+                                        },
+                                    },
+                                ),
+                                Expression(
+                                    Version {
+                                        key: PythonVersion,
+                                        specifier: VersionSpecifier {
+                                            operator: LessThan,
+                                            version: "4",
+                                        },
+                                    },
+                                ),
+                                Expression(
+                                    String {
+                                        key: PlatformSystem,
+                                        operator: Equal,
+                                        value: "Windows",
+                                    },
+                                ),
                             ],
                         ),
                     ),
-                ),
-                marker: Some(
-                    And(
-                        [
-                            Expression(
-                                MarkerExpression {
-                                    l_value: MarkerEnvVersion(
-                                        PythonVersion,
-                                    ),
-                                    operator: GreaterEqual,
-                                    r_value: QuotedString(
-                                        "3.8",
-                                    ),
-                                },
-                            ),
-                            Expression(
-                                MarkerExpression {
-                                    l_value: MarkerEnvVersion(
-                                        PythonVersion,
-                                    ),
-                                    operator: LessThan,
-                                    r_value: QuotedString(
-                                        "4",
-                                    ),
-                                },
-                            ),
-                            Expression(
-                                MarkerExpression {
-                                    l_value: MarkerEnvString(
-                                        PlatformSystem,
-                                    ),
-                                    operator: Equal,
-                                    r_value: QuotedString(
-                                        "Windows",
-                                    ),
-                                },
-                            ),
-                        ],
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/poetry-with-hashes.txt",
+                        ),
                     ),
-                ),
-            },
+                },
+            ),
             hashes: [
                 "sha256:e4d039def5768a47e4afec8e89e83ec3ae5a26bf00ad851f914d1240b444d2b1",
             ],
-            editable: false,
         },
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "requests-oauthlib",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "requests-oauthlib",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: Equal,
+                                        version: "1.3.1",
+                                    },
+                                ],
+                            ),
+                        ),
+                    ),
+                    marker: Some(
+                        And(
                             [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "1.3.1",
-                                },
+                                Expression(
+                                    Version {
+                                        key: PythonVersion,
+                                        specifier: VersionSpecifier {
+                                            operator: GreaterThanEqual,
+                                            version: "3.8",
+                                        },
+                                    },
+                                ),
+                                Expression(
+                                    Version {
+                                        key: PythonVersion,
+                                        specifier: VersionSpecifier {
+                                            operator: LessThan,
+                                            version: "4.0",
+                                        },
+                                    },
+                                ),
                             ],
                         ),
                     ),
-                ),
-                marker: Some(
-                    And(
-                        [
-                            Expression(
-                                MarkerExpression {
-                                    l_value: MarkerEnvVersion(
-                                        PythonVersion,
-                                    ),
-                                    operator: GreaterEqual,
-                                    r_value: QuotedString(
-                                        "3.8",
-                                    ),
-                                },
-                            ),
-                            Expression(
-                                MarkerExpression {
-                                    l_value: MarkerEnvVersion(
-                                        PythonVersion,
-                                    ),
-                                    operator: LessThan,
-                                    r_value: QuotedString(
-                                        "4.0",
-                                    ),
-                                },
-                            ),
-                        ],
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/poetry-with-hashes.txt",
+                        ),
                     ),
-                ),
-            },
+                },
+            ),
             hashes: [
                 "sha256:2577c501a2fb8d05a304c09d090d6e47c306fef15809d102b327cf8364bddab5",
                 "sha256:75beac4a47881eeb94d5ea5d6ad31ef88856affe2332b9aafb52c6452ccf0d7a",
             ],
-            editable: false,
         },
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "psycopg2",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "psycopg2",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: Equal,
+                                        version: "2.9.5",
+                                    },
+                                ],
+                            ),
+                        ),
+                    ),
+                    marker: Some(
+                        And(
                             [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "2.9.5",
-                                },
+                                Expression(
+                                    Version {
+                                        key: PythonVersion,
+                                        specifier: VersionSpecifier {
+                                            operator: GreaterThanEqual,
+                                            version: "3.8",
+                                        },
+                                    },
+                                ),
+                                Expression(
+                                    Version {
+                                        key: PythonVersion,
+                                        specifier: VersionSpecifier {
+                                            operator: LessThan,
+                                            version: "4.0",
+                                        },
+                                    },
+                                ),
                             ],
                         ),
                     ),
-                ),
-                marker: Some(
-                    And(
-                        [
-                            Expression(
-                                MarkerExpression {
-                                    l_value: MarkerEnvVersion(
-                                        PythonVersion,
-                                    ),
-                                    operator: GreaterEqual,
-                                    r_value: QuotedString(
-                                        "3.8",
-                                    ),
-                                },
-                            ),
-                            Expression(
-                                MarkerExpression {
-                                    l_value: MarkerEnvVersion(
-                                        PythonVersion,
-                                    ),
-                                    operator: LessThan,
-                                    r_value: QuotedString(
-                                        "4.0",
-                                    ),
-                                },
-                            ),
-                        ],
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/poetry-with-hashes.txt",
+                        ),
                     ),
-                ),
-            },
+                },
+            ),
             hashes: [
                 "sha256:093e3894d2d3c592ab0945d9eba9d139c139664dcf83a1c440b8a7aa9bb21955",
                 "sha256:190d51e8c1b25a47484e52a79638a8182451d6f6dff99f26ad9bd81e5359a0fa",
                 "sha256:1a5c7d7d577e0eabfcf15eb87d1e19314c8c4f0e722a301f98e0e3a65e238b4e",
                 "sha256:1e5a38aa85bd660c53947bd28aeaafb6a97d70423606f1ccb044a03a1203fe4a",
             ],
-            editable: false,
         },
     ],
     constraints: [],
     editables: [],
     index_url: None,
     extra_index_urls: [],
     find_links: [],
     no_index: false,
+    no_binary: None,
+    only_binary: None,
 }
```

### Comparing `uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-small.txt.snap` & `uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-small.txt.snap`

 * *Files 18% similar despite different names*

```diff
@@ -1,60 +1,74 @@
 ---
 source: crates/requirements-txt/src/lib.rs
 expression: actual
 ---
 RequirementsTxt {
     requirements: [
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "tqdm",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "4.65.0",
-                                },
-                            ],
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "tqdm",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: Equal,
+                                        version: "4.65.0",
+                                    },
+                                ],
+                            ),
+                        ),
+                    ),
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/small.txt",
                         ),
                     ),
-                ),
-                marker: None,
-            },
+                },
+            ),
             hashes: [],
-            editable: false,
         },
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "tomli-w",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "1.0.0",
-                                },
-                            ],
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "tomli-w",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: Equal,
+                                        version: "1.0.0",
+                                    },
+                                ],
+                            ),
+                        ),
+                    ),
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/small.txt",
                         ),
                     ),
-                ),
-                marker: None,
-            },
+                },
+            ),
             hashes: [],
-            editable: false,
         },
     ],
     constraints: [],
     editables: [],
     index_url: None,
     extra_index_urls: [],
     find_links: [],
     no_index: false,
+    no_binary: None,
+    only_binary: None,
 }
```

### Comparing `uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-whitespace.txt.snap` & `uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-whitespace.txt.snap`

 * *Files 9% similar despite different names*

```diff
@@ -1,65 +1,79 @@
 ---
 source: crates/requirements-txt/src/lib.rs
 expression: actual
 ---
 RequirementsTxt {
     requirements: [
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "numpy",
-                ),
-                extras: [],
-                version_or_url: None,
-                marker: None,
-            },
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "numpy",
+                    ),
+                    extras: [],
+                    version_or_url: None,
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/whitespace.txt",
+                        ),
+                    ),
+                },
+            ),
             hashes: [],
-            editable: false,
         },
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "pandas",
-                ),
-                extras: [
-                    ExtraName(
-                        "tabulate",
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "pandas",
                     ),
-                ],
-                version_or_url: Some(
-                    Url(
-                        VerbatimUrl {
-                            url: Url {
-                                scheme: "https",
-                                cannot_be_a_base: false,
-                                username: "",
-                                password: None,
-                                host: Some(
-                                    Domain(
-                                        "github.com",
+                    extras: [
+                        ExtraName(
+                            "tabulate",
+                        ),
+                    ],
+                    version_or_url: Some(
+                        Url(
+                            VerbatimUrl {
+                                url: Url {
+                                    scheme: "https",
+                                    cannot_be_a_base: false,
+                                    username: "",
+                                    password: None,
+                                    host: Some(
+                                        Domain(
+                                            "github.com",
+                                        ),
                                     ),
+                                    port: None,
+                                    path: "/pandas-dev/pandas",
+                                    query: None,
+                                    fragment: None,
+                                },
+                                given: Some(
+                                    "https://github.com/pandas-dev/pandas",
                                 ),
-                                port: None,
-                                path: "/pandas-dev/pandas",
-                                query: None,
-                                fragment: None,
                             },
-                            given: Some(
-                                "https://github.com/pandas-dev/pandas",
-                            ),
-                        },
+                        ),
+                    ),
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/whitespace.txt",
+                        ),
                     ),
-                ),
-                marker: None,
-            },
+                },
+            ),
             hashes: [],
-            editable: false,
         },
     ],
     constraints: [],
     editables: [],
     index_url: None,
     extra_index_urls: [],
     find_links: [],
     no_index: false,
+    no_binary: None,
+    only_binary: None,
 }
```

### Comparing `uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-basic.txt.snap` & `uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-for-poetry.txt.snap`

 * *Files 26% similar despite different names*

```diff
@@ -1,152 +1,129 @@
 ---
 source: crates/requirements-txt/src/lib.rs
 expression: actual
 ---
 RequirementsTxt {
     requirements: [
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "numpy",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "1.24.2",
-                                },
-                            ],
-                        ),
-                    ),
-                ),
-                marker: None,
-            },
-            hashes: [],
-            editable: false,
-        },
-        RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "pandas",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "2.0.0",
-                                },
-                            ],
-                        ),
-                    ),
-                ),
-                marker: None,
-            },
-            hashes: [],
-            editable: false,
-        },
-        RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "python-dateutil",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "2.8.2",
-                                },
-                            ],
-                        ),
-                    ),
-                ),
-                marker: None,
-            },
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "inflection",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: Equal,
+                                        version: "0.5.1",
+                                    },
+                                ],
+                            ),
+                        ),
+                    ),
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/for-poetry.txt",
+                        ),
+                    ),
+                },
+            ),
             hashes: [],
-            editable: false,
         },
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "pytz",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "2023.3",
-                                },
-                            ],
-                        ),
-                    ),
-                ),
-                marker: None,
-            },
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "upsidedown",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: Equal,
+                                        version: "0.4",
+                                    },
+                                ],
+                            ),
+                        ),
+                    ),
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/for-poetry.txt",
+                        ),
+                    ),
+                },
+            ),
             hashes: [],
-            editable: false,
         },
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "six",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "1.16.0",
-                                },
-                            ],
-                        ),
-                    ),
-                ),
-                marker: None,
-            },
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "numpy",
+                    ),
+                    extras: [],
+                    version_or_url: None,
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/for-poetry.txt",
+                        ),
+                    ),
+                },
+            ),
             hashes: [],
-            editable: false,
         },
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "tzdata",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "2023.3",
-                                },
-                            ],
-                        ),
-                    ),
-                ),
-                marker: None,
-            },
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "pandas",
+                    ),
+                    extras: [
+                        ExtraName(
+                            "tabulate",
+                        ),
+                    ],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: GreaterThanEqual,
+                                        version: "1",
+                                    },
+                                    VersionSpecifier {
+                                        operator: LessThan,
+                                        version: "2",
+                                    },
+                                ],
+                            ),
+                        ),
+                    ),
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/for-poetry.txt",
+                        ),
+                    ),
+                },
+            ),
             hashes: [],
-            editable: false,
         },
     ],
     constraints: [],
     editables: [],
     index_url: None,
     extra_index_urls: [],
     find_links: [],
     no_index: false,
+    no_binary: None,
+    only_binary: None,
 }
```

### Comparing `uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-constraints-a.txt.snap` & `uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__line-endings-constraints-a.txt.snap`

 * *Files 9% similar despite different names*

```diff
@@ -1,35 +1,41 @@
 ---
 source: crates/requirements-txt/src/lib.rs
 expression: actual
 ---
 RequirementsTxt {
     requirements: [
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "django-debug-toolbar",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: LessThan,
-                                    version: "2.2",
-                                },
-                            ],
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "django-debug-toolbar",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: LessThan,
+                                        version: "2.2",
+                                    },
+                                ],
+                            ),
                         ),
                     ),
-                ),
-                marker: None,
-            },
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/constraints-a.txt",
+                        ),
+                    ),
+                },
+            ),
             hashes: [],
-            editable: false,
         },
     ],
     constraints: [
         Requirement {
             name: PackageName(
                 "django",
             ),
@@ -43,14 +49,19 @@
                                 version: "2.1.15",
                             },
                         ],
                     ),
                 ),
             ),
             marker: None,
+            origin: Some(
+                File(
+                    "<REQUIREMENTS_DIR>/constraints-b.txt",
+                ),
+            ),
         },
         Requirement {
             name: PackageName(
                 "pytz",
             ),
             extras: [],
             version_or_url: Some(
@@ -62,15 +73,22 @@
                                 version: "2023.3",
                             },
                         ],
                     ),
                 ),
             ),
             marker: None,
+            origin: Some(
+                File(
+                    "<REQUIREMENTS_DIR>/constraints-b.txt",
+                ),
+            ),
         },
     ],
     editables: [],
     index_url: None,
     extra_index_urls: [],
     find_links: [],
     no_index: false,
+    no_binary: None,
+    only_binary: None,
 }
```

### Comparing `uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-constraints-b.txt.snap` & `uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-constraints-a.txt.snap`

 * *Files 24% similar despite different names*

```diff
@@ -1,60 +1,94 @@
 ---
 source: crates/requirements-txt/src/lib.rs
 expression: actual
 ---
 RequirementsTxt {
     requirements: [
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "django",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "2.1.15",
-                                },
-                            ],
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "django-debug-toolbar",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: LessThan,
+                                        version: "2.2",
+                                    },
+                                ],
+                            ),
                         ),
                     ),
-                ),
-                marker: None,
-            },
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/constraints-a.txt",
+                        ),
+                    ),
+                },
+            ),
             hashes: [],
-            editable: false,
         },
-        RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "pytz",
+    ],
+    constraints: [
+        Requirement {
+            name: PackageName(
+                "django",
+            ),
+            extras: [],
+            version_or_url: Some(
+                VersionSpecifier(
+                    VersionSpecifiers(
+                        [
+                            VersionSpecifier {
+                                operator: Equal,
+                                version: "2.1.15",
+                            },
+                        ],
+                    ),
                 ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "2023.3",
-                                },
-                            ],
-                        ),
+            ),
+            marker: None,
+            origin: Some(
+                File(
+                    "<REQUIREMENTS_DIR>/constraints-b.txt",
+                ),
+            ),
+        },
+        Requirement {
+            name: PackageName(
+                "pytz",
+            ),
+            extras: [],
+            version_or_url: Some(
+                VersionSpecifier(
+                    VersionSpecifiers(
+                        [
+                            VersionSpecifier {
+                                operator: Equal,
+                                version: "2023.3",
+                            },
+                        ],
                     ),
                 ),
-                marker: None,
-            },
-            hashes: [],
-            editable: false,
+            ),
+            marker: None,
+            origin: Some(
+                File(
+                    "<REQUIREMENTS_DIR>/constraints-b.txt",
+                ),
+            ),
         },
     ],
-    constraints: [],
     editables: [],
     index_url: None,
     extra_index_urls: [],
     find_links: [],
     no_index: false,
+    no_binary: None,
+    only_binary: None,
 }
```

### Comparing `uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-for-poetry.txt.snap` & `uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-small.txt.snap`

 * *Files 18% similar despite different names*

```diff
@@ -1,103 +1,74 @@
 ---
 source: crates/requirements-txt/src/lib.rs
 expression: actual
 ---
 RequirementsTxt {
     requirements: [
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "inflection",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "0.5.1",
-                                },
-                            ],
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "tqdm",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: Equal,
+                                        version: "4.65.0",
+                                    },
+                                ],
+                            ),
                         ),
                     ),
-                ),
-                marker: None,
-            },
-            hashes: [],
-            editable: false,
-        },
-        RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "upsidedown",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "0.4",
-                                },
-                            ],
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/small.txt",
                         ),
                     ),
-                ),
-                marker: None,
-            },
-            hashes: [],
-            editable: false,
-        },
-        RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "numpy",
-                ),
-                extras: [],
-                version_or_url: None,
-                marker: None,
-            },
+                },
+            ),
             hashes: [],
-            editable: false,
         },
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "pandas",
-                ),
-                extras: [
-                    ExtraName(
-                        "tabulate",
-                    ),
-                ],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: GreaterThanEqual,
-                                    version: "1",
-                                },
-                                VersionSpecifier {
-                                    operator: LessThan,
-                                    version: "2",
-                                },
-                            ],
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "tomli-w",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: Equal,
+                                        version: "1.0.0",
+                                    },
+                                ],
+                            ),
+                        ),
+                    ),
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/small.txt",
                         ),
                     ),
-                ),
-                marker: None,
-            },
+                },
+            ),
             hashes: [],
-            editable: false,
         },
     ],
     constraints: [],
     editables: [],
     index_url: None,
     extra_index_urls: [],
     find_links: [],
     no_index: false,
+    no_binary: None,
+    only_binary: None,
 }
```

### Comparing `uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-include-a.txt.snap` & `uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-include-b.txt.snap`

 * *Files 21% similar despite different names*

```diff
@@ -1,49 +1,34 @@
 ---
 source: crates/requirements-txt/src/lib.rs
 expression: actual
 ---
 RequirementsTxt {
     requirements: [
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "tomli",
-                ),
-                extras: [],
-                version_or_url: None,
-                marker: None,
-            },
-            hashes: [],
-            editable: false,
-        },
-        RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "numpy",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
-                            [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "1.24.2",
-                                },
-                            ],
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "tomli",
+                    ),
+                    extras: [],
+                    version_or_url: None,
+                    marker: None,
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/include-b.txt",
                         ),
                     ),
-                ),
-                marker: None,
-            },
+                },
+            ),
             hashes: [],
-            editable: false,
         },
     ],
     constraints: [],
     editables: [],
     index_url: None,
     extra_index_urls: [],
     find_links: [],
     no_index: false,
+    no_binary: None,
+    only_binary: None,
 }
```

### Comparing `uv-0.1.9/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-poetry-with-hashes.txt.snap` & `uv-0.2.0/crates/requirements-txt/src/snapshots/requirements_txt__test__parse-poetry-with-hashes.txt.snap`

 * *Files 16% similar despite different names*

```diff
@@ -1,289 +1,297 @@
 ---
 source: crates/requirements-txt/src/lib.rs
 expression: actual
 ---
 RequirementsTxt {
     requirements: [
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "werkzeug",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "werkzeug",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: Equal,
+                                        version: "2.2.3",
+                                    },
+                                ],
+                            ),
+                        ),
+                    ),
+                    marker: Some(
+                        And(
                             [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "2.2.3",
-                                },
+                                Expression(
+                                    Version {
+                                        key: PythonVersion,
+                                        specifier: VersionSpecifier {
+                                            operator: GreaterThanEqual,
+                                            version: "3.8",
+                                        },
+                                    },
+                                ),
+                                Expression(
+                                    Version {
+                                        key: PythonVersion,
+                                        specifier: VersionSpecifier {
+                                            operator: LessThan,
+                                            version: "4.0",
+                                        },
+                                    },
+                                ),
                             ],
                         ),
                     ),
-                ),
-                marker: Some(
-                    And(
-                        [
-                            Expression(
-                                MarkerExpression {
-                                    l_value: MarkerEnvVersion(
-                                        PythonVersion,
-                                    ),
-                                    operator: GreaterEqual,
-                                    r_value: QuotedString(
-                                        "3.8",
-                                    ),
-                                },
-                            ),
-                            Expression(
-                                MarkerExpression {
-                                    l_value: MarkerEnvVersion(
-                                        PythonVersion,
-                                    ),
-                                    operator: LessThan,
-                                    r_value: QuotedString(
-                                        "4.0",
-                                    ),
-                                },
-                            ),
-                        ],
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/poetry-with-hashes.txt",
+                        ),
                     ),
-                ),
-            },
+                },
+            ),
             hashes: [
                 "sha256:2e1ccc9417d4da358b9de6f174e3ac094391ea1d4fbef2d667865d819dfd0afe",
             ],
-            editable: false,
         },
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "urllib3",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "urllib3",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: Equal,
+                                        version: "1.26.15",
+                                    },
+                                ],
+                            ),
+                        ),
+                    ),
+                    marker: Some(
+                        And(
                             [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "1.26.15",
-                                },
+                                Expression(
+                                    Version {
+                                        key: PythonVersion,
+                                        specifier: VersionSpecifier {
+                                            operator: GreaterThanEqual,
+                                            version: "3.8",
+                                        },
+                                    },
+                                ),
+                                Expression(
+                                    Version {
+                                        key: PythonVersion,
+                                        specifier: VersionSpecifier {
+                                            operator: LessThan,
+                                            version: "4",
+                                        },
+                                    },
+                                ),
                             ],
                         ),
                     ),
-                ),
-                marker: Some(
-                    And(
-                        [
-                            Expression(
-                                MarkerExpression {
-                                    l_value: MarkerEnvVersion(
-                                        PythonVersion,
-                                    ),
-                                    operator: GreaterEqual,
-                                    r_value: QuotedString(
-                                        "3.8",
-                                    ),
-                                },
-                            ),
-                            Expression(
-                                MarkerExpression {
-                                    l_value: MarkerEnvVersion(
-                                        PythonVersion,
-                                    ),
-                                    operator: LessThan,
-                                    r_value: QuotedString(
-                                        "4",
-                                    ),
-                                },
-                            ),
-                        ],
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/poetry-with-hashes.txt",
+                        ),
                     ),
-                ),
-            },
+                },
+            ),
             hashes: [
                 "sha256:8a388717b9476f934a21484e8c8e61875ab60644d29b9b39e11e4b9dc1c6b305",
             ],
-            editable: false,
         },
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "ansicon",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "ansicon",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: Equal,
+                                        version: "1.89.0",
+                                    },
+                                ],
+                            ),
+                        ),
+                    ),
+                    marker: Some(
+                        And(
                             [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "1.89.0",
-                                },
+                                Expression(
+                                    Version {
+                                        key: PythonVersion,
+                                        specifier: VersionSpecifier {
+                                            operator: GreaterThanEqual,
+                                            version: "3.8",
+                                        },
+                                    },
+                                ),
+                                Expression(
+                                    Version {
+                                        key: PythonVersion,
+                                        specifier: VersionSpecifier {
+                                            operator: LessThan,
+                                            version: "4",
+                                        },
+                                    },
+                                ),
+                                Expression(
+                                    String {
+                                        key: PlatformSystem,
+                                        operator: Equal,
+                                        value: "Windows",
+                                    },
+                                ),
                             ],
                         ),
                     ),
-                ),
-                marker: Some(
-                    And(
-                        [
-                            Expression(
-                                MarkerExpression {
-                                    l_value: MarkerEnvVersion(
-                                        PythonVersion,
-                                    ),
-                                    operator: GreaterEqual,
-                                    r_value: QuotedString(
-                                        "3.8",
-                                    ),
-                                },
-                            ),
-                            Expression(
-                                MarkerExpression {
-                                    l_value: MarkerEnvVersion(
-                                        PythonVersion,
-                                    ),
-                                    operator: LessThan,
-                                    r_value: QuotedString(
-                                        "4",
-                                    ),
-                                },
-                            ),
-                            Expression(
-                                MarkerExpression {
-                                    l_value: MarkerEnvString(
-                                        PlatformSystem,
-                                    ),
-                                    operator: Equal,
-                                    r_value: QuotedString(
-                                        "Windows",
-                                    ),
-                                },
-                            ),
-                        ],
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/poetry-with-hashes.txt",
+                        ),
                     ),
-                ),
-            },
+                },
+            ),
             hashes: [
                 "sha256:e4d039def5768a47e4afec8e89e83ec3ae5a26bf00ad851f914d1240b444d2b1",
             ],
-            editable: false,
         },
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "requests-oauthlib",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "requests-oauthlib",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: Equal,
+                                        version: "1.3.1",
+                                    },
+                                ],
+                            ),
+                        ),
+                    ),
+                    marker: Some(
+                        And(
                             [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "1.3.1",
-                                },
+                                Expression(
+                                    Version {
+                                        key: PythonVersion,
+                                        specifier: VersionSpecifier {
+                                            operator: GreaterThanEqual,
+                                            version: "3.8",
+                                        },
+                                    },
+                                ),
+                                Expression(
+                                    Version {
+                                        key: PythonVersion,
+                                        specifier: VersionSpecifier {
+                                            operator: LessThan,
+                                            version: "4.0",
+                                        },
+                                    },
+                                ),
                             ],
                         ),
                     ),
-                ),
-                marker: Some(
-                    And(
-                        [
-                            Expression(
-                                MarkerExpression {
-                                    l_value: MarkerEnvVersion(
-                                        PythonVersion,
-                                    ),
-                                    operator: GreaterEqual,
-                                    r_value: QuotedString(
-                                        "3.8",
-                                    ),
-                                },
-                            ),
-                            Expression(
-                                MarkerExpression {
-                                    l_value: MarkerEnvVersion(
-                                        PythonVersion,
-                                    ),
-                                    operator: LessThan,
-                                    r_value: QuotedString(
-                                        "4.0",
-                                    ),
-                                },
-                            ),
-                        ],
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/poetry-with-hashes.txt",
+                        ),
                     ),
-                ),
-            },
+                },
+            ),
             hashes: [
                 "sha256:2577c501a2fb8d05a304c09d090d6e47c306fef15809d102b327cf8364bddab5",
                 "sha256:75beac4a47881eeb94d5ea5d6ad31ef88856affe2332b9aafb52c6452ccf0d7a",
             ],
-            editable: false,
         },
         RequirementEntry {
-            requirement: Requirement {
-                name: PackageName(
-                    "psycopg2",
-                ),
-                extras: [],
-                version_or_url: Some(
-                    VersionSpecifier(
-                        VersionSpecifiers(
+            requirement: Named(
+                Requirement {
+                    name: PackageName(
+                        "psycopg2",
+                    ),
+                    extras: [],
+                    version_or_url: Some(
+                        VersionSpecifier(
+                            VersionSpecifiers(
+                                [
+                                    VersionSpecifier {
+                                        operator: Equal,
+                                        version: "2.9.5",
+                                    },
+                                ],
+                            ),
+                        ),
+                    ),
+                    marker: Some(
+                        And(
                             [
-                                VersionSpecifier {
-                                    operator: Equal,
-                                    version: "2.9.5",
-                                },
+                                Expression(
+                                    Version {
+                                        key: PythonVersion,
+                                        specifier: VersionSpecifier {
+                                            operator: GreaterThanEqual,
+                                            version: "3.8",
+                                        },
+                                    },
+                                ),
+                                Expression(
+                                    Version {
+                                        key: PythonVersion,
+                                        specifier: VersionSpecifier {
+                                            operator: LessThan,
+                                            version: "4.0",
+                                        },
+                                    },
+                                ),
                             ],
                         ),
                     ),
-                ),
-                marker: Some(
-                    And(
-                        [
-                            Expression(
-                                MarkerExpression {
-                                    l_value: MarkerEnvVersion(
-                                        PythonVersion,
-                                    ),
-                                    operator: GreaterEqual,
-                                    r_value: QuotedString(
-                                        "3.8",
-                                    ),
-                                },
-                            ),
-                            Expression(
-                                MarkerExpression {
-                                    l_value: MarkerEnvVersion(
-                                        PythonVersion,
-                                    ),
-                                    operator: LessThan,
-                                    r_value: QuotedString(
-                                        "4.0",
-                                    ),
-                                },
-                            ),
-                        ],
+                    origin: Some(
+                        File(
+                            "<REQUIREMENTS_DIR>/poetry-with-hashes.txt",
+                        ),
                     ),
-                ),
-            },
+                },
+            ),
             hashes: [
                 "sha256:093e3894d2d3c592ab0945d9eba9d139c139664dcf83a1c440b8a7aa9bb21955",
                 "sha256:190d51e8c1b25a47484e52a79638a8182451d6f6dff99f26ad9bd81e5359a0fa",
                 "sha256:1a5c7d7d577e0eabfcf15eb87d1e19314c8c4f0e722a301f98e0e3a65e238b4e",
                 "sha256:1e5a38aa85bd660c53947bd28aeaafb6a97d70423606f1ccb044a03a1203fe4a",
             ],
-            editable: false,
         },
     ],
     constraints: [],
     editables: [],
     index_url: None,
     extra_index_urls: [],
     find_links: [],
     no_index: false,
+    no_binary: None,
+    only_binary: None,
 }
```

### Comparing `uv-0.1.9/crates/requirements-txt/test-data/requirements-txt/poetry-with-hashes.txt` & `uv-0.2.0/crates/requirements-txt/test-data/requirements-txt/poetry-with-hashes.txt`

 * *Files identical despite different names*

### Comparing `uv-0.1.9/crates/uv-dispatch/src/lib.rs` & `uv-0.2.0/crates/uv-dispatch/src/lib.rs`

 * *Files 14% similar despite different names*

```diff
@@ -1,292 +1,333 @@
 //! Avoid cyclic crate dependencies between [resolver][`uv_resolver`],
 //! [installer][`uv_installer`] and [build][`uv_build`] through [`BuildDispatch`]
 //! implementing [`BuildContext`].
 
-use std::future::Future;
-use std::path::{Path, PathBuf};
+use std::ffi::OsStr;
+use std::ffi::OsString;
+use std::path::Path;
 
 use anyhow::{bail, Context, Result};
+use futures::FutureExt;
 use itertools::Itertools;
+use rustc_hash::FxHashMap;
 use tracing::{debug, instrument};
 
-use distribution_types::{IndexLocations, Name, Resolution, SourceDist};
-use futures::FutureExt;
-use pep508_rs::Requirement;
+use distribution_types::{IndexLocations, Name, Requirement, Resolution, SourceDist};
 use uv_build::{SourceBuild, SourceBuildContext};
 use uv_cache::Cache;
-use uv_client::{FlatIndex, RegistryClient};
-use uv_installer::{Downloader, Installer, NoBinary, Plan, Planner, Reinstall, SitePackages};
-use uv_interpreter::{Interpreter, Virtualenv};
-use uv_resolver::{InMemoryIndex, Manifest, Options, Resolver};
-use uv_traits::{BuildContext, BuildKind, ConfigSettings, InFlight, NoBuild, SetupPyStrategy};
+use uv_client::RegistryClient;
+use uv_configuration::Concurrency;
+use uv_configuration::{BuildKind, ConfigSettings, NoBinary, NoBuild, Reinstall, SetupPyStrategy};
+use uv_distribution::DistributionDatabase;
+use uv_installer::{Downloader, Installer, Plan, Planner, SitePackages};
+use uv_interpreter::{Interpreter, PythonEnvironment};
+use uv_resolver::{FlatIndex, InMemoryIndex, Manifest, Options, PythonRequirement, Resolver};
+use uv_types::{BuildContext, BuildIsolation, EmptyInstalledPackages, HashStrategy, InFlight};
 
 /// The main implementation of [`BuildContext`], used by the CLI, see [`BuildContext`]
 /// documentation.
 pub struct BuildDispatch<'a> {
     client: &'a RegistryClient,
     cache: &'a Cache,
     interpreter: &'a Interpreter,
     index_locations: &'a IndexLocations,
     flat_index: &'a FlatIndex,
     index: &'a InMemoryIndex,
     in_flight: &'a InFlight,
-    base_python: PathBuf,
     setup_py: SetupPyStrategy,
+    build_isolation: BuildIsolation<'a>,
+    link_mode: install_wheel_rs::linker::LinkMode,
     no_build: &'a NoBuild,
     no_binary: &'a NoBinary,
     config_settings: &'a ConfigSettings,
     source_build_context: SourceBuildContext,
     options: Options,
+    build_extra_env_vars: FxHashMap<OsString, OsString>,
+    concurrency: Concurrency,
 }
 
 impl<'a> BuildDispatch<'a> {
     #[allow(clippy::too_many_arguments)]
     pub fn new(
         client: &'a RegistryClient,
         cache: &'a Cache,
         interpreter: &'a Interpreter,
         index_locations: &'a IndexLocations,
         flat_index: &'a FlatIndex,
         index: &'a InMemoryIndex,
         in_flight: &'a InFlight,
-        base_python: PathBuf,
         setup_py: SetupPyStrategy,
         config_settings: &'a ConfigSettings,
+        build_isolation: BuildIsolation<'a>,
+        link_mode: install_wheel_rs::linker::LinkMode,
         no_build: &'a NoBuild,
         no_binary: &'a NoBinary,
+        concurrency: Concurrency,
     ) -> Self {
         Self {
             client,
             cache,
             interpreter,
             index_locations,
             flat_index,
             index,
             in_flight,
-            base_python,
             setup_py,
             config_settings,
+            build_isolation,
+            link_mode,
             no_build,
             no_binary,
+            concurrency,
             source_build_context: SourceBuildContext::default(),
             options: Options::default(),
+            build_extra_env_vars: FxHashMap::default(),
         }
     }
 
     #[must_use]
     pub fn with_options(mut self, options: Options) -> Self {
         self.options = options;
         self
     }
+
+    /// Set the environment variables to be used when building a source distribution.
+    #[must_use]
+    pub fn with_build_extra_env_vars<I, K, V>(mut self, sdist_build_env_variables: I) -> Self
+    where
+        I: IntoIterator<Item = (K, V)>,
+        K: AsRef<OsStr>,
+        V: AsRef<OsStr>,
+    {
+        self.build_extra_env_vars = sdist_build_env_variables
+            .into_iter()
+            .map(|(key, value)| (key.as_ref().to_owned(), value.as_ref().to_owned()))
+            .collect();
+        self
+    }
 }
 
 impl<'a> BuildContext for BuildDispatch<'a> {
     type SourceDistBuilder = SourceBuild;
 
     fn cache(&self) -> &Cache {
         self.cache
     }
 
     fn interpreter(&self) -> &Interpreter {
         self.interpreter
     }
 
-    fn base_python(&self) -> &Path {
-        &self.base_python
+    fn build_isolation(&self) -> BuildIsolation {
+        self.build_isolation
     }
 
     fn no_build(&self) -> &NoBuild {
         self.no_build
     }
 
     fn no_binary(&self) -> &NoBinary {
         self.no_binary
     }
 
-    fn setup_py_strategy(&self) -> SetupPyStrategy {
-        self.setup_py
-    }
-
     fn index_locations(&self) -> &IndexLocations {
         self.index_locations
     }
 
+    fn setup_py_strategy(&self) -> SetupPyStrategy {
+        self.setup_py
+    }
+
     async fn resolve<'data>(&'data self, requirements: &'data [Requirement]) -> Result<Resolution> {
         let markers = self.interpreter.markers();
+        let python_requirement =
+            PythonRequirement::from_marker_environment(self.interpreter, markers);
         let tags = self.interpreter.tags()?;
         let resolver = Resolver::new(
             Manifest::simple(requirements.to_vec()),
             self.options,
-            markers,
-            self.interpreter,
+            &python_requirement,
+            Some(markers),
             tags,
-            self.client,
             self.flat_index,
             self.index,
+            &HashStrategy::None,
             self,
+            EmptyInstalledPackages,
+            DistributionDatabase::new(self.client, self, self.concurrency.downloads),
         )?;
         let graph = resolver.resolve().await.with_context(|| {
             format!(
                 "No solution found when resolving: {}",
                 requirements.iter().map(ToString::to_string).join(", "),
             )
         })?;
         Ok(Resolution::from(graph))
     }
 
-    #[allow(clippy::manual_async_fn)] // TODO(konstin): rustc 1.75 gets into a type inference cycle with async fn
     #[instrument(
         skip(self, resolution, venv),
         fields(
             resolution = resolution.distributions().map(ToString::to_string).join(", "),
             venv = ?venv.root()
         )
     )]
-    fn install<'data>(
+    async fn install<'data>(
         &'data self,
         resolution: &'data Resolution,
-        venv: &'data Virtualenv,
-    ) -> impl Future<Output = Result<()>> + Send + 'data {
-        async move {
-            debug!(
-                "Installing in {} in {}",
-                resolution
-                    .distributions()
-                    .map(ToString::to_string)
-                    .join(", "),
-                venv.root().display(),
-            );
+        venv: &'data PythonEnvironment,
+    ) -> Result<()> {
+        debug!(
+            "Installing in {} in {}",
+            resolution
+                .distributions()
+                .map(ToString::to_string)
+                .join(", "),
+            venv.root().display(),
+        );
 
-            // Determine the current environment markers.
-            let tags = self.interpreter.tags()?;
+        // Determine the current environment markers.
+        let tags = self.interpreter.tags()?;
 
-            // Determine the set of installed packages.
-            let site_packages =
-                SitePackages::from_executable(venv).context("Failed to list installed packages")?;
-
-            let Plan {
-                local,
-                remote,
-                reinstalls,
-                extraneous,
-            } = Planner::with_requirements(&resolution.requirements()).build(
-                site_packages,
-                &Reinstall::None,
-                &NoBinary::None,
-                self.index_locations,
-                self.cache(),
-                venv,
-                tags,
-            )?;
+        // Determine the set of installed packages.
+        let site_packages = SitePackages::from_executable(venv)?;
 
-            // Resolve any registry-based requirements.
-            let remote = remote
-                .iter()
-                .map(|dist| {
-                    resolution
-                        .get(&dist.name)
-                        .cloned()
-                        .expect("Resolution should contain all packages")
-                })
-                .collect::<Vec<_>>();
-
-            // Download any missing distributions.
-            let wheels = if remote.is_empty() {
-                vec![]
-            } else {
-                // TODO(konstin): Check that there is no endless recursion.
-                let downloader = Downloader::new(self.cache(), tags, self.client, self);
-                debug!(
-                    "Downloading and building requirement{} for build: {}",
-                    if remote.len() == 1 { "" } else { "s" },
-                    remote.iter().map(ToString::to_string).join(", ")
-                );
+        let Plan {
+            cached,
+            remote,
+            reinstalls,
+            extraneous: _,
+        } = Planner::with_requirements(&resolution.requirements()).build(
+            site_packages,
+            &Reinstall::None,
+            &NoBinary::None,
+            &HashStrategy::None,
+            self.index_locations,
+            self.cache(),
+            venv,
+            tags,
+        )?;
 
-                downloader
-                    .download(remote, self.in_flight)
-                    .await
-                    .context("Failed to download and build distributions")?
-            };
+        // Nothing to do.
+        if remote.is_empty() && cached.is_empty() && reinstalls.is_empty() {
+            debug!("No build requirements to install for build");
+            return Ok(());
+        }
 
-            // Remove any unnecessary packages.
-            if !extraneous.is_empty() || !reinstalls.is_empty() {
-                for dist_info in extraneous.iter().chain(reinstalls.iter()) {
-                    let summary = uv_installer::uninstall(dist_info)
-                        .await
-                        .context("Failed to uninstall build dependencies")?;
-                    debug!(
-                        "Uninstalled {} ({} file{}, {} director{})",
-                        dist_info.name(),
-                        summary.file_count,
-                        if summary.file_count == 1 { "" } else { "s" },
-                        summary.dir_count,
-                        if summary.dir_count == 1 { "y" } else { "ies" },
-                    );
-                }
-            }
+        // Resolve any registry-based requirements.
+        let remote = remote
+            .iter()
+            .map(|dist| {
+                resolution
+                    .get_remote(&dist.name)
+                    .cloned()
+                    .expect("Resolution should contain all packages")
+            })
+            .collect::<Vec<_>>();
+
+        // Download any missing distributions.
+        let wheels = if remote.is_empty() {
+            vec![]
+        } else {
+            // TODO(konstin): Check that there is no endless recursion.
+            let downloader = Downloader::new(
+                self.cache,
+                tags,
+                &HashStrategy::None,
+                DistributionDatabase::new(self.client, self, self.concurrency.downloads),
+            );
 
-            // Install the resolved distributions.
-            let wheels = wheels.into_iter().chain(local).collect::<Vec<_>>();
-            if !wheels.is_empty() {
+            debug!(
+                "Downloading and building requirement{} for build: {}",
+                if remote.len() == 1 { "" } else { "s" },
+                remote.iter().map(ToString::to_string).join(", ")
+            );
+
+            downloader
+                .download(remote, self.in_flight)
+                .await
+                .context("Failed to download and build distributions")?
+        };
+
+        // Remove any unnecessary packages.
+        if !reinstalls.is_empty() {
+            for dist_info in &reinstalls {
+                let summary = uv_installer::uninstall(dist_info)
+                    .await
+                    .context("Failed to uninstall build dependencies")?;
                 debug!(
-                    "Installing build requirement{}: {}",
-                    if wheels.len() == 1 { "" } else { "s" },
-                    wheels.iter().map(ToString::to_string).join(", ")
+                    "Uninstalled {} ({} file{}, {} director{})",
+                    dist_info.name(),
+                    summary.file_count,
+                    if summary.file_count == 1 { "" } else { "s" },
+                    summary.dir_count,
+                    if summary.dir_count == 1 { "y" } else { "ies" },
                 );
-                Installer::new(venv)
-                    .install(&wheels)
-                    .context("Failed to install build dependencies")?;
             }
+        }
 
-            Ok(())
+        // Install the resolved distributions.
+        let wheels = wheels.into_iter().chain(cached).collect::<Vec<_>>();
+        if !wheels.is_empty() {
+            debug!(
+                "Installing build requirement{}: {}",
+                if wheels.len() == 1 { "" } else { "s" },
+                wheels.iter().map(ToString::to_string).join(", ")
+            );
+            Installer::new(venv)
+                .with_link_mode(self.link_mode)
+                .install(&wheels)
+                .context("Failed to install build dependencies")?;
         }
+
+        Ok(())
     }
 
-    #[allow(clippy::manual_async_fn)] // TODO(konstin): rustc 1.75 gets into a type inference cycle with async fn
-    #[instrument(skip_all, fields(package_id = package_id, subdirectory = ?subdirectory))]
+    #[instrument(skip_all, fields(version_id = version_id, subdirectory = ?subdirectory))]
     async fn setup_build<'data>(
         &'data self,
         source: &'data Path,
         subdirectory: Option<&'data Path>,
-        package_id: &'data str,
+        version_id: &'data str,
         dist: Option<&'data SourceDist>,
         build_kind: BuildKind,
     ) -> Result<SourceBuild> {
         match self.no_build {
-            NoBuild::All => bail!("Building source distributions is disabled"),
+            NoBuild::All => debug_assert!(
+                matches!(build_kind, BuildKind::Editable),
+                "Only editable builds are exempt from 'no build' checks"
+            ),
             NoBuild::None => {}
             NoBuild::Packages(packages) => {
+                // We can only prevent builds by name for packages with names. For editable
+                // packages and unnamed requirements, we can't prevent the build.
                 if let Some(dist) = dist {
-                    // We can only prevent builds by name for packages with names
-                    // which is unknown before build of editable source distributions
                     if packages.contains(dist.name()) {
                         bail!(
                             "Building source distributions for {} is disabled",
                             dist.name()
                         );
                     }
-                } else {
-                    debug_assert!(
-                        matches!(build_kind, BuildKind::Editable),
-                        "Only editable builds are exempt from 'no build' checks"
-                    );
                 }
             }
         }
 
         let builder = SourceBuild::setup(
             source,
             subdirectory,
             self.interpreter,
             self,
             self.source_build_context.clone(),
-            package_id.to_string(),
+            version_id.to_string(),
             self.setup_py,
             self.config_settings.clone(),
+            self.build_isolation,
             build_kind,
+            self.build_extra_env_vars.clone(),
+            self.concurrency.builds,
         )
-        .boxed()
+        .boxed_local()
         .await?;
         Ok(builder)
     }
 }
```

### Comparing `uv-0.1.9/crates/uv/Cargo.toml` & `uv-0.2.0/crates/uv/Cargo.toml`

 * *Files 20% similar despite different names*

```diff
@@ -1,97 +1,106 @@
 [package]
 name = "uv"
-version = "0.1.9"
+version = "0.2.0"
 edition = { workspace = true }
 rust-version = { workspace = true }
 homepage = { workspace = true }
 documentation = { workspace = true }
 repository = { workspace = true }
 authors = { workspace = true }
 license = { workspace = true }
 default-run = "uv"
 
 [lints]
 workspace = true
 
 [dependencies]
-distribution-filename = { path = "../distribution-filename" }
-distribution-types = { path = "../distribution-types" }
-gourgeist = { path = "../gourgeist" }
-install-wheel-rs = { path = "../install-wheel-rs", default-features = false }
-pep440_rs = { path = "../pep440-rs" }
-pep508_rs = { path = "../pep508-rs" }
-platform-host = { path = "../platform-host" }
-platform-tags = { path = "../platform-tags" }
-uv-build = { path = "../uv-build" }
-uv-cache = { path = "../uv-cache", features = ["clap"] }
-uv-client = { path = "../uv-client" }
-uv-dispatch = { path = "../uv-dispatch" }
-uv-distribution = { path = "../uv-distribution" }
-uv-fs = { path = "../uv-fs" }
-uv-installer = { path = "../uv-installer" }
-uv-interpreter = { path = "../uv-interpreter" }
-uv-normalize = { path = "../uv-normalize" }
-uv-resolver = { path = "../uv-resolver", features = ["clap"] }
-uv-traits = { path = "../uv-traits" }
-uv-warnings = { path = "../uv-warnings" }
-pypi-types = { path = "../pypi-types" }
-requirements-txt = { path = "../requirements-txt" }
+distribution-types = { workspace = true }
+install-wheel-rs = { workspace = true, features = ["clap"], default-features = false }
+pep440_rs = { workspace = true }
+pep508_rs = { workspace = true }
+platform-tags = { workspace = true }
+pypi-types = { workspace = true }
+requirements-txt = { workspace = true, features = ["http"] }
+uv-auth = { workspace = true }
+uv-cache = { workspace = true, features = ["clap"] }
+uv-client = { workspace = true }
+uv-configuration = { workspace = true, features = ["clap"] }
+uv-dispatch = { workspace = true }
+uv-distribution = { workspace = true }
+uv-fs = { workspace = true }
+uv-installer = { workspace = true }
+uv-interpreter = { workspace = true }
+uv-normalize = { workspace = true }
+uv-requirements = { workspace = true }
+uv-resolver = { workspace = true, features = ["clap"] }
+uv-types = { workspace = true }
+uv-virtualenv = { workspace = true }
+uv-warnings = { workspace = true }
+uv-workspace = { workspace = true, features = ["schemars"] }
 
 anstream = { workspace = true }
 anyhow = { workspace = true }
-base64 = { workspace = true }
+axoupdater = { workspace = true, features = ["github_releases", "tokio"], optional = true }
 chrono = { workspace = true }
-clap = { workspace = true, features = ["derive"] }
+clap = { workspace = true, features = ["derive", "string", "wrap_help"] }
 clap_complete_command = { workspace = true }
-console = { workspace = true }
-ctrlc = { workspace = true  }
-dunce = { workspace = true }
 flate2 = { workspace = true, default-features = false }
 fs-err = { workspace = true, features = ["tokio"] }
-futures = { workspace = true }
+indexmap = { workspace = true }
 indicatif = { workspace = true }
 itertools = { workspace = true }
 miette = { workspace = true, features = ["fancy"] }
 owo-colors = { workspace = true }
-pubgrub = { workspace = true }
-pyproject-toml = { workspace = true }
+rayon = { workspace = true }
+regex = { workspace = true }
 rustc-hash = { workspace = true }
+serde = { workspace = true }
+serde_json = { workspace = true }
 tempfile = { workspace = true }
 textwrap = { workspace = true }
 thiserror = { workspace = true }
 tokio = { workspace = true }
 toml = { workspace = true }
 tracing = { workspace = true }
 tracing-durations-export = { workspace = true, features = ["plot"], optional = true }
-tracing-subscriber = { workspace = true }
+tracing-subscriber = { workspace = true, features = ["json"] }
 tracing-tree = { workspace = true }
+unicode-width = { workspace = true }
 url = { workspace = true }
-which = { workspace = true }
 
 [target.'cfg(target_os = "windows")'.dependencies]
-mimalloc = "0.1.39"
+mimalloc = { version = "0.1.39" }
 
 [target.'cfg(all(not(target_os = "windows"), not(target_os = "openbsd"), any(target_arch = "x86_64", target_arch = "aarch64", target_arch = "powerpc64")))'.dependencies]
-tikv-jemallocator = "0.5.4"
+tikv-jemallocator = { version = "0.5.4" }
 
 [dev-dependencies]
-assert_cmd = { version = "2.0.12" }
+assert_cmd = { version = "2.0.14" }
 assert_fs = { version = "1.1.0" }
+base64 = { version = "0.22.0" }
+byteorder = { version = "1.5.0" }
 filetime = { version = "0.2.23" }
 indoc = { version = "2.0.4" }
-insta = { version = "1.34.0", features = ["filters"] }
+insta = { version = "1.36.1", features = ["filters", "json"] }
 predicates = { version = "3.0.4" }
 regex = { version = "1.10.3" }
-reqwest = { version = "0.11.23", features = ["blocking"], default-features = false }
+reqwest = { workspace = true, features = ["blocking"], default-features = false }
+
+[package.metadata.cargo-shear]
+ignored = ["flate2"]
 
 [features]
-default = ["flate2/zlib-ng", "python", "pypi", "git", "maturin"]
+default = ["flate2/zlib-ng", "python", "pypi", "git"]
 # Introduces a dependency on a local Python installation.
 python = []
+# Introduces a dependency on a local Python installation with specific patch versions.
+python-patch = []
 # Introduces a dependency on PyPI.
 pypi = []
 # Introduces a dependency on Git.
 git = []
-# Introduces a dependency on Maturin.
-maturin = []
+# Adds self-update functionality.
+self-update = ["axoupdater"]
 
+[build-dependencies]
+fs-err = { workspace = true }
```

### Comparing `uv-0.1.9/crates/uv/src/commands/cache_clean.rs` & `uv-0.2.0/crates/uv/src/commands/cache_clean.rs`

 * *Files 25% similar despite different names*

```diff
@@ -1,136 +1,125 @@
 use std::fmt::Write;
 
 use anyhow::{Context, Result};
 use owo_colors::OwoColorize;
 
 use uv_cache::Cache;
-use uv_fs::Normalized;
+use uv_fs::Simplified;
 use uv_normalize::PackageName;
 
-use crate::commands::ExitStatus;
+use crate::commands::{human_readable_bytes, ExitStatus};
 use crate::printer::Printer;
 
-/// Clear the cache.
+/// Clear the cache, removing all entries or those linked to specific packages.
 pub(crate) fn cache_clean(
-    cache: &Cache,
     packages: &[PackageName],
-    mut printer: Printer,
+    cache: &Cache,
+    printer: Printer,
 ) -> Result<ExitStatus> {
     if !cache.root().exists() {
         writeln!(
-            printer,
+            printer.stderr(),
             "No cache found at: {}",
-            cache.root().normalized_display().cyan()
+            cache.root().user_display().cyan()
         )?;
         return Ok(ExitStatus::Success);
     }
 
     if packages.is_empty() {
         writeln!(
-            printer,
+            printer.stderr(),
             "Clearing cache at: {}",
-            cache.root().normalized_display().cyan()
+            cache.root().user_display().cyan()
         )?;
 
         let summary = cache.clear().with_context(|| {
-            format!(
-                "Failed to clear cache at: {}",
-                cache.root().normalized_display()
-            )
+            format!("Failed to clear cache at: {}", cache.root().user_display())
         })?;
 
         // Write a summary of the number of files and directories removed.
         match (summary.num_files, summary.num_dirs) {
             (0, 0) => {
-                write!(printer, "No cache entries found")?;
+                write!(printer.stderr(), "No cache entries found")?;
             }
             (0, 1) => {
-                write!(printer, "Removed 1 directory")?;
+                write!(printer.stderr(), "Removed 1 directory")?;
             }
             (0, num_dirs_removed) => {
-                write!(printer, "Removed {num_dirs_removed} directories")?;
+                write!(printer.stderr(), "Removed {num_dirs_removed} directories")?;
             }
             (1, _) => {
-                write!(printer, "Removed 1 file")?;
+                write!(printer.stderr(), "Removed 1 file")?;
             }
             (num_files_removed, _) => {
-                write!(printer, "Removed {num_files_removed} files")?;
+                write!(printer.stderr(), "Removed {num_files_removed} files")?;
             }
         }
 
         // If any, write a summary of the total byte count removed.
         if summary.total_bytes > 0 {
             let bytes = if summary.total_bytes < 1024 {
                 format!("{}B", summary.total_bytes)
             } else {
                 let (bytes, unit) = human_readable_bytes(summary.total_bytes);
                 format!("{bytes:.1}{unit}")
             };
-            write!(printer, " ({})", bytes.green())?;
+            write!(printer.stderr(), " ({})", bytes.green())?;
         }
 
-        writeln!(printer)?;
+        writeln!(printer.stderr())?;
     } else {
         for package in packages {
             let summary = cache.remove(package)?;
 
             // Write a summary of the number of files and directories removed.
             match (summary.num_files, summary.num_dirs) {
                 (0, 0) => {
-                    write!(printer, "No cache entries found for {}", package.cyan())?;
+                    write!(
+                        printer.stderr(),
+                        "No cache entries found for {}",
+                        package.cyan()
+                    )?;
                 }
                 (0, 1) => {
-                    write!(printer, "Removed 1 directory for {}", package.cyan())?;
+                    write!(
+                        printer.stderr(),
+                        "Removed 1 directory for {}",
+                        package.cyan()
+                    )?;
                 }
                 (0, num_dirs_removed) => {
                     write!(
-                        printer,
+                        printer.stderr(),
                         "Removed {num_dirs_removed} directories for {}",
                         package.cyan()
                     )?;
                 }
                 (1, _) => {
-                    write!(printer, "Removed 1 file for {}", package.cyan())?;
+                    write!(printer.stderr(), "Removed 1 file for {}", package.cyan())?;
                 }
                 (num_files_removed, _) => {
                     write!(
-                        printer,
+                        printer.stderr(),
                         "Removed {num_files_removed} files for {}",
                         package.cyan()
                     )?;
                 }
             }
 
             // If any, write a summary of the total byte count removed.
             if summary.total_bytes > 0 {
                 let bytes = if summary.total_bytes < 1024 {
                     format!("{}B", summary.total_bytes)
                 } else {
                     let (bytes, unit) = human_readable_bytes(summary.total_bytes);
                     format!("{bytes:.1}{unit}")
                 };
-                write!(printer, " ({})", bytes.green())?;
+                write!(printer.stderr(), " ({})", bytes.green())?;
             }
 
-            writeln!(printer)?;
+            writeln!(printer.stderr())?;
         }
     }
 
     Ok(ExitStatus::Success)
 }
-
-/// Formats a number of bytes into a human readable SI-prefixed size.
-///
-/// Returns a tuple of `(quantity, units)`.
-#[allow(
-    clippy::cast_possible_truncation,
-    clippy::cast_possible_wrap,
-    clippy::cast_precision_loss,
-    clippy::cast_sign_loss
-)]
-fn human_readable_bytes(bytes: u64) -> (f32, &'static str) {
-    static UNITS: [&str; 7] = ["B", "KiB", "MiB", "GiB", "TiB", "PiB", "EiB"];
-    let bytes = bytes as f32;
-    let i = ((bytes.log2() / 10.0) as usize).min(UNITS.len() - 1);
-    (bytes / 1024_f32.powi(i as i32), UNITS[i])
-}
```

### Comparing `uv-0.1.9/crates/uv/src/commands/pip_install.rs` & `uv-0.2.0/crates/uv/src/commands/project/mod.rs`

 * *Files 14% similar despite different names*

```diff
@@ -1,615 +1,392 @@
-use std::collections::HashSet;
 use std::fmt::Write;
 
-use std::path::Path;
-
-use anstream::eprint;
-use anyhow::{anyhow, Context, Result};
-use chrono::{DateTime, Utc};
+use anyhow::{Context, Result};
 use itertools::Itertools;
 use owo_colors::OwoColorize;
-use tempfile::tempdir_in;
-use tracing::debug;
 
-use distribution_types::{
-    IndexLocations, InstalledMetadata, LocalDist, LocalEditable, Name, Resolution,
-};
+use distribution_types::{IndexLocations, InstalledMetadata, LocalDist, Name, Resolution};
 use install_wheel_rs::linker::LinkMode;
-use pep508_rs::{MarkerEnvironment, Requirement};
-use platform_host::Platform;
+use pep508_rs::MarkerEnvironment;
 use platform_tags::Tags;
 use pypi_types::Yanked;
-use requirements_txt::EditableRequirement;
+use tracing::debug;
 use uv_cache::Cache;
-use uv_client::{Connectivity, FlatIndex, FlatIndexClient, RegistryClient, RegistryClientBuilder};
+use uv_client::{BaseClientBuilder, Connectivity, RegistryClient, RegistryClientBuilder};
+use uv_configuration::{
+    Concurrency, ConfigSettings, Constraints, NoBinary, NoBuild, Overrides, PreviewMode, Reinstall,
+    SetupPyStrategy,
+};
 use uv_dispatch::BuildDispatch;
-use uv_fs::Normalized;
-use uv_installer::{
-    BuiltEditable, Downloader, NoBinary, Plan, Planner, Reinstall, ResolvedEditable, SitePackages,
+use uv_distribution::DistributionDatabase;
+use uv_fs::Simplified;
+use uv_installer::{Downloader, Plan, Planner, SatisfiesResult, SitePackages};
+use uv_interpreter::{find_default_interpreter, Interpreter, PythonEnvironment};
+use uv_requirements::{
+    ExtrasSpecification, LookaheadResolver, NamedRequirementsResolver, ProjectWorkspace,
+    RequirementsSource, RequirementsSpecification, SourceTreeResolver,
 };
-use uv_interpreter::{Interpreter, Virtualenv};
-use uv_normalize::PackageName;
 use uv_resolver::{
-    DependencyMode, InMemoryIndex, Manifest, Options, OptionsBuilder, PreReleaseMode,
-    ResolutionGraph, ResolutionMode, Resolver,
+    Exclusions, FlatIndex, InMemoryIndex, Manifest, Options, OptionsBuilder, PythonRequirement,
+    ResolutionGraph, Resolver,
 };
-use uv_traits::{ConfigSettings, InFlight, NoBuild, SetupPyStrategy};
+use uv_types::{BuildIsolation, HashStrategy, InFlight, InstalledPackagesProvider};
 
 use crate::commands::reporters::{DownloadReporter, InstallReporter, ResolverReporter};
-use crate::commands::{elapsed, ChangeEvent, ChangeEventKind, ExitStatus};
+use crate::commands::{elapsed, ChangeEvent, ChangeEventKind};
+use crate::editables::ResolvedEditables;
 use crate::printer::Printer;
-use crate::requirements::{ExtrasSpecification, RequirementsSource, RequirementsSpecification};
 
-use super::Upgrade;
-
-/// Install packages into the current environment.
-#[allow(clippy::too_many_arguments)]
-pub(crate) async fn pip_install(
-    requirements: &[RequirementsSource],
-    constraints: &[RequirementsSource],
-    overrides: &[RequirementsSource],
-    extras: &ExtrasSpecification<'_>,
-    resolution_mode: ResolutionMode,
-    prerelease_mode: PreReleaseMode,
-    dependency_mode: DependencyMode,
-    upgrade: Upgrade,
-    index_locations: IndexLocations,
-    reinstall: &Reinstall,
-    link_mode: LinkMode,
-    setup_py: SetupPyStrategy,
-    connectivity: Connectivity,
-    config_settings: &ConfigSettings,
-    no_build: &NoBuild,
-    no_binary: &NoBinary,
-    strict: bool,
-    exclude_newer: Option<DateTime<Utc>>,
-    cache: Cache,
-    mut printer: Printer,
-) -> Result<ExitStatus> {
-    let start = std::time::Instant::now();
+pub(crate) mod lock;
+pub(crate) mod run;
+pub(crate) mod sync;
 
-    // Read all requirements from the provided sources.
-    let RequirementsSpecification {
-        project,
-        requirements,
-        constraints,
-        overrides,
-        editables,
-        index_url,
-        extra_index_urls,
-        no_index,
-        find_links,
-        extras: used_extras,
-    } = specification(requirements, constraints, overrides, extras)?;
-
-    // Incorporate any index locations from the provided sources.
-    let index_locations =
-        index_locations.combine(index_url, extra_index_urls, find_links, no_index);
-
-    // Check that all provided extras are used
-    if let ExtrasSpecification::Some(extras) = extras {
-        let mut unused_extras = extras
-            .iter()
-            .filter(|extra| !used_extras.contains(extra))
-            .collect::<Vec<_>>();
-        if !unused_extras.is_empty() {
-            unused_extras.sort_unstable();
-            unused_extras.dedup();
-            let s = if unused_extras.len() == 1 { "" } else { "s" };
-            return Err(anyhow!(
-                "Requested extra{s} not found: {}",
-                unused_extras.iter().join(", ")
-            ));
-        }
-    }
-
-    // Detect the current Python interpreter.
-    let platform = Platform::current()?;
-    let venv = Virtualenv::from_env(platform, &cache)?;
-    debug!(
-        "Using Python {} environment at {}",
-        venv.interpreter().python_version(),
-        venv.python_executable().normalized_display().cyan()
-    );
-
-    let _lock = venv.lock()?;
-
-    // Determine the set of installed packages.
-    let site_packages =
-        SitePackages::from_executable(&venv).context("Failed to list installed packages")?;
-
-    // If the requirements are already satisfied, we're done. Ideally, the resolver would be fast
-    // enough to let us remove this check. But right now, for large environments, it's an order of
-    // magnitude faster to validate the environment than to resolve the requirements.
-    if reinstall.is_none()
-        && upgrade.is_none()
-        && site_packages.satisfies(&requirements, &editables, &constraints)?
-    {
-        let num_requirements = requirements.len() + editables.len();
-        let s = if num_requirements == 1 { "" } else { "s" };
-        writeln!(
-            printer,
-            "{}",
-            format!(
-                "Audited {} in {}",
-                format!("{num_requirements} package{s}").bold(),
-                elapsed(start.elapsed())
-            )
-            .dimmed()
-        )?;
-        return Ok(ExitStatus::Success);
-    }
-
-    // Determine the tags, markers, and interpreter to use for resolution.
-    let interpreter = venv.interpreter().clone();
-    let tags = venv.interpreter().tags()?;
-    let markers = venv.interpreter().markers();
-
-    // Instantiate a client.
-    let client = RegistryClientBuilder::new(cache.clone())
-        .index_urls(index_locations.index_urls())
-        .connectivity(connectivity)
-        .build();
-
-    // Resolve the flat indexes from `--find-links`.
-    let flat_index = {
-        let client = FlatIndexClient::new(&client, &cache);
-        let entries = client.fetch(index_locations.flat_index()).await?;
-        FlatIndex::from_entries(entries, tags)
-    };
-
-    // Create a shared in-memory index.
-    let index = InMemoryIndex::default();
-
-    // Track in-flight downloads, builds, etc., across resolutions.
-    let in_flight = InFlight::default();
-
-    let resolve_dispatch = BuildDispatch::new(
-        &client,
-        &cache,
-        &interpreter,
-        &index_locations,
-        &flat_index,
-        &index,
-        &in_flight,
-        venv.python_executable(),
-        setup_py,
-        config_settings,
-        no_build,
-        no_binary,
-    )
-    .with_options(OptionsBuilder::new().exclude_newer(exclude_newer).build());
-
-    // Build all editable distributions. The editables are shared between resolution and
-    // installation, and should live for the duration of the command. If an editable is already
-    // installed in the environment, we'll still re-build it here.
-    let editable_wheel_dir;
-    let editables = if editables.is_empty() {
-        vec![]
-    } else {
-        editable_wheel_dir = tempdir_in(venv.root())?;
-        build_editables(
-            &editables,
-            editable_wheel_dir.path(),
-            &cache,
-            tags,
-            &client,
-            &resolve_dispatch,
-            printer,
-        )
-        .await?
-    };
-
-    let options = OptionsBuilder::new()
-        .resolution_mode(resolution_mode)
-        .prerelease_mode(prerelease_mode)
-        .dependency_mode(dependency_mode)
-        .exclude_newer(exclude_newer)
-        .build();
+#[derive(thiserror::Error, Debug)]
+pub(crate) enum Error {
+    #[error(transparent)]
+    Resolve(#[from] uv_resolver::ResolveError),
 
-    // Resolve the requirements.
-    let resolution = match resolve(
-        requirements,
-        constraints,
-        overrides,
-        project,
-        &editables,
-        &site_packages,
-        reinstall,
-        &upgrade,
-        &interpreter,
-        tags,
-        markers,
-        &client,
-        &flat_index,
-        &index,
-        &resolve_dispatch,
-        options,
-        printer,
-    )
-    .await
-    {
-        Ok(resolution) => Resolution::from(resolution),
-        Err(Error::Resolve(uv_resolver::ResolveError::NoSolution(err))) => {
-            let report = miette::Report::msg(format!("{err}"))
-                .context("No solution found when resolving dependencies:");
-            eprint!("{report:?}");
-            return Ok(ExitStatus::Failure);
-        }
-        Err(err) => return Err(err.into()),
-    };
+    #[error(transparent)]
+    Client(#[from] uv_client::Error),
 
-    // Re-initialize the in-flight map.
-    let in_flight = InFlight::default();
+    #[error(transparent)]
+    Platform(#[from] platform_tags::PlatformError),
 
-    // If we're running with `--reinstall`, initialize a separate `BuildDispatch`, since we may
-    // end up removing some distributions from the environment.
-    let install_dispatch = if reinstall.is_none() {
-        resolve_dispatch
-    } else {
-        BuildDispatch::new(
-            &client,
-            &cache,
-            &interpreter,
-            &index_locations,
-            &flat_index,
-            &index,
-            &in_flight,
-            venv.python_executable(),
-            setup_py,
-            config_settings,
-            no_build,
-            no_binary,
-        )
-        .with_options(OptionsBuilder::new().exclude_newer(exclude_newer).build())
-    };
+    #[error(transparent)]
+    Interpreter(#[from] uv_interpreter::Error),
 
-    // Sync the environment.
-    install(
-        &resolution,
-        editables,
-        site_packages,
-        reinstall,
-        no_binary,
-        link_mode,
-        &index_locations,
-        tags,
-        &client,
-        &in_flight,
-        &install_dispatch,
-        &cache,
-        &venv,
-        printer,
-    )
-    .await?;
+    #[error(transparent)]
+    Virtualenv(#[from] uv_virtualenv::Error),
 
-    // Validate the environment.
-    if strict {
-        validate(&resolution, &venv, printer)?;
-    }
+    #[error(transparent)]
+    Hash(#[from] uv_types::HashStrategyError),
 
-    Ok(ExitStatus::Success)
-}
+    #[error(transparent)]
+    Io(#[from] std::io::Error),
 
-/// Consolidate the requirements for an installation.
-fn specification(
-    requirements: &[RequirementsSource],
-    constraints: &[RequirementsSource],
-    overrides: &[RequirementsSource],
-    extras: &ExtrasSpecification<'_>,
-) -> Result<RequirementsSpecification, Error> {
-    // If the user requests `extras` but does not provide a pyproject toml source
-    if !matches!(extras, ExtrasSpecification::None)
-        && !requirements
-            .iter()
-            .any(|source| matches!(source, RequirementsSource::PyprojectToml(_)))
-    {
-        return Err(anyhow!("Requesting extras requires a pyproject.toml input file.").into());
-    }
+    #[error(transparent)]
+    Fmt(#[from] std::fmt::Error),
 
-    // Read all requirements from the provided sources.
-    let spec =
-        RequirementsSpecification::from_sources(requirements, constraints, overrides, extras)?;
+    #[error(transparent)]
+    Lookahead(#[from] uv_requirements::LookaheadError),
 
-    // Check that all provided extras are used
-    if let ExtrasSpecification::Some(extras) = extras {
-        let mut unused_extras = extras
-            .iter()
-            .filter(|extra| !spec.extras.contains(extra))
-            .collect::<Vec<_>>();
-        if !unused_extras.is_empty() {
-            unused_extras.sort_unstable();
-            unused_extras.dedup();
-            let s = if unused_extras.len() == 1 { "" } else { "s" };
-            return Err(anyhow!(
-                "Requested extra{s} not found: {}",
-                unused_extras.iter().join(", ")
-            )
-            .into());
-        }
-    }
+    #[error(transparent)]
+    ParsedUrl(#[from] Box<distribution_types::ParsedUrlError>),
 
-    Ok(spec)
+    #[error(transparent)]
+    Anyhow(#[from] anyhow::Error),
 }
 
-/// Build a set of editable distributions.
-async fn build_editables(
-    editables: &[EditableRequirement],
-    editable_wheel_dir: &Path,
+/// Initialize a virtual environment for the current project.
+pub(crate) fn init(
+    project: &ProjectWorkspace,
     cache: &Cache,
-    tags: &Tags,
-    client: &RegistryClient,
-    build_dispatch: &BuildDispatch<'_>,
-    mut printer: Printer,
-) -> Result<Vec<BuiltEditable>, Error> {
-    let start = std::time::Instant::now();
-
-    let downloader = Downloader::new(cache, tags, client, build_dispatch)
-        .with_reporter(DownloadReporter::from(printer).with_length(editables.len() as u64));
-
-    let editables: Vec<LocalEditable> = editables
-        .iter()
-        .map(|editable| {
-            let EditableRequirement { url, extras, path } = editable;
-            Ok(LocalEditable {
-                url: url.clone(),
-                extras: extras.clone(),
-                path: path.clone(),
-            })
-        })
-        .collect::<Result<_>>()?;
+    printer: Printer,
+) -> Result<PythonEnvironment, Error> {
+    let venv = project.workspace().root().join(".venv");
+
+    // Discover or create the virtual environment.
+    // TODO(charlie): If the environment isn't compatible with `--python`, recreate it.
+    match PythonEnvironment::from_root(&venv, cache) {
+        Ok(venv) => Ok(venv),
+        Err(uv_interpreter::Error::NotFound(_)) => {
+            // TODO(charlie): Respect `--python`; if unset, respect `Requires-Python`.
+            let interpreter = find_default_interpreter(cache)
+                .map_err(uv_interpreter::Error::from)?
+                .map_err(uv_interpreter::Error::from)?
+                .into_interpreter();
 
-    let editables: Vec<_> = downloader
-        .build_editables(editables, editable_wheel_dir)
-        .await
-        .context("Failed to build editables")?
-        .into_iter()
-        .collect();
+            writeln!(
+                printer.stderr(),
+                "Using Python {} interpreter at: {}",
+                interpreter.python_version(),
+                interpreter.sys_executable().user_display().cyan()
+            )?;
 
-    let s = if editables.len() == 1 { "" } else { "s" };
-    writeln!(
-        printer,
-        "{}",
-        format!(
-            "Built {} in {}",
-            format!("{} editable{}", editables.len(), s).bold(),
-            elapsed(start.elapsed())
-        )
-        .dimmed()
-    )?;
+            writeln!(
+                printer.stderr(),
+                "Creating virtualenv at: {}",
+                venv.user_display().cyan()
+            )?;
 
-    Ok(editables)
+            Ok(uv_virtualenv::create_venv(
+                &venv,
+                interpreter,
+                uv_virtualenv::Prompt::None,
+                false,
+                false,
+            )?)
+        }
+        Err(e) => Err(e.into()),
+    }
 }
 
 /// Resolve a set of requirements, similar to running `pip compile`.
 #[allow(clippy::too_many_arguments)]
-async fn resolve(
-    requirements: Vec<Requirement>,
-    constraints: Vec<Requirement>,
-    overrides: Vec<Requirement>,
-    project: Option<PackageName>,
-    editables: &[BuiltEditable],
-    site_packages: &SitePackages<'_>,
-    reinstall: &Reinstall,
-    upgrade: &Upgrade,
+pub(crate) async fn resolve<InstalledPackages: InstalledPackagesProvider>(
+    spec: RequirementsSpecification,
+    installed_packages: InstalledPackages,
+    editables: &ResolvedEditables,
+    hasher: &HashStrategy,
     interpreter: &Interpreter,
     tags: &Tags,
     markers: &MarkerEnvironment,
     client: &RegistryClient,
     flat_index: &FlatIndex,
     index: &InMemoryIndex,
     build_dispatch: &BuildDispatch<'_>,
     options: Options,
-    mut printer: Printer,
+    printer: Printer,
+    concurrency: Concurrency,
 ) -> Result<ResolutionGraph, Error> {
     let start = std::time::Instant::now();
 
-    let preferences = if upgrade.is_all() || reinstall.is_all() {
-        vec![]
-    } else {
-        // Combine upgrade and reinstall lists
-        let mut exclusions: HashSet<&PackageName> = if let Reinstall::Packages(packages) = reinstall
-        {
-            HashSet::from_iter(packages)
-        } else {
-            HashSet::default()
-        };
-        if let Upgrade::Packages(packages) = upgrade {
-            exclusions.extend(packages);
-        };
+    let exclusions = Exclusions::None;
+    let preferences = Vec::new();
+    let constraints = Constraints::default();
+    let overrides = Overrides::default();
+    let python_requirement = PythonRequirement::from_marker_environment(interpreter, markers);
+
+    let RequirementsSpecification {
+        project,
+        requirements,
+        constraints: _,
+        overrides: _,
+        editables: _,
+        source_trees,
+        extras: _,
+        index_url: _,
+        extra_index_urls: _,
+        no_index: _,
+        find_links: _,
+        no_binary: _,
+        no_build: _,
+    } = spec;
+
+    // Resolve the requirements from the provided sources.
+    let requirements = {
+        // Convert from unnamed to named requirements.
+        let mut requirements = NamedRequirementsResolver::new(
+            requirements,
+            hasher,
+            index,
+            DistributionDatabase::new(client, build_dispatch, concurrency.downloads),
+        )
+        .with_reporter(ResolverReporter::from(printer))
+        .resolve()
+        .await?;
+
+        // Resolve any source trees into requirements.
+        if !source_trees.is_empty() {
+            requirements.extend(
+                SourceTreeResolver::new(
+                    source_trees,
+                    &ExtrasSpecification::None,
+                    hasher,
+                    index,
+                    DistributionDatabase::new(client, build_dispatch, concurrency.downloads),
+                )
+                .with_reporter(ResolverReporter::from(printer))
+                .resolve()
+                .await?,
+            );
+        }
 
-        // Prefer current site packages, unless in the upgrade or reinstall lists
-        site_packages
-            .requirements()
-            .filter(|requirement| !exclusions.contains(&requirement.name))
-            .collect()
+        requirements
     };
 
-    // Map the editables to their metadata.
-    let editables = editables
-        .iter()
-        .map(|built_editable| {
-            (
-                built_editable.editable.clone(),
-                built_editable.metadata.clone(),
-            )
-        })
-        .collect();
+    // Determine any lookahead requirements.
+    let editable_metadata = editables.as_metadata().map_err(Error::ParsedUrl)?;
+    let lookaheads = LookaheadResolver::new(
+        &requirements,
+        &constraints,
+        &overrides,
+        &editable_metadata,
+        hasher,
+        index,
+        DistributionDatabase::new(client, build_dispatch, concurrency.downloads),
+    )
+    .with_reporter(ResolverReporter::from(printer))
+    .resolve(Some(markers))
+    .await?;
 
     // Create a manifest of the requirements.
     let manifest = Manifest::new(
         requirements,
         constraints,
         overrides,
         preferences,
         project,
-        editables,
+        editable_metadata,
+        exclusions,
+        lookaheads,
     );
 
     // Resolve the dependencies.
     let resolver = Resolver::new(
         manifest,
         options,
-        markers,
-        interpreter,
+        &python_requirement,
+        Some(markers),
         tags,
-        client,
         flat_index,
         index,
+        hasher,
         build_dispatch,
+        installed_packages,
+        DistributionDatabase::new(client, build_dispatch, concurrency.downloads),
     )?
     .with_reporter(ResolverReporter::from(printer));
     let resolution = resolver.resolve().await?;
 
     let s = if resolution.len() == 1 { "" } else { "s" };
     writeln!(
-        printer,
+        printer.stderr(),
         "{}",
         format!(
             "Resolved {} in {}",
             format!("{} package{}", resolution.len(), s).bold(),
             elapsed(start.elapsed())
         )
         .dimmed()
     )?;
 
+    // Notify the user of any diagnostics.
+    for diagnostic in resolution.diagnostics() {
+        writeln!(
+            printer.stderr(),
+            "{}{} {}",
+            "warning".yellow().bold(),
+            ":".bold(),
+            diagnostic.message().bold()
+        )?;
+    }
+
     Ok(resolution)
 }
 
 /// Install a set of requirements into the current environment.
 #[allow(clippy::too_many_arguments)]
-async fn install(
+pub(crate) async fn install(
     resolution: &Resolution,
-    built_editables: Vec<BuiltEditable>,
-    site_packages: SitePackages<'_>,
-    reinstall: &Reinstall,
+    resolved_editables: ResolvedEditables,
+    site_packages: SitePackages,
     no_binary: &NoBinary,
     link_mode: LinkMode,
     index_urls: &IndexLocations,
+    hasher: &HashStrategy,
     tags: &Tags,
     client: &RegistryClient,
     in_flight: &InFlight,
     build_dispatch: &BuildDispatch<'_>,
     cache: &Cache,
-    venv: &Virtualenv,
-    mut printer: Printer,
+    venv: &PythonEnvironment,
+    printer: Printer,
+    concurrency: Concurrency,
 ) -> Result<(), Error> {
     let start = std::time::Instant::now();
 
-    // Partition into those that should be linked from the cache (`local`), those that need to be
-    // downloaded (`remote`), and those that should be removed (`extraneous`).
     let requirements = resolution.requirements();
-    let editables = built_editables
-        .into_iter()
-        .map(ResolvedEditable::Built)
-        .collect::<Vec<_>>();
 
-    let Plan {
-        local,
-        remote,
-        reinstalls,
-        extraneous: _,
-    } = Planner::with_requirements(&requirements)
-        .with_editable_requirements(&editables)
+    // Partition into those that should be linked from the cache (`local`), those that need to be
+    // downloaded (`remote`), and those that should be removed (`extraneous`).
+    let plan = Planner::with_requirements(&requirements)
+        .with_editable_requirements(&resolved_editables.editables)
         .build(
             site_packages,
-            reinstall,
+            &Reinstall::None,
             no_binary,
+            hasher,
             index_urls,
             cache,
             venv,
             tags,
         )
         .context("Failed to determine installation plan")?;
 
+    let Plan {
+        cached,
+        remote,
+        reinstalls,
+        extraneous: _,
+    } = plan;
+
     // Nothing to do.
-    if remote.is_empty() && local.is_empty() && reinstalls.is_empty() {
+    if remote.is_empty() && cached.is_empty() {
         let s = if resolution.len() == 1 { "" } else { "s" };
         writeln!(
-            printer,
+            printer.stderr(),
             "{}",
             format!(
                 "Audited {} in {}",
                 format!("{} package{}", resolution.len(), s).bold(),
                 elapsed(start.elapsed())
             )
             .dimmed()
         )?;
-
         return Ok(());
     }
 
     // Map any registry-based requirements back to those returned by the resolver.
     let remote = remote
         .iter()
         .map(|dist| {
             resolution
-                .get(&dist.name)
+                .get_remote(&dist.name)
                 .cloned()
                 .expect("Resolution should contain all packages")
         })
         .collect::<Vec<_>>();
 
     // Download, build, and unzip any missing distributions.
     let wheels = if remote.is_empty() {
         vec![]
     } else {
         let start = std::time::Instant::now();
 
-        let downloader = Downloader::new(cache, tags, client, build_dispatch)
-            .with_reporter(DownloadReporter::from(printer).with_length(remote.len() as u64));
+        let downloader = Downloader::new(
+            cache,
+            tags,
+            hasher,
+            DistributionDatabase::new(client, build_dispatch, concurrency.downloads),
+        )
+        .with_reporter(DownloadReporter::from(printer).with_length(remote.len() as u64));
 
         let wheels = downloader
             .download(remote.clone(), in_flight)
             .await
             .context("Failed to download distributions")?;
 
         let s = if wheels.len() == 1 { "" } else { "s" };
         writeln!(
-            printer,
+            printer.stderr(),
             "{}",
             format!(
                 "Downloaded {} in {}",
                 format!("{} package{}", wheels.len(), s).bold(),
                 elapsed(start.elapsed())
             )
             .dimmed()
         )?;
 
         wheels
     };
 
-    // Remove any existing installations.
-    if !reinstalls.is_empty() {
-        for dist_info in &reinstalls {
-            let summary = uv_installer::uninstall(dist_info).await?;
-            debug!(
-                "Uninstalled {} ({} file{}, {} director{})",
-                dist_info.name(),
-                summary.file_count,
-                if summary.file_count == 1 { "" } else { "s" },
-                summary.dir_count,
-                if summary.dir_count == 1 { "y" } else { "ies" },
-            );
-        }
-    }
-
     // Install the resolved distributions.
-    let wheels = wheels.into_iter().chain(local).collect::<Vec<_>>();
+    let wheels = wheels.into_iter().chain(cached).collect::<Vec<_>>();
     if !wheels.is_empty() {
         let start = std::time::Instant::now();
         uv_installer::Installer::new(venv)
             .with_link_mode(link_mode)
             .with_reporter(InstallReporter::from(printer).with_length(wheels.len() as u64))
             .install(&wheels)?;
 
         let s = if wheels.len() == 1 { "" } else { "s" };
         writeln!(
-            printer,
+            printer.stderr(),
             "{}",
             format!(
                 "Installed {} in {}",
                 format!("{} package{}", wheels.len(), s).bold(),
                 elapsed(start.elapsed())
             )
             .dimmed()
@@ -627,29 +404,30 @@
             kind: ChangeEventKind::Added,
         }))
         .sorted_unstable_by(|a, b| {
             a.dist
                 .name()
                 .cmp(b.dist.name())
                 .then_with(|| a.kind.cmp(&b.kind))
+                .then_with(|| a.dist.installed_version().cmp(&b.dist.installed_version()))
         })
     {
         match event.kind {
             ChangeEventKind::Added => {
                 writeln!(
-                    printer,
+                    printer.stderr(),
                     " {} {}{}",
                     "+".green(),
                     event.dist.name().as_ref().bold(),
                     event.dist.installed_version().to_string().dimmed()
                 )?;
             }
             ChangeEventKind::Removed => {
                 writeln!(
-                    printer,
+                    printer.stderr(),
                     " {} {}{}",
                     "-".red(),
                     event.dist.name().as_ref().bold(),
                     event.dist.installed_version().to_string().dimmed()
                 )?;
             }
         }
@@ -661,69 +439,203 @@
         let Some(file) = dist.file() else {
             continue;
         };
         match &file.yanked {
             None | Some(Yanked::Bool(false)) => {}
             Some(Yanked::Bool(true)) => {
                 writeln!(
-                    printer,
+                    printer.stderr(),
                     "{}{} {dist} is yanked.",
                     "warning".yellow().bold(),
                     ":".bold(),
                 )?;
             }
             Some(Yanked::Reason(reason)) => {
                 writeln!(
-                    printer,
+                    printer.stderr(),
                     "{}{} {dist} is yanked (reason: \"{reason}\").",
                     "warning".yellow().bold(),
                     ":".bold(),
                 )?;
             }
         }
     }
 
     Ok(())
 }
 
-/// Validate the installed packages in the virtual environment.
-fn validate(resolution: &Resolution, venv: &Virtualenv, mut printer: Printer) -> Result<(), Error> {
-    let site_packages = SitePackages::from_executable(venv)?;
-    let diagnostics = site_packages.diagnostics()?;
-    for diagnostic in diagnostics {
-        // Only surface diagnostics that are "relevant" to the current resolution.
-        if resolution
-            .packages()
-            .any(|package| diagnostic.includes(package))
-        {
-            writeln!(
-                printer,
-                "{}{} {}",
-                "warning".yellow().bold(),
-                ":".bold(),
-                diagnostic.message().bold()
-            )?;
+/// Update a [`PythonEnvironment`] to satisfy a set of [`RequirementsSource`]s.
+pub(crate) async fn update_environment(
+    venv: PythonEnvironment,
+    requirements: &[RequirementsSource],
+    preview: PreviewMode,
+    connectivity: Connectivity,
+    cache: &Cache,
+    printer: Printer,
+) -> Result<PythonEnvironment> {
+    // TODO(zanieb): Support client configuration
+    let client_builder = BaseClientBuilder::default().connectivity(connectivity);
+
+    // Read all requirements from the provided sources.
+    // TODO(zanieb): Consider allowing constraints and extras
+    // TODO(zanieb): Allow specifying extras somehow
+    let spec = RequirementsSpecification::from_sources(
+        requirements,
+        &[],
+        &[],
+        &ExtrasSpecification::None,
+        &client_builder,
+        preview,
+    )
+    .await?;
+
+    // Check if the current environment satisfies the requirements
+    let site_packages = SitePackages::from_executable(&venv)?;
+
+    // If the requirements are already satisfied, we're done.
+    if spec.source_trees.is_empty() {
+        match site_packages.satisfies(&spec.requirements, &spec.editables, &spec.constraints)? {
+            SatisfiesResult::Fresh {
+                recursive_requirements,
+            } => {
+                debug!(
+                    "All requirements satisfied: {}",
+                    recursive_requirements
+                        .iter()
+                        .map(|entry| entry.requirement.to_string())
+                        .sorted()
+                        .join(" | ")
+                );
+                debug!(
+                    "All editables satisfied: {}",
+                    spec.editables.iter().map(ToString::to_string).join(", ")
+                );
+                return Ok(venv);
+            }
+            SatisfiesResult::Unsatisfied(requirement) => {
+                debug!("At least one requirement is not satisfied: {requirement}");
+            }
         }
     }
-    Ok(())
-}
 
-#[derive(thiserror::Error, Debug)]
-enum Error {
-    #[error(transparent)]
-    Resolve(#[from] uv_resolver::ResolveError),
+    // Determine the tags, markers, and interpreter to use for resolution.
+    let interpreter = venv.interpreter().clone();
+    let tags = venv.interpreter().tags()?;
+    let markers = venv.interpreter().markers();
 
-    #[error(transparent)]
-    Client(#[from] uv_client::Error),
+    // Initialize the registry client.
+    // TODO(zanieb): Support client options e.g. offline, tls, etc.
+    let client = RegistryClientBuilder::new(cache.clone())
+        .connectivity(connectivity)
+        .markers(markers)
+        .platform(venv.interpreter().platform())
+        .build();
 
-    #[error(transparent)]
-    Platform(#[from] platform_host::PlatformError),
+    // TODO(charlie): Respect project configuration.
+    let build_isolation = BuildIsolation::default();
+    let config_settings = ConfigSettings::default();
+    let flat_index = FlatIndex::default();
+    let hasher = HashStrategy::default();
+    let in_flight = InFlight::default();
+    let index = InMemoryIndex::default();
+    let index_locations = IndexLocations::default();
+    let link_mode = LinkMode::default();
+    let no_binary = NoBinary::default();
+    let no_build = NoBuild::default();
+    let setup_py = SetupPyStrategy::default();
+    let concurrency = Concurrency::default();
+    let reinstall = Reinstall::None;
 
-    #[error(transparent)]
-    Io(#[from] std::io::Error),
+    // Create a build dispatch.
+    let build_dispatch = BuildDispatch::new(
+        &client,
+        cache,
+        &interpreter,
+        &index_locations,
+        &flat_index,
+        &index,
+        &in_flight,
+        setup_py,
+        &config_settings,
+        build_isolation,
+        link_mode,
+        &no_build,
+        &no_binary,
+        concurrency,
+    );
 
-    #[error(transparent)]
-    Fmt(#[from] std::fmt::Error),
+    let options = OptionsBuilder::new()
+        // TODO(zanieb): Support resolver options
+        // .resolution_mode(resolution_mode)
+        // .prerelease_mode(prerelease_mode)
+        // .dependency_mode(dependency_mode)
+        // .exclude_newer(exclude_newer)
+        .build();
 
-    #[error(transparent)]
-    Anyhow(#[from] anyhow::Error),
+    // Build all editable distributions. The editables are shared between resolution and
+    // installation, and should live for the duration of the command.
+    let editables = ResolvedEditables::resolve(
+        spec.editables
+            .iter()
+            .cloned()
+            .map(ResolvedEditables::from_requirement),
+        &site_packages,
+        &reinstall,
+        &hasher,
+        &interpreter,
+        tags,
+        cache,
+        &client,
+        &build_dispatch,
+        concurrency,
+        printer,
+    )
+    .await?;
+
+    // Resolve the requirements.
+    let resolution = match resolve(
+        spec,
+        site_packages.clone(),
+        &editables,
+        &hasher,
+        &interpreter,
+        tags,
+        markers,
+        &client,
+        &flat_index,
+        &index,
+        &build_dispatch,
+        options,
+        printer,
+        concurrency,
+    )
+    .await
+    {
+        Ok(resolution) => Resolution::from(resolution),
+        Err(err) => return Err(err.into()),
+    };
+
+    // Re-initialize the in-flight map.
+    let in_flight = InFlight::default();
+
+    // Sync the environment.
+    install(
+        &resolution,
+        editables,
+        site_packages,
+        &no_binary,
+        link_mode,
+        &index_locations,
+        &hasher,
+        tags,
+        &client,
+        &in_flight,
+        &build_dispatch,
+        cache,
+        &venv,
+        printer,
+        concurrency,
+    )
+    .await?;
+
+    Ok(venv)
 }
```

### Comparing `uv-0.1.9/crates/uv/src/commands/reporters.rs` & `uv-0.2.0/crates/uv/src/commands/reporters.rs`

 * *Files 17% similar despite different names*

```diff
@@ -2,56 +2,22 @@
 use std::time::Duration;
 
 use indicatif::{MultiProgress, ProgressBar, ProgressStyle};
 use owo_colors::OwoColorize;
 use url::Url;
 
 use distribution_types::{
-    CachedDist, Dist, DistributionMetadata, LocalEditable, Name, SourceDist, VersionOrUrl,
+    BuildableSource, CachedDist, DistributionMetadata, LocalEditable, Name, SourceDist,
+    VersionOrUrlRef,
 };
 use uv_normalize::PackageName;
 
 use crate::printer::Printer;
 
 #[derive(Debug)]
-pub(crate) struct FinderReporter {
-    progress: ProgressBar,
-}
-
-impl From<Printer> for FinderReporter {
-    fn from(printer: Printer) -> Self {
-        let progress = ProgressBar::with_draw_target(None, printer.target());
-        progress.set_style(
-            ProgressStyle::with_template("{bar:20} [{pos}/{len}] {wide_msg:.dim}").unwrap(),
-        );
-        progress.set_message("Resolving dependencies...");
-        Self { progress }
-    }
-}
-
-impl FinderReporter {
-    #[must_use]
-    pub(crate) fn with_length(self, length: u64) -> Self {
-        self.progress.set_length(length);
-        self
-    }
-}
-
-impl uv_resolver::FinderReporter for FinderReporter {
-    fn on_progress(&self, dist: &Dist) {
-        self.progress.set_message(format!("{dist}"));
-        self.progress.inc(1);
-    }
-
-    fn on_complete(&self) {
-        self.progress.finish_and_clear();
-    }
-}
-
-#[derive(Debug)]
 pub(crate) struct DownloadReporter {
     printer: Printer,
     multi_progress: MultiProgress,
     progress: ProgressBar,
     bars: Arc<Mutex<Vec<ProgressBar>>>,
 }
 
@@ -110,20 +76,20 @@
         self.progress.inc(1);
     }
 
     fn on_complete(&self) {
         self.progress.finish_and_clear();
     }
 
-    fn on_build_start(&self, dist: &SourceDist) -> usize {
-        self.on_any_build_start(&dist.to_color_string())
+    fn on_build_start(&self, source: &BuildableSource) -> usize {
+        self.on_any_build_start(&source.to_color_string())
     }
 
-    fn on_build_complete(&self, dist: &SourceDist, index: usize) {
-        self.on_any_build_complete(&dist.to_color_string(), index);
+    fn on_build_complete(&self, source: &BuildableSource, index: usize) {
+        self.on_any_build_complete(&source.to_color_string(), index);
     }
 
     fn on_editable_build_start(&self, dist: &LocalEditable) -> usize {
         self.on_any_build_start(&dist.to_color_string())
     }
 
     fn on_editable_build_complete(&self, dist: &LocalEditable, id: usize) {
@@ -223,55 +189,61 @@
             multi_progress,
             progress,
             bars: Arc::new(Mutex::new(Vec::new())),
         }
     }
 }
 
-impl uv_resolver::ResolverReporter for ResolverReporter {
-    fn on_progress(&self, name: &PackageName, version_or_url: VersionOrUrl) {
+impl ResolverReporter {
+    #[must_use]
+    pub(crate) fn with_length(self, length: u64) -> Self {
+        self.progress.set_length(length);
+        self
+    }
+
+    fn on_progress(&self, name: &PackageName, version_or_url: &VersionOrUrlRef) {
         match version_or_url {
-            VersionOrUrl::Version(version) => {
+            VersionOrUrlRef::Version(version) => {
                 self.progress.set_message(format!("{name}=={version}"));
             }
-            VersionOrUrl::Url(url) => {
+            VersionOrUrlRef::Url(url) => {
                 self.progress.set_message(format!("{name} @ {url}"));
             }
         }
     }
 
     fn on_complete(&self) {
         self.progress.finish_and_clear();
     }
 
-    fn on_build_start(&self, dist: &SourceDist) -> usize {
+    fn on_build_start(&self, source: &BuildableSource) -> usize {
         let progress = self.multi_progress.insert_before(
             &self.progress,
             ProgressBar::with_draw_target(None, self.printer.target()),
         );
 
         progress.set_style(ProgressStyle::with_template("{wide_msg}").unwrap());
         progress.set_message(format!(
             "{} {}",
             "Building".bold().cyan(),
-            dist.to_color_string(),
+            source.to_color_string(),
         ));
 
         let mut bars = self.bars.lock().unwrap();
         bars.push(progress);
         bars.len() - 1
     }
 
-    fn on_build_complete(&self, dist: &SourceDist, index: usize) {
+    fn on_build_complete(&self, source: &BuildableSource, index: usize) {
         let bars = self.bars.lock().unwrap();
         let progress = &bars[index];
         progress.finish_with_message(format!(
             "   {} {}",
             "Built".bold().green(),
-            dist.to_color_string(),
+            source.to_color_string(),
         ));
     }
 
     fn on_checkout_start(&self, url: &Url, rev: &str) -> usize {
         let progress = self.multi_progress.insert_before(
             &self.progress,
             ProgressBar::with_draw_target(None, self.printer.target()),
@@ -299,25 +271,78 @@
             "Updated".bold().green(),
             url,
             rev.dimmed()
         ));
     }
 }
 
+impl uv_resolver::ResolverReporter for ResolverReporter {
+    fn on_progress(&self, name: &PackageName, version_or_url: &VersionOrUrlRef) {
+        self.on_progress(name, version_or_url);
+    }
+
+    fn on_complete(&self) {
+        self.on_complete();
+    }
+
+    fn on_build_start(&self, source: &BuildableSource) -> usize {
+        self.on_build_start(source)
+    }
+
+    fn on_build_complete(&self, source: &BuildableSource, index: usize) {
+        self.on_build_complete(source, index);
+    }
+
+    fn on_checkout_start(&self, url: &Url, rev: &str) -> usize {
+        self.on_checkout_start(url, rev)
+    }
+
+    fn on_checkout_complete(&self, url: &Url, rev: &str, index: usize) {
+        self.on_checkout_complete(url, rev, index);
+    }
+}
+
+impl uv_distribution::Reporter for ResolverReporter {
+    fn on_build_start(&self, source: &BuildableSource) -> usize {
+        self.on_build_start(source)
+    }
+
+    fn on_build_complete(&self, source: &BuildableSource, index: usize) {
+        self.on_build_complete(source, index);
+    }
+
+    fn on_checkout_start(&self, url: &Url, rev: &str) -> usize {
+        self.on_checkout_start(url, rev)
+    }
+
+    fn on_checkout_complete(&self, url: &Url, rev: &str, index: usize) {
+        self.on_checkout_complete(url, rev, index);
+    }
+}
+
 /// Like [`std::fmt::Display`], but with colors.
 trait ColorDisplay {
     fn to_color_string(&self) -> String;
 }
 
 impl ColorDisplay for SourceDist {
     fn to_color_string(&self) -> String {
         let name = self.name();
         let version_or_url = self.version_or_url();
         format!("{}{}", name, version_or_url.to_string().dimmed())
     }
 }
 
+impl ColorDisplay for BuildableSource<'_> {
+    fn to_color_string(&self) -> String {
+        match self {
+            BuildableSource::Dist(dist) => dist.to_color_string(),
+            BuildableSource::Url(url) => url.to_string(),
+        }
+    }
+}
+
 impl ColorDisplay for LocalEditable {
     fn to_color_string(&self) -> String {
         format!("{}", self.to_string().dimmed())
     }
 }
```

### Comparing `uv-0.1.9/crates/uv/src/commands/venv.rs` & `uv-0.2.0/crates/uv/src/commands/venv.rs`

 * *Files 25% similar despite different names*

```diff
@@ -1,57 +1,75 @@
 use std::fmt::Write;
 use std::path::Path;
 use std::str::FromStr;
 use std::vec;
 
 use anstream::eprint;
 use anyhow::Result;
-use chrono::{DateTime, Utc};
 use itertools::Itertools;
 use miette::{Diagnostic, IntoDiagnostic};
 use owo_colors::OwoColorize;
 use thiserror::Error;
 
-use distribution_types::{DistributionMetadata, IndexLocations, Name};
-use gourgeist::Prompt;
-use pep508_rs::Requirement;
-use platform_host::Platform;
+use distribution_types::{DistributionMetadata, IndexLocations, Name, Requirement, ResolvedDist};
+use install_wheel_rs::linker::LinkMode;
+use uv_auth::store_credentials_from_url;
 use uv_cache::Cache;
-use uv_client::{Connectivity, FlatIndex, FlatIndexClient, RegistryClientBuilder};
+use uv_client::{Connectivity, FlatIndexClient, RegistryClientBuilder};
+use uv_configuration::{Concurrency, KeyringProviderType};
+use uv_configuration::{ConfigSettings, IndexStrategy, NoBinary, NoBuild, SetupPyStrategy};
 use uv_dispatch::BuildDispatch;
-use uv_fs::Normalized;
-use uv_installer::NoBinary;
-use uv_interpreter::{find_default_python, find_requested_python, Error};
-use uv_resolver::{InMemoryIndex, OptionsBuilder};
-use uv_traits::{BuildContext, ConfigSettings, InFlight, NoBuild, SetupPyStrategy};
+use uv_fs::Simplified;
+use uv_interpreter::{
+    find_default_interpreter, find_interpreter, InterpreterRequest, SourceSelector,
+};
+use uv_resolver::{ExcludeNewer, FlatIndex, InMemoryIndex, OptionsBuilder};
+use uv_types::{BuildContext, BuildIsolation, HashStrategy, InFlight};
 
 use crate::commands::ExitStatus;
 use crate::printer::Printer;
+use crate::shell::Shell;
 
 /// Create a virtual environment.
-#[allow(clippy::unnecessary_wraps, clippy::too_many_arguments)]
+#[allow(
+    clippy::unnecessary_wraps,
+    clippy::too_many_arguments,
+    clippy::fn_params_excessive_bools
+)]
 pub(crate) async fn venv(
     path: &Path,
     python_request: Option<&str>,
+    link_mode: LinkMode,
     index_locations: &IndexLocations,
-    prompt: Prompt,
+    index_strategy: IndexStrategy,
+    keyring_provider: KeyringProviderType,
+    prompt: uv_virtualenv::Prompt,
+    system_site_packages: bool,
     connectivity: Connectivity,
     seed: bool,
-    exclude_newer: Option<DateTime<Utc>>,
+    allow_existing: bool,
+    exclude_newer: Option<ExcludeNewer>,
+    native_tls: bool,
     cache: &Cache,
     printer: Printer,
 ) -> Result<ExitStatus> {
     match venv_impl(
         path,
         python_request,
+        link_mode,
         index_locations,
+        index_strategy,
+        keyring_provider,
         prompt,
+        system_site_packages,
         connectivity,
         seed,
+        allow_existing,
         exclude_newer,
+        native_tls,
         cache,
         printer,
     )
     .await
     {
         Ok(status) => Ok(status),
         Err(err) => {
@@ -61,15 +79,15 @@
     }
 }
 
 #[derive(Error, Debug, Diagnostic)]
 enum VenvError {
     #[error("Failed to create virtualenv")]
     #[diagnostic(code(uv::venv::creation))]
-    Creation(#[source] gourgeist::Error),
+    Creation(#[source] uv_virtualenv::Error),
 
     #[error("Failed to install seed packages")]
     #[diagnostic(code(uv::venv::seed))]
     Seed(#[source] anyhow::Error),
 
     #[error("Failed to extract interpreter tags")]
     #[diagnostic(code(uv::venv::tags))]
@@ -77,153 +95,242 @@
 
     #[error("Failed to resolve `--find-links` entry")]
     #[diagnostic(code(uv::venv::flat_index))]
     FlatIndex(#[source] uv_client::FlatIndexError),
 }
 
 /// Create a virtual environment.
-#[allow(clippy::too_many_arguments)]
+#[allow(clippy::too_many_arguments, clippy::fn_params_excessive_bools)]
 async fn venv_impl(
     path: &Path,
     python_request: Option<&str>,
+    link_mode: LinkMode,
     index_locations: &IndexLocations,
-    prompt: Prompt,
+    index_strategy: IndexStrategy,
+    keyring_provider: KeyringProviderType,
+    prompt: uv_virtualenv::Prompt,
+    system_site_packages: bool,
     connectivity: Connectivity,
     seed: bool,
-    exclude_newer: Option<DateTime<Utc>>,
+    allow_existing: bool,
+    exclude_newer: Option<ExcludeNewer>,
+    native_tls: bool,
     cache: &Cache,
-    mut printer: Printer,
+    printer: Printer,
 ) -> miette::Result<ExitStatus> {
     // Locate the Python interpreter.
-    let platform = Platform::current().into_diagnostic()?;
-    let interpreter = if let Some(python_request) = python_request {
-        find_requested_python(python_request, &platform, cache)
-            .into_diagnostic()?
-            .ok_or(Error::NoSuchPython(python_request.to_string()))
-            .into_diagnostic()?
+    let interpreter = if let Some(python) = python_request.as_ref() {
+        let system = uv_interpreter::SystemPython::Required;
+        let request = InterpreterRequest::parse(python);
+        let sources = SourceSelector::from_settings(system);
+        find_interpreter(&request, system, &sources, cache)
     } else {
-        find_default_python(&platform, cache).into_diagnostic()?
-    };
+        find_default_interpreter(cache)
+    }
+    .into_diagnostic()?
+    .into_diagnostic()?
+    .into_interpreter();
+
+    // Add all authenticated sources to the cache.
+    for url in index_locations.urls() {
+        store_credentials_from_url(url);
+    }
 
     writeln!(
-        printer,
-        "Using Python {} interpreter at {}",
+        printer.stderr(),
+        "Using Python {} interpreter at: {}",
         interpreter.python_version(),
-        interpreter.sys_executable().normalized_display().cyan()
+        interpreter.sys_executable().user_display().cyan()
     )
     .into_diagnostic()?;
 
     writeln!(
-        printer,
+        printer.stderr(),
         "Creating virtualenv at: {}",
-        path.normalized_display().cyan()
+        path.user_display().cyan()
     )
     .into_diagnostic()?;
 
-    // Extra cfg for pyvenv.cfg to specify uv version
-    let extra_cfg = vec![("uv".to_string(), env!("CARGO_PKG_VERSION").to_string())];
-
     // Create the virtual environment.
-    let venv = gourgeist::create_venv(path, interpreter, prompt, extra_cfg)
-        .map_err(VenvError::Creation)?;
+    let venv = uv_virtualenv::create_venv(
+        path,
+        interpreter,
+        prompt,
+        system_site_packages,
+        allow_existing,
+    )
+    .map_err(VenvError::Creation)?;
 
     // Install seed packages.
     if seed {
         // Extract the interpreter.
         let interpreter = venv.interpreter();
 
         // Instantiate a client.
         let client = RegistryClientBuilder::new(cache.clone())
+            .native_tls(native_tls)
             .index_urls(index_locations.index_urls())
+            .index_strategy(index_strategy)
+            .keyring(keyring_provider)
             .connectivity(connectivity)
+            .markers(interpreter.markers())
+            .platform(interpreter.platform())
             .build();
 
         // Resolve the flat indexes from `--find-links`.
         let flat_index = {
             let tags = interpreter.tags().map_err(VenvError::Tags)?;
             let client = FlatIndexClient::new(&client, cache);
             let entries = client
                 .fetch(index_locations.flat_index())
                 .await
                 .map_err(VenvError::FlatIndex)?;
-            FlatIndex::from_entries(entries, tags)
+            FlatIndex::from_entries(
+                entries,
+                tags,
+                &HashStrategy::None,
+                &NoBuild::All,
+                &NoBinary::None,
+            )
         };
 
         // Create a shared in-memory index.
         let index = InMemoryIndex::default();
 
         // Track in-flight downloads, builds, etc., across resolutions.
         let in_flight = InFlight::default();
 
-        // For seed packages, assume the default settings are sufficient.
+        // For seed packages, assume the default settings and concurrency is sufficient.
         let config_settings = ConfigSettings::default();
+        let concurrency = Concurrency::default();
 
         // Prep the build context.
         let build_dispatch = BuildDispatch::new(
             &client,
             cache,
             interpreter,
             index_locations,
             &flat_index,
             &index,
             &in_flight,
-            venv.python_executable(),
             SetupPyStrategy::default(),
             &config_settings,
+            BuildIsolation::Isolated,
+            link_mode,
             &NoBuild::All,
             &NoBinary::None,
+            concurrency,
         )
         .with_options(OptionsBuilder::new().exclude_newer(exclude_newer).build());
 
         // Resolve the seed packages.
-        let mut requirements = vec![Requirement::from_str("pip").unwrap()];
+        let mut requirements =
+            vec![
+                Requirement::from_pep508(pep508_rs::Requirement::from_str("pip").unwrap()).unwrap(),
+            ];
 
         // Only include `setuptools` and `wheel` on Python <3.12
         if interpreter.python_tuple() < (3, 12) {
-            requirements.push(Requirement::from_str("setuptools").unwrap());
-            requirements.push(Requirement::from_str("wheel").unwrap());
+            requirements.push(
+                Requirement::from_pep508(pep508_rs::Requirement::from_str("setuptools").unwrap())
+                    .unwrap(),
+            );
+            requirements.push(
+                Requirement::from_pep508(pep508_rs::Requirement::from_str("wheel").unwrap())
+                    .unwrap(),
+            );
         }
         let resolution = build_dispatch
             .resolve(&requirements)
             .await
             .map_err(VenvError::Seed)?;
 
         // Install into the environment.
         build_dispatch
             .install(&resolution, &venv)
             .await
             .map_err(VenvError::Seed)?;
 
         for distribution in resolution
             .distributions()
+            .filter_map(|dist| match dist {
+                ResolvedDist::Installable(dist) => Some(dist),
+                ResolvedDist::Installed(_) => None,
+            })
             .sorted_unstable_by(|a, b| a.name().cmp(b.name()).then(a.version().cmp(&b.version())))
         {
             writeln!(
-                printer,
+                printer.stderr(),
                 " {} {}{}",
                 "+".green(),
                 distribution.name().as_ref().bold(),
                 distribution.version_or_url().dimmed()
             )
             .into_diagnostic()?;
         }
     }
 
-    if cfg!(windows) {
-        writeln!(
-            printer,
-            // This should work whether the user is on CMD or PowerShell:
-            "Activate with: {}\\Scripts\\activate",
-            path.normalized_display().cyan()
-        )
-        .into_diagnostic()?;
-    } else {
-        writeln!(
-            printer,
-            "Activate with: source {}/bin/activate",
-            path.normalized_display().cyan()
-        )
-        .into_diagnostic()?;
+    // Determine the appropriate activation command.
+    let activation = match Shell::from_env() {
+        None => None,
+        Some(Shell::Bash | Shell::Zsh) => Some(format!(
+            "source {}",
+            shlex_posix(venv.scripts().join("activate"))
+        )),
+        Some(Shell::Fish) => Some(format!(
+            "source {}",
+            shlex_posix(venv.scripts().join("activate.fish"))
+        )),
+        Some(Shell::Nushell) => Some(format!(
+            "overlay use {}",
+            shlex_posix(venv.scripts().join("activate.nu"))
+        )),
+        Some(Shell::Csh) => Some(format!(
+            "source {}",
+            shlex_posix(venv.scripts().join("activate.csh"))
+        )),
+        Some(Shell::Powershell) => Some(shlex_windows(
+            venv.scripts().join("activate"),
+            Shell::Powershell,
+        )),
+        Some(Shell::Cmd) => Some(shlex_windows(venv.scripts().join("activate"), Shell::Cmd)),
     };
+    if let Some(act) = activation {
+        writeln!(printer.stderr(), "Activate with: {}", act.green()).into_diagnostic()?;
+    }
 
     Ok(ExitStatus::Success)
 }
+
+/// Quote a path, if necessary, for safe use in a POSIX-compatible shell command.
+fn shlex_posix(executable: impl AsRef<Path>) -> String {
+    // Convert to a display path.
+    let executable = executable.as_ref().user_display().to_string();
+
+    // Like Python's `shlex.quote`:
+    // > Use single quotes, and put single quotes into double quotes
+    // > The string $'b is then quoted as '$'"'"'b'
+    if executable.contains(' ') {
+        format!("'{}'", executable.replace('\'', r#"'"'"'"#))
+    } else {
+        executable
+    }
+}
+
+/// Quote a path, if necessary, for safe use in `PowerShell` and `cmd`.
+fn shlex_windows(executable: impl AsRef<Path>, shell: Shell) -> String {
+    // Convert to a display path.
+    let executable = executable.as_ref().user_display().to_string();
+
+    // Wrap the executable in quotes (and a `&` invocation on PowerShell), if it contains spaces.
+    if executable.contains(' ') {
+        if shell == Shell::Powershell {
+            // For PowerShell, wrap in a `&` invocation.
+            format!("& \"{executable}\"")
+        } else {
+            // Otherwise, assume `cmd`, which doesn't need the `&`.
+            format!("\"{executable}\"")
+        }
+    } else {
+        executable
+    }
+}
```

### Comparing `uv-0.1.9/crates/uv/src/compat/mod.rs` & `uv-0.2.0/crates/uv/src/compat/mod.rs`

 * *Files 9% similar despite different names*

```diff
@@ -24,20 +24,14 @@
     #[clap(long, hide = true)]
     reuse_hashes: bool,
 
     #[clap(long, hide = true)]
     no_reuse_hashes: bool,
 
     #[clap(long, hide = true)]
-    build_isolation: bool,
-
-    #[clap(long, hide = true)]
-    no_build_isolation: bool,
-
-    #[clap(long, hide = true)]
     resolver: Option<Resolver>,
 
     #[clap(long, hide = true)]
     max_rounds: Option<usize>,
 
     #[clap(long, hide = true)]
     cert: Option<String>,
@@ -51,41 +45,26 @@
     #[clap(long, hide = true)]
     emit_trusted_host: bool,
 
     #[clap(long, hide = true)]
     no_emit_trusted_host: bool,
 
     #[clap(long, hide = true)]
-    unsafe_package: Vec<String>,
-
-    #[clap(long, hide = true)]
     config: Option<String>,
 
     #[clap(long, hide = true)]
     no_config: bool,
 
     #[clap(long, hide = true)]
-    no_emit_index_url: bool,
-
-    #[clap(long, hide = true)]
-    no_emit_find_links: bool,
-
-    #[clap(long, hide = true)]
     emit_options: bool,
 
     #[clap(long, hide = true)]
     no_emit_options: bool,
 
     #[clap(long, hide = true)]
-    strip_extras: bool,
-
-    #[clap(long, hide = true)]
-    no_strip_extras: bool,
-
-    #[clap(long, hide = true)]
     pip_args: Option<String>,
 }
 
 impl CompatArgs for PipCompileCompatArgs {
     /// Validate the arguments passed for `pip-compile` compatibility.
     ///
     /// This method will warn when an argument is passed that has no effect but matches uv's
@@ -110,26 +89,14 @@
 
         if self.no_reuse_hashes {
             warn_user!(
                 "pip-compile's `--no-reuse-hashes` has no effect (uv doesn't reuse hashes)."
             );
         }
 
-        if self.build_isolation {
-            warn_user!(
-                "pip-compile's `--build-isolation` has no effect (uv always uses build isolation)."
-            );
-        }
-
-        if self.no_build_isolation {
-            return Err(anyhow!(
-                "pip-compile's `--no-build-isolation` is unsupported (uv always uses build isolation)."
-            ));
-        }
-
         if let Some(resolver) = self.resolver {
             match resolver {
                 Resolver::Backtracking => {
                     warn_user!(
                         "pip-compile's `--resolver=backtracking` has no effect (uv always backtracks)."
                     );
                 }
@@ -167,74 +134,71 @@
 
         if self.no_emit_trusted_host {
             warn_user!(
                 "pip-compile's `--no-emit-trusted-host` has no effect (uv never emits trusted hosts)."
             );
         }
 
-        if !self.unsafe_package.is_empty() {
-            return Err(anyhow!(
-                "pip-compile's `--unsafe-package` is not supported."
-            ));
-        }
-
         if self.config.is_some() {
             return Err(anyhow!(
                 "pip-compile's `--config` is unsupported (uv does not use a configuration file)."
             ));
         }
 
         if self.no_config {
             warn_user!(
                 "pip-compile's `--no-config` has no effect (uv does not use a configuration file)."
             );
         }
 
-        if self.no_emit_index_url {
-            warn_user!(
-                "pip-compile's `--no-emit-index-url` has no effect (uv excludes index URLs by default)."
-            );
-        }
-
-        if self.no_emit_find_links {
-            warn_user!(
-                "pip-compile's `--no-emit-find-links` has no effect (uv excludes `--find-links` URLs by default)."
-            );
-        }
-
         if self.emit_options {
             return Err(anyhow!(
                 "pip-compile's `--emit-options` is unsupported (uv never emits options)."
             ));
         }
 
         if self.no_emit_options {
             warn_user!("pip-compile's `--no-emit-options` has no effect (uv never emits options).");
         }
 
-        if self.strip_extras {
-            warn_user!("pip-compile's `--strip-extras` has no effect (uv always strips extras).");
-        }
-
-        if self.no_strip_extras {
-            return Err(anyhow!(
-                "pip-compile's `--no-strip-extras` is unsupported (uv always strips extras)."
-            ));
-        }
-
         if self.pip_args.is_some() {
             return Err(anyhow!(
                 "pip-compile's `--pip-args` is unsupported (try passing arguments to uv directly)."
             ));
         }
 
         Ok(())
     }
 }
 
+/// Arguments for `pip list` compatibility.
+///
+/// These represent a subset of the `pip list` interface that uv supports by default.
+#[derive(Args)]
+#[allow(clippy::struct_excessive_bools)]
+pub(crate) struct PipListCompatArgs {
+    #[clap(long, hide = true)]
+    outdated: bool,
+}
+
+impl CompatArgs for crate::compat::PipListCompatArgs {
+    /// Validate the arguments passed for `pip list` compatibility.
+    ///
+    /// This method will warn when an argument is passed that has no effect but matches uv's
+    /// behavior. If an argument is passed that does _not_ match uv's behavior (e.g.,
+    /// `--outdated`), this method will return an error.
+    fn validate(&self) -> Result<()> {
+        if self.outdated {
+            return Err(anyhow!("pip list's `--outdated` is unsupported."));
+        }
+
+        Ok(())
+    }
+}
+
 /// Arguments for `pip-sync` compatibility.
 ///
 /// These represent a subset of the `pip-sync` interface that uv supports by default.
 #[derive(Args)]
 #[allow(clippy::struct_excessive_bools)]
 pub(crate) struct PipSyncCompatArgs {
     #[clap(short, long, hide = true)]
@@ -281,15 +245,17 @@
         if self.python_executable.is_some() {
             return Err(anyhow!(
                 "pip-sync's `--python-executable` is unsupported (to install into a separate Python environment, try setting `VIRTUAL_ENV` instead)."
             ));
         }
 
         if self.user {
-            return Err(anyhow!("pip-sync's `--user` is unsupported."));
+            return Err(anyhow!(
+                "pip-sync's `--user` is unsupported (use a virtual environment instead)."
+            ));
         }
 
         if self.client_cert.is_some() {
             return Err(anyhow!(
                 "pip-sync's `--client-cert` is unsupported (uv doesn't support dedicated client certificates)."
             ));
         }
@@ -382,7 +348,33 @@
         if self.no_wheel {
             warn_user!("virtualenv's `--no-wheel` has no effect (uv omits `wheel` by default).");
         }
 
         Ok(())
     }
 }
+
+/// Arguments for `pip install` compatibility.
+///
+/// These represent a subset of the `pip install` interface that uv supports by default.
+#[derive(Args)]
+#[allow(clippy::struct_excessive_bools)]
+pub(crate) struct PipInstallCompatArgs {
+    #[clap(long, hide = false)]
+    user: bool,
+}
+
+impl CompatArgs for PipInstallCompatArgs {
+    /// Validate the arguments passed for `pip install` compatibility.
+    ///
+    /// This method will warn when an argument is passed that has no effect but matches uv's
+    /// behavior. If an argument is passed that does _not_ match uv's behavior, this method will
+    /// return an error.
+    fn validate(&self) -> Result<()> {
+        if self.user {
+            return Err(anyhow!(
+                "pip install's `--user` is unsupported (use a virtual environment instead)."
+            ));
+        }
+        Ok(())
+    }
+}
```

### Comparing `uv-0.1.9/crates/uv/src/confirm.rs` & `uv-0.2.0/crates/uv-requirements/src/confirm.rs`

 * *Files 16% similar despite different names*

```diff
@@ -1,27 +1,37 @@
 use anyhow::Result;
 use console::{style, Key, Term};
 
 /// Prompt the user for confirmation in the given [`Term`].
 ///
-/// This is a slimmed-down version of [`dialoguer::Confirm`], with the post-confirmation report
+/// This is a slimmed-down version of `dialoguer::Confirm`, with the post-confirmation report
 /// enabled.
 pub(crate) fn confirm(message: &str, term: &Term, default: bool) -> Result<bool> {
-    ctrlc::set_handler(move || {
+    // Set the Ctrl-C handler to exit the process.
+    let result = ctrlc::set_handler(move || {
         let term = Term::stderr();
         term.show_cursor().ok();
         term.flush().ok();
 
         #[allow(clippy::exit, clippy::cast_possible_wrap)]
         std::process::exit(if cfg!(windows) {
             0xC000_013A_u32 as i32
         } else {
             130
         });
-    })?;
+    });
+
+    match result {
+        Ok(()) => {}
+        Err(ctrlc::Error::MultipleHandlers) => {
+            // If multiple handlers were set, we assume that the existing handler is our
+            // confirmation handler, and continue.
+        }
+        Err(e) => return Err(e.into()),
+    }
 
     let prompt = format!(
         "{} {} {} {} {}",
         style("?".to_string()).for_stderr().yellow(),
         style(message).for_stderr().bold(),
         style("[y/n]").for_stderr().black().bright(),
         style("›").for_stderr().black().bright(),
```

### Comparing `uv-0.1.9/crates/uv/src/requirements.rs` & `uv-0.2.0/crates/uv-requirements/src/specification.rs`

 * *Files 12% similar despite different names*

```diff
@@ -1,199 +1,108 @@
-//! A standard interface for working with heterogeneous sources of requirements.
-
+use std::collections::BTreeMap;
 use std::path::{Path, PathBuf};
 
 use anyhow::{Context, Result};
-use console::Term;
+use itertools::{Either, Itertools};
 use rustc_hash::FxHashSet;
+use tracing::{debug, instrument};
 
-use distribution_types::{FlatIndexLocation, IndexUrl};
-use pep508_rs::Requirement;
-use requirements_txt::{EditableRequirement, FindLink, RequirementsTxt};
-use tracing::{instrument, Level};
-use uv_fs::Normalized;
+use cache_key::CanonicalUrl;
+use distribution_types::{
+    FlatIndexLocation, IndexUrl, Requirement, RequirementSource, UnresolvedRequirement,
+    UnresolvedRequirementSpecification,
+};
+use pep508_rs::{UnnamedRequirement, VerbatimUrl};
+use requirements_txt::{
+    EditableRequirement, FindLink, RequirementEntry, RequirementsTxt, RequirementsTxtRequirement,
+};
+use uv_client::BaseClientBuilder;
+use uv_configuration::{NoBinary, NoBuild, PreviewMode};
+use uv_fs::Simplified;
 use uv_normalize::{ExtraName, PackageName};
 
-use crate::confirm;
-
-use uv_warnings::warn_user;
-
-#[derive(Debug)]
-pub(crate) enum RequirementsSource {
-    /// A package was provided on the command line (e.g., `pip install flask`).
-    Package(String),
-    /// An editable path was provided on the command line (e.g., `pip install -e ../flask`).
-    Editable(String),
-    /// Dependencies were provided via a `requirements.txt` file (e.g., `pip install -r requirements.txt`).
-    RequirementsTxt(PathBuf),
-    /// Dependencies were provided via a `pyproject.toml` file (e.g., `pip-compile pyproject.toml`).
-    PyprojectToml(PathBuf),
-}
-
-impl RequirementsSource {
-    /// Parse a [`RequirementsSource`] from a [`PathBuf`].
-    pub(crate) fn from_path(path: PathBuf) -> Self {
-        if path.ends_with("pyproject.toml") {
-            Self::PyprojectToml(path)
-        } else {
-            Self::RequirementsTxt(path)
-        }
-    }
-
-    /// Parse a [`RequirementsSource`] from a user-provided string, assumed to be a package.
-    ///
-    /// If the user provided a value that appears to be a `requirements.txt` file or a local
-    /// directory, prompt them to correct it (if the terminal is interactive).
-    pub(crate) fn from_package(name: String) -> Self {
-        // If the user provided a `requirements.txt` file without `-r` (as in
-        // `uv pip install requirements.txt`), prompt them to correct it.
-        #[allow(clippy::case_sensitive_file_extension_comparisons)]
-        if name.ends_with(".txt") || name.ends_with(".in") {
-            if Path::new(&name).is_file() {
-                let term = Term::stderr();
-                if term.is_term() {
-                    let prompt = format!(
-                        "`{name}` looks like a requirements file but was passed as a package name. Did you mean `-r {name}`?"
-                    );
-                    let confirmation = confirm::confirm(&prompt, &term, true).unwrap();
-                    if confirmation {
-                        return Self::RequirementsTxt(name.into());
-                    }
-                }
-            }
-        }
-
-        // If the user provided a path to a local directory without `-e` (as in
-        // `uv pip install ../flask`), prompt them to correct it.
-        if name.contains('/') || name.contains('\\') {
-            if Path::new(&name).is_dir() {
-                let term = Term::stderr();
-                if term.is_term() {
-                    let prompt =
-                        format!("`{name}` looks like a local directory but was passed as a package name. Did you mean `-e {name}`?");
-                    let confirmation = confirm::confirm(&prompt, &term, true).unwrap();
-                    if confirmation {
-                        return Self::RequirementsTxt(name.into());
-                    }
-                }
-            }
-        }
-
-        Self::Package(name)
-    }
-}
-
-impl std::fmt::Display for RequirementsSource {
-    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-        match self {
-            Self::Editable(path) => write!(f, "-e {path}"),
-            Self::RequirementsTxt(path) | Self::PyprojectToml(path) => {
-                write!(f, "{}", path.display())
-            }
-            Self::Package(package) => write!(f, "{package}"),
-        }
-    }
-}
-
-#[derive(Debug, Default, Clone)]
-pub(crate) enum ExtrasSpecification<'a> {
-    #[default]
-    None,
-    All,
-    Some(&'a [ExtraName]),
-}
-
-impl ExtrasSpecification<'_> {
-    /// Returns true if a name is included in the extra specification.
-    fn contains(&self, name: &ExtraName) -> bool {
-        match self {
-            ExtrasSpecification::All => true,
-            ExtrasSpecification::None => false,
-            ExtrasSpecification::Some(extras) => extras.contains(name),
-        }
-    }
-}
+use crate::pyproject::{Pep621Metadata, PyProjectToml};
+use crate::{ExtrasSpecification, RequirementsSource};
 
 #[derive(Debug, Default)]
-pub(crate) struct RequirementsSpecification {
+pub struct RequirementsSpecification {
     /// The name of the project specifying requirements.
-    pub(crate) project: Option<PackageName>,
+    pub project: Option<PackageName>,
     /// The requirements for the project.
-    pub(crate) requirements: Vec<Requirement>,
+    pub requirements: Vec<UnresolvedRequirementSpecification>,
     /// The constraints for the project.
-    pub(crate) constraints: Vec<Requirement>,
+    pub constraints: Vec<Requirement>,
     /// The overrides for the project.
-    pub(crate) overrides: Vec<Requirement>,
+    pub overrides: Vec<UnresolvedRequirementSpecification>,
     /// Package to install as editable installs
-    pub(crate) editables: Vec<EditableRequirement>,
+    pub editables: Vec<EditableRequirement>,
+    /// The source trees from which to extract requirements.
+    pub source_trees: Vec<PathBuf>,
     /// The extras used to collect requirements.
-    pub(crate) extras: FxHashSet<ExtraName>,
+    pub extras: FxHashSet<ExtraName>,
     /// The index URL to use for fetching packages.
-    pub(crate) index_url: Option<IndexUrl>,
+    pub index_url: Option<IndexUrl>,
     /// The extra index URLs to use for fetching packages.
-    pub(crate) extra_index_urls: Vec<IndexUrl>,
+    pub extra_index_urls: Vec<IndexUrl>,
     /// Whether to disallow index usage.
-    pub(crate) no_index: bool,
+    pub no_index: bool,
     /// The `--find-links` locations to use for fetching packages.
-    pub(crate) find_links: Vec<FlatIndexLocation>,
+    pub find_links: Vec<FlatIndexLocation>,
+    /// The `--no-binary` flags to enforce when selecting distributions.
+    pub no_binary: NoBinary,
+    /// The `--no-build` flags to enforce when selecting distributions.
+    pub no_build: NoBuild,
 }
 
 impl RequirementsSpecification {
     /// Read the requirements and constraints from a source.
-    #[instrument(skip_all, level = Level::DEBUG, fields(source = % source))]
-    pub(crate) fn from_source(
+    #[instrument(skip_all, level = tracing::Level::DEBUG, fields(source = % source))]
+    pub async fn from_source(
         source: &RequirementsSource,
         extras: &ExtrasSpecification,
+        client_builder: &BaseClientBuilder<'_>,
+        preview: PreviewMode,
     ) -> Result<Self> {
         Ok(match source {
             RequirementsSource::Package(name) => {
-                let requirement = Requirement::parse(name, std::env::current_dir()?)
+                let requirement = RequirementsTxtRequirement::parse(name, std::env::current_dir()?)
                     .with_context(|| format!("Failed to parse `{name}`"))?;
                 Self {
-                    project: None,
-                    requirements: vec![requirement],
-                    constraints: vec![],
-                    overrides: vec![],
-                    editables: vec![],
-                    extras: FxHashSet::default(),
-                    index_url: None,
-                    extra_index_urls: vec![],
-                    no_index: false,
-                    find_links: vec![],
+                    requirements: vec![UnresolvedRequirementSpecification::try_from(
+                        RequirementEntry {
+                            requirement,
+                            hashes: vec![],
+                        },
+                    )?],
+                    ..Self::default()
                 }
             }
             RequirementsSource::Editable(name) => {
-                let requirement = EditableRequirement::parse(name, std::env::current_dir()?)
+                let requirement = EditableRequirement::parse(name, None, std::env::current_dir()?)
                     .with_context(|| format!("Failed to parse `{name}`"))?;
                 Self {
-                    project: None,
-                    requirements: vec![],
-                    constraints: vec![],
-                    overrides: vec![],
                     editables: vec![requirement],
-                    extras: FxHashSet::default(),
-                    index_url: None,
-                    extra_index_urls: vec![],
-                    no_index: false,
-                    find_links: vec![],
+                    ..Self::default()
                 }
             }
             RequirementsSource::RequirementsTxt(path) => {
-                let requirements_txt = RequirementsTxt::parse(path, std::env::current_dir()?)?;
+                let requirements_txt =
+                    RequirementsTxt::parse(path, std::env::current_dir()?, client_builder).await?;
                 Self {
-                    project: None,
                     requirements: requirements_txt
                         .requirements
                         .into_iter()
-                        .map(|entry| entry.requirement)
-                        .collect(),
-                    constraints: requirements_txt.constraints,
+                        .map(UnresolvedRequirementSpecification::try_from)
+                        .collect::<Result<_, _>>()?,
+                    constraints: requirements_txt
+                        .constraints
+                        .into_iter()
+                        .map(Requirement::from_pep508)
+                        .collect::<Result<_, _>>()?,
                     editables: requirements_txt.editables,
-                    overrides: vec![],
-                    extras: FxHashSet::default(),
                     index_url: requirements_txt.index_url.map(IndexUrl::from),
                     extra_index_urls: requirements_txt
                         .extra_index_urls
                         .into_iter()
                         .map(IndexUrl::from)
                         .collect(),
                     no_index: requirements_txt.no_index,
@@ -201,151 +110,244 @@
                         .find_links
                         .into_iter()
                         .map(|link| match link {
                             FindLink::Url(url) => FlatIndexLocation::Url(url),
                             FindLink::Path(path) => FlatIndexLocation::Path(path),
                         })
                         .collect(),
+                    no_binary: requirements_txt.no_binary,
+                    no_build: requirements_txt.only_binary,
+                    ..Self::default()
                 }
             }
             RequirementsSource::PyprojectToml(path) => {
-                let contents = uv_fs::read_to_string(path)?;
-                let pyproject_toml = toml::from_str::<pyproject_toml::PyProjectToml>(&contents)
-                    .with_context(|| format!("Failed to parse `{}`", path.normalized_display()))?;
-                let mut used_extras = FxHashSet::default();
-                let mut requirements = Vec::new();
-                let mut project_name = None;
-                if let Some(project) = pyproject_toml.project {
-                    requirements.extend(project.dependencies.unwrap_or_default());
-                    // Include any optional dependencies specified in `extras`
-                    if !matches!(extras, ExtrasSpecification::None) {
-                        for (name, optional_requirements) in
-                            project.optional_dependencies.unwrap_or_default()
+                let contents = uv_fs::read_to_string(&path).await?;
+                Self::parse_direct_pyproject_toml(&contents, extras, path.as_ref(), preview)
+                    .with_context(|| format!("Failed to parse `{}`", path.user_display()))?
+            }
+            RequirementsSource::SetupPy(path) | RequirementsSource::SetupCfg(path) => Self {
+                source_trees: vec![path.clone()],
+                ..Self::default()
+            },
+            RequirementsSource::SourceTree(path) => Self {
+                project: None,
+                requirements: vec![UnresolvedRequirementSpecification {
+                    requirement: UnresolvedRequirement::Unnamed(UnnamedRequirement {
+                        url: VerbatimUrl::from_path(path)?,
+                        extras: vec![],
+                        marker: None,
+                        origin: None,
+                    }),
+                    hashes: vec![],
+                }],
+                ..Self::default()
+            },
+        })
+    }
+
+    /// Attempt to read metadata from the `pyproject.toml` directly.
+    ///
+    /// Since we only use this path for directly included pyproject.toml, we are strict about
+    /// PEP 621 and don't allow invalid `project.dependencies` (e.g., Hatch's relative path
+    /// support).
+    pub(crate) fn parse_direct_pyproject_toml(
+        contents: &str,
+        extras: &ExtrasSpecification,
+        pyproject_path: &Path,
+        preview: PreviewMode,
+    ) -> Result<Self> {
+        let pyproject = toml::from_str::<PyProjectToml>(contents)?;
+
+        // We need use this path as base for the relative paths inside pyproject.toml, so
+        // we need the absolute path instead of a potentially relative path. E.g. with
+        // `foo = { path = "../foo" }`, we will join `../foo` onto this path.
+        let absolute_path = uv_fs::absolutize_path(pyproject_path)?;
+        let project_dir = absolute_path
+            .parent()
+            .context("`pyproject.toml` has no parent directory")?;
+
+        let workspace_sources = BTreeMap::default();
+        let workspace_packages = BTreeMap::default();
+        match Pep621Metadata::try_from(
+            pyproject,
+            extras,
+            pyproject_path,
+            project_dir,
+            &workspace_sources,
+            &workspace_packages,
+            preview,
+        ) {
+            Ok(Some(project)) => {
+                // Partition into editable and non-editable requirements.
+                let (editables, requirements): (Vec<_>, Vec<_>) = project
+                    .requirements
+                    .into_iter()
+                    .partition_map(|requirement| {
+                        if let RequirementSource::Path {
+                            path,
+                            editable: true,
+                            url,
+                        } = requirement.source
                         {
-                            // TODO(konstin): It's not ideal that pyproject-toml doesn't use
-                            // `ExtraName`
-                            let normalized_name = ExtraName::new(name)?;
-                            if extras.contains(&normalized_name) {
-                                used_extras.insert(normalized_name);
-                                requirements.extend(optional_requirements);
-                            }
+                            Either::Left(EditableRequirement {
+                                url,
+                                path,
+                                marker: requirement.marker,
+                                extras: requirement.extras,
+                                origin: requirement.origin,
+                            })
+                        } else {
+                            Either::Right(UnresolvedRequirementSpecification {
+                                requirement: UnresolvedRequirement::Named(requirement),
+                                hashes: vec![],
+                            })
                         }
-                    }
-                    // Parse the project name
-                    project_name = Some(PackageName::new(project.name).with_context(|| {
-                        format!("Invalid `project.name` in {}", path.normalized_display())
-                    })?);
-                }
-
-                if requirements.is_empty() {
-                    if pyproject_toml.build_system.is_some_and(|build_system| {
-                        build_system
-                            .requires
-                            .iter()
-                            .any(|v| v.name.as_dist_info_name().starts_with("poetry"))
-                    }) {
-                        warn_user!("`{}` does not contain any dependencies (hint: specify dependencies in the `project.dependencies` section; `tool.poetry.dependencies` is not currently supported)", path.normalized_display());
-                    }
-                }
+                    });
 
-                Self {
-                    project: project_name,
+                Ok(Self {
+                    project: Some(project.name),
+                    editables,
                     requirements,
-                    constraints: vec![],
-                    overrides: vec![],
-                    editables: vec![],
-                    extras: used_extras,
-                    index_url: None,
-                    extra_index_urls: vec![],
-                    no_index: false,
-                    find_links: vec![],
-                }
+                    extras: project.used_extras,
+                    ..Self::default()
+                })
+            }
+            Ok(None) => {
+                debug!(
+                    "Dynamic pyproject.toml at: `{}`",
+                    pyproject_path.user_display()
+                );
+                Ok(Self {
+                    project: None,
+                    requirements: vec![],
+                    source_trees: vec![pyproject_path.to_path_buf()],
+                    ..Self::default()
+                })
             }
-        })
+            Err(err) => Err(err.into()),
+        }
     }
 
     /// Read the combined requirements and constraints from a set of sources.
-    pub(crate) fn from_sources(
+    pub async fn from_sources(
         requirements: &[RequirementsSource],
         constraints: &[RequirementsSource],
         overrides: &[RequirementsSource],
         extras: &ExtrasSpecification,
+        client_builder: &BaseClientBuilder<'_>,
+        preview: PreviewMode,
     ) -> Result<Self> {
         let mut spec = Self::default();
 
         // Read all requirements, and keep track of all requirements _and_ constraints.
         // A `requirements.txt` can contain a `-c constraints.txt` directive within it, so reading
         // a requirements file can also add constraints.
         for source in requirements {
-            let source = Self::from_source(source, extras)?;
+            let source = Self::from_source(source, extras, client_builder, preview).await?;
             spec.requirements.extend(source.requirements);
             spec.constraints.extend(source.constraints);
             spec.overrides.extend(source.overrides);
             spec.extras.extend(source.extras);
             spec.editables.extend(source.editables);
+            spec.source_trees.extend(source.source_trees);
 
             // Use the first project name discovered.
             if spec.project.is_none() {
                 spec.project = source.project;
             }
 
-            if let Some(url) = source.index_url {
+            if let Some(index_url) = source.index_url {
                 if let Some(existing) = spec.index_url {
-                    return Err(anyhow::anyhow!(
-                        "Multiple index URLs specified: `{existing}` vs.` {url}",
-                    ));
+                    if CanonicalUrl::new(index_url.url()) != CanonicalUrl::new(existing.url()) {
+                        return Err(anyhow::anyhow!(
+                            "Multiple index URLs specified: `{existing}` vs. `{index_url}`",
+                        ));
+                    }
                 }
-                spec.index_url = Some(url);
+                spec.index_url = Some(index_url);
             }
             spec.no_index |= source.no_index;
             spec.extra_index_urls.extend(source.extra_index_urls);
             spec.find_links.extend(source.find_links);
+            spec.no_binary.extend(source.no_binary);
+            spec.no_build.extend(source.no_build);
         }
 
-        // Read all constraints, treating _everything_ as a constraint.
+        // Read all constraints, treating both requirements _and_ constraints as constraints.
+        // Overrides are ignored, as are the hashes, as they are not relevant for constraints.
         for source in constraints {
-            let source = Self::from_source(source, extras)?;
-            spec.constraints.extend(source.requirements);
+            let source = Self::from_source(source, extras, client_builder, preview).await?;
+            for entry in source.requirements {
+                match entry.requirement {
+                    UnresolvedRequirement::Named(requirement) => {
+                        spec.constraints.push(requirement);
+                    }
+                    UnresolvedRequirement::Unnamed(requirement) => {
+                        return Err(anyhow::anyhow!(
+                            "Unnamed requirements are not allowed as constraints (found: `{requirement}`)"
+                        ));
+                    }
+                }
+            }
             spec.constraints.extend(source.constraints);
-            spec.constraints.extend(source.overrides);
 
-            if let Some(url) = source.index_url {
+            if let Some(index_url) = source.index_url {
                 if let Some(existing) = spec.index_url {
-                    return Err(anyhow::anyhow!(
-                        "Multiple index URLs specified: `{existing}` vs.` {url}",
-                    ));
+                    if CanonicalUrl::new(index_url.url()) != CanonicalUrl::new(existing.url()) {
+                        return Err(anyhow::anyhow!(
+                            "Multiple index URLs specified: `{existing}` vs. `{index_url}`",
+                        ));
+                    }
                 }
-                spec.index_url = Some(url);
+                spec.index_url = Some(index_url);
             }
             spec.no_index |= source.no_index;
             spec.extra_index_urls.extend(source.extra_index_urls);
             spec.find_links.extend(source.find_links);
+            spec.no_binary.extend(source.no_binary);
+            spec.no_build.extend(source.no_build);
         }
 
-        // Read all overrides, treating both requirements _and_ constraints as overrides.
+        // Read all overrides, treating both requirements _and_ overrides as overrides.
+        // Constraints are ignored.
         for source in overrides {
-            let source = Self::from_source(source, extras)?;
+            let source = Self::from_source(source, extras, client_builder, preview).await?;
             spec.overrides.extend(source.requirements);
-            spec.overrides.extend(source.constraints);
             spec.overrides.extend(source.overrides);
 
-            if let Some(url) = source.index_url {
+            if let Some(index_url) = source.index_url {
                 if let Some(existing) = spec.index_url {
-                    return Err(anyhow::anyhow!(
-                        "Multiple index URLs specified: `{existing}` vs.` {url}",
-                    ));
+                    if CanonicalUrl::new(index_url.url()) != CanonicalUrl::new(existing.url()) {
+                        return Err(anyhow::anyhow!(
+                            "Multiple index URLs specified: `{existing}` vs. `{index_url}`",
+                        ));
+                    }
                 }
-                spec.index_url = Some(url);
+                spec.index_url = Some(index_url);
             }
             spec.no_index |= source.no_index;
             spec.extra_index_urls.extend(source.extra_index_urls);
             spec.find_links.extend(source.find_links);
+            spec.no_binary.extend(source.no_binary);
+            spec.no_build.extend(source.no_build);
         }
 
         Ok(spec)
     }
 
     /// Read the requirements from a set of sources.
-    pub(crate) fn from_simple_sources(requirements: &[RequirementsSource]) -> Result<Self> {
-        Self::from_sources(requirements, &[], &[], &ExtrasSpecification::None)
+    pub async fn from_simple_sources(
+        requirements: &[RequirementsSource],
+        client_builder: &BaseClientBuilder<'_>,
+        preview: PreviewMode,
+    ) -> Result<Self> {
+        Self::from_sources(
+            requirements,
+            &[],
+            &[],
+            &ExtrasSpecification::None,
+            client_builder,
+            preview,
+        )
+        .await
     }
 }
```

### Comparing `uv-0.1.9/crates/uv/tests/common/mod.rs` & `uv-0.2.0/crates/uv/tests/common/mod.rs`

 * *Files 24% similar despite different names*

```diff
@@ -1,39 +1,44 @@
 // The `unreachable_pub` is to silence false positives in RustRover.
 #![allow(dead_code, unreachable_pub)]
 
 use assert_cmd::assert::{Assert, OutputAssertExt};
 use assert_cmd::Command;
 use assert_fs::assert::PathAssert;
 use assert_fs::fixture::PathChild;
-#[cfg(unix)]
-use fs_err::os::unix::fs::symlink as symlink_file;
-#[cfg(windows)]
-use fs_err::os::windows::fs::symlink_file;
-use regex::{self, Regex};
+use regex::Regex;
 use std::borrow::BorrowMut;
 use std::env;
 use std::ffi::OsString;
 use std::path::{Path, PathBuf};
 use std::process::Output;
-use uv_fs::Normalized;
+use std::str::FromStr;
 
-use platform_host::Platform;
 use uv_cache::Cache;
-use uv_interpreter::find_requested_python;
+use uv_fs::Simplified;
+use uv_interpreter::managed::toolchains_for_version;
+use uv_interpreter::{
+    find_interpreter, InterpreterRequest, PythonVersion, SourceSelector, VersionRequest,
+};
 
 // Exclude any packages uploaded after this date.
-pub static EXCLUDE_NEWER: &str = "2023-11-18T12:00:00Z";
+pub static EXCLUDE_NEWER: &str = "2024-03-25T00:00:00Z";
 
+/// Using a find links url allows using `--index-url` instead of `--extra-index-url` in tests
+/// to prevent dependency confusion attacks against our test suite.
+pub const BUILD_VENDOR_LINKS_URL: &str =
+    "https://raw.githubusercontent.com/astral-sh/packse/0.3.15/vendor/links.html";
+
+#[doc(hidden)] // Macro and test context only, don't use directly.
 pub const INSTA_FILTERS: &[(&str, &str)] = &[
     (r"--cache-dir [^\s]+", "--cache-dir [CACHE_DIR]"),
     // Operation times
-    (r"(\d+m )?(\d+\.)?\d+(ms|s)", "[TIME]"),
+    (r"(\s|\()(\d+m )?(\d+\.)?\d+(ms|s)", "$1[TIME]"),
     // File sizes
-    (r"(\d+\.)?\d+([KM]i)?B", "[SIZE]"),
+    (r"(\s|\()(\d+\.)?\d+([KM]i)?B", "$1[SIZE]"),
     // Rewrite Windows output to Unix output
     (r"\\([\w\d])", "/$1"),
     (r"uv.exe", "uv"),
     // The exact message is host language dependent
     (
         r"Caused by: .* \(os error 2\)",
         "Caused by: No such file or directory (os error 2)",
@@ -41,68 +46,186 @@
 ];
 
 #[derive(Debug)]
 pub struct TestContext {
     pub temp_dir: assert_fs::TempDir,
     pub cache_dir: assert_fs::TempDir,
     pub venv: PathBuf,
+    pub python_version: String,
+    pub workspace_root: PathBuf,
 
     // Standard filters for this test context
     filters: Vec<(String, String)>,
 }
 
 impl TestContext {
     pub fn new(python_version: &str) -> Self {
         let temp_dir = assert_fs::TempDir::new().expect("Failed to create temp dir");
         let cache_dir = assert_fs::TempDir::new().expect("Failed to create cache dir");
         let venv = create_venv(&temp_dir, &cache_dir, python_version);
 
+        // The workspace root directory is not available without walking up the tree
+        // https://github.com/rust-lang/cargo/issues/3946
+        let workspace_root = Path::new(&std::env::var("CARGO_MANIFEST_DIR").unwrap())
+            .parent()
+            .expect("CARGO_MANIFEST_DIR should be nested in workspace")
+            .parent()
+            .expect("CARGO_MANIFEST_DIR should be doubly nested in workspace")
+            .to_path_buf();
+
+        let site_packages = site_packages_path(&venv, format!("python{python_version}"));
+
+        let python_version =
+            PythonVersion::from_str(python_version).expect("Tests must use valid Python versions");
+
         let mut filters = Vec::new();
         filters.extend(
             Self::path_patterns(&cache_dir)
                 .into_iter()
                 .map(|pattern| (pattern, "[CACHE_DIR]/".to_string())),
         );
         filters.extend(
-            Self::path_patterns(&temp_dir)
+            Self::path_patterns(&site_packages)
                 .into_iter()
-                .map(|pattern| (pattern, "[TEMP_DIR]/".to_string())),
+                .map(|pattern| (pattern, "[SITE_PACKAGES]/".to_string())),
         );
         filters.extend(
             Self::path_patterns(&venv)
                 .into_iter()
                 .map(|pattern| (pattern, "[VENV]/".to_string())),
         );
+        filters.extend(
+            Self::path_patterns(&temp_dir)
+                .into_iter()
+                .map(|pattern| (pattern, "[TEMP_DIR]/".to_string())),
+        );
+        filters.extend(
+            Self::path_patterns(&workspace_root)
+                .into_iter()
+                .map(|pattern| (pattern, "[WORKSPACE]/".to_string())),
+        );
+
+        // Account for [`Simplified::user_display`] which is relative to the command working directory
+        filters.push((
+            Self::path_pattern(
+                site_packages
+                    .strip_prefix(&temp_dir)
+                    .expect("The test site-packages directory is always in the tempdir"),
+            ),
+            "[SITE_PACKAGES]/".to_string(),
+        ));
+        filters.push((
+            Self::path_pattern(
+                venv.strip_prefix(&temp_dir)
+                    .expect("The test virtual environment directory is always in the tempdir"),
+            ),
+            "[VENV]/".to_string(),
+        ));
+
+        // Filter non-deterministic temporary directory names
+        // Note we apply this _after_ all the full paths to avoid breaking their matching
+        filters.push((r"(\\|\/)\.tmp.*(\\|\/)".to_string(), "/[TMP]/".to_string()));
+
+        // Account for platform prefix differences `file://` (Unix) vs `file:///` (Windows)
+        filters.push((r"file:///".to_string(), "file://".to_string()));
+
+        // Destroy any remaining UNC prefixes (Windows only)
+        filters.push((r"\\\\\?\\".to_string(), String::new()));
+
+        // Add Python patch version filtering unless explicitly requested to ensure
+        // snapshots are patch version agnostic when it is not a part of the test.
+        if python_version.patch().is_none() {
+            filters.push((
+                format!(
+                    r"({})\.\d+",
+                    regex::escape(python_version.to_string().as_str())
+                ),
+                "$1.[X]".to_string(),
+            ));
+        }
 
         Self {
             temp_dir,
             cache_dir,
             venv,
+            python_version: python_version.to_string(),
             filters,
+            workspace_root,
         }
     }
 
     /// Set shared defaults between tests:
     /// * Set the current directory to a temporary directory (`temp_dir`).
     /// * Set the cache dir to a different temporary directory (`cache_dir`).
     /// * Set a cutoff for versions used in the resolution so the snapshots don't change after a new release.
     /// * Set the venv to a fresh `.venv` in `temp_dir`.
     pub fn compile(&self) -> std::process::Command {
+        let mut command = self.compile_without_exclude_newer();
+        command.arg("--exclude-newer").arg(EXCLUDE_NEWER);
+        command
+    }
+
+    /// Create a `pip compile` command with no `--exclude-newer` option.
+    ///
+    /// One should avoid using this in tests to the extent possible because
+    /// it can result in tests failing when the index state changes. Therefore,
+    /// if you use this, there should be some other kind of mitigation in place.
+    /// For example, pinning package versions.
+    pub fn compile_without_exclude_newer(&self) -> std::process::Command {
         let mut cmd = std::process::Command::new(get_bin());
         cmd.arg("pip")
             .arg("compile")
             .arg("--cache-dir")
             .arg(self.cache_dir.path())
-            .arg("--exclude-newer")
-            .arg(EXCLUDE_NEWER)
             .env("VIRTUAL_ENV", self.venv.as_os_str())
+            .env("UV_NO_WRAP", "1")
             .current_dir(self.temp_dir.path());
+
+        if cfg!(all(windows, debug_assertions)) {
+            // TODO(konstin): Reduce stack usage in debug mode enough that the tests pass with the
+            // default windows stack of 1MB
+            cmd.env("UV_STACK_SIZE", (8 * 1024 * 1024).to_string());
+        }
+
         cmd
     }
 
+    /// Create a `pip install` command with options shared across scenarios.
+    pub fn install(&self) -> std::process::Command {
+        let mut command = self.install_without_exclude_newer();
+        command.arg("--exclude-newer").arg(EXCLUDE_NEWER);
+        command
+    }
+
+    /// Create a `pip install` command with no `--exclude-newer` option.
+    ///
+    /// One should avoid using this in tests to the extent possible because
+    /// it can result in tests failing when the index state changes. Therefore,
+    /// if you use this, there should be some other kind of mitigation in place.
+    /// For example, pinning package versions.
+    pub fn install_without_exclude_newer(&self) -> std::process::Command {
+        let mut command = std::process::Command::new(get_bin());
+        command
+            .arg("pip")
+            .arg("install")
+            .arg("--cache-dir")
+            .arg(self.cache_dir.path())
+            .env("VIRTUAL_ENV", self.venv.as_os_str())
+            .env("UV_NO_WRAP", "1")
+            .current_dir(&self.temp_dir);
+
+        if cfg!(all(windows, debug_assertions)) {
+            // TODO(konstin): Reduce stack usage in debug mode enough that the tests pass with the
+            // default windows stack of 1MB
+            command.env("UV_STACK_SIZE", (4 * 1024 * 1024).to_string());
+        }
+
+        command
+    }
+
     /// Run the given python code and check whether it succeeds.
     pub fn assert_command(&self, command: &str) -> Assert {
         std::process::Command::new(venv_to_interpreter(&self.venv))
             // Our tests change files in <1s, so we must disable CPython bytecode caching or we'll get stale files
             // https://github.com/python/cpython/issues/75953
             .arg("-B")
             .arg("-c")
@@ -116,140 +239,118 @@
         self.assert_command(
             format!("import {package} as package; print(package.__version__, end='')").as_str(),
         )
         .success()
         .stdout(version);
     }
 
-    /// Generate an escaped regex pattern for the given path.
+    /// Generate various escaped regex patterns for the given path.
     fn path_patterns(path: impl AsRef<Path>) -> Vec<String> {
-        vec![
-            format!(
-                // Trim the trailing separator for cross-platform directories filters
-                r"{}\\?/?",
-                regex::escape(
-                    &path
-                        .as_ref()
-                        .canonicalize()
-                        .expect("Failed to create canonical path")
-                        // Normalize the path to match display and remove UNC prefixes on Windows
-                        .normalized()
-                        .display()
-                        .to_string(),
-                )
-                // Make seprators platform agnostic because on Windows we will display
+        let mut patterns = Vec::new();
+
+        // We can only canonicalize paths that exist already
+        if path.as_ref().exists() {
+            patterns.push(Self::path_pattern(
+                path.as_ref()
+                    .canonicalize()
+                    .expect("Failed to create canonical path"),
+            ));
+        }
+
+        // Include a non-canonicalized version
+        patterns.push(Self::path_pattern(path));
+
+        patterns
+    }
+
+    /// Generate an escaped regex pattern for the given path.
+    fn path_pattern(path: impl AsRef<Path>) -> String {
+        format!(
+            // Trim the trailing separator for cross-platform directories filters
+            r"{}\\?/?",
+            regex::escape(&path.as_ref().simplified_display().to_string())
+                // Make separators platform agnostic because on Windows we will display
                 // paths with Unix-style separators sometimes
                 .replace(r"\\", r"(\\|\/)")
-            ),
-            // Include a non-canonicalized version
-            format!(
-                r"{}\\?/?",
-                regex::escape(&path.as_ref().normalized().display().to_string())
-                    .replace(r"\\", r"(\\|\/)")
-            ),
-        ]
+        )
     }
 
     /// Standard snapshot filters _plus_ those for this test context.
     pub fn filters(&self) -> Vec<(&str, &str)> {
         // Put test context snapshots before the default filters
         // This ensures we don't replace other patterns inside paths from the test context first
         self.filters
             .iter()
             .map(|(p, r)| (p.as_str(), r.as_str()))
             .chain(INSTA_FILTERS.iter().copied())
             .collect()
     }
+
+    /// For when we add pypy to the test suite.
+    #[allow(clippy::unused_self)]
+    pub fn python_kind(&self) -> &str {
+        "python"
+    }
+
+    /// Returns the site-packages folder inside the venv.
+    pub fn site_packages(&self) -> PathBuf {
+        site_packages_path(
+            &self.venv,
+            format!("{}{}", self.python_kind(), self.python_version),
+        )
+    }
 }
 
-pub fn venv_to_interpreter(venv: &Path) -> PathBuf {
+fn site_packages_path(venv: &Path, python: String) -> PathBuf {
     if cfg!(unix) {
-        venv.join("bin").join("python")
+        venv.join("lib").join(python).join("site-packages")
     } else if cfg!(windows) {
-        venv.join("Scripts").join("python.exe")
+        venv.join("Lib").join("site-packages")
     } else {
         unimplemented!("Only Windows and Unix are supported")
     }
 }
 
-/// If bootstrapped python build standalone pythons exists in `<project root>/bin`,
-/// return the paths to the directories containing the python binaries (i.e. as paths that
-/// `which::which_in` can use).
-///
-/// Use `scripts/bootstrap/install.py` to bootstrap.
-///
-/// Python versions are sorted from newest to oldest.
-pub fn bootstrapped_pythons() -> Option<Vec<PathBuf>> {
-    // Current dir is `<project root>/crates/uv`.
-    let project_root = std::env::current_dir()
-        .unwrap()
-        .parent()
-        .unwrap()
-        .parent()
-        .unwrap()
-        .to_path_buf();
-    let boostrap_dir = if let Some(boostrap_dir) = env::var_os("UV_BOOTSTRAP_DIR") {
-        let boostrap_dir = PathBuf::from(boostrap_dir);
-        if boostrap_dir.is_absolute() {
-            boostrap_dir
-        } else {
-            // cargo test changes directory to the test crate, but doesn't tell us from where the user is running the
-            // tests. We'll assume that it's the project root.
-            project_root.join(boostrap_dir)
-        }
+pub fn venv_bin_path(venv: &Path) -> PathBuf {
+    if cfg!(unix) {
+        venv.join("bin")
+    } else if cfg!(windows) {
+        venv.join("Scripts")
     } else {
-        project_root.join("bin")
-    };
-    let bootstrapped_pythons = boostrap_dir.join("versions");
-    let Ok(bootstrapped_pythons) = fs_err::read_dir(bootstrapped_pythons) else {
-        return None;
-    };
-
-    let mut bootstrapped_pythons: Vec<PathBuf> = bootstrapped_pythons
-        .map(Result::unwrap)
-        .filter(|entry| entry.metadata().unwrap().is_dir())
-        .map(|entry| {
-            if cfg!(unix) {
-                entry.path().join("install").join("bin")
-            } else if cfg!(windows) {
-                entry.path().join("install")
-            } else {
-                unimplemented!("Only Windows and Unix are supported")
-            }
-        })
-        .collect();
-    bootstrapped_pythons.sort();
-    // Prefer the most recent patch version.
-    bootstrapped_pythons.reverse();
-    Some(bootstrapped_pythons)
+        unimplemented!("Only Windows and Unix are supported")
+    }
+}
+
+pub fn venv_to_interpreter(venv: &Path) -> PathBuf {
+    if cfg!(unix) {
+        venv.join("bin").join("python")
+    } else if cfg!(windows) {
+        venv.join("Scripts").join("python.exe")
+    } else {
+        unimplemented!("Only Windows and Unix are supported")
+    }
 }
 
 /// Create a virtual environment named `.venv` in a temporary directory with the given
-/// Python version. Expected format for `python` is "python<version>".
-pub fn create_venv(
-    temp_dir: &assert_fs::TempDir,
+/// Python version. Expected format for `python` is "<version>".
+pub fn create_venv<Parent: assert_fs::prelude::PathChild + AsRef<std::path::Path>>(
+    temp_dir: &Parent,
     cache_dir: &assert_fs::TempDir,
     python: &str,
 ) -> PathBuf {
-    let python = if let Some(bootstrapped_pythons) = bootstrapped_pythons() {
-        bootstrapped_pythons
-            .into_iter()
-            // Good enough since we control the directory
-            .find(|path| path.to_str().unwrap().contains(&format!("@{python}")))
-            .expect("Missing python bootstrap version")
-            .join(if cfg!(unix) {
-                "python3"
-            } else if cfg!(windows) {
-                "python.exe"
-            } else {
-                unimplemented!("Only Windows and Unix are supported")
-            })
-    } else {
-        PathBuf::from(python)
-    };
+    let python = toolchains_for_version(
+        &PythonVersion::from_str(python).expect("Tests should use a valid Python version"),
+    )
+    .expect("Tests are run on a supported platform")
+    .first()
+    .map(uv_interpreter::managed::Toolchain::executable)
+    // We'll search for the request Python on the PATH if not found in the toolchain versions
+    // We hack this into a `PathBuf` to satisfy the compiler but it's just a string
+    .unwrap_or(PathBuf::from(python));
+
     let venv = temp_dir.child(".venv");
     Command::new(get_bin())
         .arg("venv")
         .arg(venv.as_os_str())
         .arg("--cache-dir")
         .arg(cache_dir.path())
         .arg("--python")
@@ -264,62 +365,104 @@
 /// Returns the uv binary that cargo built before launching the tests.
 ///
 /// <https://doc.rust-lang.org/cargo/reference/environment-variables.html#environment-variables-cargo-sets-for-crates>
 pub fn get_bin() -> PathBuf {
     PathBuf::from(env!("CARGO_BIN_EXE_uv"))
 }
 
-/// Create a directory with the requested Python binaries available.
-pub fn create_bin_with_executables(
+/// Create a `PATH` with the requested Python versions available in order.
+///
+/// Generally this should be used with `UV_TEST_PYTHON_PATH`.
+pub fn python_path_with_versions(
     temp_dir: &assert_fs::TempDir,
     python_versions: &[&str],
 ) -> anyhow::Result<OsString> {
-    if let Some(bootstrapped_pythons) = bootstrapped_pythons() {
-        let selected_pythons = bootstrapped_pythons.into_iter().filter(|path| {
-            python_versions.iter().any(|python_version| {
-                // Good enough since we control the directory
-                path.to_str()
-                    .unwrap()
-                    .contains(&format!("@{python_version}"))
+    let cache = Cache::from_path(temp_dir.child("cache").to_path_buf())?.init()?;
+    let selected_pythons = python_versions
+        .iter()
+        .flat_map(|python_version| {
+            let inner = toolchains_for_version(
+                &PythonVersion::from_str(python_version)
+                    .expect("Tests should use a valid Python version"),
+            )
+            .expect("Tests are run on a supported platform")
+            .iter()
+            .map(|toolchain| {
+                toolchain
+                    .executable()
+                    .parent()
+                    .expect("Executables must exist in a directory")
+                    .to_path_buf()
             })
-        });
-        return Ok(env::join_paths(selected_pythons)?);
-    }
+            .collect::<Vec<_>>();
+            if inner.is_empty() {
+                // Fallback to a system lookup if we failed to find one in the toolchain directory
+                let request = InterpreterRequest::Version(
+                    VersionRequest::from_str(python_version)
+                        .expect("The test version request must be valid"),
+                );
+                let sources = SourceSelector::All;
+                if let Ok(found) = find_interpreter(
+                    &request,
+                    uv_interpreter::SystemPython::Allowed,
+                    &sources,
+                    &cache,
+                )
+                .unwrap()
+                {
+                    vec![found
+                        .into_interpreter()
+                        .sys_executable()
+                        .parent()
+                        .expect("Python executable should always be in a directory")
+                        .to_path_buf()]
+                } else {
+                    panic!("Could not find Python {python_version} for test");
+                }
+            } else {
+                inner
+            }
+        })
+        .collect::<Vec<_>>();
 
-    let bin = temp_dir.child("bin");
-    fs_err::create_dir(&bin)?;
-    for &request in python_versions {
-        let interpreter = find_requested_python(
-            request,
-            &Platform::current().unwrap(),
-            &Cache::temp().unwrap(),
-        )?
-        .ok_or(uv_interpreter::Error::NoSuchPython(request.to_string()))?;
-        let name = interpreter
-            .sys_executable()
-            .file_name()
-            .expect("Discovered executable must have a filename");
-        symlink_file(interpreter.sys_executable(), bin.child(name))?;
-    }
-    Ok(bin.canonicalize()?.into())
+    assert!(
+        python_versions.is_empty() || !selected_pythons.is_empty(),
+        "Failed to fulfill requested test Python versions: {selected_pythons:?}"
+    );
+
+    Ok(env::join_paths(selected_pythons)?)
 }
 
 /// Execute the command and format its output status, stdout and stderr into a snapshot string.
 ///
 /// This function is derived from `insta_cmd`s `spawn_with_info`.
-pub fn run_and_format<'a>(
+pub fn run_and_format<T: AsRef<str>>(
     mut command: impl BorrowMut<std::process::Command>,
-    filters: impl AsRef<[(&'a str, &'a str)]>,
+    filters: impl AsRef<[(T, T)]>,
+    function_name: &str,
     windows_filters: bool,
 ) -> (String, Output) {
     let program = command
         .borrow_mut()
         .get_program()
         .to_string_lossy()
         .to_string();
+
+    // Support profiling test run commands with traces.
+    if let Ok(root) = env::var("TRACING_DURATIONS_TEST_ROOT") {
+        assert!(
+            cfg!(feature = "tracing-durations-export"),
+            "You need to enable the tracing-durations-export feature to use `TRACING_DURATIONS_TEST_ROOT`"
+        );
+        command.borrow_mut().env(
+            "TRACING_DURATIONS_FILE",
+            Path::new(&root).join(function_name).with_extension("jsonl"),
+        );
+    }
+
     let output = command
         .borrow_mut()
         .output()
         .unwrap_or_else(|err| panic!("Failed to spawn {program}: {err}"));
 
     let mut snapshot = format!(
         "success: {:?}\nexit_code: {}\n----- stdout -----\n{}\n----- stderr -----\n{}",
@@ -327,20 +470,24 @@
         output.status.code().unwrap_or(!0),
         String::from_utf8_lossy(&output.stdout),
         String::from_utf8_lossy(&output.stderr)
     );
 
     for (matcher, replacement) in filters.as_ref() {
         // TODO(konstin): Cache regex compilation
-        let re = Regex::new(matcher).expect("Do you need to regex::escape your filter?");
+        let re = Regex::new(matcher.as_ref()).expect("Do you need to regex::escape your filter?");
         if re.is_match(&snapshot) {
-            snapshot = re.replace_all(&snapshot, *replacement).to_string();
+            snapshot = re.replace_all(&snapshot, replacement.as_ref()).to_string();
         }
     }
 
+    // This is a heuristic filter meant to try and make *most* of our tests
+    // pass whether it's on Windows or Unix. In particular, there are some very
+    // common Windows-only dependencies that, when removed from a resolution,
+    // cause the set of dependencies to be the same across platforms.
     if cfg!(windows) && windows_filters {
         // The optional leading +/- is for install logs, the optional next line is for lock files
         let windows_only_deps = [
             ("( [+-] )?colorama==\\d+(\\.[\\d+])+\n(    # via .*\n)?"),
             ("( [+-] )?colorama==\\d+(\\.[\\d+])+\\s+(# via .*\n)?"),
             ("( [+-] )?tzdata==\\d+(\\.[\\d+])+\n(    # via .*\n)?"),
             ("( [+-] )?tzdata==\\d+(\\.[\\d+])+\\s+(# via .*\n)?"),
@@ -363,32 +510,66 @@
             }
         }
     }
 
     (snapshot, output)
 }
 
+/// Recursively copy a directory and its contents.
+pub fn copy_dir_all(src: impl AsRef<Path>, dst: impl AsRef<Path>) -> std::io::Result<()> {
+    fs_err::create_dir_all(&dst)?;
+    for entry in fs_err::read_dir(src.as_ref())? {
+        let entry = entry?;
+        let ty = entry.file_type()?;
+        if ty.is_dir() {
+            copy_dir_all(entry.path(), dst.as_ref().join(entry.file_name()))?;
+        } else {
+            fs_err::copy(entry.path(), dst.as_ref().join(entry.file_name()))?;
+        }
+    }
+    Ok(())
+}
+
+/// Utility macro to return the name of the current function.
+///
+/// https://stackoverflow.com/a/40234666/3549270
+#[doc(hidden)]
+#[macro_export]
+macro_rules! function_name {
+    () => {{
+        fn f() {}
+        fn type_name_of_val<T>(_: T) -> &'static str {
+            std::any::type_name::<T>()
+        }
+        let mut name = type_name_of_val(f).strip_suffix("::f").unwrap_or("");
+        while let Some(rest) = name.strip_suffix("::{{closure}}") {
+            name = rest;
+        }
+        name
+    }};
+}
+
 /// Run [`assert_cmd_snapshot!`], with default filters or with custom filters.
 ///
 /// By default, the filters will search for the generally windows-only deps colorama and tzdata,
 /// filter them out and decrease the package counts by one for each match.
 #[allow(unused_macros)]
 macro_rules! uv_snapshot {
     ($spawnable:expr, @$snapshot:literal) => {{
         uv_snapshot!($crate::common::INSTA_FILTERS.to_vec(), $spawnable, @$snapshot)
     }};
     ($filters:expr, $spawnable:expr, @$snapshot:literal) => {{
         // Take a reference for backwards compatibility with the vec-expecting insta filters.
-        let (snapshot, output) = $crate::common::run_and_format($spawnable, &$filters, true);
+        let (snapshot, output) = $crate::common::run_and_format($spawnable, &$filters, function_name!(), true);
         ::insta::assert_snapshot!(snapshot, @$snapshot);
         output
     }};
     ($filters:expr, windows_filters=false, $spawnable:expr, @$snapshot:literal) => {{
         // Take a reference for backwards compatibility with the vec-expecting insta filters.
-        let (snapshot, output) = $crate::common::run_and_format($spawnable, &$filters, false);
+        let (snapshot, output) = $crate::common::run_and_format($spawnable, &$filters, function_name!(), false);
         ::insta::assert_snapshot!(snapshot, @$snapshot);
         output
     }};
 }
 
 /// <https://stackoverflow.com/a/31749071/3549270>
 #[allow(unused_imports)]
```

### Comparing `uv-0.1.9/crates/uv/tests/pip_compile.rs` & `uv-0.2.0/crates/uv/tests/pip_install.rs`

 * *Files 18% similar despite different names*

```diff
@@ -1,4296 +1,5024 @@
 #![cfg(all(feature = "python", feature = "pypi"))]
 
-use std::fs;
-use std::path::PathBuf;
 use std::process::Command;
 
-use anyhow::{bail, Context, Result};
+use anyhow::Result;
+use assert_cmd::prelude::*;
 use assert_fs::prelude::*;
-use assert_fs::TempDir;
+use base64::{prelude::BASE64_STANDARD as base64, Engine};
 use indoc::indoc;
-use insta::assert_snapshot;
 use itertools::Itertools;
-use url::Url;
 
-use common::{uv_snapshot, TestContext, INSTA_FILTERS};
-use uv_fs::Normalized;
+use common::{uv_snapshot, TestContext};
+use uv_fs::Simplified;
 
-use crate::common::{get_bin, EXCLUDE_NEWER};
+use crate::common::{get_bin, venv_bin_path, BUILD_VENDOR_LINKS_URL};
 
 mod common;
 
-/// Resolve a specific version of Django from a `requirements.in` file.
+// This is a fine-grained token that only has read-only access to the `uv-private-pypackage` repository
+const READ_ONLY_GITHUB_TOKEN: &[&str] = &[
+    "Z2l0aHViX3BhdA==",
+    "MTFCR0laQTdRMGdXeGsweHV6ekR2Mg==",
+    "NVZMaExzZmtFMHZ1ZEVNd0pPZXZkV040WUdTcmk2WXREeFB4TFlybGlwRTZONEpHV01FMnFZQWJVUm4=",
+];
+
+// This is a fine-grained token that only has read-only access to the `uv-private-pypackage-2` repository
+#[cfg(not(windows))]
+const READ_ONLY_GITHUB_TOKEN_2: &[&str] = &[
+    "Z2l0aHViX3BhdA==",
+    "MTFCR0laQTdRMHV1MEpwaFp4dFFyRwo=",
+    "cnNmNXJwMHk2WWpteVZvb2ZFc0c5WUs5b2NPcFY1aVpYTnNmdE05eEhaM0lGSExSSktDWTcxeVBVZXkK",
+];
+
+/// Decode a split, base64 encoded authentication token.
+/// We split and encode the token to bypass revoke by GitHub's secret scanning
+fn decode_token(content: &[&str]) -> String {
+    let token = content
+        .iter()
+        .map(|part| base64.decode(part).unwrap())
+        .map(|decoded| {
+            std::str::from_utf8(decoded.as_slice())
+                .unwrap()
+                .trim_end()
+                .to_string()
+        })
+        .join("_");
+    token
+}
+
+/// Create a `pip uninstall` command with options shared across scenarios.
+fn uninstall_command(context: &TestContext) -> Command {
+    let mut command = Command::new(get_bin());
+    command
+        .arg("pip")
+        .arg("uninstall")
+        .arg("--cache-dir")
+        .arg(context.cache_dir.path())
+        .env("VIRTUAL_ENV", context.venv.as_os_str())
+        .current_dir(&context.temp_dir);
+
+    if cfg!(all(windows, debug_assertions)) {
+        // TODO(konstin): Reduce stack usage in debug mode enough that the tests pass with the
+        // default windows stack of 1MB
+        command.env("UV_STACK_SIZE", (2 * 1024 * 1024).to_string());
+    }
+
+    command
+}
+
 #[test]
-fn compile_requirements_in() -> Result<()> {
+fn missing_requirements_txt() {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("django==5.0b1")?;
+    let requirements_txt = context.temp_dir.child("requirements.txt");
 
-    uv_snapshot!(context
-        .compile()
-        .arg("requirements.in"), @r###"
-    success: true
-    exit_code: 0
+    uv_snapshot!(context.filters(), context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--strict"), @r###"
+    success: false
+    exit_code: 2
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    asgiref==3.7.2
-        # via django
-    django==5.0b1
-    sqlparse==0.4.4
-        # via django
 
     ----- stderr -----
-    Resolved 3 packages in [TIME]
-    "###);
+    error: failed to read from file `requirements.txt`
+      Caused by: No such file or directory (os error 2)
+    "###
+    );
 
-    Ok(())
+    requirements_txt.assert(predicates::path::missing());
 }
 
-/// Resolve a specific version of Django from a `requirements.in` file with a `--annotation-style=line` flag.
 #[test]
-fn compile_requirements_in_annotation_line() -> Result<()> {
+fn empty_requirements_txt() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("django==5.0b1")?;
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.touch()?;
 
-    uv_snapshot!(context
-        .compile()
-        .arg("--annotation-style=line")
-        .arg("requirements.in"), @r###"
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--strict"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z --annotation-style=line requirements.in
-    asgiref==3.7.2            # via django
-    django==5.0b1
-    sqlparse==0.4.4           # via django
 
     ----- stderr -----
-    Resolved 3 packages in [TIME]
-    "###);
+    warning: Requirements file requirements.txt does not contain any dependencies
+    Audited 0 packages in [TIME]
+    "###
+    );
 
     Ok(())
 }
 
-/// Resolve a specific version of Django from a `requirements.in` file on stdin
-/// when passed a path of `-`.
 #[test]
-fn compile_requirements_in_stdin() -> Result<()> {
+fn missing_pyproject_toml() {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("django==5.0b1")?;
 
-    uv_snapshot!(context
-        .compile()
-        .stdin(fs::File::open(requirements_in)?)
-        .arg("-"), @r###"
-    success: true
-    exit_code: 0
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("pyproject.toml"), @r###"
+    success: false
+    exit_code: 2
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z -
-    asgiref==3.7.2
-        # via django
-    django==5.0b1
-    sqlparse==0.4.4
-        # via django
 
     ----- stderr -----
-    Resolved 3 packages in [TIME]
-    "###);
-
-    Ok(())
+    error: failed to read from file `pyproject.toml`
+      Caused by: No such file or directory (os error 2)
+    "###
+    );
 }
 
 #[test]
-fn missing_requirements_in() {
+fn invalid_pyproject_toml_syntax() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
+    let pyproject_toml = context.temp_dir.child("pyproject.toml");
+    pyproject_toml.write_str("123 - 456")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("pyproject.toml"), @r###"
     success: false
     exit_code: 2
     ----- stdout -----
 
     ----- stderr -----
-    error: failed to open file `requirements.in`
-      Caused by: No such file or directory (os error 2)
+    error: Failed to parse `pyproject.toml`
+      Caused by: TOML parse error at line 1, column 5
+      |
+    1 | 123 - 456
+      |     ^
+    expected `.`, `=`
+
     "###
     );
 
-    requirements_in.assert(predicates::path::missing());
+    Ok(())
 }
 
 #[test]
-fn missing_venv() -> Result<()> {
-    let temp_dir = TempDir::new()?;
-    let cache_dir = TempDir::new()?;
-    let venv = temp_dir.child(".venv");
+fn invalid_pyproject_toml_schema() -> Result<()> {
+    let context = TestContext::new("3.12");
+    let pyproject_toml = context.temp_dir.child("pyproject.toml");
+    pyproject_toml.write_str("[project]")?;
 
-    uv_snapshot!(Command::new(get_bin())
-            .arg("pip")
-            .arg("compile")
-            .arg("requirements.in")
-            .arg("--cache-dir")
-            .arg(cache_dir.path())
-            .env("VIRTUAL_ENV", venv.as_os_str())
-            .current_dir(&temp_dir), @r###"
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("pyproject.toml"), @r###"
     success: false
     exit_code: 2
     ----- stdout -----
 
     ----- stderr -----
-    error: failed to open file `requirements.in`
-      Caused by: No such file or directory (os error 2)
+    error: Failed to parse `pyproject.toml`
+      Caused by: TOML parse error at line 1, column 1
+      |
+    1 | [project]
+      | ^^^^^^^^^
+    missing field `name`
+
     "###
     );
 
-    venv.assert(predicates::path::missing());
-
     Ok(())
 }
 
-/// Resolve a specific version of Django from a `pyproject.toml` file.
+/// For user controlled pyproject.toml files, we enforce PEP 621.
 #[test]
-fn compile_pyproject_toml() -> Result<()> {
+fn invalid_pyproject_toml_requirement_direct() -> Result<()> {
     let context = TestContext::new("3.12");
     let pyproject_toml = context.temp_dir.child("pyproject.toml");
     pyproject_toml.write_str(
-        r#"[build-system]
-requires = ["setuptools", "wheel"]
-
-[project]
+        r#"[project]
 name = "project"
-dependencies = [
-    "django==5.0b1",
-]
+dependencies = ["flask==1.0.x"]
 "#,
     )?;
 
-    uv_snapshot!(context.compile()
-            .arg("pyproject.toml"), @r###"
-    success: true
-    exit_code: 0
+    let filters = [("exit status", "exit code")]
+        .into_iter()
+        .chain(context.filters())
+        .collect::<Vec<_>>();
+
+    uv_snapshot!(filters, context.install()
+        .arg("-r")
+        .arg("pyproject.toml"), @r###"
+    success: false
+    exit_code: 2
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z pyproject.toml
-    asgiref==3.7.2
-        # via django
-    django==5.0b1
-    sqlparse==0.4.4
-        # via django
 
     ----- stderr -----
-    Resolved 3 packages in [TIME]
+    error: Failed to parse `pyproject.toml`
+      Caused by: after parsing '1.0', found '.x', which is not part of a valid version
+    flask==1.0.x
+         ^^^^^^^
     "###
     );
 
     Ok(())
 }
 
-/// Resolve a package from a `requirements.in` file, with a `constraints.txt` file.
+/// For indirect, non-user controlled pyproject.toml, we don't enforce correctness.
+///
+/// If we fail to extract the PEP 621 metadata, we fall back to treating it as a source
+/// tree, as there are some cases where the `pyproject.toml` may not be a valid PEP
+/// 621 file, but might still resolve under PEP 517. (If the source tree doesn't
+/// resolve under PEP 517, we'll catch that later.)
+///
+/// For example, Hatch's "Context formatting" API is not compliant with PEP 621, as
+/// it expects dynamic processing by the build backend for the static metadata
+/// fields. See: <https://hatch.pypa.io/latest/config/context/>
 #[test]
-fn compile_constraints_txt() -> Result<()> {
+fn invalid_pyproject_toml_requirement_indirect() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("django==5.0b1")?;
+    let pyproject_toml = context.temp_dir.child("path_dep/pyproject.toml");
+    pyproject_toml.write_str(
+        r#"[project]
+name = "project"
+dependencies = ["flask==1.0.x"]
+"#,
+    )?;
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str("./path_dep")?;
 
-    let constraints_txt = context.temp_dir.child("constraints.txt");
-    constraints_txt.write_str("sqlparse<0.4.4")?;
+    let filters = [("exit status", "exit code")]
+        .into_iter()
+        .chain(context.filters())
+        .collect::<Vec<_>>();
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--constraint")
-            .arg("constraints.txt"), @r###"
-    success: true
-    exit_code: 0
+    uv_snapshot!(filters, context.install()
+        .arg("-r")
+        .arg("requirements.txt"), @r###"
+    success: false
+    exit_code: 2
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --constraint constraints.txt
-    asgiref==3.7.2
-        # via django
-    django==5.0b1
-    sqlparse==0.4.3
-        # via django
 
     ----- stderr -----
-    Resolved 3 packages in [TIME]
+    error: Failed to download and build: `project @ file://[TEMP_DIR]/path_dep`
+      Caused by: Failed to build: `project @ file://[TEMP_DIR]/path_dep`
+      Caused by: Build backend failed to determine extra requires with `build_wheel()` with exit code: 1
+    --- stdout:
+    configuration error: `project.dependencies[0]` must be pep508
+    DESCRIPTION:
+        Project dependency specification according to PEP 508
+
+    GIVEN VALUE:
+        "flask==1.0.x"
+
+    OFFENDING RULE: 'format'
+
+    DEFINITION:
+        {
+            "$id": "#/definitions/dependency",
+            "title": "Dependency",
+            "type": "string",
+            "format": "pep508"
+        }
+    --- stderr:
+    Traceback (most recent call last):
+      File "<string>", line 14, in <module>
+      File "[CACHE_DIR]/[TMP]/build_meta.py", line 325, in get_requires_for_build_wheel
+        return self._get_build_requires(config_settings, requirements=['wheel'])
+               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+      File "[CACHE_DIR]/[TMP]/build_meta.py", line 295, in _get_build_requires
+        self.run_setup()
+      File "[CACHE_DIR]/[TMP]/build_meta.py", line 487, in run_setup
+        super().run_setup(setup_script=setup_script)
+      File "[CACHE_DIR]/[TMP]/build_meta.py", line 311, in run_setup
+        exec(code, locals())
+      File "<string>", line 1, in <module>
+      File "[CACHE_DIR]/[TMP]/__init__.py", line 104, in setup
+        return distutils.core.setup(**attrs)
+               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+      File "[CACHE_DIR]/[TMP]/core.py", line 159, in setup
+        dist.parse_config_files()
+      File "[CACHE_DIR]/[TMP]/_virtualenv.py", line 22, in parse_config_files
+        result = old_parse_config_files(self, *args, **kwargs)
+                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+      File "[CACHE_DIR]/[TMP]/dist.py", line 631, in parse_config_files
+        pyprojecttoml.apply_configuration(self, filename, ignore_option_errors)
+      File "[CACHE_DIR]/[TMP]/pyprojecttoml.py", line 68, in apply_configuration
+        config = read_configuration(filepath, True, ignore_option_errors, dist)
+                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+      File "[CACHE_DIR]/[TMP]/pyprojecttoml.py", line 129, in read_configuration
+        validate(subset, filepath)
+      File "[CACHE_DIR]/[TMP]/pyprojecttoml.py", line 57, in validate
+        raise ValueError(f"{error}/n{summary}") from None
+    ValueError: invalid pyproject.toml config: `project.dependencies[0]`.
+    configuration error: `project.dependencies[0]` must be pep508
+    ---
     "###
     );
 
     Ok(())
 }
 
-/// Resolve a package from a `requirements.in` file, with an inline constraint.
 #[test]
-fn compile_constraints_inline() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("django==5.0b1")?;
-    requirements_in.write_str("-c constraints.txt")?;
-
-    let constraints_txt = context.temp_dir.child("constraints.txt");
-    constraints_txt.write_str("sqlparse<0.4.4")?;
-
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
-    success: true
-    exit_code: 0
+fn missing_pip() {
+    uv_snapshot!(Command::new(get_bin()).arg("install"), @r###"
+    success: false
+    exit_code: 2
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
 
     ----- stderr -----
-    Resolved 0 packages in [TIME]
-    "###
-    );
+    error: unrecognized subcommand 'install'
 
-    Ok(())
+      tip: a similar subcommand exists: 'uv pip install'
+
+    Usage: uv [OPTIONS] <COMMAND>
+
+    For more information, try '--help'.
+    "###);
 }
 
-/// Resolve a package from a `requirements.in` file, with a `constraints.txt` file that
-/// uses markers.
 #[test]
-fn compile_constraints_markers() -> Result<()> {
+fn no_solution() {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("anyio")?;
 
-    // Constrain a transitive dependency based on the Python version
-    let constraints_txt = context.temp_dir.child("constraints.txt");
-    // If constraints are ignored, these will conflict
-    constraints_txt.write_str("sniffio==1.2.0;python_version<='3.7'")?;
-    constraints_txt.write_str("sniffio==1.3.0;python_version>'3.7'")?;
+    uv_snapshot!(context.install()
+        .arg("flask>=3.0.2")
+        .arg("WerkZeug<1.0.0")
+        .arg("--strict"), @r###"
+    success: false
+    exit_code: 1
+    ----- stdout -----
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--constraint")
-            .arg("constraints.txt"), @r###"
+    ----- stderr -----
+      × No solution found when resolving dependencies:
+      ╰─▶ Because only flask<=3.0.2 is available and flask==3.0.2 depends on werkzeug>=3.0.0, we can conclude that flask>=3.0.2 depends on werkzeug>=3.0.0.
+          And because you require flask>=3.0.2 and werkzeug<1.0.0, we can conclude that the requirements are unsatisfiable.
+    "###);
+}
+
+/// Install a package from the command line into a virtual environment.
+#[test]
+fn install_package() {
+    let context = TestContext::new("3.12");
+
+    // Install Flask.
+    uv_snapshot!(context.install()
+        .arg("Flask")
+        .arg("--strict"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --constraint constraints.txt
-    anyio==4.0.0
-    idna==3.4
-        # via anyio
-    sniffio==1.3.0
-        # via anyio
 
     ----- stderr -----
-    Resolved 3 packages in [TIME]
+    Resolved 7 packages in [TIME]
+    Downloaded 7 packages in [TIME]
+    Installed 7 packages in [TIME]
+     + blinker==1.7.0
+     + click==8.1.7
+     + flask==3.0.2
+     + itsdangerous==2.1.2
+     + jinja2==3.1.3
+     + markupsafe==2.1.5
+     + werkzeug==3.0.1
     "###
     );
 
-    Ok(())
+    context.assert_command("import flask").success();
 }
 
-/// Resolve a package from an optional dependency group in a `pyproject.toml` file.
+/// Install a package from a `requirements.txt` into a virtual environment.
 #[test]
-fn compile_pyproject_toml_extra() -> Result<()> {
+fn install_requirements_txt() -> Result<()> {
     let context = TestContext::new("3.12");
-    let pyproject_toml = context.temp_dir.child("pyproject.toml");
-    pyproject_toml.write_str(
-        r#"[build-system]
-requires = ["setuptools", "wheel"]
 
-[project]
-name = "project"
-dependencies = []
-optional-dependencies.foo = [
-    "django==5.0b1",
-]
-"#,
-    )?;
+    // Install Flask.
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str("Flask")?;
 
-    uv_snapshot!(context.compile()
-            .arg("pyproject.toml")
-            .arg("--extra")
-            .arg("foo"), @r###"
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--strict"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z pyproject.toml --extra foo
-    asgiref==3.7.2
-        # via django
-    django==5.0b1
-    sqlparse==0.4.4
-        # via django
 
     ----- stderr -----
-    Resolved 3 packages in [TIME]
+    Resolved 7 packages in [TIME]
+    Downloaded 7 packages in [TIME]
+    Installed 7 packages in [TIME]
+     + blinker==1.7.0
+     + click==8.1.7
+     + flask==3.0.2
+     + itsdangerous==2.1.2
+     + jinja2==3.1.3
+     + markupsafe==2.1.5
+     + werkzeug==3.0.1
     "###
     );
 
-    Ok(())
-}
-
-/// Resolve a package from an extra with unnormalized names in a `pyproject.toml` file.
-#[test]
-fn compile_pyproject_toml_extra_name_normalization() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let pyproject_toml = context.temp_dir.child("pyproject.toml");
-    pyproject_toml.write_str(
-        r#"[build-system]
-requires = ["setuptools", "wheel"]
+    context.assert_command("import flask").success();
 
-[project]
-name = "project"
-dependencies = []
-optional-dependencies."FrIeNdLy-._.-bArD" = [
-    "django==5.0b1",
-]
-"#,
-    )?;
+    // Install Jinja2 (which should already be installed, but shouldn't remove other packages).
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str("Jinja2")?;
 
-    uv_snapshot!(context.compile()
-            .arg("pyproject.toml")
-            .arg("--extra")
-            .arg("FRiENDlY-...-_-BARd"), @r###"
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--strict"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z pyproject.toml --extra FRiENDlY-...-_-BARd
-    asgiref==3.7.2
-        # via django
-    django==5.0b1
-    sqlparse==0.4.4
-        # via django
 
     ----- stderr -----
-    Resolved 3 packages in [TIME]
+    Audited 1 package in [TIME]
     "###
     );
 
+    context.assert_command("import flask").success();
+
     Ok(())
 }
 
-/// Request an extra that does not exist as a dependency group in a `pyproject.toml` file.
+/// Install a requirements file with pins that conflict
+///
+/// This is likely to occur in the real world when compiled on one platform then installed on another.
 #[test]
-fn compile_pyproject_toml_extra_missing() -> Result<()> {
+fn install_requirements_txt_conflicting_pins() -> Result<()> {
     let context = TestContext::new("3.12");
-    let pyproject_toml = context.temp_dir.child("pyproject.toml");
-    pyproject_toml.write_str(
-        r#"[build-system]
-requires = ["setuptools", "wheel"]
 
-[project]
-name = "project"
-dependencies = []
-optional-dependencies.foo = [
-    "django==5.0b1",
-]
-"#,
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+
+    // We pin `click` to a conflicting requirement
+    requirements_txt.write_str(
+        r"
+blinker==1.7.0
+click==7.0.0
+flask==3.0.2
+itsdangerous==2.1.2
+jinja2==3.1.3
+markupsafe==2.1.5
+werkzeug==3.0.1
+",
     )?;
 
-    uv_snapshot!(context.compile()
-            .arg("pyproject.toml")
-            .arg("--extra")
-            .arg("bar"), @r###"
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--strict"), @r###"
     success: false
-    exit_code: 2
+    exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
-    error: Requested extra not found: bar
+      × No solution found when resolving dependencies:
+      ╰─▶ Because flask==3.0.2 depends on click>=8.1.3 and you require click==7.0.0, we can conclude that your requirements and flask==3.0.2 are incompatible.
+          And because you require flask==3.0.2, we can conclude that the requirements are unsatisfiable.
     "###
     );
 
     Ok(())
 }
 
-/// Show a dedicated warning if the user tries to compile a `pyproject.toml` file with a `poetry`
-/// section.
+/// Install a `pyproject.toml` file with a `poetry` section.
 #[test]
-fn compile_empty_pyproject_toml_poetry() -> Result<()> {
+fn install_pyproject_toml_poetry() -> Result<()> {
     let context = TestContext::new("3.12");
     let pyproject_toml = context.temp_dir.child("pyproject.toml");
     pyproject_toml.write_str(
         r#"[tool.poetry]
 name = "poetry-editable"
 version = "0.1.0"
 description = ""
 authors = ["Astral Software Inc. <hey@astral.sh>"]
 
 [tool.poetry.dependencies]
 python = "^3.10"
-numpy = "^1"
+anyio = "^3"
+iniconfig = { version = "*", optional = true }
+
+[tool.poetry.extras]
+test = ["iniconfig"]
 
 [build-system]
 requires = ["poetry-core"]
 build-backend = "poetry.core.masonry.api"
 "#,
     )?;
 
-    uv_snapshot!(context.compile()
-            .arg("pyproject.toml"), @r###"
+    uv_snapshot!(context.install()
+            .arg("-r")
+            .arg("pyproject.toml")
+            .arg("--extra")
+            .arg("test"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z pyproject.toml
 
     ----- stderr -----
-    warning: `pyproject.toml` does not contain any dependencies (hint: specify dependencies in the `project.dependencies` section; `tool.poetry.dependencies` is not currently supported)
-    Resolved 0 packages in [TIME]
+    Resolved 4 packages in [TIME]
+    Downloaded 4 packages in [TIME]
+    Installed 4 packages in [TIME]
+     + anyio==3.7.1
+     + idna==3.6
+     + iniconfig==2.0.0
+     + sniffio==1.3.1
     "###
     );
 
     Ok(())
 }
 
-/// Request multiple extras that do not exist as a dependency group in a `pyproject.toml` file.
+/// Respect installed versions when resolving.
 #[test]
-fn compile_pyproject_toml_extras_missing() -> Result<()> {
+fn respect_installed_and_reinstall() -> Result<()> {
     let context = TestContext::new("3.12");
-    let pyproject_toml = context.temp_dir.child("pyproject.toml");
-    pyproject_toml.write_str(
-        r#"[build-system]
-requires = ["setuptools", "wheel"]
 
-[project]
-name = "project"
-dependencies = []
-optional-dependencies.foo = [
-    "django==5.0b1",
-]
-"#,
-    )?;
+    // Install Flask.
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str("Flask==2.3.2")?;
 
-    uv_snapshot!(context.compile()
-            .arg("pyproject.toml")
-            .arg("--extra")
-            .arg("foo")
-            .arg("--extra")
-            .arg("bar")
-            .arg("--extra")
-            .arg("foobar"), @r###"
-    success: false
-    exit_code: 2
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--strict"), @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    error: Requested extras not found: bar, foobar
+    Resolved 7 packages in [TIME]
+    Downloaded 7 packages in [TIME]
+    Installed 7 packages in [TIME]
+     + blinker==1.7.0
+     + click==8.1.7
+     + flask==2.3.2
+     + itsdangerous==2.1.2
+     + jinja2==3.1.3
+     + markupsafe==2.1.5
+     + werkzeug==3.0.1
     "###
     );
 
-    Ok(())
-}
+    context.assert_command("import flask").success();
 
-/// Request extras when using a `requirements.in` file which does not support extras.
-#[test]
-fn compile_requirements_file_extra() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("django==5.0b1")?;
+    // Re-install Flask. We should respect the existing version.
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str("Flask")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--all-extras"),
-            @r###"
-    success: false
-    exit_code: 2
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--strict"), @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    error: Requesting extras requires a pyproject.toml input file.
+    Audited 1 package in [TIME]
     "###
     );
 
-    Ok(())
-}
-
-/// Request an extra with a name that does not conform to the specification.
-#[test]
-fn invalid_extra_name() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let pyproject_toml = context.temp_dir.child("pyproject.toml");
-    pyproject_toml.write_str(
-        r#"[build-system]
-requires = ["setuptools", "wheel"]
+    context.assert_command("import flask").success();
 
-[project]
-name = "project"
-dependencies = []
-optional-dependencies.foo = [
-    "django==5.0b1",
-]
-"#,
-    )?;
+    // Install a newer version of Flask. We should upgrade it.
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str("Flask==2.3.3")?;
 
-    uv_snapshot!(context.compile()
-            .arg("pyproject.toml")
-            .arg("--extra")
-            .arg("invalid name!"), @r###"
-    success: false
-    exit_code: 2
+    let filters = if cfg!(windows) {
+        // Remove the colorama count on windows
+        context
+            .filters()
+            .into_iter()
+            .chain([("Resolved 8 packages", "Resolved 7 packages")])
+            .collect()
+    } else {
+        context.filters()
+    };
+    uv_snapshot!(filters, context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--strict"), @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    error: invalid value 'invalid name!' for '--extra <EXTRA>': Extra names must start and end with a letter or digit and may only contain -, _, ., and alphanumeric characters
-
-    For more information, try '--help'.
+    Resolved 7 packages in [TIME]
+    Downloaded 1 package in [TIME]
+    Uninstalled 1 package in [TIME]
+    Installed 1 package in [TIME]
+     - flask==2.3.2
+     + flask==2.3.3
     "###
     );
 
-    Ok(())
-}
-
-/// Resolve a specific version of Black at Python 3.12.
-#[test]
-fn compile_python_312() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("black==23.10.1")?;
+    // Re-install Flask. We should upgrade it.
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str("Flask")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--python-version")
-            .arg("3.12"), @r###"
+    uv_snapshot!(filters, context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--reinstall-package")
+        .arg("Flask")
+        .arg("--strict"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --python-version 3.12
-    black==23.10.1
-    click==8.1.7
-        # via black
-    mypy-extensions==1.0.0
-        # via black
-    packaging==23.2
-        # via black
-    pathspec==0.11.2
-        # via black
-    platformdirs==4.0.0
-        # via black
 
     ----- stderr -----
-    Resolved 6 packages in [TIME]
+    Resolved 7 packages in [TIME]
+    Downloaded 1 package in [TIME]
+    Uninstalled 1 package in [TIME]
+    Installed 1 package in [TIME]
+     - flask==2.3.3
+     + flask==3.0.2
     "###
     );
 
-    Ok(())
-}
-
-/// Resolve a specific version of Black at Python 3.12 with `--annotation-style=line`.
-#[test]
-fn compile_python_312_annotation_line() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("black==23.10.1")?;
+    // Re-install Flask. We should install even though the version is current
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str("Flask")?;
 
-    uv_snapshot!(context.compile()
-            .arg("--annotation-style=line")
-            .arg("requirements.in")
-            .arg("--python-version")
-            .arg("3.12"), @r###"
+    uv_snapshot!(filters, context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--reinstall-package")
+        .arg("Flask")
+        .arg("--strict"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z --annotation-style=line requirements.in --python-version 3.12
-    black==23.10.1
-    click==8.1.7              # via black
-    mypy-extensions==1.0.0    # via black
-    packaging==23.2           # via black
-    pathspec==0.11.2          # via black
-    platformdirs==4.0.0       # via black
 
     ----- stderr -----
-    Resolved 6 packages in [TIME]
+    Resolved 7 packages in [TIME]
+    Uninstalled 1 package in [TIME]
+    Installed 1 package in [TIME]
+     - flask==3.0.2
+     + flask==3.0.2
     "###
     );
 
     Ok(())
 }
 
-/// Resolve a specific version of Black at Python 3.12 without deps.
+/// Respect installed versions when resolving.
 #[test]
-fn compile_python_312_no_deps() -> Result<()> {
+fn reinstall_extras() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("black==23.10.1")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--no-deps")
-            .arg("--python-version")
-            .arg("3.12"), @r###"
+    // Install httpx.
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str("httpx")?;
+
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--strict"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --no-deps --python-version 3.12
-    black==23.10.1
 
     ----- stderr -----
-    Resolved 1 package in [TIME]
+    Resolved 7 packages in [TIME]
+    Downloaded 7 packages in [TIME]
+    Installed 7 packages in [TIME]
+     + anyio==4.3.0
+     + certifi==2024.2.2
+     + h11==0.14.0
+     + httpcore==1.0.4
+     + httpx==0.27.0
+     + idna==3.6
+     + sniffio==1.3.1
     "###
     );
 
-    Ok(())
-}
+    context.assert_command("import httpx").success();
 
-/// Resolve a specific version of Black at Python 3.7.
-#[test]
-fn compile_python_37() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("black==23.10.1")?;
-
-    let filters: Vec<_> = [
-        // 3.7 may not be installed
-        (
-            "warning: The requested Python version 3.7 is not available; .* will be used to build dependencies instead.\n",
-            "",
-        ),
-    ]
-        .into_iter()
-        .chain(INSTA_FILTERS.to_vec())
-        .collect();
+    // Re-install httpx, with an extra.
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str("httpx[http2]")?;
 
-    uv_snapshot!(filters, context.compile()
-            .arg("requirements.in")
-            .arg("--python-version")
-            .arg("3.7"), @r###"
-    success: false
-    exit_code: 1
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--strict"), @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-      × No solution found when resolving dependencies:
-      ╰─▶ Because the requested Python version (3.7) does not satisfy Python>=3.8
-          and black==23.10.1 depends on Python>=3.8, we can conclude that
-          black==23.10.1 cannot be used.
-          And because you require black==23.10.1, we can conclude that the
-          requirements are unsatisfiable.
-    "###);
+    Resolved 10 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 3 packages in [TIME]
+     + h2==4.1.0
+     + hpack==4.0.0
+     + hyperframe==6.0.1
+    "###
+    );
+
+    context.assert_command("import httpx").success();
 
     Ok(())
 }
 
-/// Resolve a source distribution with `--resolution=lowest-direct`, to ensure that the build
-/// requirements aren't resolved at their lowest compatible version.
+/// Warn, but don't fail, when uninstalling incomplete packages.
 #[test]
-fn compile_sdist_resolution_lowest() -> Result<()> {
+fn reinstall_incomplete() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("anyio @ https://files.pythonhosted.org/packages/2d/b8/7333d87d5f03247215d86a86362fd3e324111788c6cdd8d2e6196a6ba833/anyio-4.2.0.tar.gz")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--resolution=lowest-direct")
-            .arg("--python-version")
-            .arg("3.12"), @r###"
+    // Install anyio.
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str("anyio==3.7.0")?;
+
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --resolution=lowest-direct --python-version 3.12
-    anyio @ https://files.pythonhosted.org/packages/2d/b8/7333d87d5f03247215d86a86362fd3e324111788c6cdd8d2e6196a6ba833/anyio-4.2.0.tar.gz
-    idna==3.4
-        # via anyio
-    sniffio==1.3.0
-        # via anyio
 
     ----- stderr -----
     Resolved 3 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 3 packages in [TIME]
+     + anyio==3.7.0
+     + idna==3.6
+     + sniffio==1.3.1
     "###
     );
 
-    Ok(())
-}
+    // Manually remove the `RECORD` file.
+    fs_err::remove_file(context.site_packages().join("anyio-3.7.0.dist-info/RECORD"))?;
 
-/// Resolve a specific version of Black against an invalid Python version.
-#[test]
-fn compile_python_invalid_version() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("black==23.10.1")?;
+    // Re-install anyio.
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str("anyio==4.0.0")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--python-version")
-            .arg("3.7.x"), @r###"
-    success: false
-    exit_code: 2
+    uv_snapshot!(context.filters(), context.install()
+        .arg("-r")
+        .arg("requirements.txt"), @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    error: invalid value '3.7.x' for '--python-version <PYTHON_VERSION>': after parsing 3.7, found ".x" after it, which is not part of a valid version
-
-    For more information, try '--help'.
+    Resolved 3 packages in [TIME]
+    Downloaded 1 package in [TIME]
+    warning: Failed to uninstall package at [SITE_PACKAGES]/anyio-3.7.0.dist-info due to missing RECORD file. Installation may result in an incomplete environment.
+    Uninstalled 1 package in [TIME]
+    Installed 1 package in [TIME]
+     - anyio==3.7.0
+     + anyio==4.0.0
     "###
     );
 
     Ok(())
 }
 
-/// Resolve a specific version of Black against an invalid Python version.
+/// Like `pip`, we (unfortunately) allow incompatible environments.
 #[test]
-fn compile_python_dev_version() -> Result<()> {
+fn allow_incompatibilities() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("black==23.10.1")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--python-version")
-            .arg("3.7-dev"), @r###"
-    success: false
-    exit_code: 2
+    // Install Flask, which relies on `Werkzeug>=3.0.0`.
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str("Flask")?;
+
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--strict"), @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    error: invalid value '3.7-dev' for '--python-version <PYTHON_VERSION>': Python version 3.7-dev is a development release
-
-    For more information, try '--help'.
+    Resolved 7 packages in [TIME]
+    Downloaded 7 packages in [TIME]
+    Installed 7 packages in [TIME]
+     + blinker==1.7.0
+     + click==8.1.7
+     + flask==3.0.2
+     + itsdangerous==2.1.2
+     + jinja2==3.1.3
+     + markupsafe==2.1.5
+     + werkzeug==3.0.1
     "###
     );
 
-    Ok(())
-}
-
-/// Test that we select the last 3.8 compatible numpy version instead of trying to compile an
-/// incompatible sdist <https://github.com/astral-sh/uv/issues/388>
-#[test]
-fn compile_numpy_py38() -> Result<()> {
-    let context = TestContext::new("3.8");
+    context.assert_command("import flask").success();
 
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("numpy")?;
+    // Install an incompatible version of Jinja2.
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str("jinja2==2.11.3")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--no-build"), @r###"
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--strict"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --no-build
-    numpy==1.24.4
 
     ----- stderr -----
-    Resolved 1 package in [TIME]
+    Resolved 2 packages in [TIME]
+    Downloaded 1 package in [TIME]
+    Uninstalled 1 package in [TIME]
+    Installed 1 package in [TIME]
+     - jinja2==3.1.3
+     + jinja2==2.11.3
+    warning: The package `flask` requires `jinja2>=3.1.2`, but `2.11.3` is installed.
     "###
     );
 
+    // This no longer works, since we have an incompatible version of Jinja2.
+    context.assert_command("import flask").failure();
+
     Ok(())
 }
 
-/// Resolve a specific Flask wheel via a URL dependency.
 #[test]
-fn compile_wheel_url_dependency() -> Result<()> {
+fn install_editable() {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("flask @ https://files.pythonhosted.org/packages/36/42/015c23096649b908c809c69388a805a571a3bea44362fe87e33fc3afa01f/flask-3.0.0-py3-none-any.whl")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    // Install the editable package.
+    uv_snapshot!(context.filters(), context.install()
+        .arg("-e")
+        .arg(context.workspace_root.join("scripts/packages/poetry_editable")), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    blinker==1.7.0
-        # via flask
-    click==8.1.7
-        # via flask
-    flask @ https://files.pythonhosted.org/packages/36/42/015c23096649b908c809c69388a805a571a3bea44362fe87e33fc3afa01f/flask-3.0.0-py3-none-any.whl
-    itsdangerous==2.1.2
-        # via flask
-    jinja2==3.1.2
-        # via flask
-    markupsafe==2.1.3
-        # via
-        #   jinja2
-        #   werkzeug
-    werkzeug==3.0.1
-        # via flask
 
     ----- stderr -----
-    Resolved 7 packages in [TIME]
+    Built 1 editable in [TIME]
+    Resolved 4 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 4 packages in [TIME]
+     + anyio==4.3.0
+     + idna==3.6
+     + poetry-editable==0.1.0 (from file://[WORKSPACE]/scripts/packages/poetry_editable)
+     + sniffio==1.3.1
     "###
     );
 
-    Ok(())
-}
-
-/// Resolve a specific Flask source distribution via a URL dependency.
-///
-/// Exercises the `prepare_metadata_for_build_wheel` hooks.
-#[test]
-fn compile_sdist_url_dependency() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("flask @ https://files.pythonhosted.org/packages/d8/09/c1a7354d3925a3c6c8cfdebf4245bae67d633ffda1ba415add06ffc839c5/flask-3.0.0.tar.gz")?;
-
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    // Install it again (no-op).
+    uv_snapshot!(context.filters(), context.install()
+        .arg("-e")
+        .arg(context.workspace_root.join("scripts/packages/poetry_editable")), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    blinker==1.7.0
-        # via flask
-    click==8.1.7
-        # via flask
-    flask @ https://files.pythonhosted.org/packages/d8/09/c1a7354d3925a3c6c8cfdebf4245bae67d633ffda1ba415add06ffc839c5/flask-3.0.0.tar.gz
-    itsdangerous==2.1.2
-        # via flask
-    jinja2==3.1.2
-        # via flask
-    markupsafe==2.1.3
-        # via
-        #   jinja2
-        #   werkzeug
-    werkzeug==3.0.1
-        # via flask
 
     ----- stderr -----
-    Resolved 7 packages in [TIME]
+    Audited 1 package in [TIME]
     "###
     );
 
-    Ok(())
-}
-
-/// Resolve a specific Flask source distribution via a Git HTTPS dependency.
-#[test]
-#[cfg(feature = "git")]
-fn compile_git_https_dependency() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("flask @ git+https://github.com/pallets/flask.git")?;
-
-    // In addition to the standard filters, remove the `main` commit, which will change frequently.
-    let filters: Vec<_> = [(r"@(\d|\w){40}", "@[COMMIT]")]
-        .into_iter()
-        .chain(INSTA_FILTERS.to_vec())
-        .collect();
-
-    uv_snapshot!(filters, context.compile()
-            .arg("requirements.in"), @r###"
+    // Add another, non-editable dependency.
+    uv_snapshot!(context.filters(), context.install()
+        .arg("-e")
+        .arg(context.workspace_root.join("scripts/packages/poetry_editable"))
+        .arg("black"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    blinker==1.7.0
-        # via flask
-    click==8.1.7
-        # via flask
-    flask @ git+https://github.com/pallets/flask.git@[COMMIT]
-    itsdangerous==2.1.2
-        # via flask
-    jinja2==3.1.2
-        # via flask
-    markupsafe==2.1.3
-        # via
-        #   jinja2
-        #   werkzeug
-    werkzeug==3.0.1
-        # via flask
 
     ----- stderr -----
-    Resolved 7 packages in [TIME]
-    "###);
-
-    Ok(())
+    Resolved 10 packages in [TIME]
+    Downloaded 6 packages in [TIME]
+    Installed 6 packages in [TIME]
+     + black==24.3.0
+     + click==8.1.7
+     + mypy-extensions==1.0.0
+     + packaging==24.0
+     + pathspec==0.12.1
+     + platformdirs==4.2.0
+    "###
+    );
 }
 
-/// Resolve a specific Flask branch via a Git HTTPS dependency.
 #[test]
-#[cfg(feature = "git")]
-fn compile_git_branch_https_dependency() -> Result<()> {
+fn install_editable_and_registry() {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("flask @ git+https://github.com/pallets/flask.git@1.0.x")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    // Install the registry-based version of Black.
+    uv_snapshot!(context.filters(), context.install()
+        .arg("black"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    click==8.1.7
-        # via flask
-    flask @ git+https://github.com/pallets/flask.git@d92b64aa275841b0c9aea3903aba72fbc4275d91
-    itsdangerous==2.1.2
-        # via flask
-    jinja2==3.1.2
-        # via flask
-    markupsafe==2.1.3
-        # via
-        #   jinja2
-        #   werkzeug
-    werkzeug==3.0.1
-        # via flask
 
     ----- stderr -----
     Resolved 6 packages in [TIME]
+    Downloaded 6 packages in [TIME]
+    Installed 6 packages in [TIME]
+     + black==24.3.0
+     + click==8.1.7
+     + mypy-extensions==1.0.0
+     + packaging==24.0
+     + pathspec==0.12.1
+     + platformdirs==4.2.0
     "###
     );
 
-    Ok(())
-}
-
-/// Resolve a specific Flask tag via a Git HTTPS dependency.
-#[test]
-#[cfg(feature = "git")]
-fn compile_git_tag_https_dependency() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("flask @ git+https://github.com/pallets/flask.git@3.0.0")?;
-
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    // Install the editable version of Black. This should remove the registry-based version.
+    uv_snapshot!(context.filters(), context.install()
+        .arg("-e")
+        .arg(context.workspace_root.join("scripts/packages/black_editable")), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    blinker==1.7.0
-        # via flask
-    click==8.1.7
-        # via flask
-    flask @ git+https://github.com/pallets/flask.git@735a4701d6d5e848241e7d7535db898efb62d400
-    itsdangerous==2.1.2
-        # via flask
-    jinja2==3.1.2
-        # via flask
-    markupsafe==2.1.3
-        # via
-        #   jinja2
-        #   werkzeug
-    werkzeug==3.0.1
-        # via flask
 
     ----- stderr -----
-    Resolved 7 packages in [TIME]
+    Built 1 editable in [TIME]
+    Resolved 1 package in [TIME]
+    Uninstalled 1 package in [TIME]
+    Installed 1 package in [TIME]
+     - black==24.3.0
+     + black==0.1.0 (from file://[WORKSPACE]/scripts/packages/black_editable)
     "###
     );
 
-    Ok(())
-}
-
-/// Resolve a specific Flask commit via a Git HTTPS dependency.
-#[test]
-#[cfg(feature = "git")]
-fn compile_git_long_commit_https_dependency() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str(
-        "flask @ git+https://github.com/pallets/flask.git@d92b64aa275841b0c9aea3903aba72fbc4275d91",
-    )?;
-
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    // Re-install the registry-based version of Black. This should be a no-op, since we have a
+    // version of Black installed (the editable version) that satisfies the requirements.
+    uv_snapshot!(context.filters(), context.install()
+        .arg("black")
+        .arg("--strict"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    click==8.1.7
-        # via flask
-    flask @ git+https://github.com/pallets/flask.git@d92b64aa275841b0c9aea3903aba72fbc4275d91
-    itsdangerous==2.1.2
-        # via flask
-    jinja2==3.1.2
-        # via flask
-    markupsafe==2.1.3
-        # via
-        #   jinja2
-        #   werkzeug
-    werkzeug==3.0.1
-        # via flask
 
     ----- stderr -----
-    Resolved 6 packages in [TIME]
+    Audited 1 package in [TIME]
     "###
     );
 
-    Ok(())
-}
-
-/// Resolve a specific Flask commit via a Git HTTPS dependency.
-#[test]
-#[cfg(feature = "git")]
-fn compile_git_short_commit_https_dependency() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("flask @ git+https://github.com/pallets/flask.git@d92b64a")?;
+    let filters: Vec<_> = context
+        .filters()
+        .into_iter()
+        .chain([
+            // Remove colorama
+            ("Resolved 7 packages", "Resolved 6 packages"),
+        ])
+        .collect();
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    // Re-install Black at a specific version. This should replace the editable version.
+    uv_snapshot!(filters, context.install()
+        .arg("black==23.10.0"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    click==8.1.7
-        # via flask
-    flask @ git+https://github.com/pallets/flask.git@d92b64aa275841b0c9aea3903aba72fbc4275d91
-    itsdangerous==2.1.2
-        # via flask
-    jinja2==3.1.2
-        # via flask
-    markupsafe==2.1.3
-        # via
-        #   jinja2
-        #   werkzeug
-    werkzeug==3.0.1
-        # via flask
 
     ----- stderr -----
     Resolved 6 packages in [TIME]
+    Downloaded 1 package in [TIME]
+    Uninstalled 1 package in [TIME]
+    Installed 1 package in [TIME]
+     - black==0.1.0 (from file://[WORKSPACE]/scripts/packages/black_editable)
+     + black==23.10.0
     "###
     );
-
-    Ok(())
 }
 
-/// Resolve a specific Flask ref via a Git HTTPS dependency.
 #[test]
-#[cfg(feature = "git")]
-fn compile_git_refs_https_dependency() -> Result<()> {
+fn install_editable_no_binary() {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in
-        .write_str("flask @ git+https://github.com/pallets/flask.git@refs/pull/5313/head")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    // Install the editable package with no-binary enabled
+    uv_snapshot!(context.filters(), context.install()
+        .arg("-e")
+        .arg(context.workspace_root.join("scripts/packages/black_editable"))
+        .arg("--no-binary")
+        .arg(":all:"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    blinker==1.7.0
-        # via flask
-    click==8.1.7
-        # via flask
-    flask @ git+https://github.com/pallets/flask.git@7af0271f4703a71beef8e26d1f5f6f8da04100e6
-    itsdangerous==2.1.2
-        # via flask
-    jinja2==3.1.2
-        # via flask
-    markupsafe==2.1.3
-        # via
-        #   jinja2
-        #   werkzeug
-    werkzeug==3.0.1
-        # via flask
 
     ----- stderr -----
-    Resolved 7 packages in [TIME]
+    Built 1 editable in [TIME]
+    Resolved 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + black==0.1.0 (from file://[WORKSPACE]/scripts/packages/black_editable)
     "###
     );
-
-    Ok(())
 }
 
-/// Resolve a specific Git dependency with a subdirectory.
 #[test]
-#[cfg(feature = "git")]
-fn compile_git_subdirectory_dependency() -> Result<()> {
+fn install_editable_compatible_constraint() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("example-pkg-a @ git+https://github.com/pypa/sample-namespace-packages.git@df7530eeb8fa0cb7dbb8ecb28363e8e36bfa2f45#subdirectory=pkg_resources/pkg_a")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    let constraints_txt = context.temp_dir.child("constraints.txt");
+    constraints_txt.write_str("black==0.1.0")?;
+
+    // Install the editable package with a compatible constraint.
+    uv_snapshot!(context.filters(), context.install()
+        .arg("-e")
+        .arg(context.workspace_root.join("scripts/packages/black_editable"))
+        .arg("--constraint")
+        .arg("constraints.txt"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    example-pkg-a @ git+https://github.com/pypa/sample-namespace-packages.git@df7530eeb8fa0cb7dbb8ecb28363e8e36bfa2f45#subdirectory=pkg_resources/pkg_a
 
     ----- stderr -----
+    Built 1 editable in [TIME]
     Resolved 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + black==0.1.0 (from file://[WORKSPACE]/scripts/packages/black_editable)
     "###
     );
 
     Ok(())
 }
 
-/// Resolve two packages from a `requirements.in` file with the same Git HTTPS dependency.
 #[test]
-#[cfg(feature = "git")]
-fn compile_git_concurrent_access() -> Result<()> {
+fn install_editable_incompatible_constraint_version() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in
-        .write_str("example-pkg-a @ git+https://github.com/pypa/sample-namespace-packages.git@df7530eeb8fa0cb7dbb8ecb28363e8e36bfa2f45#subdirectory=pkg_resources/pkg_a\nexample-pkg-b @ git+https://github.com/pypa/sample-namespace-packages.git@df7530eeb8fa0cb7dbb8ecb28363e8e36bfa2f45#subdirectory=pkg_resources/pkg_b")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
-    success: true
-    exit_code: 0
+    let constraints_txt = context.temp_dir.child("constraints.txt");
+    constraints_txt.write_str("black>0.1.0")?;
+
+    // Install the editable package with an incompatible constraint.
+    uv_snapshot!(context.filters(), context.install()
+        .arg("-e")
+        .arg(context.workspace_root.join("scripts/packages/black_editable"))
+        .arg("--constraint")
+        .arg("constraints.txt"), @r###"
+    success: false
+    exit_code: 1
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    example-pkg-a @ git+https://github.com/pypa/sample-namespace-packages.git@df7530eeb8fa0cb7dbb8ecb28363e8e36bfa2f45#subdirectory=pkg_resources/pkg_a
-    example-pkg-b @ git+https://github.com/pypa/sample-namespace-packages.git@df7530eeb8fa0cb7dbb8ecb28363e8e36bfa2f45#subdirectory=pkg_resources/pkg_b
 
     ----- stderr -----
-    Resolved 2 packages in [TIME]
+    Built 1 editable in [TIME]
+      × No solution found when resolving dependencies:
+      ╰─▶ Because you require black==0.1.0 and black>0.1.0, we can conclude that the requirements are unsatisfiable.
     "###
     );
 
     Ok(())
 }
 
-/// Resolve a Git dependency with a declared name that differs from the true name of the package.
 #[test]
-#[cfg(feature = "git")]
-fn compile_git_mismatched_name() -> Result<()> {
+fn install_editable_incompatible_constraint_url() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in
-        .write_str("flask @ git+https://github.com/pallets/flask.git@2.0.0\ndask @ git+https://github.com/pallets/flask.git@3.0.0")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    let constraints_txt = context.temp_dir.child("constraints.txt");
+    constraints_txt.write_str("black @ https://files.pythonhosted.org/packages/0f/89/294c9a6b6c75a08da55e9d05321d0707e9418735e3062b12ef0f54c33474/black-24.4.2-py3-none-any.whl")?;
+
+    // Install the editable package with an incompatible constraint.
+    uv_snapshot!(context.filters(), context.install()
+        .arg("-e")
+        .arg(context.workspace_root.join("scripts/packages/black_editable"))
+        .arg("--constraint")
+        .arg("constraints.txt"), @r###"
     success: false
     exit_code: 2
     ----- stdout -----
 
     ----- stderr -----
-    error: Failed to download and build: dask @ git+https://github.com/pallets/flask.git@3.0.0
-      Caused by: Package metadata name `flask` does not match given name `dask`
+    Built 1 editable in [TIME]
+    error: Requirements contain conflicting URLs for package `black`:
+    - [WORKSPACE]/scripts/packages/black_editable
+    - https://files.pythonhosted.org/packages/0f/89/294c9a6b6c75a08da55e9d05321d0707e9418735e3062b12ef0f54c33474/black-24.4.2-py3-none-any.whl
     "###
     );
 
     Ok(())
 }
 
-/// Request Flask, but include a URL dependency for Werkzeug, which should avoid adding a
-/// duplicate dependency from `PyPI`.
+/// Install a source distribution that uses the `flit` build system, along with `flit`
+/// at the top-level, along with `--reinstall` to force a re-download after resolution, to ensure
+/// that the `flit` install and the source distribution build don't conflict.
 #[test]
-fn mixed_url_dependency() -> Result<()> {
+fn reinstall_build_system() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("flask==3.0.0\nwerkzeug @ https://files.pythonhosted.org/packages/c3/fc/254c3e9b5feb89ff5b9076a23218dafbc99c96ac5941e900b71206e6313b/werkzeug-3.0.1-py3-none-any.whl")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str(indoc! {r"
+        flit_core<4.0.0
+        flask @ https://files.pythonhosted.org/packages/d8/09/c1a7354d3925a3c6c8cfdebf4245bae67d633ffda1ba415add06ffc839c5/flask-3.0.0.tar.gz
+        "
+    })?;
+
+    uv_snapshot!(context.install()
+        .arg("--reinstall")
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--strict"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    blinker==1.7.0
-        # via flask
-    click==8.1.7
-        # via flask
-    flask==3.0.0
-    itsdangerous==2.1.2
-        # via flask
-    jinja2==3.1.2
-        # via flask
-    markupsafe==2.1.3
-        # via
-        #   jinja2
-        #   werkzeug
-    werkzeug @ https://files.pythonhosted.org/packages/c3/fc/254c3e9b5feb89ff5b9076a23218dafbc99c96ac5941e900b71206e6313b/werkzeug-3.0.1-py3-none-any.whl
-        # via flask
 
     ----- stderr -----
-    Resolved 7 packages in [TIME]
+    Resolved 8 packages in [TIME]
+    Downloaded 8 packages in [TIME]
+    Installed 8 packages in [TIME]
+     + blinker==1.7.0
+     + click==8.1.7
+     + flask==3.0.0 (from https://files.pythonhosted.org/packages/d8/09/c1a7354d3925a3c6c8cfdebf4245bae67d633ffda1ba415add06ffc839c5/flask-3.0.0.tar.gz)
+     + flit-core==3.9.0
+     + itsdangerous==2.1.2
+     + jinja2==3.1.3
+     + markupsafe==2.1.5
+     + werkzeug==3.0.1
     "###
     );
 
     Ok(())
 }
 
-/// Request Werkzeug via both a version and a URL dependency at a _different_ version, which
-/// should result in a conflict.
+/// Install a package without using the remote index
 #[test]
-fn conflicting_direct_url_dependency() -> Result<()> {
+fn install_no_index() {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("werkzeug==3.0.0\nwerkzeug @ https://files.pythonhosted.org/packages/ff/1d/960bb4017c68674a1cb099534840f18d3def3ce44aed12b5ed8b78e0153e/Werkzeug-2.0.0-py3-none-any.whl")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    uv_snapshot!(context.install()
+        .arg("Flask")
+        .arg("--no-index"), @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because there is no version of werkzeug==3.0.0 and you require
-          werkzeug==3.0.0, we can conclude that the requirements are
-          unsatisfiable.
+      ╰─▶ Because flask was not found in the provided package locations and you require flask, we can conclude that the requirements are unsatisfiable.
+
+          hint: Packages were unavailable because index lookups were disabled and no additional package locations were provided (try: `--find-links <uri>`)
     "###
     );
 
-    Ok(())
+    context.assert_command("import flask").failure();
 }
 
-/// Request Werkzeug via both a version and a URL dependency at _the same_ version, which
-/// should prefer the direct URL dependency.
+/// Install a package without using the remote index
+/// Covers a case where the user requests a version which should be included in the error
 #[test]
-fn compatible_direct_url_dependency() -> Result<()> {
+fn install_no_index_version() {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("werkzeug==2.0.0\nwerkzeug @ https://files.pythonhosted.org/packages/ff/1d/960bb4017c68674a1cb099534840f18d3def3ce44aed12b5ed8b78e0153e/Werkzeug-2.0.0-py3-none-any.whl")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
-    success: true
-    exit_code: 0
+    uv_snapshot!(context.install()
+        .arg("Flask==3.0.0")
+        .arg("--no-index"), @r###"
+    success: false
+    exit_code: 1
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    werkzeug @ https://files.pythonhosted.org/packages/ff/1d/960bb4017c68674a1cb099534840f18d3def3ce44aed12b5ed8b78e0153e/Werkzeug-2.0.0-py3-none-any.whl
 
     ----- stderr -----
-    Resolved 1 package in [TIME]
+      × No solution found when resolving dependencies:
+      ╰─▶ Because flask was not found in the provided package locations and you require flask==3.0.0, we can conclude that the requirements are unsatisfiable.
+
+          hint: Packages were unavailable because index lookups were disabled and no additional package locations were provided (try: `--find-links <uri>`)
     "###
     );
 
-    Ok(())
+    context.assert_command("import flask").failure();
 }
 
-/// Request Werkzeug via two different URLs at different versions, which should result in a conflict.
+/// Install a package via --extra-index-url.
+///
+/// This is a regression test where previously `uv` would consult test.pypi.org
+/// first, and if the package was found there, `uv` would not look at any other
+/// indexes. We fixed this by flipping the priority order of indexes so that
+/// test.pypi.org becomes the fallback (in this example) and the extra indexes
+/// (regular PyPI) are checked first.
+///
+/// (Neither approach matches `pip`'s behavior, which considers versions of
+/// each package from all indexes. `uv` stops at the first index it finds a
+/// package in.)
+///
+/// Ref: <https://github.com/astral-sh/uv/issues/1600>
 #[test]
-fn conflicting_repeated_url_dependency_version_mismatch() -> Result<()> {
+fn install_extra_index_url_has_priority() {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("werkzeug @ https://files.pythonhosted.org/packages/bd/24/11c3ea5a7e866bf2d97f0501d0b4b1c9bbeade102bb4b588f0d2919a5212/Werkzeug-2.0.1-py3-none-any.whl\nwerkzeug @ https://files.pythonhosted.org/packages/ff/1d/960bb4017c68674a1cb099534840f18d3def3ce44aed12b5ed8b78e0153e/Werkzeug-2.0.0-py3-none-any.whl")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
-    success: false
-    exit_code: 2
+    uv_snapshot!(context.install_without_exclude_newer()
+        .arg("--index-url")
+        .arg("https://test.pypi.org/simple")
+        .arg("--extra-index-url")
+        .arg("https://pypi.org/simple")
+        // This tests what we want because BOTH of the following
+        // are true: `black` is on pypi.org and test.pypi.org, AND
+        // `black==24.2.0` is on pypi.org and NOT test.pypi.org. So
+        // this would previously check for `black` on test.pypi.org,
+        // find it, but then not find a compatible version. After
+        // the fix, `uv` will check pypi.org first since it is given
+        // priority via --extra-index-url.
+        .arg("black==24.2.0")
+        .arg("--no-deps")
+        .arg("--exclude-newer")
+        .arg("2024-03-09"), @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    error: Requirements contain conflicting URLs for package `werkzeug`:
-    - https://files.pythonhosted.org/packages/bd/24/11c3ea5a7e866bf2d97f0501d0b4b1c9bbeade102bb4b588f0d2919a5212/Werkzeug-2.0.1-py3-none-any.whl
-    - https://files.pythonhosted.org/packages/ff/1d/960bb4017c68674a1cb099534840f18d3def3ce44aed12b5ed8b78e0153e/Werkzeug-2.0.0-py3-none-any.whl
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + black==24.2.0
     "###
     );
 
-    Ok(())
+    context.assert_command("import flask").failure();
 }
 
-/// Request Werkzeug via two different URLs at different versions. However, only one of the
-/// URLs is compatible with the requested Python version, so there shouldn't be any conflict.
+/// Install a package from a public GitHub repository
 #[test]
-fn conflicting_repeated_url_dependency_markers() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str(indoc! {r"
-        werkzeug @ https://files.pythonhosted.org/packages/bd/24/11c3ea5a7e866bf2d97f0501d0b4b1c9bbeade102bb4b588f0d2919a5212/Werkzeug-2.0.1-py3-none-any.whl ; python_version >= '3.10'
-        werkzeug @ https://files.pythonhosted.org/packages/ff/1d/960bb4017c68674a1cb099534840f18d3def3ce44aed12b5ed8b78e0153e/Werkzeug-2.0.0-py3-none-any.whl ; python_version < '3.10'
-    "})?;
+#[cfg(feature = "git")]
+fn install_git_public_https() {
+    let context = TestContext::new("3.8");
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    uv_snapshot!(
+        context
+        .install()
+        .arg("uv-public-pypackage @ git+https://github.com/astral-test/uv-public-pypackage"),
+        @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    werkzeug @ https://files.pythonhosted.org/packages/bd/24/11c3ea5a7e866bf2d97f0501d0b4b1c9bbeade102bb4b588f0d2919a5212/Werkzeug-2.0.1-py3-none-any.whl
 
     ----- stderr -----
     Resolved 1 package in [TIME]
-    "###
-    );
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + uv-public-pypackage==0.1.0 (from git+https://github.com/astral-test/uv-public-pypackage@b270df1a2fb5d012294e9aaf05e7e0bab1e6a389)
+    "###);
 
-    Ok(())
+    context.assert_installed("uv_public_pypackage", "0.1.0");
 }
 
-/// Request Werkzeug via two different URLs at the same version. Despite mapping to the same
-/// version, it should still result in a conflict.
+/// Install a package from a public GitHub repository at a ref that does not exist
 #[test]
 #[cfg(feature = "git")]
-fn conflicting_repeated_url_dependency_version_match() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("werkzeug @ git+https://github.com/pallets/werkzeug.git@2.0.0\nwerkzeug @ https://files.pythonhosted.org/packages/ff/1d/960bb4017c68674a1cb099534840f18d3def3ce44aed12b5ed8b78e0153e/Werkzeug-2.0.0-py3-none-any.whl")?;
+fn install_git_public_https_missing_branch_or_tag() {
+    let context = TestContext::new("3.8");
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    let mut filters = context.filters();
+    // Windows does not style the command the same as Unix, so we must omit it from the snapshot
+    filters.push(("`git fetch .*`", "`git fetch [...]`"));
+    filters.push(("exit status", "exit code"));
+
+    uv_snapshot!(filters, context.install()
+        // 2.0.0 does not exist
+        .arg("uv-public-pypackage @ git+https://github.com/astral-test/uv-public-pypackage@2.0.0"), @r###"
     success: false
     exit_code: 2
     ----- stdout -----
 
     ----- stderr -----
-    error: Requirements contain conflicting URLs for package `werkzeug`:
-    - git+https://github.com/pallets/werkzeug.git@2.0.0
-    - https://files.pythonhosted.org/packages/ff/1d/960bb4017c68674a1cb099534840f18d3def3ce44aed12b5ed8b78e0153e/Werkzeug-2.0.0-py3-none-any.whl
-    "###
-    );
+    error: Failed to download and build: `uv-public-pypackage @ git+https://github.com/astral-test/uv-public-pypackage@2.0.0`
+      Caused by: Git operation failed
+      Caused by: failed to clone into: [CACHE_DIR]/git-v0/db/8dab139913c4b566
+      Caused by: failed to fetch branch or tag `2.0.0`
+      Caused by: process didn't exit successfully: `git fetch [...]` (exit code: 128)
+    --- stderr
+    fatal: couldn't find remote ref refs/tags/2.0.0
 
-    Ok(())
+    "###);
 }
 
-/// Request Flask, but include a URL dependency for a conflicting version of Werkzeug.
+/// Install a package from a public GitHub repository at a ref that does not exist
 #[test]
-fn conflicting_transitive_url_dependency() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("flask==3.0.0\nwerkzeug @ https://files.pythonhosted.org/packages/ff/1d/960bb4017c68674a1cb099534840f18d3def3ce44aed12b5ed8b78e0153e/Werkzeug-2.0.0-py3-none-any.whl")?;
+#[cfg(feature = "git")]
+fn install_git_public_https_missing_commit() {
+    let context = TestContext::new("3.8");
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    let mut filters = context.filters();
+    // Windows does not style the command the same as Unix, so we must omit it from the snapshot
+    filters.push(("`git fetch .*`", "`git fetch [...]`"));
+    filters.push(("exit status", "exit code"));
+
+    // There are flakes on Windows where this irrelevant error is appended
+    filters.push((
+        "fatal: unable to write response end packet: Broken pipe\n",
+        "",
+    ));
+
+    uv_snapshot!(filters, context.install()
+        // 2.0.0 does not exist
+        .arg("uv-public-pypackage @ git+https://github.com/astral-test/uv-public-pypackage@79a935a7a1a0ad6d0bdf72dce0e16cb0a24a1b3b")
+        , @r###"
     success: false
-    exit_code: 1
+    exit_code: 2
     ----- stdout -----
 
     ----- stderr -----
-      × No solution found when resolving dependencies:
-      ╰─▶ Because flask==3.0.0 depends on werkzeug>=3.0.0 and only werkzeug<3.0.0
-          is available, we can conclude that flask==3.0.0 cannot be used.
-          And because you require flask==3.0.0, we can conclude that the
-          requirements are unsatisfiable.
-    "###
-    );
+    error: Failed to download and build: `uv-public-pypackage @ git+https://github.com/astral-test/uv-public-pypackage@79a935a7a1a0ad6d0bdf72dce0e16cb0a24a1b3b`
+      Caused by: Git operation failed
+      Caused by: failed to clone into: [CACHE_DIR]/git-v0/db/8dab139913c4b566
+      Caused by: failed to fetch commit `79a935a7a1a0ad6d0bdf72dce0e16cb0a24a1b3b`
+      Caused by: process didn't exit successfully: `git fetch [...]` (exit code: 128)
+    --- stderr
+    fatal: remote error: upload-pack: not our ref 79a935a7a1a0ad6d0bdf72dce0e16cb0a24a1b3b
 
-    Ok(())
+    "###);
 }
 
-/// Request Werkzeug via two different URLs which resolve to the same canonical version.
+/// Install a package from a private GitHub repository using a PAT
 #[test]
-fn compatible_repeated_url_dependency() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("werkzeug @ git+https://github.com/pallets/werkzeug.git@2.0.0\nwerkzeug @ git+https://github.com/pallets/werkzeug@2.0.0")?;
+#[cfg(all(not(windows), feature = "git"))]
+fn install_git_private_https_pat() {
+    let context = TestContext::new("3.8");
+    let token = decode_token(READ_ONLY_GITHUB_TOKEN);
+
+    let filters: Vec<_> = [(token.as_str(), "***")]
+        .into_iter()
+        .chain(context.filters())
+        .collect();
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    let package = format!(
+        "uv-private-pypackage@ git+https://{token}@github.com/astral-test/uv-private-pypackage"
+    );
+
+    uv_snapshot!(filters, context.install().arg(package)
+        , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    werkzeug @ git+https://github.com/pallets/werkzeug@af160e0b6b7ddd81c22f1652c728ff5ac72d5c74
 
     ----- stderr -----
     Resolved 1 package in [TIME]
-    "###
-    );
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + uv-private-pypackage==0.1.0 (from git+https://***@github.com/astral-test/uv-private-pypackage@d780faf0ac91257d4d5a4f0c5a0e4509608c0071)
+    "###);
 
-    Ok(())
+    context.assert_installed("uv_private_pypackage", "0.1.0");
 }
 
-/// Request `transitive_url_dependency`, which depends on `git+https://github.com/pallets/werkzeug@2.0.0`.
-/// Since this URL isn't declared upfront, we should reject it.
+/// Install a package from a private GitHub repository using a PAT
+/// Include a public GitHub repository too, to ensure that the authentication is not erroneously copied over.
 #[test]
-#[cfg(feature = "git")]
-fn disallowed_transitive_url_dependency() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("transitive_url_dependency @ https://github.com/astral-sh/ruff/files/14078476/transitive_url_dependency.zip")?;
+#[cfg(all(not(windows), feature = "git"))]
+fn install_git_private_https_pat_mixed_with_public() {
+    let context = TestContext::new("3.8");
+    let token = decode_token(READ_ONLY_GITHUB_TOKEN);
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
-    success: false
-    exit_code: 2
+    let filters: Vec<_> = [(token.as_str(), "***")]
+        .into_iter()
+        .chain(context.filters())
+        .collect();
+
+    let package = format!(
+        "uv-private-pypackage @ git+https://{token}@github.com/astral-test/uv-private-pypackage"
+    );
+
+    uv_snapshot!(filters, context.install().arg(package).arg("uv-public-pypackage @ git+https://github.com/astral-test/uv-public-pypackage"),
+    @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    error: Package `werkzeug` attempted to resolve via URL: git+https://github.com/pallets/werkzeug@2.0.0. URL dependencies must be expressed as direct requirements or constraints. Consider adding `werkzeug @ git+https://github.com/pallets/werkzeug@2.0.0` to your dependencies or constraints file.
-    "###
-    );
+    Resolved 2 packages in [TIME]
+    Downloaded 2 packages in [TIME]
+    Installed 2 packages in [TIME]
+     + uv-private-pypackage==0.1.0 (from git+https://***@github.com/astral-test/uv-private-pypackage@d780faf0ac91257d4d5a4f0c5a0e4509608c0071)
+     + uv-public-pypackage==0.1.0 (from git+https://github.com/astral-test/uv-public-pypackage@b270df1a2fb5d012294e9aaf05e7e0bab1e6a389)
+    "###);
 
-    Ok(())
+    context.assert_installed("uv_private_pypackage", "0.1.0");
 }
 
-/// Request `transitive_url_dependency`, which depends on `git+https://github.com/pallets/werkzeug@2.0.0`.
-/// Since this URL is declared as a constraint, we should accept it.
+/// Install packages from multiple private GitHub repositories with separate PATS
 #[test]
-#[cfg(feature = "git")]
-fn allowed_transitive_url_dependency() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("transitive_url_dependency @ https://github.com/astral-sh/ruff/files/14078476/transitive_url_dependency.zip")?;
+#[cfg(all(not(windows), feature = "git"))]
+fn install_git_private_https_multiple_pat() {
+    let context = TestContext::new("3.8");
+    let token_1 = decode_token(READ_ONLY_GITHUB_TOKEN);
+    let token_2 = decode_token(READ_ONLY_GITHUB_TOKEN_2);
 
-    let constraints_txt = context.temp_dir.child("constraints.txt");
-    constraints_txt.write_str("werkzeug @ git+https://github.com/pallets/werkzeug@2.0.0")?;
+    let filters: Vec<_> = [(token_1.as_str(), "***_1"), (token_2.as_str(), "***_2")]
+        .into_iter()
+        .chain(context.filters())
+        .collect();
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--constraint")
-            .arg("constraints.txt"), @r###"
+    let package_1 = format!(
+        "uv-private-pypackage @ git+https://{token_1}@github.com/astral-test/uv-private-pypackage"
+    );
+    let package_2 = format!(
+        "uv-private-pypackage-2 @ git+https://{token_2}@github.com/astral-test/uv-private-pypackage-2"
+    );
+
+    uv_snapshot!(filters, context.install().arg(package_1).arg(package_2)
+        , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --constraint constraints.txt
-    transitive-url-dependency @ https://github.com/astral-sh/ruff/files/14078476/transitive_url_dependency.zip
-    werkzeug @ git+https://github.com/pallets/werkzeug@af160e0b6b7ddd81c22f1652c728ff5ac72d5c74
-        # via transitive-url-dependency
 
     ----- stderr -----
     Resolved 2 packages in [TIME]
-    "###
-    );
+    Downloaded 2 packages in [TIME]
+    Installed 2 packages in [TIME]
+     + uv-private-pypackage==0.1.0 (from git+https://***_1@github.com/astral-test/uv-private-pypackage@d780faf0ac91257d4d5a4f0c5a0e4509608c0071)
+     + uv-private-pypackage-2==0.1.0 (from git+https://***_2@github.com/astral-test/uv-private-pypackage-2@45c0bec7365710f09b1f4dbca61c86dde9537e4e)
+    "###);
 
-    Ok(())
+    context.assert_installed("uv_private_pypackage", "0.1.0");
 }
 
-/// Request `transitive_url_dependency`, which depends on `git+https://github.com/pallets/werkzeug@2.0.0`.
-/// Since this `git+https://github.com/pallets/werkzeug@2.0.0.git` is declared as a constraint, and
-/// those map to the same canonical URL, we should accept it.
+/// Install a package from a private GitHub repository at a specific commit using a PAT
 #[test]
 #[cfg(feature = "git")]
-fn allowed_transitive_canonical_url_dependency() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("transitive_url_dependency @ https://github.com/astral-sh/ruff/files/14078476/transitive_url_dependency.zip")?;
+fn install_git_private_https_pat_at_ref() {
+    let context = TestContext::new("3.8");
+    let token = decode_token(READ_ONLY_GITHUB_TOKEN);
 
-    let constraints_txt = context.temp_dir.child("constraints.txt");
-    constraints_txt.write_str("werkzeug @ git+https://github.com/pallets/werkzeug.git@2.0.0")?;
+    let mut filters: Vec<_> = [(token.as_str(), "***")]
+        .into_iter()
+        .chain(context.filters())
+        .collect();
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--constraint")
-            .arg("constraints.txt"), @r###"
+    filters.push((r"git\+https://", ""));
+
+    // A user is _required_ on Windows
+    let user = if cfg!(windows) {
+        filters.push((r"git:", ""));
+        "git:"
+    } else {
+        ""
+    };
+
+    let package = format!("uv-private-pypackage @ git+https://{user}{token}@github.com/astral-test/uv-private-pypackage@6c09ce9ae81f50670a60abd7d95f30dd416d00ac");
+    uv_snapshot!(filters, context.install()
+        .arg(package), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --constraint constraints.txt
-    transitive-url-dependency @ https://github.com/astral-sh/ruff/files/14078476/transitive_url_dependency.zip
-    werkzeug @ git+https://github.com/pallets/werkzeug.git@af160e0b6b7ddd81c22f1652c728ff5ac72d5c74
-        # via transitive-url-dependency
 
     ----- stderr -----
-    Resolved 2 packages in [TIME]
-    "###
-    );
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + uv-private-pypackage==0.1.0 (from ***@github.com/astral-test/uv-private-pypackage@6c09ce9ae81f50670a60abd7d95f30dd416d00ac)
+    "###);
 
-    Ok(())
+    context.assert_installed("uv_private_pypackage", "0.1.0");
 }
 
-/// Resolve packages from all optional dependency groups in a `pyproject.toml` file.
+/// Install a package from a private GitHub repository using a PAT and username
+/// An arbitrary username is supported when using a PAT.
+///
+/// TODO(charlie): This test modifies the user's keyring.
+/// See: <https://github.com/astral-sh/uv/issues/1980>.
 #[test]
-fn compile_pyproject_toml_all_extras() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let pyproject_toml = context.temp_dir.child("pyproject.toml");
-    pyproject_toml.write_str(
-        r#"[build-system]
-requires = ["setuptools", "wheel"]
+#[cfg(feature = "git")]
+#[ignore]
+fn install_git_private_https_pat_and_username() {
+    let context = TestContext::new("3.8");
+    let token = decode_token(READ_ONLY_GITHUB_TOKEN);
+    let user = "astral-test-bot";
 
-[project]
-name = "project"
-dependencies = ["django==5.0b1"]
-optional-dependencies.foo = [
-    "anyio==4.0.0",
-]
-optional-dependencies.bar = [
-    "httpcore==0.18.0",
-]
-"#,
-    )?;
+    let filters: Vec<_> = [(token.as_str(), "***")]
+        .into_iter()
+        .chain(context.filters())
+        .collect();
 
-    uv_snapshot!(context.compile()
-            .arg("pyproject.toml")
-            .arg("--all-extras"), @r###"
+    uv_snapshot!(filters, context.install().arg(format!("uv-private-pypackage @ git+https://{user}:{token}@github.com/astral-test/uv-private-pypackage"))
+        , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z pyproject.toml --all-extras
-    anyio==4.0.0
-        # via httpcore
-    asgiref==3.7.2
-        # via django
-    certifi==2023.11.17
-        # via httpcore
-    django==5.0b1
-    h11==0.14.0
-        # via httpcore
-    httpcore==0.18.0
-    idna==3.4
-        # via anyio
-    sniffio==1.3.0
-        # via
-        #   anyio
-        #   httpcore
-    sqlparse==0.4.4
-        # via django
 
     ----- stderr -----
-    Resolved 9 packages in [TIME]
-    "###
-    );
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + uv-private-pypackage==0.1.0 (from git+https://astral-test-bot:***@github.com/astral-test/uv-private-pypackage@6c09ce9ae81f50670a60abd7d95f30dd416d00ac)
+    "###);
 
-    Ok(())
+    context.assert_installed("uv_private_pypackage", "0.1.0");
 }
 
+/// Install a package from a private GitHub repository using a PAT
 #[test]
-fn compile_pyproject_toml_all_extras_annotation_line() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let pyproject_toml = context.temp_dir.child("pyproject.toml");
-    pyproject_toml.write_str(
-        r#"[build-system]
-requires = ["setuptools", "wheel"]
+#[cfg(all(not(windows), feature = "git"))]
+fn install_git_private_https_pat_not_authorized() {
+    let context = TestContext::new("3.8");
 
-[project]
-name = "project"
-dependencies = ["django==5.0b1"]
-optional-dependencies.foo = [
-    "anyio==4.0.0",
-]
-optional-dependencies.bar = [
-    "httpcore==0.18.0",
-]
-"#,
-    )?;
+    // A revoked token
+    let token = "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";
 
-    uv_snapshot!(context.compile()
-            .arg("--annotation-style=line")
-            .arg("pyproject.toml")
-            .arg("--all-extras"), @r###"
+    let mut filters = context.filters();
+    filters.insert(0, (token, "***"));
+
+    // We provide a username otherwise (since the token is invalid), the git cli will prompt for a password
+    // and hang the test
+    uv_snapshot!(filters, context.install()
+        .arg(format!("uv-private-pypackage @ git+https://git:{token}@github.com/astral-test/uv-private-pypackage"))
+        , @r###"
+    success: false
+    exit_code: 2
+    ----- stdout -----
+
+    ----- stderr -----
+    error: Failed to download and build: `uv-private-pypackage @ git+https://git:***@github.com/astral-test/uv-private-pypackage`
+      Caused by: Git operation failed
+      Caused by: failed to clone into: [CACHE_DIR]/git-v0/db/8401f5508e3e612d
+      Caused by: process didn't exit successfully: `git fetch --force --update-head-ok 'https://git:***@github.com/astral-test/uv-private-pypackage' '+HEAD:refs/remotes/origin/HEAD'` (exit status: 128)
+    --- stderr
+    remote: Support for password authentication was removed on August 13, 2021.
+    remote: Please see https://docs.github.com/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.
+    fatal: Authentication failed for 'https://github.com/astral-test/uv-private-pypackage/'
+
+    "###);
+}
+
+/// Install a package from a private GitHub repository using a PAT
+/// Does not use `git`, instead installs a distribution artifact.
+/// Include a public GitHub repository too, to ensure that the authentication is not erroneously copied over.
+#[test]
+#[cfg(not(windows))]
+fn install_github_artifact_private_https_pat_mixed_with_public() {
+    let context = TestContext::new("3.8");
+    let token = decode_token(READ_ONLY_GITHUB_TOKEN);
+
+    let filters: Vec<_> = [(token.as_str(), "***")]
+        .into_iter()
+        .chain(context.filters())
+        .collect();
+
+    let private_package = format!(
+        "uv-private-pypackage @ https://{token}@raw.githubusercontent.com/astral-test/uv-private-pypackage/main/dist/uv_private_pypackage-0.1.0-py3-none-any.whl"
+    );
+    let public_package = "uv-public-pypackage @ https://raw.githubusercontent.com/astral-test/uv-public-pypackage/main/dist/uv_public_pypackage-0.1.0-py3-none-any.whl";
+
+    uv_snapshot!(filters, context.install().arg(private_package).arg(public_package),
+    @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z --annotation-style=line pyproject.toml --all-extras
-    anyio==4.0.0              # via httpcore
-    asgiref==3.7.2            # via django
-    certifi==2023.11.17       # via httpcore
-    django==5.0b1
-    h11==0.14.0               # via httpcore
-    httpcore==0.18.0
-    idna==3.4                 # via anyio
-    sniffio==1.3.0            # via anyio, httpcore
-    sqlparse==0.4.4           # via django
 
     ----- stderr -----
-    Resolved 9 packages in [TIME]
-    "###
+    Resolved 2 packages in [TIME]
+    Downloaded 2 packages in [TIME]
+    Installed 2 packages in [TIME]
+     + uv-private-pypackage==0.1.0 (from https://***@raw.githubusercontent.com/astral-test/uv-private-pypackage/main/dist/uv_private_pypackage-0.1.0-py3-none-any.whl)
+     + uv-public-pypackage==0.1.0 (from https://raw.githubusercontent.com/astral-test/uv-public-pypackage/main/dist/uv_public_pypackage-0.1.0-py3-none-any.whl)
+    "###);
+
+    context.assert_installed("uv_private_pypackage", "0.1.0");
+}
+
+/// Install packages from multiple private GitHub repositories with separate PATS
+/// Does not use `git`, instead installs a distribution artifact.
+#[test]
+#[cfg(not(windows))]
+fn install_github_artifact_private_https_multiple_pat() {
+    let context = TestContext::new("3.8");
+    let token_1 = decode_token(READ_ONLY_GITHUB_TOKEN);
+    let token_2 = decode_token(READ_ONLY_GITHUB_TOKEN_2);
+
+    let filters: Vec<_> = [(token_1.as_str(), "***_1"), (token_2.as_str(), "***_2")]
+        .into_iter()
+        .chain(context.filters())
+        .collect();
+
+    let package_1 = format!(
+        "uv-private-pypackage @ https://astral-test-bot:{token_1}@raw.githubusercontent.com/astral-test/uv-private-pypackage/main/dist/uv_private_pypackage-0.1.0-py3-none-any.whl"
+    );
+    let package_2 = format!(
+        "uv-private-pypackage-2 @ https://astral-test-bot:{token_2}@raw.githubusercontent.com/astral-test/uv-private-pypackage-2/main/dist/uv_private_pypackage_2-0.1.0-py3-none-any.whl"
     );
 
-    Ok(())
+    uv_snapshot!(filters, context.install().arg(package_1).arg(package_2)
+        , @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 2 packages in [TIME]
+    Downloaded 2 packages in [TIME]
+    Installed 2 packages in [TIME]
+     + uv-private-pypackage==0.1.0 (from https://astral-test-bot:***_1@raw.githubusercontent.com/astral-test/uv-private-pypackage/main/dist/uv_private_pypackage-0.1.0-py3-none-any.whl)
+     + uv-private-pypackage-2==0.1.0 (from https://astral-test-bot:***_2@raw.githubusercontent.com/astral-test/uv-private-pypackage-2/main/dist/uv_private_pypackage_2-0.1.0-py3-none-any.whl)
+    "###);
+
+    context.assert_installed("uv_private_pypackage", "0.1.0");
 }
 
-/// Resolve packages from all optional dependency groups in a `pyproject.toml` file.
+/// Install a package without using pre-built wheels.
 #[test]
-fn compile_does_not_allow_both_extra_and_all_extras() -> Result<()> {
+fn reinstall_no_binary() {
     let context = TestContext::new("3.12");
-    let pyproject_toml = context.temp_dir.child("pyproject.toml");
-    pyproject_toml.write_str(
-        r#"[build-system]
-requires = ["setuptools", "wheel"]
 
-[project]
-name = "project"
-dependencies = ["django==5.0b1"]
-optional-dependencies.foo = [
-    "anyio==4.0.0",
-]
-optional-dependencies.bar = [
-    "httpcore==0.18.0",
-]
-"#,
-    )?;
+    // The first installation should use a pre-built wheel
+    let mut command = context.install();
+    command.arg("anyio").arg("--strict");
+    uv_snapshot!(
+        command,
+        @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
 
-    uv_snapshot!(context.compile()
-            .arg("pyproject.toml")
-            .arg("--all-extras")
-            .arg("--extra")
-            .arg("foo"),
-            @r###"
-    success: false
-    exit_code: 2
+    ----- stderr -----
+    Resolved 3 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 3 packages in [TIME]
+     + anyio==4.3.0
+     + idna==3.6
+     + sniffio==1.3.1
+    "###
+    );
+
+    context.assert_command("import anyio").success();
+
+    // Running installation again with `--no-binary` should be a no-op
+    // The first installation should use a pre-built wheel
+    let mut command = context.install();
+    command
+        .arg("anyio")
+        .arg("--no-binary")
+        .arg(":all:")
+        .arg("--strict");
+    uv_snapshot!(command, @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    error: the argument '--all-extras' cannot be used with '--extra <EXTRA>'
+    Audited 1 package in [TIME]
+    "###
+    );
 
-    Usage: uv pip compile --cache-dir [CACHE_DIR] --all-extras <SRC_FILE>...
+    context.assert_command("import anyio").success();
 
-    For more information, try '--help'.
+    // With `--reinstall`, `--no-binary` should have an affect
+    let filters = if cfg!(windows) {
+        // Remove the colorama count on windows
+        context
+            .filters()
+            .into_iter()
+            .chain([("Resolved 8 packages", "Resolved 7 packages")])
+            .collect()
+    } else {
+        context.filters()
+    };
+
+    let mut command = context.install();
+    command
+        .arg("anyio")
+        .arg("--no-binary")
+        .arg(":all:")
+        .arg("--reinstall-package")
+        .arg("anyio")
+        .arg("--strict");
+    uv_snapshot!(filters, command, @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 3 packages in [TIME]
+    Uninstalled 1 package in [TIME]
+    Installed 1 package in [TIME]
+     - anyio==4.3.0
+     + anyio==4.3.0
     "###
     );
 
-    Ok(())
+    context.assert_command("import anyio").success();
 }
 
-/// Compile requirements that cannot be solved due to conflict in a `pyproject.toml` fil;e.
+/// Respect `--only-binary` flags in `requirements.txt`
 #[test]
-fn compile_unsolvable_requirements() -> Result<()> {
+fn only_binary_requirements_txt() {
     let context = TestContext::new("3.12");
-    let pyproject_toml = context.temp_dir.child("pyproject.toml");
-    pyproject_toml.write_str(
-        r#"[build-system]
-requires = ["setuptools", "wheel"]
 
-[project]
-name = "my-project"
-dependencies = ["django==5.0b1", "django==5.0a1"]
-"#,
-    )?;
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt
+        .write_str(indoc! {r"
+        django_allauth==0.51.0
+        --only-binary django_allauth
+        "
+        })
+        .unwrap();
 
-    uv_snapshot!(context.compile()
-            .arg("pyproject.toml"), @r###"
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--strict"), @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because my-project depends on django==5.0b1 and my-project depends on
-          django==5.0a1, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because django-allauth==0.51.0 has no usable wheels and building from source is disabled and you require django-allauth==0.51.0, we can conclude that the requirements are unsatisfiable.
     "###
     );
-
-    Ok(())
 }
 
-/// Compile requirements in a `pyproject.toml` file that cannot be resolved due to
-/// a requirement with a version that is not available online.
+/// `--only-binary` does not apply to editable requirements
 #[test]
-fn compile_unsolvable_requirements_version_not_available() -> Result<()> {
+fn only_binary_editable() {
     let context = TestContext::new("3.12");
-    let pyproject_toml = context.temp_dir.child("pyproject.toml");
-    pyproject_toml.write_str(
-        r#"[build-system]
-requires = ["setuptools", "wheel"]
-
-[project]
-name = "my-project"
-dependencies = ["django==300.1.4"]
-"#,
-    )?;
 
-    uv_snapshot!(context.compile()
-            .arg("pyproject.toml"), @r###"
-    success: false
-    exit_code: 1
+    // Install the editable package.
+    uv_snapshot!(context.filters(), context.install()
+        .arg("--only-binary")
+        .arg(":all:")
+        .arg("-e")
+        .arg(context.workspace_root.join("scripts/packages/anyio_local")), @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-      × No solution found when resolving dependencies:
-      ╰─▶ Because there is no version of django==300.1.4 and my-project
-          depends on django==300.1.4, we can conclude that the requirements are
-          unsatisfiable.
+    Built 1 editable in [TIME]
+    Resolved 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + anyio==4.3.0+foo (from file://[WORKSPACE]/scripts/packages/anyio_local)
     "###
     );
-
-    Ok(())
 }
 
-/// Resolve at a specific time in the past
+/// `--only-binary` does not apply to editable requirements that depend on each other
 #[test]
-fn compile_exclude_newer() -> Result<()> {
+fn only_binary_dependent_editables() {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("tqdm")?;
+    let root_path = context
+        .workspace_root
+        .join("scripts/packages/dependent_editables");
 
-    uv_snapshot!(Command::new(get_bin())
-            .arg("pip")
-            .arg("compile")
-            .arg("requirements.in")
-            .arg("--exclude-newer")
-            // 4.64.0: 2022-04-04T01:48:46.194635Z1
-            // 4.64.1: 2022-09-03T11:10:27.148080Z
-            .arg("2022-04-04T12:00:00Z")
-            .arg("--cache-dir")
-            .arg(context.cache_dir.path())
-            .env("VIRTUAL_ENV", context.venv.as_os_str())
-            .current_dir(context.temp_dir.path()), @r###"
+    // Install the editable package.
+    uv_snapshot!(context.filters(), context.install()
+        .arg("--only-binary")
+        .arg(":all:")
+        .arg("-e")
+        .arg(root_path.join("first_editable"))
+        .arg("-e")
+        .arg(root_path.join("second_editable")), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile requirements.in --exclude-newer 2022-04-04T12:00:00Z --cache-dir [CACHE_DIR]
-    tqdm==4.64.0
 
     ----- stderr -----
-    Resolved 1 package in [TIME]
+    Built 2 editables in [TIME]
+    Resolved 2 packages in [TIME]
+    Installed 2 packages in [TIME]
+     + first-editable==0.0.1 (from file://[WORKSPACE]/scripts/packages/dependent_editables/first_editable)
+     + second-editable==0.0.1 (from file://[WORKSPACE]/scripts/packages/dependent_editables/second_editable)
     "###
     );
+}
 
-    // Use a date as input instead.
-    // We interpret a date as including this day
-    uv_snapshot!(Command::new(get_bin())
-            .arg("pip")
-            .arg("compile")
-            .arg("requirements.in")
-            .arg("--exclude-newer")
-            .arg("2022-04-04")
-            .arg("--cache-dir")
-            .arg(context.cache_dir.path())
-            .env("VIRTUAL_ENV", context.venv.as_os_str())
-            .current_dir(context.temp_dir.path()), @r###"
+/// `--only-binary` does not apply to editable requirements, with a `setup.py` config
+#[test]
+fn only_binary_editable_setup_py() {
+    let context = TestContext::new("3.12");
+
+    // Install the editable package.
+    uv_snapshot!(context.filters(), context.install()
+        .arg("--only-binary")
+        .arg(":all:")
+        .arg("-e")
+        .arg(context.workspace_root.join("scripts/packages/setup_py_editable")), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile requirements.in --exclude-newer 2022-04-04 --cache-dir [CACHE_DIR]
-    tqdm==4.64.0
 
     ----- stderr -----
-    Resolved 1 package in [TIME]
+    Built 1 editable in [TIME]
+    Resolved 8 packages in [TIME]
+    Downloaded 7 packages in [TIME]
+    Installed 8 packages in [TIME]
+     + anyio==4.3.0
+     + certifi==2024.2.2
+     + h11==0.14.0
+     + httpcore==1.0.4
+     + httpx==0.27.0
+     + idna==3.6
+     + setup-py-editable==0.0.1 (from file://[WORKSPACE]/scripts/packages/setup_py_editable)
+     + sniffio==1.3.1
     "###
     );
+}
 
-    // Check the error message for invalid datetime
-    uv_snapshot!(Command::new(get_bin())
-            .arg("pip")
-            .arg("compile")
-            .arg("requirements.in")
-            .arg("--exclude-newer")
-            .arg("2022-04-04+02:00")
-            .arg("--cache-dir")
-            .arg(context.cache_dir.path())
-            .env("VIRTUAL_ENV", context.venv.as_os_str())
-            .current_dir(context.temp_dir.path()), @r###"
-    success: false
-    exit_code: 2
+/// Install a package into a virtual environment, and ensuring that the executable permissions
+/// are retained.
+///
+/// This test uses the default link semantics. (On macOS, this is `clone`.)
+#[test]
+fn install_executable() {
+    let context = TestContext::new("3.12");
+
+    uv_snapshot!(context.install()
+        .arg("pylint==3.0.0"), @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    error: invalid value '2022-04-04+02:00' for '--exclude-newer <EXCLUDE_NEWER>': Neither a valid date (trailing input) not a valid datetime (input contains invalid characters)
-
-    For more information, try '--help'.
+    Resolved 7 packages in [TIME]
+    Downloaded 7 packages in [TIME]
+    Installed 7 packages in [TIME]
+     + astroid==3.0.3
+     + dill==0.3.8
+     + isort==5.13.2
+     + mccabe==0.7.0
+     + platformdirs==4.2.0
+     + pylint==3.0.0
+     + tomlkit==0.12.4
     "###
     );
 
-    Ok(())
+    // Verify that `pylint` is executable.
+    let executable = context
+        .venv
+        .join(if cfg!(windows) { "Scripts" } else { "bin" })
+        .join(format!("pylint{}", std::env::consts::EXE_SUFFIX));
+    Command::new(executable).arg("--version").assert().success();
 }
 
-/// Resolve a local path dependency on a specific wheel.
+/// Install a package into a virtual environment using copy semantics, and ensure that the
+/// executable permissions are retained.
 #[test]
-fn compile_wheel_path_dependency() -> Result<()> {
+fn install_executable_copy() {
     let context = TestContext::new("3.12");
 
-    // Download a wheel.
-    let response = reqwest::blocking::get("https://files.pythonhosted.org/packages/36/42/015c23096649b908c809c69388a805a571a3bea44362fe87e33fc3afa01f/flask-3.0.0-py3-none-any.whl")?;
-    let flask_wheel = context.temp_dir.child("flask-3.0.0-py3-none-any.whl");
-    let mut flask_wheel_file = fs::File::create(&flask_wheel)?;
-    std::io::copy(&mut response.bytes()?.as_ref(), &mut flask_wheel_file)?;
+    uv_snapshot!(context.install()
+        .arg("pylint==3.0.0")
+        .arg("--link-mode")
+        .arg("copy"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
 
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str(&format!(
-        "flask @ {}",
-        Url::from_file_path(flask_wheel.path()).unwrap()
-    ))?;
+    ----- stderr -----
+    Resolved 7 packages in [TIME]
+    Downloaded 7 packages in [TIME]
+    Installed 7 packages in [TIME]
+     + astroid==3.0.3
+     + dill==0.3.8
+     + isort==5.13.2
+     + mccabe==0.7.0
+     + platformdirs==4.2.0
+     + pylint==3.0.0
+     + tomlkit==0.12.4
+    "###
+    );
 
-    // In addition to the standard filters, remove the temporary directory from the snapshot.
-    let filters: Vec<_> = [(r"file://.*/", "file://[TEMP_DIR]/")]
-        .into_iter()
-        .chain(INSTA_FILTERS.to_vec())
-        .collect();
+    // Verify that `pylint` is executable.
+    let executable = context
+        .venv
+        .join(if cfg!(windows) { "Scripts" } else { "bin" })
+        .join(format!("pylint{}", std::env::consts::EXE_SUFFIX));
+    Command::new(executable).arg("--version").assert().success();
+}
+
+/// Install a package into a virtual environment using hardlink semantics, and ensure that the
+/// executable permissions are retained.
+#[test]
+fn install_executable_hardlink() {
+    let context = TestContext::new("3.12");
 
-    uv_snapshot!(filters, context.compile()
-            .arg("requirements.in"), @r###"
+    uv_snapshot!(context.install()
+        .arg("pylint==3.0.0")
+        .arg("--link-mode")
+        .arg("hardlink"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    blinker==1.7.0
-        # via flask
-    click==8.1.7
-        # via flask
-    flask @ file://[TEMP_DIR]/flask-3.0.0-py3-none-any.whl
-    itsdangerous==2.1.2
-        # via flask
-    jinja2==3.1.2
-        # via flask
-    markupsafe==2.1.3
-        # via
-        #   jinja2
-        #   werkzeug
-    werkzeug==3.0.1
-        # via flask
 
     ----- stderr -----
     Resolved 7 packages in [TIME]
-    "###);
+    Downloaded 7 packages in [TIME]
+    Installed 7 packages in [TIME]
+     + astroid==3.0.3
+     + dill==0.3.8
+     + isort==5.13.2
+     + mccabe==0.7.0
+     + platformdirs==4.2.0
+     + pylint==3.0.0
+     + tomlkit==0.12.4
+    "###
+    );
 
-    // Run the same operation, but this time with a relative path, omitting the `//`.
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("flask @ file:flask-3.0.0-py3-none-any.whl")?;
+    // Verify that `pylint` is executable.
+    let executable = context
+        .venv
+        .join(if cfg!(windows) { "Scripts" } else { "bin" })
+        .join(format!("pylint{}", std::env::consts::EXE_SUFFIX));
+    Command::new(executable).arg("--version").assert().success();
+}
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+/// Install a package from the command line into a virtual environment, ignoring its dependencies.
+#[test]
+fn no_deps() {
+    let context = TestContext::new("3.12");
+
+    // Install Flask.
+    uv_snapshot!(context.install()
+        .arg("Flask")
+        .arg("--no-deps")
+        .arg("--strict"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    blinker==1.7.0
-        # via flask
-    click==8.1.7
-        # via flask
-    flask @ file:flask-3.0.0-py3-none-any.whl
-    itsdangerous==2.1.2
-        # via flask
-    jinja2==3.1.2
-        # via flask
-    markupsafe==2.1.3
-        # via
-        #   jinja2
-        #   werkzeug
-    werkzeug==3.0.1
-        # via flask
 
     ----- stderr -----
-    Resolved 7 packages in [TIME]
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + flask==3.0.2
+    warning: The package `flask` requires `werkzeug>=3.0.0`, but it's not installed.
+    warning: The package `flask` requires `jinja2>=3.1.2`, but it's not installed.
+    warning: The package `flask` requires `itsdangerous>=2.1.2`, but it's not installed.
+    warning: The package `flask` requires `click>=8.1.3`, but it's not installed.
+    warning: The package `flask` requires `blinker>=1.6.2`, but it's not installed.
     "###
     );
 
-    // Run the same operation, but this time with a relative path, including the `//`.
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("flask @ file://flask-3.0.0-py3-none-any.whl")?;
+    context.assert_command("import flask").failure();
+}
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+/// Install an editable package from the command line into a virtual environment, ignoring its
+/// dependencies.
+#[test]
+fn no_deps_editable() {
+    let context = TestContext::new("3.12");
+
+    // Install the editable version of Black. This should remove the registry-based version.
+    uv_snapshot!(context.filters(), context.install()
+        .arg("--no-deps")
+        .arg("-e")
+        .arg(context.workspace_root.join("scripts/packages/black_editable[dev]")), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    blinker==1.7.0
-        # via flask
-    click==8.1.7
-        # via flask
-    flask @ file://flask-3.0.0-py3-none-any.whl
-    itsdangerous==2.1.2
-        # via flask
-    jinja2==3.1.2
-        # via flask
-    markupsafe==2.1.3
-        # via
-        #   jinja2
-        #   werkzeug
-    werkzeug==3.0.1
-        # via flask
 
     ----- stderr -----
-    Resolved 7 packages in [TIME]
+    Built 1 editable in [TIME]
+    Resolved 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + black==0.1.0 (from file://[WORKSPACE]/scripts/packages/black_editable)
     "###
     );
 
-    // Run the same operation, but this time with a relative path, exclusive of any scheme.
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("flask @ ./flask-3.0.0-py3-none-any.whl")?;
+    context.assert_command("import black").success();
+    context.assert_command("import aiohttp").failure();
+}
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+/// Upgrade a package.
+#[test]
+fn install_upgrade() {
+    let context = TestContext::new("3.12");
+
+    // Install an old version of anyio and httpcore.
+    uv_snapshot!(context.install()
+        .arg("anyio==3.6.2")
+        .arg("httpcore==0.16.3")
+        .arg("--strict"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    blinker==1.7.0
-        # via flask
-    click==8.1.7
-        # via flask
-    flask @ ./flask-3.0.0-py3-none-any.whl
-    itsdangerous==2.1.2
-        # via flask
-    jinja2==3.1.2
-        # via flask
-    markupsafe==2.1.3
-        # via
-        #   jinja2
-        #   werkzeug
-    werkzeug==3.0.1
-        # via flask
 
     ----- stderr -----
-    Resolved 7 packages in [TIME]
+    Resolved 6 packages in [TIME]
+    Downloaded 6 packages in [TIME]
+    Installed 6 packages in [TIME]
+     + anyio==3.6.2
+     + certifi==2024.2.2
+     + h11==0.14.0
+     + httpcore==0.16.3
+     + idna==3.6
+     + sniffio==1.3.1
     "###
     );
 
-    // Run the same operation, but this time with an absolute path (rather than a URL).
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str(&format!("flask @ {}", flask_wheel.path().display()))?;
+    context.assert_command("import anyio").success();
+
+    // Upgrade anyio.
+    uv_snapshot!(context.install()
+        .arg("anyio")
+        .arg("--upgrade-package")
+        .arg("anyio"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
 
-    // In addition to the standard filters, remove the temporary directory from the snapshot.
-    let filter_path = regex::escape(&flask_wheel.normalized_display().to_string());
-    let filters: Vec<_> = [(filter_path.as_str(), "/[TEMP_DIR]/")]
-        .into_iter()
-        .chain(INSTA_FILTERS.to_vec())
-        .collect();
+    ----- stderr -----
+    Resolved 3 packages in [TIME]
+    Downloaded 1 package in [TIME]
+    Uninstalled 1 package in [TIME]
+    Installed 1 package in [TIME]
+     - anyio==3.6.2
+     + anyio==4.3.0
+    "###
+    );
 
-    uv_snapshot!(filters, context.compile()
-            .arg("requirements.in"), @r###"
+    // Upgrade anyio again, should not reinstall.
+    uv_snapshot!(context.install()
+        .arg("anyio")
+        .arg("--upgrade-package")
+        .arg("anyio"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    blinker==1.7.0
-        # via flask
-    click==8.1.7
-        # via flask
-    flask @ /[TEMP_DIR]/
-    itsdangerous==2.1.2
-        # via flask
-    jinja2==3.1.2
-        # via flask
-    markupsafe==2.1.3
-        # via
-        #   jinja2
-        #   werkzeug
-    werkzeug==3.0.1
-        # via flask
 
     ----- stderr -----
-    Resolved 7 packages in [TIME]
+    Resolved 3 packages in [TIME]
+    Audited 3 packages in [TIME]
     "###
     );
 
-    Ok(())
+    // Install httpcore, request anyio upgrade should not reinstall
+    uv_snapshot!(context.install()
+        .arg("httpcore")
+        .arg("--upgrade-package")
+        .arg("anyio"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 6 packages in [TIME]
+    Audited 6 packages in [TIME]
+    "###
+    );
+
+    // Upgrade httpcore with global flag
+    uv_snapshot!(context.install()
+        .arg("httpcore")
+        .arg("--upgrade"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 3 packages in [TIME]
+    Downloaded 1 package in [TIME]
+    Uninstalled 1 package in [TIME]
+    Installed 1 package in [TIME]
+     - httpcore==0.16.3
+     + httpcore==1.0.4
+    "###
+    );
 }
 
-/// Resolve a local path dependency on a specific source distribution.
+/// Install a package from a `requirements.txt` file, with a `constraints.txt` file.
 #[test]
-fn compile_source_distribution_path_dependency() -> Result<()> {
+fn install_constraints_txt() -> Result<()> {
     let context = TestContext::new("3.12");
-    // Download a source distribution.
-    let response = reqwest::blocking::get("https://files.pythonhosted.org/packages/d8/09/c1a7354d3925a3c6c8cfdebf4245bae67d633ffda1ba415add06ffc839c5/flask-3.0.0.tar.gz")?;
-    let flask_wheel = context.temp_dir.child("flask-3.0.0.tar.gz");
-    let mut flask_wheel_file = std::fs::File::create(&flask_wheel)?;
-    std::io::copy(&mut response.bytes()?.as_ref(), &mut flask_wheel_file)?;
-
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str(&format!(
-        "flask @ {}",
-        Url::from_file_path(flask_wheel.path()).unwrap()
-    ))?;
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str("anyio==3.7.0")?;
 
-    // In addition to the standard filters, remove the temporary directory from the snapshot.
-    let filters: Vec<_> = [(r"file://.*/", "file://[TEMP_DIR]/")]
-        .into_iter()
-        .chain(INSTA_FILTERS.to_vec())
-        .collect();
+    let constraints_txt = context.temp_dir.child("constraints.txt");
+    constraints_txt.write_str("idna<3.4")?;
 
-    uv_snapshot!(filters, context.compile()
-            .arg("requirements.in"), @r###"
+    uv_snapshot!(context.install()
+            .arg("-r")
+            .arg("requirements.txt")
+            .arg("--constraint")
+            .arg("constraints.txt"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    blinker==1.7.0
-        # via flask
-    click==8.1.7
-        # via flask
-    flask @ file://[TEMP_DIR]/flask-3.0.0.tar.gz
-    itsdangerous==2.1.2
-        # via flask
-    jinja2==3.1.2
-        # via flask
-    markupsafe==2.1.3
-        # via
-        #   jinja2
-        #   werkzeug
-    werkzeug==3.0.1
-        # via flask
 
     ----- stderr -----
-    Resolved 7 packages in [TIME]
-    "###);
+    Resolved 3 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 3 packages in [TIME]
+     + anyio==3.7.0
+     + idna==3.3
+     + sniffio==1.3.1
+    "###
+    );
 
     Ok(())
 }
 
-/// Resolve a local path dependency to a non-existent file.
+/// Install a package from a `requirements.txt` file, with an inline constraint.
 #[test]
-fn compile_wheel_path_dependency_missing() -> Result<()> {
+fn install_constraints_inline() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("flask @ file:///path/to/flask-3.0.0-py3-none-any.whl")?;
+    let requirementstxt = context.temp_dir.child("requirements.txt");
+    requirementstxt.write_str("anyio==3.7.0\n-c constraints.txt")?;
 
-    // In addition to the standard filters, remove the temporary directory from the snapshot.
-    let filters: Vec<_> = [(r"file://.*/", "file://[TEMP_DIR]/")]
-        .into_iter()
-        .chain(INSTA_FILTERS.to_vec())
-        .collect();
+    let constraints_txt = context.temp_dir.child("constraints.txt");
+    constraints_txt.write_str("idna<3.4")?;
 
-    uv_snapshot!(filters, context.compile()
-            .arg("requirements.in"), @r###"
-    success: false
-    exit_code: 2
+    uv_snapshot!(context.install()
+            .arg("-r")
+            .arg("requirements.txt"), @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    error: Distribution not found at: file://[TEMP_DIR]/flask-3.0.0-py3-none-any.whl
-    "###);
+    Resolved 3 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 3 packages in [TIME]
+     + anyio==3.7.0
+     + idna==3.3
+     + sniffio==1.3.1
+    "###
+    );
 
     Ok(())
 }
 
-/// Resolve a yanked version of `attrs` by specifying the version directly.
+/// Install a package from a `constraints.txt` file on a remote http server.
 #[test]
-fn compile_yanked_version_direct() -> Result<()> {
+fn install_constraints_remote() {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("attrs==21.1.0")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    uv_snapshot!(context.install()
+            .arg("-c")
+            .arg("https://raw.githubusercontent.com/apache/airflow/constraints-2-6/constraints-3.11.txt")
+            .arg("typing_extensions>=4.0"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    attrs==21.1.0
 
     ----- stderr -----
     Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + typing-extensions==4.7.1
     "###
+    ); // would yield typing-extensions==4.8.2 without constraint file
+}
+
+/// Install a package from a `requirements.txt` file, with an inline constraint, which points
+/// to a remote http server.
+#[test]
+fn install_constraints_inline_remote() -> Result<()> {
+    let context = TestContext::new("3.12");
+    let requirementstxt = context.temp_dir.child("requirements.txt");
+    requirementstxt.write_str("typing-extensions>=4.0\n-c https://raw.githubusercontent.com/apache/airflow/constraints-2-6/constraints-3.11.txt")?;
+
+    uv_snapshot!(context.install()
+            .arg("-r")
+            .arg("requirements.txt"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + typing-extensions==4.7.1
+    "### // would yield typing-extensions==4.8.2 without constraint file
     );
 
     Ok(())
 }
 
-/// Fail to resolve `attrs` due to the indirect use of a yanked version (`21.1.0`).
 #[test]
-fn compile_yanked_version_indirect() -> Result<()> {
+fn install_constraints_respects_offline_mode() {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("attrs>20.3.0,<21.2.0")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    uv_snapshot!(context.install()
+            .arg("--offline")
+            .arg("-r")
+            .arg("http://example.com/requirements.txt"), @r###"
     success: false
-    exit_code: 1
+    exit_code: 2
     ----- stdout -----
 
     ----- stderr -----
-      × No solution found when resolving dependencies:
-      ╰─▶ Because only the following versions of attrs are available:
-              attrs<=20.3.0
-              attrs==21.1.0
-              attrs>=21.2.0
-          and attrs==21.1.0 is unusable because it was yanked (reason:
-          Installable but not importable on Python 3.4), we can conclude that
-          attrs>20.3.0,<21.2.0 cannot be used.
-          And because you require attrs>20.3.0,<21.2.0, we can conclude that the
-          requirements are unsatisfiable.
+    error: Network connectivity is disabled, but a remote requirements file was requested: http://example.com/requirements.txt
     "###
     );
-
-    Ok(())
 }
 
-/// Flask==3.0.0 depends on Werkzeug>=3.0.0. Demonstrate that we can override this
-/// requirement with an incompatible version.
+/// Tests that we can install `polars==0.14.0`, which has this odd dependency
+/// requirement in its wheel metadata: `pyarrow>=4.0.*; extra == 'pyarrow'`.
+///
+/// The `>=4.0.*` is invalid, but is something we "fix" because it is out
+/// of the control of the end user. However, our fix for this case ends up
+/// stripping the quotes around `pyarrow` and thus produces an irrevocably
+/// invalid dependency requirement.
+///
+/// See: <https://github.com/astral-sh/uv/issues/1477>
 #[test]
-fn override_dependency() -> Result<()> {
+fn install_pinned_polars_invalid_metadata() {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("flask==3.0.0")?;
-
-    let overrides_txt = context.temp_dir.child("overrides.txt");
-    overrides_txt.write_str("werkzeug==2.3.0")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--override")
-            .arg("overrides.txt"), @r###"
+    // Install Flask.
+    uv_snapshot!(context.install()
+        .arg("polars==0.14.0"),
+        @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --override overrides.txt
-    blinker==1.7.0
-        # via flask
-    click==8.1.7
-        # via flask
-    flask==3.0.0
-    itsdangerous==2.1.2
-        # via flask
-    jinja2==3.1.2
-        # via flask
-    markupsafe==2.1.3
-        # via
-        #   jinja2
-        #   werkzeug
-    werkzeug==2.3.0
-        # via flask
 
     ----- stderr -----
-    Resolved 7 packages in [TIME]
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + polars==0.14.0
     "###
     );
 
-    Ok(())
+    context.assert_command("import polars").success();
 }
 
-/// Black==23.10.1 depends on tomli>=1.1.0 for Python versions below 3.11. Demonstrate that we can
-/// override it with a multi-line override.
+/// Install a source distribution with `--resolution=lowest-direct`, to ensure that the build
+/// requirements aren't resolved at their lowest compatible version.
 #[test]
-fn override_multi_dependency() -> Result<()> {
+fn install_sdist_resolution_lowest() -> Result<()> {
     let context = TestContext::new("3.12");
     let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("black==23.10.1")?;
-
-    let overrides_txt = context.temp_dir.child("overrides.txt");
-    overrides_txt.write_str(
-        "tomli>=1.1.0; python_version >= '3.11'\ntomli<1.0.0; python_version < '3.11'",
-    )?;
+    requirements_in.write_str("anyio @ https://files.pythonhosted.org/packages/2d/b8/7333d87d5f03247215d86a86362fd3e324111788c6cdd8d2e6196a6ba833/anyio-4.2.0.tar.gz")?;
 
-    uv_snapshot!(context.compile()
+    uv_snapshot!(context.install()
+            .arg("-r")
             .arg("requirements.in")
-            .arg("--override")
-            .arg("overrides.txt"), @r###"
+            .arg("--resolution=lowest-direct"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --override overrides.txt
-    black==23.10.1
-    click==8.1.7
-        # via black
-    mypy-extensions==1.0.0
-        # via black
-    packaging==23.2
-        # via black
-    pathspec==0.11.2
-        # via black
-    platformdirs==4.0.0
-        # via black
-    tomli==2.0.1
-        # via black
 
     ----- stderr -----
-    Resolved 7 packages in [TIME]
+    Resolved 3 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 3 packages in [TIME]
+     + anyio==4.2.0 (from https://files.pythonhosted.org/packages/2d/b8/7333d87d5f03247215d86a86362fd3e324111788c6cdd8d2e6196a6ba833/anyio-4.2.0.tar.gz)
+     + idna==3.6
+     + sniffio==1.3.1
     "###
     );
 
     Ok(())
 }
 
-/// Request an extra that doesn't exist on the specified package.
+/// Tests that we can install a package from a zip file that has bunk
+/// permissions.
+///
+/// See: <https://github.com/astral-sh/uv/issues/1453>
 #[test]
-fn missing_registry_extra() -> Result<()> {
+fn direct_url_zip_file_bunk_permissions() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("black[tensorboard]==23.10.1")?;
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str(
+        "opensafely-pipeline @ https://github.com/opensafely-core/pipeline/archive/refs/tags/v2023.11.06.145820.zip",
+    )?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--strict"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    black==23.10.1
-    click==8.1.7
-        # via black
-    mypy-extensions==1.0.0
-        # via black
-    packaging==23.2
-        # via black
-    pathspec==0.11.2
-        # via black
-    platformdirs==4.0.0
-        # via black
 
     ----- stderr -----
     Resolved 6 packages in [TIME]
-    warning: The package `black==23.10.1` does not have an extra named `tensorboard`.
+    Downloaded 5 packages in [TIME]
+    Installed 6 packages in [TIME]
+     + distro==1.9.0
+     + opensafely-pipeline==2023.11.6.145820 (from https://github.com/opensafely-core/pipeline/archive/refs/tags/v2023.11.06.145820.zip)
+     + pydantic==1.10.14
+     + ruyaml==0.91.0
+     + setuptools==69.2.0
+     + typing-extensions==4.10.0
     "###
     );
 
     Ok(())
 }
 
-/// Request an extra that doesn't exist on the specified package.
 #[test]
-fn missing_url_extra() -> Result<()> {
+fn launcher() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("flask[tensorboard] @ https://files.pythonhosted.org/packages/36/42/015c23096649b908c809c69388a805a571a3bea44362fe87e33fc3afa01f/flask-3.0.0-py3-none-any.whl")?;
+    let project_root = fs_err::canonicalize(std::env::current_dir()?.join("../.."))?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    let filters = [
+        (r"(\d+m )?(\d+\.)?\d+(ms|s)", "[TIME]"),
+        (
+            r"simple-launcher==0\.1\.0 \(from .+\.whl\)",
+            "simple_launcher.whl",
+        ),
+    ];
+
+    uv_snapshot!(
+        filters,
+        context.install()
+        .arg(format!("simple_launcher@{}", project_root.join("scripts/links/simple_launcher-0.1.0-py3-none-any.whl").display()))
+        .arg("--strict"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    blinker==1.7.0
-        # via flask
-    click==8.1.7
-        # via flask
-    flask @ https://files.pythonhosted.org/packages/36/42/015c23096649b908c809c69388a805a571a3bea44362fe87e33fc3afa01f/flask-3.0.0-py3-none-any.whl
-    itsdangerous==2.1.2
-        # via flask
-    jinja2==3.1.2
-        # via flask
-    markupsafe==2.1.3
-        # via
-        #   jinja2
-        #   werkzeug
-    werkzeug==3.0.1
-        # via flask
 
     ----- stderr -----
-    Resolved 7 packages in [TIME]
-    warning: The package `flask @ https://files.pythonhosted.org/packages/36/42/015c23096649b908c809c69388a805a571a3bea44362fe87e33fc3afa01f/flask-3.0.0-py3-none-any.whl` does not have an extra named `tensorboard`.
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + simple_launcher.whl
     "###
     );
 
+    let bin_path = if cfg!(windows) { "Scripts" } else { "bin" };
+
+    uv_snapshot!(Command::new(
+        context.venv.join(bin_path).join("simple_launcher")
+    ), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+    Hi from the simple launcher!
+
+    ----- stderr -----
+    "###);
+
     Ok(())
 }
 
-/// Resolve a dependency from a URL, preserving the exact casing of the URL as specified in the
-/// requirements file.
 #[test]
-fn preserve_url() -> Result<()> {
+fn launcher_with_symlink() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("flask @ https://files.PYTHONHOSTED.org/packages/36/42/015c23096649b908c809c69388a805a571a3bea44362fe87e33fc3afa01f/flask-3.0.0-py3-none-any.whl")?;
+    let project_root = fs_err::canonicalize(std::env::current_dir()?.join("../.."))?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    let filters = [
+        (r"(\d+m )?(\d+\.)?\d+(ms|s)", "[TIME]"),
+        (
+            r"simple-launcher==0\.1\.0 \(from .+\.whl\)",
+            "simple_launcher.whl",
+        ),
+    ];
+
+    uv_snapshot!(filters,
+        context.install()
+            .arg(format!("simple_launcher@{}", project_root.join("scripts/links/simple_launcher-0.1.0-py3-none-any.whl").display()))
+            .arg("--strict"),
+        @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    blinker==1.7.0
-        # via flask
-    click==8.1.7
-        # via flask
-    flask @ https://files.PYTHONHOSTED.org/packages/36/42/015c23096649b908c809c69388a805a571a3bea44362fe87e33fc3afa01f/flask-3.0.0-py3-none-any.whl
-    itsdangerous==2.1.2
-        # via flask
-    jinja2==3.1.2
-        # via flask
-    markupsafe==2.1.3
-        # via
-        #   jinja2
-        #   werkzeug
-    werkzeug==3.0.1
-        # via flask
 
     ----- stderr -----
-    Resolved 7 packages in [TIME]
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + simple_launcher.whl
     "###
     );
 
+    #[cfg(windows)]
+    if let Err(error) = std::os::windows::fs::symlink_file(
+        context.venv.join("Scripts\\simple_launcher.exe"),
+        context.temp_dir.join("simple_launcher.exe"),
+    ) {
+        // Os { code: 1314, kind: Uncategorized, message: "A required privilege is not held by the client." }
+        // where `Uncategorized` is unstable.
+        if error.raw_os_error() == Some(1314) {
+            return Ok(());
+        }
+        return Err(error.into());
+    }
+
+    #[cfg(unix)]
+    std::os::unix::fs::symlink(
+        context.venv.join("bin/simple_launcher"),
+        context.temp_dir.join("simple_launcher"),
+    )?;
+
+    // Only support windows or linux
+    #[cfg(not(any(windows, unix)))]
+    return Ok(());
+
+    uv_snapshot!(Command::new(
+        context.temp_dir.join("simple_launcher")
+    ), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+    Hi from the simple launcher!
+
+    ----- stderr -----
+    "###);
+
     Ok(())
 }
 
-/// Resolve a dependency from a URL, preserving the unexpanded environment variable as specified in
-/// the requirements file.
 #[test]
-fn preserve_env_var() -> Result<()> {
+fn config_settings() {
     let context = TestContext::new("3.12");
-    // Download a wheel.
-    let response = reqwest::blocking::get("https://files.pythonhosted.org/packages/36/42/015c23096649b908c809c69388a805a571a3bea44362fe87e33fc3afa01f/flask-3.0.0-py3-none-any.whl")?;
-    let flask_wheel = context.temp_dir.child("flask-3.0.0-py3-none-any.whl");
-    let mut flask_wheel_file = std::fs::File::create(flask_wheel)?;
-    std::io::copy(&mut response.bytes()?.as_ref(), &mut flask_wheel_file)?;
 
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("flask @ file://${PROJECT_ROOT}/flask-3.0.0-py3-none-any.whl")?;
-
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    // Install the editable package.
+    uv_snapshot!(context.filters(), context.install()
+        .arg("-e")
+        .arg(context.workspace_root.join("scripts/packages/setuptools_editable")), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    blinker==1.7.0
-        # via flask
-    click==8.1.7
-        # via flask
-    flask @ file://${PROJECT_ROOT}/flask-3.0.0-py3-none-any.whl
-    itsdangerous==2.1.2
-        # via flask
-    jinja2==3.1.2
-        # via flask
-    markupsafe==2.1.3
-        # via
-        #   jinja2
-        #   werkzeug
-    werkzeug==3.0.1
-        # via flask
 
     ----- stderr -----
-    Resolved 7 packages in [TIME]
+    Built 1 editable in [TIME]
+    Resolved 2 packages in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 2 packages in [TIME]
+     + iniconfig==2.0.0
+     + setuptools-editable==0.1.0 (from file://[WORKSPACE]/scripts/packages/setuptools_editable)
     "###
     );
 
-    Ok(())
-}
+    // When installed without `--editable_mode=compat`, the `finder.py` file should be present.
+    let finder = context
+        .site_packages()
+        .join("__editable___setuptools_editable_0_1_0_finder.py");
+    assert!(finder.exists());
 
-#[test]
-#[cfg(feature = "maturin")]
-fn compile_editable() -> Result<()> {
+    // Install the editable package with `--editable_mode=compat`.
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str(indoc! {r"
-        -e ../../scripts/editable-installs/poetry_editable
-        -e ${PROJECT_ROOT}/../../scripts/editable-installs/maturin_editable
-        -e file://../../scripts/editable-installs/black_editable[dev]
-        boltons # normal dependency for comparison
-        "
-    })?;
 
-    let filter_path = regex::escape(&requirements_in.normalized_display().to_string());
-    let filters: Vec<_> = [(filter_path.as_str(), "requirements.in")]
-        .into_iter()
-        .chain(INSTA_FILTERS.to_vec())
-        .collect();
+    uv_snapshot!(context.filters(), context.install()
+        .arg("-e")
+        .arg(context.workspace_root.join("scripts/packages/setuptools_editable"))
+        .arg("-C")
+        .arg("editable_mode=compat")
+        , @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
 
-    uv_snapshot!(filters, Command::new(get_bin())
-            .arg("pip")
-            .arg("compile")
-            .arg(requirements_in.path())
-            .arg("--cache-dir")
-            .arg(context.cache_dir.path())
-            .arg("--exclude-newer")
-            .arg(EXCLUDE_NEWER)
-            .env("VIRTUAL_ENV", context.venv.as_os_str()), @r###"
+    ----- stderr -----
+    Built 1 editable in [TIME]
+    Resolved 2 packages in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 2 packages in [TIME]
+     + iniconfig==2.0.0
+     + setuptools-editable==0.1.0 (from file://[WORKSPACE]/scripts/packages/setuptools_editable)
+    "###
+    );
+
+    // When installed without `--editable_mode=compat`, the `finder.py` file should _not_ be present.
+    let finder = context
+        .site_packages()
+        .join("__editable___setuptools_editable_0_1_0_finder.py");
+    assert!(!finder.exists());
+}
+
+/// Reinstall a duplicate package in a virtual environment.
+#[test]
+fn reinstall_duplicate() -> Result<()> {
+    use crate::common::copy_dir_all;
+
+    // Sync a version of `pip` into a virtual environment.
+    let context1 = TestContext::new("3.12");
+    let requirements_txt = context1.temp_dir.child("requirements.txt");
+    requirements_txt.write_str("pip==21.3.1")?;
+
+    // Run `pip sync`.
+    context1
+        .install()
+        .arg("-r")
+        .arg(requirements_txt.path())
+        .assert()
+        .success();
+
+    // Sync a different version of `pip` into a virtual environment.
+    let context2 = TestContext::new("3.12");
+    let requirements_txt = context2.temp_dir.child("requirements.txt");
+    requirements_txt.write_str("pip==22.1.1")?;
+
+    // Run `pip sync`.
+    context2
+        .install()
+        .arg("-r")
+        .arg(requirements_txt.path())
+        .assert()
+        .success();
+
+    // Copy the virtual environment to a new location.
+    copy_dir_all(
+        context2.site_packages().join("pip-22.1.1.dist-info"),
+        context1.site_packages().join("pip-22.1.1.dist-info"),
+    )?;
+
+    // Run `pip install`.
+    uv_snapshot!(context1.install()
+        .arg("pip")
+        .arg("--reinstall"),
+        @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile requirements.in --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z
-    -e ${PROJECT_ROOT}/../../scripts/editable-installs/maturin_editable
-    -e ../../scripts/editable-installs/poetry_editable
-    -e file://../../scripts/editable-installs/black_editable
-    aiohttp==3.9.0
-        # via black
-    aiosignal==1.3.1
-        # via aiohttp
-    attrs==23.1.0
-        # via aiohttp
-    boltons==23.1.1
-    frozenlist==1.4.0
-        # via
-        #   aiohttp
-        #   aiosignal
-    idna==3.4
-        # via yarl
-    multidict==6.0.4
-        # via
-        #   aiohttp
-        #   yarl
-    numpy==1.26.2
-        # via poetry-editable
-    uvloop==0.19.0
-        # via black
-    yarl==1.9.2
-        # via aiohttp
 
     ----- stderr -----
-    Built 3 editables in [TIME]
-    Resolved 13 packages in [TIME]
-    "###);
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Uninstalled 2 packages in [TIME]
+    Installed 1 package in [TIME]
+     - pip==21.3.1
+     - pip==22.1.1
+     + pip==24.0
+    "###
+    );
 
     Ok(())
 }
 
+/// Install a package that contains a symlink within the archive.
 #[test]
-fn recursive_extras_direct_url() -> Result<()> {
+fn install_symlink() {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("black[dev] @ ../../scripts/editable-installs/black_editable")?;
 
-    let filter_path = regex::escape(&requirements_in.normalized_display().to_string());
-    let filters: Vec<_> = [(filter_path.as_str(), "requirements.in")]
-        .into_iter()
-        .chain(INSTA_FILTERS.to_vec())
-        .collect();
-
-    uv_snapshot!(filters, Command::new(get_bin())
-            .arg("pip")
-            .arg("compile")
-            .arg(requirements_in.path())
-            .arg("--cache-dir")
-            .arg(context.cache_dir.path())
-            .arg("--exclude-newer")
-            .arg(EXCLUDE_NEWER)
-            .env("VIRTUAL_ENV", context.venv.as_os_str()), @r###"
+    uv_snapshot!(context.install()
+        .arg("pgpdump==1.5")
+        .arg("--strict"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile requirements.in --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z
-    aiohttp==3.9.0
-        # via black
-    aiosignal==1.3.1
-        # via aiohttp
-    attrs==23.1.0
-        # via aiohttp
-    black @ ../../scripts/editable-installs/black_editable
-    frozenlist==1.4.0
-        # via
-        #   aiohttp
-        #   aiosignal
-    idna==3.4
-        # via yarl
-    multidict==6.0.4
-        # via
-        #   aiohttp
-        #   yarl
-    uvloop==0.19.0
-        # via black
-    yarl==1.9.2
-        # via aiohttp
 
     ----- stderr -----
-    Resolved 9 packages in [TIME]
-    "###);
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + pgpdump==1.5
+    "###
+    );
 
-    Ok(())
+    context.assert_command("import pgpdump").success();
+
+    uv_snapshot!(uninstall_command(&context)
+        .arg("pgpdump"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Uninstalled 1 package in [TIME]
+     - pgpdump==1.5
+    "###
+    );
 }
 
-/// Compile an editable package with a direct URL requirement.
 #[test]
-fn compile_editable_url_requirement() -> Result<()> {
+fn invalidate_editable_on_change() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("-e ../../scripts/editable-installs/hatchling_editable")?;
 
-    let filter_path = regex::escape(&requirements_in.normalized_display().to_string());
-    let filters: Vec<_> = [(filter_path.as_str(), "requirements.in")]
-        .into_iter()
-        .chain(INSTA_FILTERS.to_vec())
-        .collect();
+    // Create an editable package.
+    let editable_dir = context.temp_dir.child("editable");
+    editable_dir.create_dir_all()?;
+    let pyproject_toml = editable_dir.child("pyproject.toml");
+    pyproject_toml.write_str(
+        r#"[project]
+name = "example"
+version = "0.0.0"
+dependencies = [
+  "anyio==4.0.0"
+]
+requires-python = ">=3.8"
+"#,
+    )?;
 
-    uv_snapshot!(filters, Command::new(get_bin())
-            .arg("pip")
-            .arg("compile")
-            .arg(requirements_in.path())
-            .arg("--cache-dir")
-            .arg(context.cache_dir.path())
-            .arg("--exclude-newer")
-            .arg(EXCLUDE_NEWER)
-            .env("VIRTUAL_ENV", context.venv.as_os_str()), @r###"
+    uv_snapshot!(context.filters(), context.install()
+        .arg("--editable")
+        .arg(editable_dir.path()), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile requirements.in --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z
-    -e ../../scripts/editable-installs/hatchling_editable
-    iniconfig @ https://files.pythonhosted.org/packages/ef/a6/62565a6e1cf69e10f5727360368e451d4b7f58beeac6173dc9db836a5b46/iniconfig-2.0.0-py3-none-any.whl
-        # via hatchling-editable
 
     ----- stderr -----
     Built 1 editable in [TIME]
-    Resolved 2 packages in [TIME]
-    "###);
+    Resolved 4 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 4 packages in [TIME]
+     + anyio==4.0.0
+     + example==0.0.0 (from file://[TEMP_DIR]/editable)
+     + idna==3.6
+     + sniffio==1.3.1
+    "###
+    );
+
+    // Re-installing should be a no-op.
+    uv_snapshot!(context.filters(), context.install()
+        .arg("--editable")
+        .arg(editable_dir.path()), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Audited 1 package in [TIME]
+    "###
+    );
+
+    // Modify the editable package.
+    pyproject_toml.write_str(
+        r#"[project]
+name = "example"
+version = "0.0.0"
+dependencies = [
+  "anyio==3.7.1"
+]
+requires-python = ">=3.8"
+"#,
+    )?;
+
+    // Re-installing should update the package.
+    uv_snapshot!(context.filters(), context.install()
+        .arg("--editable")
+        .arg(editable_dir.path()), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Built 1 editable in [TIME]
+    Resolved 4 packages in [TIME]
+    Downloaded 1 package in [TIME]
+    Uninstalled 2 packages in [TIME]
+    Installed 2 packages in [TIME]
+     - anyio==4.0.0
+     + anyio==3.7.1
+     - example==0.0.0 (from file://[TEMP_DIR]/editable)
+     + example==0.0.0 (from file://[TEMP_DIR]/editable)
+    "###
+    );
 
     Ok(())
 }
 
 #[test]
-#[ignore]
-fn cache_errors_are_non_fatal() -> Result<()> {
+fn invalidate_editable_dynamic() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    // No git dep, git has its own locking strategy
-    requirements_in.write_str(indoc! {r"
-        # pypi wheel
-        pandas
-        # url wheel
-        flask @ https://files.pythonhosted.org/packages/36/42/015c23096649b908c809c69388a805a571a3bea44362fe87e33fc3afa01f/flask-3.0.0-py3-none-any.whl
-        # url source dist
-        werkzeug @ https://files.pythonhosted.org/packages/0d/cc/ff1904eb5eb4b455e442834dabf9427331ac0fa02853bf83db817a7dd53d/werkzeug-3.0.1.tar.gz
-    "
-    })?;
-
-    // Pick a file from each kind of cache
-    let interpreter_cache = context
-        .cache_dir
-        .path()
-        .join("interpreter-v0")
-        .read_dir()?
-        .next()
-        .context("Expected a python interpreter cache file")??
-        .path();
-    let cache_files = [
-        PathBuf::from("simple-v0/pypi/numpy.msgpack"),
-        PathBuf::from(
-            "wheels-v0/pypi/python-dateutil/python_dateutil-2.8.2-py2.py3-none-any.msgpack",
-        ),
-        PathBuf::from("wheels-v0/url/4b8be67c801a7ecb/flask/flask-3.0.0-py3-none-any.msgpack"),
-        PathBuf::from("built-wheels-v0/url/6781bd6440ae72c2/werkzeug/metadata.msgpack"),
-        interpreter_cache,
-    ];
 
-    let check = || {
-        uv_snapshot!(context.compile()
-                .arg("pip")
-                .arg("compile")
-                .arg(requirements_in.path())
-                // It's sufficient to check that we resolve to a fix number of packages
-                .stdout(std::process::Stdio::null()), @r###"
-            success: true
-            exit_code: 0
-            ----- stdout -----
-
-            ----- stderr -----
-            Resolved 13 packages in [TIME]
-            "###
-        );
-    };
-
-    insta::allow_duplicates! {
-        check();
+    // Create an editable package with dynamic metadata
+    let editable_dir = context.temp_dir.child("editable");
+    editable_dir.create_dir_all()?;
+    let pyproject_toml = editable_dir.child("pyproject.toml");
+    pyproject_toml.write_str(
+        r#"
+[project]
+name = "example"
+version = "0.1.0"
+dynamic = ["dependencies"]
+requires-python = ">=3.11,<3.13"
 
-        // Replace some cache files with invalid contents
-        for file in &cache_files {
-            let file = context.cache_dir.join(file);
-            if !file.is_file() {
-                bail!("Missing cache file {}", file.normalized_display());
-            }
-            fs_err::write(file, "I borken you cache")?;
-        }
+[tool.setuptools.dynamic]
+dependencies = {file = ["requirements.txt"]}
+"#,
+    )?;
 
-        check();
+    let requirements_txt = editable_dir.child("requirements.txt");
+    requirements_txt.write_str("anyio==4.0.0")?;
 
-        #[cfg(unix)]
-        {
-            use fs_err::os::unix::fs::OpenOptionsExt;
+    uv_snapshot!(context.filters(), context.install()
+        .arg("--editable")
+        .arg(editable_dir.path()), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
 
-            // Make some files unreadable, so that the read instead of the deserialization will fail
-            for file in cache_files {
-                let file = context.cache_dir.join(file);
-                if !file.is_file() {
-                    bail!("Missing cache file {}", file.normalized_display());
-                }
-
-                fs_err::OpenOptions::new()
-                    .create(true)
-                    .write(true)
-                    .mode(0o000)
-                    .open(file)?;
-            }
-        }
+    ----- stderr -----
+    Built 1 editable in [TIME]
+    Resolved 4 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 4 packages in [TIME]
+     + anyio==4.0.0
+     + example==0.1.0 (from file://[TEMP_DIR]/editable)
+     + idna==3.6
+     + sniffio==1.3.1
+    "###
+    );
 
-        check();
+    // Re-installing should re-install.
+    uv_snapshot!(context.filters(), context.install()
+        .arg("--editable")
+        .arg(editable_dir.path()), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
 
-        Ok(())
-    }
-}
+    ----- stderr -----
+    Built 1 editable in [TIME]
+    Resolved 4 packages in [TIME]
+    Uninstalled 1 package in [TIME]
+    Installed 1 package in [TIME]
+     - example==0.1.0 (from file://[TEMP_DIR]/editable)
+     + example==0.1.0 (from file://[TEMP_DIR]/editable)
+    "###
+    );
 
-/// Resolve a distribution from an HTML-only registry.
-#[test]
-#[cfg(not(target_env = "musl"))] // No musllinux wheels in the torch index
-fn compile_html() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("jinja2<=3.1.2")?;
+    // Modify the requirements.
+    requirements_txt.write_str("anyio==3.7.1")?;
 
-    uv_snapshot!(Command::new(get_bin())
-            .arg("pip")
-            .arg("compile")
-            .arg("requirements.in")
-            .arg("--cache-dir")
-            .arg(context.cache_dir.path())
-            .arg("--index-url")
-            .arg("https://download.pytorch.org/whl")
-            .env("VIRTUAL_ENV", context.venv.as_os_str())
-            .current_dir(context.temp_dir.path()), @r###"
+    // Re-installing should update the package.
+    uv_snapshot!(context.filters(), context.install()
+        .arg("--editable")
+        .arg(editable_dir.path()), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile requirements.in --cache-dir [CACHE_DIR]
-    jinja2==3.1.2
-    markupsafe==2.1.3
-        # via jinja2
 
     ----- stderr -----
-    Resolved 2 packages in [TIME]
+    Built 1 editable in [TIME]
+    Resolved 4 packages in [TIME]
+    Downloaded 1 package in [TIME]
+    Uninstalled 2 packages in [TIME]
+    Installed 2 packages in [TIME]
+     - anyio==4.0.0
+     + anyio==3.7.1
+     - example==0.1.0 (from file://[TEMP_DIR]/editable)
+     + example==0.1.0 (from file://[TEMP_DIR]/editable)
     "###
     );
 
     Ok(())
 }
 
-/// Resolve a distribution from a registry with and without a trailing slash.
 #[test]
-fn trailing_slash() -> Result<()> {
+fn invalidate_path_on_change() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("jinja2")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--index-url")
-            .arg("https://test.pypi.org/simple"), @r###"
+    // Create a local package.
+    let editable_dir = context.temp_dir.child("editable");
+    editable_dir.create_dir_all()?;
+    let pyproject_toml = editable_dir.child("pyproject.toml");
+    pyproject_toml.write_str(
+        r#"[project]
+name = "example"
+version = "0.0.0"
+dependencies = [
+  "anyio==4.0.0"
+]
+requires-python = ">=3.8"
+"#,
+    )?;
+
+    uv_snapshot!(context.filters(), context.install()
+        .arg("example @ .")
+        .current_dir(editable_dir.path()), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    jinja2==3.1.2
-    markupsafe==2.1.3
-        # via jinja2
 
     ----- stderr -----
-    Resolved 2 packages in [TIME]
+    Resolved 4 packages in [TIME]
+    Downloaded 4 packages in [TIME]
+    Installed 4 packages in [TIME]
+     + anyio==4.0.0
+     + example==0.0.0 (from file://[TEMP_DIR]/editable)
+     + idna==3.6
+     + sniffio==1.3.1
     "###
     );
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--index-url")
-            .arg("https://test.pypi.org/simple/"), @r###"
+    // Re-installing should be a no-op.
+    uv_snapshot!(context.filters(), context.install()
+        .arg("example @ .")
+        .current_dir(editable_dir.path()), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    jinja2==3.1.2
-    markupsafe==2.1.3
-        # via jinja2
 
     ----- stderr -----
-    Resolved 2 packages in [TIME]
+    Audited 1 package in [TIME]
+    "###
+    );
+
+    // Modify the package.
+    pyproject_toml.write_str(
+        r#"[project]
+name = "example"
+version = "0.0.0"
+dependencies = [
+  "anyio==3.7.1"
+]
+requires-python = ">=3.8"
+"#,
+    )?;
+
+    // Re-installing should update the package.
+    uv_snapshot!(context.filters(), context.install()
+        .arg("example @ .")
+        .current_dir(editable_dir.path()), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 4 packages in [TIME]
+    Downloaded 2 packages in [TIME]
+    Uninstalled 2 packages in [TIME]
+    Installed 2 packages in [TIME]
+     - anyio==4.0.0
+     + anyio==3.7.1
+     - example==0.0.0 (from file://[TEMP_DIR]/editable)
+     + example==0.0.0 (from file://[TEMP_DIR]/editable)
     "###
     );
 
     Ok(())
 }
 
-/// Resolve a project without a `pyproject.toml`, using the PEP 517 build backend (default).
+/// Ignore a URL dependency with a non-matching marker.
 #[test]
-fn compile_legacy_sdist_pep_517() -> Result<()> {
+fn editable_url_with_marker() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("flake8 @ https://files.pythonhosted.org/packages/66/53/3ad4a3b74d609b3b9008a10075c40e7c8909eae60af53623c3888f7a529a/flake8-6.0.0.tar.gz")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    let editable_dir = context.temp_dir.child("editable");
+    editable_dir.create_dir_all()?;
+    let pyproject_toml = editable_dir.child("pyproject.toml");
+    pyproject_toml.write_str(
+        r#"
+[project]
+name = "example"
+version = "0.1.0"
+dependencies = [
+  "anyio==4.0.0; python_version >= '3.11'",
+  "anyio @ https://files.pythonhosted.org/packages/2d/b8/7333d87d5f03247215d86a86362fd3e324111788c6cdd8d2e6196a6ba833/anyio-4.2.0.tar.gz ; python_version < '3.11'"
+]
+requires-python = ">=3.11,<3.13"
+"#,
+    )?;
+
+    uv_snapshot!(context.filters(), context.install()
+        .arg("--editable")
+        .arg(editable_dir.path()), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    flake8 @ https://files.pythonhosted.org/packages/66/53/3ad4a3b74d609b3b9008a10075c40e7c8909eae60af53623c3888f7a529a/flake8-6.0.0.tar.gz
-    mccabe==0.7.0
-        # via flake8
-    pycodestyle==2.10.0
-        # via flake8
-    pyflakes==3.0.1
-        # via flake8
 
     ----- stderr -----
+    Built 1 editable in [TIME]
     Resolved 4 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 4 packages in [TIME]
+     + anyio==4.0.0
+     + example==0.1.0 (from file://[TEMP_DIR]/editable)
+     + idna==3.6
+     + sniffio==1.3.1
     "###
     );
 
     Ok(())
 }
 
-/// Resolve a project without a `pyproject.toml`, using `setuptools` directly.
+/// Raise an error when an editable's `Requires-Python` constraint is not met.
 #[test]
-fn compile_legacy_sdist_setuptools() -> Result<()> {
+fn requires_python_editable() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("flake8 @ https://files.pythonhosted.org/packages/66/53/3ad4a3b74d609b3b9008a10075c40e7c8909eae60af53623c3888f7a529a/flake8-6.0.0.tar.gz")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--legacy-setup-py"), @r###"
-    success: true
-    exit_code: 0
+    // Create an editable package with a `Requires-Python` constraint that is not met.
+    let editable_dir = context.temp_dir.child("editable");
+    editable_dir.create_dir_all()?;
+    let pyproject_toml = editable_dir.child("pyproject.toml");
+    pyproject_toml.write_str(
+        r#"[project]
+name = "example"
+version = "0.0.0"
+dependencies = [
+  "anyio==4.0.0"
+]
+requires-python = "<=3.8"
+"#,
+    )?;
+
+    uv_snapshot!(context.filters(), context.install()
+        .arg("--editable")
+        .arg(editable_dir.path()), @r###"
+    success: false
+    exit_code: 2
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --legacy-setup-py
-    flake8 @ https://files.pythonhosted.org/packages/66/53/3ad4a3b74d609b3b9008a10075c40e7c8909eae60af53623c3888f7a529a/flake8-6.0.0.tar.gz
-    mccabe==0.7.0
-        # via flake8
-    pycodestyle==2.10.0
-        # via flake8
-    pyflakes==3.0.1
-        # via flake8
 
     ----- stderr -----
-    Resolved 4 packages in [TIME]
+    error: Editable `example` requires Python <=3.8, but 3.12.[X] is installed
     "###
     );
 
     Ok(())
 }
 
-/// Include hashes in the generated output.
+/// Install with `--no-build-isolation`, to disable isolation during PEP 517 builds.
 #[test]
-fn generate_hashes() -> Result<()> {
+fn no_build_isolation() -> Result<()> {
     let context = TestContext::new("3.12");
     let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("flask==3.0.0")?;
+    requirements_in.write_str("anyio @ https://files.pythonhosted.org/packages/db/4d/3970183622f0330d3c23d9b8a5f52e365e50381fd484d08e3285104333d3/anyio-4.3.0.tar.gz")?;
 
-    let colorama_locked = regex::escape(indoc! {r"
-        colorama==0.4.6 \
-            --hash=sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44 \
-            --hash=sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6
-            # via click
-    "});
-    let filters: Vec<_> = if cfg!(windows) {
-        // Remove colorama
-        vec![
-            (colorama_locked.as_str(), ""),
-            ("Resolved 8 packages", "Resolved 7 packages"),
-        ]
-    } else {
-        vec![]
-    }
-    .into_iter()
-    .chain(INSTA_FILTERS.to_vec())
-    .collect();
+    // We expect the build to fail, because `setuptools` is not installed.
+    let filters = std::iter::once((r"exit code: 1", "exit status: 1"))
+        .chain(context.filters())
+        .collect::<Vec<_>>();
+    uv_snapshot!(filters, context.install()
+        .arg("-r")
+        .arg("requirements.in")
+        .arg("--no-build-isolation"), @r###"
+    success: false
+    exit_code: 2
+    ----- stdout -----
 
-    uv_snapshot!(filters, context.compile()
+    ----- stderr -----
+    error: Failed to download and build: `anyio @ https://files.pythonhosted.org/packages/db/4d/3970183622f0330d3c23d9b8a5f52e365e50381fd484d08e3285104333d3/anyio-4.3.0.tar.gz`
+      Caused by: Failed to build: `anyio @ https://files.pythonhosted.org/packages/db/4d/3970183622f0330d3c23d9b8a5f52e365e50381fd484d08e3285104333d3/anyio-4.3.0.tar.gz`
+      Caused by: Build backend failed to determine metadata through `prepare_metadata_for_build_wheel` with exit status: 1
+    --- stdout:
+
+    --- stderr:
+    Traceback (most recent call last):
+      File "<string>", line 8, in <module>
+    ModuleNotFoundError: No module named 'setuptools'
+    ---
+    "###
+    );
+
+    // Install `setuptools` and `wheel`.
+    uv_snapshot!(context.install()
+        .arg("setuptools")
+        .arg("wheel"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 2 packages in [TIME]
+    Downloaded 2 packages in [TIME]
+    Installed 2 packages in [TIME]
+     + setuptools==69.2.0
+     + wheel==0.43.0
+    "###);
+
+    // We expect the build to succeed, since `setuptools` is now installed.
+    uv_snapshot!(context.install()
+        .arg("-r")
         .arg("requirements.in")
-        .arg("--generate-hashes"), @r###"
+        .arg("--no-build-isolation"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --generate-hashes
-    blinker==1.7.0 \
-        --hash=sha256:c3f865d4d54db7abc53758a01601cf343fe55b84c1de4e3fa910e420b438d5b9 \
-        --hash=sha256:e6820ff6fa4e4d1d8e2747c2283749c3f547e4fee112b98555cdcdae32996182
-        # via flask
-    click==8.1.7 \
-        --hash=sha256:ae74fb96c20a0277a1d615f1e4d73c8414f5a98db8b799a7931d1582f3390c28 \
-        --hash=sha256:ca9853ad459e787e2192211578cc907e7594e294c7ccc834310722b41b9ca6de
-        # via flask
-    flask==3.0.0 \
-        --hash=sha256:21128f47e4e3b9d597a3e8521a329bf56909b690fcc3fa3e477725aa81367638 \
-        --hash=sha256:cfadcdb638b609361d29ec22360d6070a77d7463dcb3ab08d2c2f2f168845f58
-    itsdangerous==2.1.2 \
-        --hash=sha256:2c2349112351b88699d8d4b6b075022c0808887cb7ad10069318a8b0bc88db44 \
-        --hash=sha256:5dbbc68b317e5e42f327f9021763545dc3fc3bfe22e6deb96aaf1fc38874156a
-        # via flask
-    jinja2==3.1.2 \
-        --hash=sha256:31351a702a408a9e7595a8fc6150fc3f43bb6bf7e319770cbc0db9df9437e852 \
-        --hash=sha256:6088930bfe239f0e6710546ab9c19c9ef35e29792895fed6e6e31a023a182a61
-        # via flask
-    markupsafe==2.1.3 \
-        --hash=sha256:05fb21170423db021895e1ea1e1f3ab3adb85d1c2333cbc2310f2a26bc77272e \
-        --hash=sha256:0a4e4a1aff6c7ac4cd55792abf96c915634c2b97e3cc1c7129578aa68ebd754e \
-        --hash=sha256:10bbfe99883db80bdbaff2dcf681dfc6533a614f700da1287707e8a5d78a8431 \
-        --hash=sha256:134da1eca9ec0ae528110ccc9e48041e0828d79f24121a1a146161103c76e686 \
-        --hash=sha256:14ff806850827afd6b07a5f32bd917fb7f45b046ba40c57abdb636674a8b559c \
-        --hash=sha256:1577735524cdad32f9f694208aa75e422adba74f1baee7551620e43a3141f559 \
-        --hash=sha256:1b40069d487e7edb2676d3fbdb2b0829ffa2cd63a2ec26c4938b2d34391b4ecc \
-        --hash=sha256:1b8dd8c3fd14349433c79fa8abeb573a55fc0fdd769133baac1f5e07abf54aeb \
-        --hash=sha256:1f67c7038d560d92149c060157d623c542173016c4babc0c1913cca0564b9939 \
-        --hash=sha256:282c2cb35b5b673bbcadb33a585408104df04f14b2d9b01d4c345a3b92861c2c \
-        --hash=sha256:2c1b19b3aaacc6e57b7e25710ff571c24d6c3613a45e905b1fde04d691b98ee0 \
-        --hash=sha256:2ef12179d3a291be237280175b542c07a36e7f60718296278d8593d21ca937d4 \
-        --hash=sha256:338ae27d6b8745585f87218a3f23f1512dbf52c26c28e322dbe54bcede54ccb9 \
-        --hash=sha256:3c0fae6c3be832a0a0473ac912810b2877c8cb9d76ca48de1ed31e1c68386575 \
-        --hash=sha256:3fd4abcb888d15a94f32b75d8fd18ee162ca0c064f35b11134be77050296d6ba \
-        --hash=sha256:42de32b22b6b804f42c5d98be4f7e5e977ecdd9ee9b660fda1a3edf03b11792d \
-        --hash=sha256:47d4f1c5f80fc62fdd7777d0d40a2e9dda0a05883ab11374334f6c4de38adffd \
-        --hash=sha256:504b320cd4b7eff6f968eddf81127112db685e81f7e36e75f9f84f0df46041c3 \
-        --hash=sha256:525808b8019e36eb524b8c68acdd63a37e75714eac50e988180b169d64480a00 \
-        --hash=sha256:56d9f2ecac662ca1611d183feb03a3fa4406469dafe241673d521dd5ae92a155 \
-        --hash=sha256:5bbe06f8eeafd38e5d0a4894ffec89378b6c6a625ff57e3028921f8ff59318ac \
-        --hash=sha256:65c1a9bcdadc6c28eecee2c119465aebff8f7a584dd719facdd9e825ec61ab52 \
-        --hash=sha256:68e78619a61ecf91e76aa3e6e8e33fc4894a2bebe93410754bd28fce0a8a4f9f \
-        --hash=sha256:69c0f17e9f5a7afdf2cc9fb2d1ce6aabdb3bafb7f38017c0b77862bcec2bbad8 \
-        --hash=sha256:6b2b56950d93e41f33b4223ead100ea0fe11f8e6ee5f641eb753ce4b77a7042b \
-        --hash=sha256:715d3562f79d540f251b99ebd6d8baa547118974341db04f5ad06d5ea3eb8007 \
-        --hash=sha256:787003c0ddb00500e49a10f2844fac87aa6ce977b90b0feaaf9de23c22508b24 \
-        --hash=sha256:7ef3cb2ebbf91e330e3bb937efada0edd9003683db6b57bb108c4001f37a02ea \
-        --hash=sha256:8023faf4e01efadfa183e863fefde0046de576c6f14659e8782065bcece22198 \
-        --hash=sha256:8758846a7e80910096950b67071243da3e5a20ed2546e6392603c096778d48e0 \
-        --hash=sha256:8afafd99945ead6e075b973fefa56379c5b5c53fd8937dad92c662da5d8fd5ee \
-        --hash=sha256:8c41976a29d078bb235fea9b2ecd3da465df42a562910f9022f1a03107bd02be \
-        --hash=sha256:8e254ae696c88d98da6555f5ace2279cf7cd5b3f52be2b5cf97feafe883b58d2 \
-        --hash=sha256:8f9293864fe09b8149f0cc42ce56e3f0e54de883a9de90cd427f191c346eb2e1 \
-        --hash=sha256:9402b03f1a1b4dc4c19845e5c749e3ab82d5078d16a2a4c2cd2df62d57bb0707 \
-        --hash=sha256:962f82a3086483f5e5f64dbad880d31038b698494799b097bc59c2edf392fce6 \
-        --hash=sha256:9aad3c1755095ce347e26488214ef77e0485a3c34a50c5a5e2471dff60b9dd9c \
-        --hash=sha256:9dcdfd0eaf283af041973bff14a2e143b8bd64e069f4c383416ecd79a81aab58 \
-        --hash=sha256:aa57bd9cf8ae831a362185ee444e15a93ecb2e344c8e52e4d721ea3ab6ef1823 \
-        --hash=sha256:aa7bd130efab1c280bed0f45501b7c8795f9fdbeb02e965371bbef3523627779 \
-        --hash=sha256:ab4a0df41e7c16a1392727727e7998a467472d0ad65f3ad5e6e765015df08636 \
-        --hash=sha256:ad9e82fb8f09ade1c3e1b996a6337afac2b8b9e365f926f5a61aacc71adc5b3c \
-        --hash=sha256:af598ed32d6ae86f1b747b82783958b1a4ab8f617b06fe68795c7f026abbdcad \
-        --hash=sha256:b076b6226fb84157e3f7c971a47ff3a679d837cf338547532ab866c57930dbee \
-        --hash=sha256:b7ff0f54cb4ff66dd38bebd335a38e2c22c41a8ee45aa608efc890ac3e3931bc \
-        --hash=sha256:bfce63a9e7834b12b87c64d6b155fdd9b3b96191b6bd334bf37db7ff1fe457f2 \
-        --hash=sha256:c011a4149cfbcf9f03994ec2edffcb8b1dc2d2aede7ca243746df97a5d41ce48 \
-        --hash=sha256:c9c804664ebe8f83a211cace637506669e7890fec1b4195b505c214e50dd4eb7 \
-        --hash=sha256:ca379055a47383d02a5400cb0d110cef0a776fc644cda797db0c5696cfd7e18e \
-        --hash=sha256:cb0932dc158471523c9637e807d9bfb93e06a95cbf010f1a38b98623b929ef2b \
-        --hash=sha256:cd0f502fe016460680cd20aaa5a76d241d6f35a1c3350c474bac1273803893fa \
-        --hash=sha256:ceb01949af7121f9fc39f7d27f91be8546f3fb112c608bc4029aef0bab86a2a5 \
-        --hash=sha256:d080e0a5eb2529460b30190fcfcc4199bd7f827663f858a226a81bc27beaa97e \
-        --hash=sha256:dd15ff04ffd7e05ffcb7fe79f1b98041b8ea30ae9234aed2a9168b5797c3effb \
-        --hash=sha256:df0be2b576a7abbf737b1575f048c23fb1d769f267ec4358296f31c2479db8f9 \
-        --hash=sha256:e09031c87a1e51556fdcb46e5bd4f59dfb743061cf93c4d6831bf894f125eb57 \
-        --hash=sha256:e4dd52d80b8c83fdce44e12478ad2e85c64ea965e75d66dbeafb0a3e77308fcc \
-        --hash=sha256:f698de3fd0c4e6972b92290a45bd9b1536bffe8c6759c62471efaa8acb4c37bc \
-        --hash=sha256:fec21693218efe39aa7f8599346e90c705afa52c5b31ae019b2e57e8f6542bb2 \
-        --hash=sha256:ffcc3f7c66b5f5b7931a5aa68fc9cecc51e685ef90282f4a82f0f5e9b704ad11
-        # via
-        #   jinja2
-        #   werkzeug
-    werkzeug==3.0.1 \
-        --hash=sha256:507e811ecea72b18a404947aded4b3390e1db8f826b494d76550ef45bb3b1dcc \
-        --hash=sha256:90a285dc0e42ad56b34e696398b8122ee4c681833fb35b8334a095d82c56da10
-        # via flask
 
     ----- stderr -----
-    Resolved 7 packages in [TIME]
+    Resolved 3 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 3 packages in [TIME]
+     + anyio==0.0.0 (from https://files.pythonhosted.org/packages/db/4d/3970183622f0330d3c23d9b8a5f52e365e50381fd484d08e3285104333d3/anyio-4.3.0.tar.gz)
+     + idna==3.6
+     + sniffio==1.3.1
     "###
     );
 
     Ok(())
 }
 
-/// Compile using `--find-links` with a local directory.
+/// Ensure that `UV_NO_BUILD_ISOLATION` env var does the same as the `--no-build-isolation` flag
 #[test]
-fn find_links_directory() -> Result<()> {
+fn respect_no_build_isolation_env_var() -> Result<()> {
     let context = TestContext::new("3.12");
     let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str(indoc! {r"
-        tqdm
-        numpy
-        werkzeug @ https://files.pythonhosted.org/packages/c3/fc/254c3e9b5feb89ff5b9076a23218dafbc99c96ac5941e900b71206e6313b/werkzeug-3.0.1-py3-none-any.whl
-    "})?;
+    requirements_in.write_str("anyio @ https://files.pythonhosted.org/packages/db/4d/3970183622f0330d3c23d9b8a5f52e365e50381fd484d08e3285104333d3/anyio-4.3.0.tar.gz")?;
 
-    let project_root = fs_err::canonicalize(std::env::current_dir()?.join("..").join(".."))?;
-    let project_root_string = regex::escape(&project_root.normalized_display().to_string());
-    let filters: Vec<_> = [
-        (project_root_string.as_str(), "[PROJECT_ROOT]"),
-        // Unify trailing (back)slash between Windows and Unix.
-        (
-            "[PROJECT_ROOT]/scripts/wheels/",
-            "[PROJECT_ROOT]/scripts/wheels",
-        ),
-    ]
-    .into_iter()
-    .chain(INSTA_FILTERS.to_vec())
-    .collect();
+    // We expect the build to fail, because `setuptools` is not installed.
+    let filters = std::iter::once((r"exit code: 1", "exit status: 1"))
+        .chain(context.filters())
+        .collect::<Vec<_>>();
+    uv_snapshot!(filters, context.install()
+        .arg("-r")
+        .arg("requirements.in")
+        .env("UV_NO_BUILD_ISOLATION", "yes"), @r###"
+    success: false
+    exit_code: 2
+    ----- stdout -----
 
-    uv_snapshot!(filters, context.compile()
-            .arg("requirements.in")
-            .arg("--find-links")
-            .arg(project_root.join("scripts").join("wheels")), @r###"
+    ----- stderr -----
+    error: Failed to download and build: `anyio @ https://files.pythonhosted.org/packages/db/4d/3970183622f0330d3c23d9b8a5f52e365e50381fd484d08e3285104333d3/anyio-4.3.0.tar.gz`
+      Caused by: Failed to build: `anyio @ https://files.pythonhosted.org/packages/db/4d/3970183622f0330d3c23d9b8a5f52e365e50381fd484d08e3285104333d3/anyio-4.3.0.tar.gz`
+      Caused by: Build backend failed to determine metadata through `prepare_metadata_for_build_wheel` with exit status: 1
+    --- stdout:
+
+    --- stderr:
+    Traceback (most recent call last):
+      File "<string>", line 8, in <module>
+    ModuleNotFoundError: No module named 'setuptools'
+    ---
+    "###
+    );
+
+    // Install `setuptools` and `wheel`.
+    uv_snapshot!(context.install()
+        .arg("setuptools")
+        .arg("wheel"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --find-links [PROJECT_ROOT]/scripts/wheels
-    markupsafe==2.1.3
-        # via werkzeug
-    numpy==1.26.2
-    tqdm==1000.0.0
-    werkzeug @ https://files.pythonhosted.org/packages/c3/fc/254c3e9b5feb89ff5b9076a23218dafbc99c96ac5941e900b71206e6313b/werkzeug-3.0.1-py3-none-any.whl
 
     ----- stderr -----
-    Resolved 4 packages in [TIME]
+    Resolved 2 packages in [TIME]
+    Downloaded 2 packages in [TIME]
+    Installed 2 packages in [TIME]
+     + setuptools==69.2.0
+     + wheel==0.43.0
     "###);
 
+    // We expect the build to succeed, since `setuptools` is now installed.
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.in")
+        .env("UV_NO_BUILD_ISOLATION", "yes"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 3 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 3 packages in [TIME]
+     + anyio==0.0.0 (from https://files.pythonhosted.org/packages/db/4d/3970183622f0330d3c23d9b8a5f52e365e50381fd484d08e3285104333d3/anyio-4.3.0.tar.gz)
+     + idna==3.6
+     + sniffio==1.3.1
+    "###
+    );
+
     Ok(())
 }
 
-/// Compile using `--find-links` with a URL by resolving `tqdm` from the `PyTorch` wheels index.
+/// This tests that `uv` can read UTF-16LE encoded requirements.txt files.
+///
+/// Ref: <https://github.com/astral-sh/uv/issues/2276>
 #[test]
-fn find_links_url() -> Result<()> {
+fn install_utf16le_requirements() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("tqdm")?;
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_binary(&utf8_to_utf16_with_bom_le("tomli"))?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--no-index")
-            .arg("--find-links")
-            .arg("https://download.pytorch.org/whl/torch_stable.html"), @r###"
+    uv_snapshot!(context.install_without_exclude_newer()
+        .arg("-r")
+        .arg("requirements.txt"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --no-index --find-links https://download.pytorch.org/whl/torch_stable.html
-    tqdm==4.64.1
 
     ----- stderr -----
     Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + tomli==2.0.1
     "###
     );
-
     Ok(())
 }
 
-/// Compile using `--find-links` with a URL by resolving `tqdm` from the `PyTorch` wheels index,
-/// with the URL itself provided in a `requirements.txt` file.
+/// This tests that `uv` can read UTF-16BE encoded requirements.txt files.
+///
+/// Ref: <https://github.com/astral-sh/uv/issues/2276>
 #[test]
-fn find_links_requirements_txt() -> Result<()> {
+fn install_utf16be_requirements() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("-f https://download.pytorch.org/whl/torch_stable.html\ntqdm")?;
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_binary(&utf8_to_utf16_with_bom_be("tomli"))?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--no-index")
-            .arg("--emit-find-links"), @r###"
+    uv_snapshot!(context.install_without_exclude_newer()
+        .arg("-r")
+        .arg("requirements.txt"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --no-index --emit-find-links
-    --find-links https://download.pytorch.org/whl/torch_stable.html
-
-    tqdm==4.64.1
 
     ----- stderr -----
     Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + tomli==2.0.1
     "###
     );
-
     Ok(())
 }
 
-/// Use an existing resolution for `black==23.10.1`, with stale versions of `click` and `pathspec`.
-/// Nothing should change.
+fn utf8_to_utf16_with_bom_le(s: &str) -> Vec<u8> {
+    use byteorder::ByteOrder;
+
+    let mut u16s = vec![0xFEFF];
+    u16s.extend(s.encode_utf16());
+    let mut u8s = vec![0; u16s.len() * 2];
+    byteorder::LittleEndian::write_u16_into(&u16s, &mut u8s);
+    u8s
+}
+
+fn utf8_to_utf16_with_bom_be(s: &str) -> Vec<u8> {
+    use byteorder::ByteOrder;
+
+    let mut u16s = vec![0xFEFF];
+    u16s.extend(s.encode_utf16());
+    let mut u8s = vec![0; u16s.len() * 2];
+    byteorder::BigEndian::write_u16_into(&u16s, &mut u8s);
+    u8s
+}
+
 #[test]
-fn upgrade_none() -> Result<()> {
+fn dry_run_install() -> std::result::Result<(), Box<dyn std::error::Error>> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("black==23.10.1")?;
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str("httpx==0.25.1")?;
+
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--dry-run")
+        .arg("--strict"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 7 packages in [TIME]
+    Would download 7 packages
+    Would install 7 packages
+     + anyio==4.3.0
+     + certifi==2024.2.2
+     + h11==0.14.0
+     + httpcore==1.0.4
+     + httpx==0.25.1
+     + idna==3.6
+     + sniffio==1.3.1
+    "###
+    );
+
+    Ok(())
+}
 
+#[test]
+fn dry_run_install_url_dependency() -> std::result::Result<(), Box<dyn std::error::Error>> {
+    let context = TestContext::new("3.12");
     let requirements_txt = context.temp_dir.child("requirements.txt");
-    requirements_txt.write_str(indoc! {r"
-        black==23.10.1
-        click==8.1.2
-            # via black
-        mypy-extensions==1.0.0
-            # via black
-        packaging==23.2
-            # via black
-        pathspec==0.11.0
-            # via black
-        platformdirs==4.0.0
-            # via black
-    "})?;
+    requirements_txt.write_str("anyio @ https://files.pythonhosted.org/packages/2d/b8/7333d87d5f03247215d86a86362fd3e324111788c6cdd8d2e6196a6ba833/anyio-4.2.0.tar.gz")?;
 
-    let filters = if cfg!(windows) {
-        [("Resolved 7 packages", "Resolved 6 packages")]
-            .into_iter()
-            .chain(INSTA_FILTERS.to_vec())
-            .collect()
-    } else {
-        INSTA_FILTERS.to_vec()
-    };
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--dry-run")
+        .arg("--strict"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
 
-    uv_snapshot!(filters, context.compile()
-            .arg("requirements.in")
-            .arg("--output-file")
-            .arg("requirements.txt"), @r###"
+    ----- stderr -----
+    Resolved 3 packages in [TIME]
+    Would download 3 packages
+    Would install 3 packages
+     + anyio @ https://files.pythonhosted.org/packages/2d/b8/7333d87d5f03247215d86a86362fd3e324111788c6cdd8d2e6196a6ba833/anyio-4.2.0.tar.gz
+     + idna==3.6
+     + sniffio==1.3.1
+    "###
+    );
+
+    Ok(())
+}
+
+#[test]
+fn dry_run_uninstall_url_dependency() -> std::result::Result<(), Box<dyn std::error::Error>> {
+    let context = TestContext::new("3.12");
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str("anyio @ https://files.pythonhosted.org/packages/2d/b8/7333d87d5f03247215d86a86362fd3e324111788c6cdd8d2e6196a6ba833/anyio-4.2.0.tar.gz")?;
+
+    // Install the URL dependency
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--strict"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    Resolved 6 packages in [TIME]
+    Resolved 3 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 3 packages in [TIME]
+     + anyio==4.2.0 (from https://files.pythonhosted.org/packages/2d/b8/7333d87d5f03247215d86a86362fd3e324111788c6cdd8d2e6196a6ba833/anyio-4.2.0.tar.gz)
+     + idna==3.6
+     + sniffio==1.3.1
     "###
     );
 
-    // Read the output requirements, but skip the header.
-    let resolution = fs::read_to_string(requirements_txt.path())?
-        .lines()
-        .skip_while(|line| line.trim_start().starts_with('#'))
-        .join("\n");
-    if cfg!(unix) {
-        assert_snapshot!(resolution, @r###"
-        black==23.10.1
-        click==8.1.2
-            # via black
-        mypy-extensions==1.0.0
-            # via black
-        packaging==23.2
-            # via black
-        pathspec==0.11.0
-            # via black
-        platformdirs==4.0.0
-            # via black
-        "###);
-    }
+    // Then switch to a registry dependency
+    requirements_txt.write_str("anyio")?;
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--upgrade-package")
+        .arg("anyio")
+        .arg("--dry-run")
+        .arg("--strict"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 3 packages in [TIME]
+    Would download 1 package
+    Would uninstall 1 package
+    Would install 1 package
+     - anyio==4.2.0 (from https://files.pythonhosted.org/packages/2d/b8/7333d87d5f03247215d86a86362fd3e324111788c6cdd8d2e6196a6ba833/anyio-4.2.0.tar.gz)
+     + anyio==4.3.0
+    "###
+    );
 
     Ok(())
 }
 
-/// Use an existing resolution for `black==23.10.1`, with stale versions of `click` and `pathspec`.
-/// Both packages should be upgraded.
 #[test]
-fn upgrade_all() -> Result<()> {
+fn dry_run_install_already_installed() -> std::result::Result<(), Box<dyn std::error::Error>> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("black==23.10.1")?;
-
     let requirements_txt = context.temp_dir.child("requirements.txt");
-    requirements_txt.write_str(indoc! {r"
-        # This file was autogenerated by uv via the following command:
-        #    uv pip compile requirements.in --python-version 3.12 --cache-dir [CACHE_DIR]
-        black==23.10.1
-        click==8.1.2
-            # via black
-        mypy-extensions==1.0.0
-            # via black
-        packaging==23.2
-            # via black
-        pathspec==0.11.0
-            # via black
-        platformdirs==4.0.0
-            # via black
-    "})?;
-
-    let filters = if cfg!(windows) {
-        [("Resolved 7 packages", "Resolved 6 packages")]
-            .into_iter()
-            .chain(INSTA_FILTERS.to_vec())
-            .collect()
-    } else {
-        INSTA_FILTERS.to_vec()
-    };
+    requirements_txt.write_str("httpx==0.25.1")?;
 
-    uv_snapshot!(filters, context.compile()
-            .arg("requirements.in")
-            .arg("--output-file")
-            .arg("requirements.txt")
-            .arg("--upgrade"), @r###"
+    // Install the package
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--strict"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    Resolved 6 packages in [TIME]
+    Resolved 7 packages in [TIME]
+    Downloaded 7 packages in [TIME]
+    Installed 7 packages in [TIME]
+     + anyio==4.3.0
+     + certifi==2024.2.2
+     + h11==0.14.0
+     + httpcore==1.0.4
+     + httpx==0.25.1
+     + idna==3.6
+     + sniffio==1.3.1
     "###
     );
 
-    // Read the output requirements, but skip the header.
-    let resolution = fs::read_to_string(requirements_txt.path())?
-        .lines()
-        .skip_while(|line| line.trim_start().starts_with('#'))
-        .join("\n");
-    if cfg!(unix) {
-        assert_snapshot!(resolution, @r###"
-        black==23.10.1
-        click==8.1.7
-            # via black
-        mypy-extensions==1.0.0
-            # via black
-        packaging==23.2
-            # via black
-        pathspec==0.11.2
-            # via black
-        platformdirs==4.0.0
-            # via black
-        "###);
-    } else if cfg!(windows) {
-        assert_snapshot!(resolution, @r###"
-        black==23.10.1
-        click==8.1.7
-            # via black
-        colorama==0.4.6
-            # via click
-        mypy-extensions==1.0.0
-            # via black
-        packaging==23.2
-            # via black
-        pathspec==0.11.2
-            # via black
-        platformdirs==4.0.0
-            # via black
-        "###);
-    } else {
-        unimplemented!("Only Windows and Unix are supported");
-    }
+    // Install again with dry run enabled
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--dry-run")
+        .arg("--strict"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Audited 1 package in [TIME]
+    Would make no changes
+    "###
+    );
 
     Ok(())
 }
 
-/// Use an existing resolution for `black==23.10.1`, with stale versions of `click` and `pathspec`.
-/// Only `click` should be upgraded.
 #[test]
-fn upgrade_package() -> Result<()> {
+fn dry_run_install_transitive_dependency_already_installed(
+) -> std::result::Result<(), Box<dyn std::error::Error>> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("black==23.10.1")?;
 
     let requirements_txt = context.temp_dir.child("requirements.txt");
-    requirements_txt.write_str(indoc! {r"
-        # This file was autogenerated by uv via the following command:
-        #    uv pip compile requirements.in --python-version 3.12 --cache-dir [CACHE_DIR]
-        black==23.10.1
-        click==8.1.2
-            # via black
-        mypy-extensions==1.0.0
-            # via black
-        packaging==23.2
-            # via black
-        pathspec==0.11.0
-            # via black
-        platformdirs==4.0.0
-            # via black
-    "})?;
-
-    let filters = if cfg!(windows) {
-        [("Resolved 7 packages", "Resolved 6 packages")]
-            .into_iter()
-            .chain(INSTA_FILTERS.to_vec())
-            .collect()
-    } else {
-        INSTA_FILTERS.to_vec()
-    };
+    requirements_txt.write_str("httpcore==1.0.2")?;
 
-    uv_snapshot!(filters, context.compile()
-            .arg("requirements.in")
-            .arg("--output-file")
-            .arg("requirements.txt")
-            .arg("--upgrade-package")
-            .arg("click"), @r###"
+    // Install a dependency of httpx
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--strict"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    Resolved 6 packages in [TIME]
+    Resolved 3 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 3 packages in [TIME]
+     + certifi==2024.2.2
+     + h11==0.14.0
+     + httpcore==1.0.2
     "###
     );
 
-    // Read the output requirements, but skip the header.
-    let resolution = fs::read_to_string(requirements_txt.path())?
-        .lines()
-        .skip_while(|line| line.trim_start().starts_with('#'))
-        .join("\n");
-    if cfg!(unix) {
-        assert_snapshot!(resolution, @r###"
-        black==23.10.1
-        click==8.1.7
-            # via black
-        mypy-extensions==1.0.0
-            # via black
-        packaging==23.2
-            # via black
-        pathspec==0.11.0
-            # via black
-        platformdirs==4.0.0
-            # via black
-        "###
-        );
-    }
+    // Install it httpx with dry run enabled
+    requirements_txt.write_str("httpx==0.25.1")?;
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--dry-run")
+        .arg("--strict"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 7 packages in [TIME]
+    Would download 4 packages
+    Would install 4 packages
+     + anyio==4.3.0
+     + httpx==0.25.1
+     + idna==3.6
+     + sniffio==1.3.1
+    "###
+    );
 
     Ok(())
 }
 
-/// Attempt to resolve a requirement at a path that doesn't exist.
 #[test]
-fn missing_path_requirement() -> Result<()> {
+fn dry_run_install_then_upgrade() -> std::result::Result<(), Box<dyn std::error::Error>> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str(if cfg!(windows) {
-        "django @ file://C:/tmp/django-3.2.8.tar.gz"
-    } else {
-        "django @ file:///tmp/django-3.2.8.tar.gz"
-    })?;
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str("httpx==0.25.0")?;
 
-    let filters: Vec<_> = [(r"/C:/", "/")]
-        .into_iter()
-        .chain(INSTA_FILTERS.to_vec())
-        .collect();
+    // Install the package
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--strict"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
 
-    uv_snapshot!(filters, context.compile()
-            .arg("requirements.in"), @r###"
-    success: false
-    exit_code: 2
+    ----- stderr -----
+    Resolved 7 packages in [TIME]
+    Downloaded 7 packages in [TIME]
+    Installed 7 packages in [TIME]
+     + anyio==4.3.0
+     + certifi==2024.2.2
+     + h11==0.14.0
+     + httpcore==0.18.0
+     + httpx==0.25.0
+     + idna==3.6
+     + sniffio==1.3.1
+    "###
+    );
+
+    // Bump the version and install with dry run enabled
+    requirements_txt.write_str("httpx==0.25.1")?;
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--dry-run"), @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    error: Distribution not found at: file:///tmp/django-3.2.8.tar.gz
-    "###);
+    Resolved 7 packages in [TIME]
+    Would download 1 package
+    Would uninstall 1 package
+    Would install 1 package
+     - httpx==0.25.0
+     + httpx==0.25.1
+    "###
+    );
 
     Ok(())
 }
 
-/// Attempt to resolve an editable requirement at a path that doesn't exist.
+/// Raise an error when a direct URL's `Requires-Python` constraint is not met.
 #[test]
-fn missing_editable_requirement() -> Result<()> {
+fn requires_python_direct_url() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("-e ../tmp/django-3.2.8.tar.gz")?;
 
-    // File url, absolute Unix path or absolute Windows path
-    let filters: Vec<_> = [
-        (r" file://.*/", " file://[TEMP_DIR]/"),
-        (r" /.*/", " /[TEMP_DIR]/"),
-        (r" [A-Z]:\\.*\\", " /[TEMP_DIR]/"),
-    ]
-    .into_iter()
-    .chain(INSTA_FILTERS.to_vec())
-    .collect::<Vec<_>>();
+    // Create an editable package with a `Requires-Python` constraint that is not met.
+    let editable_dir = context.temp_dir.child("editable");
+    editable_dir.create_dir_all()?;
+    let pyproject_toml = editable_dir.child("pyproject.toml");
+    pyproject_toml.write_str(
+        r#"[project]
+name = "example"
+version = "0.0.0"
+dependencies = [
+  "anyio==4.0.0"
+]
+requires-python = "<=3.8"
+"#,
+    )?;
 
-    uv_snapshot!(filters, context.compile()
-            .arg("requirements.in"), @r###"
+    uv_snapshot!(context.filters(), context.install()
+        .arg(format!("example @ {}", editable_dir.path().display())), @r###"
     success: false
-    exit_code: 2
+    exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
-    error: Failed to build editables
-      Caused by: Failed to build editable: file://[TEMP_DIR]/django-3.2.8.tar.gz
-      Caused by: Source distribution not found at: /[TEMP_DIR]/django-3.2.8.tar.gz
-    "###);
+      × No solution found when resolving dependencies:
+      ╰─▶ Because the current Python version (3.12.[X]) does not satisfy Python<=3.8 and example==0.0.0 depends on Python<=3.8, we can conclude that example==0.0.0 cannot be used.
+          And because only example==0.0.0 is available and you require example, we can conclude that the requirements are unsatisfiable.
+    "###
+    );
 
     Ok(())
 }
 
-/// Attempt to resolve a URL requirement without a package name.
+/// Install a package from an index that requires authentication
 #[test]
-fn missing_package_name() -> Result<()> {
+fn install_package_basic_auth_from_url() {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("https://files.pythonhosted.org/packages/36/42/015c23096649b908c809c69388a805a571a3bea44362fe87e33fc3afa01f/flask-3.0.0-py3-none-any.whl")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
-    success: false
-    exit_code: 2
+    uv_snapshot!(context.install()
+        .arg("anyio")
+        .arg("--index-url")
+        .arg("https://public:heron@pypi-proxy.fly.dev/basic-auth/simple")
+        .arg("--strict"), @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    error: Unsupported requirement in requirements.in at position 0
-      Caused by: URL requirement must be preceded by a package name. Add the name of the package before the URL (e.g., `package_name @ https://...`).
-    https://files.pythonhosted.org/packages/36/42/015c23096649b908c809c69388a805a571a3bea44362fe87e33fc3afa01f/flask-3.0.0-py3-none-any.whl
-    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+    Resolved 3 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 3 packages in [TIME]
+     + anyio==4.3.0
+     + idna==3.6
+     + sniffio==1.3.1
     "###
     );
 
-    Ok(())
+    context.assert_command("import anyio").success();
 }
 
-/// Exclude annotations from the output.
+/// Install a package from an index that requires authentication
 #[test]
-fn no_annotate() -> Result<()> {
+fn install_package_basic_auth_from_netrc_default() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("black==23.10.1")?;
+    let netrc = context.temp_dir.child(".netrc");
+    netrc.write_str("default login public password heron")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--no-annotate"), @r###"
+    uv_snapshot!(context.install()
+        .arg("anyio")
+        .arg("--index-url")
+        .arg("https://pypi-proxy.fly.dev/basic-auth/simple")
+        .env("NETRC", netrc.to_str().unwrap())
+        .arg("--strict"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --no-annotate
-    black==23.10.1
-    click==8.1.7
-    mypy-extensions==1.0.0
-    packaging==23.2
-    pathspec==0.11.2
-    platformdirs==4.0.0
 
     ----- stderr -----
-    Resolved 6 packages in [TIME]
+    Resolved 3 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 3 packages in [TIME]
+     + anyio==4.3.0
+     + idna==3.6
+     + sniffio==1.3.1
     "###
     );
 
+    context.assert_command("import anyio").success();
+
     Ok(())
 }
 
-/// Exclude header from the output.
+/// Install a package from an index that requires authentication
 #[test]
-fn no_header() -> Result<()> {
+fn install_package_basic_auth_from_netrc() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("black==23.10.1")?;
+    let netrc = context.temp_dir.child(".netrc");
+    netrc.write_str("machine pypi-proxy.fly.dev login public password heron")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--no-header"), @r###"
+    uv_snapshot!(context.install()
+        .arg("anyio")
+        .arg("--index-url")
+        .arg("https://pypi-proxy.fly.dev/basic-auth/simple")
+        .env("NETRC", netrc.to_str().unwrap())
+        .arg("--strict"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    black==23.10.1
-    click==8.1.7
-        # via black
-    mypy-extensions==1.0.0
-        # via black
-    packaging==23.2
-        # via black
-    pathspec==0.11.2
-        # via black
-    platformdirs==4.0.0
-        # via black
 
     ----- stderr -----
-    Resolved 6 packages in [TIME]
+    Resolved 3 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 3 packages in [TIME]
+     + anyio==4.3.0
+     + idna==3.6
+     + sniffio==1.3.1
     "###
     );
 
+    context.assert_command("import anyio").success();
+
     Ok(())
 }
 
-/// Emit warnings when users pass redundant options from `pip-compile`.
+/// Install a package from an index that requires authentication
+/// Define the `--index-url` in the requirements file
 #[test]
-fn allow_unsafe() -> Result<()> {
+fn install_package_basic_auth_from_netrc_index_in_requirements() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("werkzeug==3.0.1")?;
+    let netrc = context.temp_dir.child(".netrc");
+    netrc.write_str("machine pypi-proxy.fly.dev login public password heron")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--allow-unsafe"), @r###"
+    let requirements = context.temp_dir.child("requirements.txt");
+    requirements.write_str(
+        r"
+anyio
+--index-url https://pypi-proxy.fly.dev/basic-auth/simple
+    ",
+    )?;
+
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .env("NETRC", netrc.to_str().unwrap())
+        .arg("--strict"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --allow-unsafe
-    markupsafe==2.1.3
-        # via werkzeug
-    werkzeug==3.0.1
 
     ----- stderr -----
-    warning: pip-compile's `--allow-unsafe` has no effect (uv can safely pin `pip` and other packages).
-    Resolved 2 packages in [TIME]
+    Resolved 3 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 3 packages in [TIME]
+     + anyio==4.3.0
+     + idna==3.6
+     + sniffio==1.3.1
     "###
     );
 
+    context.assert_command("import anyio").success();
+
     Ok(())
 }
 
-/// Emit warnings when users pass redundant options from `pip-compile`.
+/// Install a package from an index that provides relative links
 #[test]
-fn resolver_legacy() -> Result<()> {
+fn install_index_with_relative_links() {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("werkzeug==3.0.1")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--resolver=legacy"), @r###"
+    uv_snapshot!(context.install()
+        .arg("anyio")
+        .arg("--index-url")
+        .arg("https://pypi-proxy.fly.dev/relative/simple")
+        .arg("--strict"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 3 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 3 packages in [TIME]
+     + anyio==4.3.0
+     + idna==3.6
+     + sniffio==1.3.1
+    "###
+    );
+
+    context.assert_command("import anyio").success();
+}
+
+/// Install a package from an index that requires authentication from the keyring.
+#[test]
+fn install_package_basic_auth_from_keyring() {
+    let context = TestContext::new("3.12");
+
+    // Install our keyring plugin
+    context
+        .install()
+        .arg(
+            context
+                .workspace_root
+                .join("scripts")
+                .join("packages")
+                .join("keyring_test_plugin"),
+        )
+        .assert()
+        .success();
+
+    uv_snapshot!(context.install()
+        .arg("anyio")
+        .arg("--index-url")
+        .arg("https://public@pypi-proxy.fly.dev/basic-auth/simple")
+        .arg("--keyring-provider")
+        .arg("subprocess")
+        .arg("--strict")
+        .env("KEYRING_TEST_CREDENTIALS", r#"{"pypi-proxy.fly.dev": {"public": "heron"}}"#)
+        .env("PATH", venv_bin_path(context.venv.as_path())), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 3 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 3 packages in [TIME]
+     + anyio==4.3.0
+     + idna==3.6
+     + sniffio==1.3.1
+    "###
+    );
+
+    context.assert_command("import anyio").success();
+}
+
+/// Install a package from an index that requires authentication
+/// but the keyring has the wrong password
+#[test]
+fn install_package_basic_auth_from_keyring_wrong_password() {
+    let context = TestContext::new("3.12");
+
+    // Install our keyring plugin
+    context
+        .install()
+        .arg(
+            context
+                .workspace_root
+                .join("scripts")
+                .join("packages")
+                .join("keyring_test_plugin"),
+        )
+        .assert()
+        .success();
+
+    uv_snapshot!(context.install()
+        .arg("anyio")
+        .arg("--index-url")
+        .arg("https://public@pypi-proxy.fly.dev/basic-auth/simple")
+        .arg("--keyring-provider")
+        .arg("subprocess")
+        .arg("--strict")
+        .env("KEYRING_TEST_CREDENTIALS", r#"{"pypi-proxy.fly.dev": {"public": "foobar"}}"#)
+        .env("PATH", venv_bin_path(context.venv.as_path())), @r###"
     success: false
     exit_code: 2
     ----- stdout -----
 
     ----- stderr -----
-    error: pip-compile's `--resolver=legacy` is unsupported (uv always backtracks).
+    error: Failed to download `anyio==4.3.0`
+      Caused by: HTTP status client error (401 Unauthorized) for url (https://pypi-proxy.fly.dev/basic-auth/files/packages/14/fd/2f20c40b45e4fb4324834aea24bd4afdf1143390242c0b33774da0e2e34f/anyio-4.3.0-py3-none-any.whl.metadata)
     "###
     );
+}
 
-    Ok(())
+/// Install a package from an index that requires authentication
+/// but the keyring has the wrong username
+#[test]
+fn install_package_basic_auth_from_keyring_wrong_username() {
+    let context = TestContext::new("3.12");
+
+    // Install our keyring plugin
+    context
+        .install()
+        .arg(
+            context
+                .workspace_root
+                .join("scripts")
+                .join("packages")
+                .join("keyring_test_plugin"),
+        )
+        .assert()
+        .success();
+
+    uv_snapshot!(context.install()
+        .arg("anyio")
+        .arg("--index-url")
+        .arg("https://public@pypi-proxy.fly.dev/basic-auth/simple")
+        .arg("--keyring-provider")
+        .arg("subprocess")
+        .arg("--strict")
+        .env("KEYRING_TEST_CREDENTIALS", r#"{"pypi-proxy.fly.dev": {"other": "heron"}}"#)
+        .env("PATH", venv_bin_path(context.venv.as_path())), @r###"
+    success: false
+    exit_code: 2
+    ----- stdout -----
+
+    ----- stderr -----
+    error: Failed to download `anyio==4.3.0`
+      Caused by: HTTP status client error (401 Unauthorized) for url (https://pypi-proxy.fly.dev/basic-auth/files/packages/14/fd/2f20c40b45e4fb4324834aea24bd4afdf1143390242c0b33774da0e2e34f/anyio-4.3.0-py3-none-any.whl.metadata)
+    "###
+    );
 }
 
-/// Emit the `--index-url` and `--extra-index-url` locations.
-/// Also, preserve the `--index-url` and `--extra-index-url` flags in the command in the header.
+/// Install a package from an index that provides relative links and requires authentication
 #[test]
-fn emit_index_urls() -> Result<()> {
+fn install_index_with_relative_links_authenticated() {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("black==23.10.1")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--emit-index-url")
-            .arg("--extra-index-url")
-            .arg("https://test.pypi.org/simple/"), @r###"
+    uv_snapshot!(context.install()
+        .arg("anyio")
+        .arg("--index-url")
+        .arg("https://public:heron@pypi-proxy.fly.dev/basic-auth/relative/simple")
+        .arg("--strict"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --emit-index-url --extra-index-url https://test.pypi.org/simple/
-    --index-url https://pypi.org/simple
-    --extra-index-url https://test.pypi.org/simple/
-
-    black==23.10.1
-    click==8.1.7
-        # via black
-    mypy-extensions==1.0.0
-        # via black
-    packaging==23.2
-        # via black
-    pathspec==0.11.2
-        # via black
-    platformdirs==4.0.0
-        # via black
 
     ----- stderr -----
-    Resolved 6 packages in [TIME]
+    Resolved 3 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 3 packages in [TIME]
+     + anyio==4.3.0
+     + idna==3.6
+     + sniffio==1.3.1
     "###
     );
 
-    Ok(())
+    context.assert_command("import anyio").success();
 }
 
-/// Emit the `--find-links` locations.
+/// The modified time of `site-packages` should change on package installation.
+#[cfg(unix)]
 #[test]
-fn emit_find_links() -> Result<()> {
+fn install_site_packages_mtime_updated() -> Result<()> {
+    use std::os::unix::fs::MetadataExt;
+
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("black==23.10.1")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--emit-find-links")
-            .arg("--find-links")
-            .arg("./"), @r###"
+    let site_packages = context.site_packages();
+
+    // `mtime` is only second-resolution so we include the nanoseconds as well
+    let metadata = site_packages.metadata()?;
+    let pre_mtime = metadata.mtime();
+    let pre_mtime_ns = metadata.mtime_nsec();
+
+    // Install a package.
+    uv_snapshot!(context.install()
+        .arg("anyio")
+        .arg("--strict"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --emit-find-links --find-links ./
-    --find-links ./
-
-    black==23.10.1
-    click==8.1.7
-        # via black
-    mypy-extensions==1.0.0
-        # via black
-    packaging==23.2
-        # via black
-    pathspec==0.11.2
-        # via black
-    platformdirs==4.0.0
-        # via black
 
     ----- stderr -----
-    Resolved 6 packages in [TIME]
+    Resolved 3 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 3 packages in [TIME]
+     + anyio==4.3.0
+     + idna==3.6
+     + sniffio==1.3.1
     "###
     );
 
+    let metadata = site_packages.metadata()?;
+    let post_mtime = metadata.mtime();
+    let post_mtime_ns = metadata.mtime_nsec();
+
+    assert!(
+        (post_mtime, post_mtime_ns) > (pre_mtime, pre_mtime_ns),
+        "Expected newer mtime than {pre_mtime}.{pre_mtime_ns} but got {post_mtime}.{post_mtime_ns}"
+    );
+
     Ok(())
 }
 
-/// Respect the `--no-index` flag in a `requirements.txt` file.
+/// We had a bug where maturin would walk up to the top level gitignore of the cache with a `*`
+/// entry (because we want to ignore the entire cache from outside), ignoring all python source
+/// files.
 #[test]
-fn no_index_requirements_txt() -> Result<()> {
+fn deptry_gitignore() {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("--no-index\ntqdm")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    let source_dist_dir = context
+        .workspace_root
+        .join("scripts/packages/deptry_reproducer");
+
+    uv_snapshot!(context.filters(), context.install()
+        .arg(format!("deptry_reproducer @ {}", source_dist_dir.join("deptry_reproducer-0.1.0.tar.gz").simplified_display()))
+        .arg("--strict")
+        .current_dir(source_dist_dir), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 3 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 3 packages in [TIME]
+     + cffi==1.16.0
+     + deptry-reproducer==0.1.0 (from file://[WORKSPACE]/scripts/packages/deptry_reproducer/deptry_reproducer-0.1.0.tar.gz)
+     + pycparser==2.21
+    "###
+    );
+
+    // Check that we packed the python source files
+    context
+        .assert_command("import deptry_reproducer.foo")
+        .success();
+}
+
+/// Reinstall an installed package with `--no-index`
+#[test]
+fn reinstall_no_index() {
+    let context = TestContext::new("3.12");
+
+    // Install anyio
+    uv_snapshot!(context.install()
+        .arg("anyio")
+        .arg("--strict"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 3 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 3 packages in [TIME]
+     + anyio==4.3.0
+     + idna==3.6
+     + sniffio==1.3.1
+    "###
+    );
+
+    // Install anyio again
+    uv_snapshot!(context.install()
+        .arg("anyio")
+        .arg("--no-index")
+        .arg("--strict"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Audited 1 package in [TIME]
+    "###
+    );
+
+    // Reinstall
+    // We should not consider the already installed package as a source and
+    // should attempt to pull from the index
+    uv_snapshot!(context.install()
+        .arg("anyio")
+        .arg("--no-index")
+        .arg("--reinstall")
+        .arg("--strict"), @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because tqdm was not found in the provided package locations and you
-          require tqdm, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because anyio was not found in the provided package locations and you require anyio, we can conclude that the requirements are unsatisfiable.
 
-          hint: Packages were unavailable because index lookups were disabled
-          and no additional package locations were provided (try: `--find-links
-          <uri>`)
+          hint: Packages were unavailable because index lookups were disabled and no additional package locations were provided (try: `--find-links <uri>`)
     "###
     );
-
-    Ok(())
 }
 
-/// Prefer the `--index-url` from the command line over the `--index-url` in a `requirements.txt`
-/// file. Also, `--index-url` and `--extra-index-url` should not be presented in the output
-/// unless we specify `--emit-index-url`.
 #[test]
-fn index_url_requirements_txt() -> Result<()> {
+fn already_installed_remote_dependencies() {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("--index-url https://google.com\ntqdm")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--index-url")
-            .arg("https://pypi.org/simple"), @r###"
+    // Install anyio's dependencies.
+    uv_snapshot!(context.install()
+        .arg("idna")
+        .arg("sniffio")
+        .arg("--strict"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    tqdm==4.66.1
 
     ----- stderr -----
-    Resolved 1 package in [TIME]
+    Resolved 2 packages in [TIME]
+    Downloaded 2 packages in [TIME]
+    Installed 2 packages in [TIME]
+     + idna==3.6
+     + sniffio==1.3.1
     "###
     );
 
-    Ok(())
+    // Install anyio.
+    uv_snapshot!(context.install()
+        .arg("anyio")
+        .arg("--strict"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 3 packages in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + anyio==4.3.0
+    "###
+    );
 }
 
-/// Raise an error when multiple `requirements.txt` files include `--index-url` flags.
+/// Install an editable package that depends on a previously installed editable package.
 #[test]
-fn conflicting_index_urls_requirements_txt() -> Result<()> {
+fn already_installed_dependent_editable() {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("--index-url https://google.com\ntqdm")?;
+    let root_path = context
+        .workspace_root
+        .join("scripts/packages/dependent_editables");
 
-    let constraints_in = context.temp_dir.child("constraints.in");
-    constraints_in.write_str("--index-url https://wikipedia.org\nflask")?;
+    // Install the first editable
+    uv_snapshot!(context.filters(), context.install()
+        .arg(root_path.join("first_editable")), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--constraint")
-            .arg("constraints.in"), @r###"
-    success: false
-    exit_code: 2
+    ----- stderr -----
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + first-editable==0.0.1 (from file://[WORKSPACE]/scripts/packages/dependent_editables/first_editable)
+    "###
+    );
+
+    // Install the second editable which depends on the first editable
+    // The already installed first editable package should satisfy the requirement
+    uv_snapshot!(context.filters(), context.install()
+        .arg(root_path.join("second_editable"))
+        // Disable the index to guard this test against dependency confusion attacks
+        .arg("--no-index")
+        .arg("--find-links")
+        .arg(BUILD_VENDOR_LINKS_URL), @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    error: Multiple index URLs specified: `https://google.com/` vs.` https://wikipedia.org/
+    Resolved 2 packages in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + second-editable==0.0.1 (from file://[WORKSPACE]/scripts/packages/dependent_editables/second_editable)
     "###
     );
 
-    Ok(())
-}
+    // Request install of the first editable by full path again
+    // We should audit the installed package
+    uv_snapshot!(context.filters(), context.install()
+        .arg(root_path.join("first_editable")), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
 
-/// Resolve without network access via the `--offline` flag.
-#[test]
-fn offline() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("black==23.10.1")?;
+    ----- stderr -----
+    Audited 1 package in [TIME]
+    "###
+    );
 
-    // Resolve with `--offline` with an empty cache.
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--offline"), @r###"
+    // Request reinstallation of the first package during install of the second
+    // It's not available on an index and the user has not specified the path so we fail
+    uv_snapshot!(context.filters(), context.install()
+        .arg(root_path.join("second_editable"))
+        .arg("--reinstall-package")
+        .arg("first-editable")
+        // Disable the index to guard this test against dependency confusion attacks
+        .arg("--no-index")
+        .arg("--find-links")
+        .arg(BUILD_VENDOR_LINKS_URL), @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because black==23.10.1 was not found in the cache and you require
-          black==23.10.1, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because first-editable was not found in the provided package locations and second-editable==0.0.1 depends on first-editable, we can conclude that second-editable==0.0.1 cannot be used.
+          And because only second-editable==0.0.1 is available and you require second-editable, we can conclude that the requirements are unsatisfiable.
+    "###
+    );
+
+    // Request reinstallation of the first package
+    // We include it in the install command with a full path so we should succeed
+    uv_snapshot!(context.filters(), context.install()
+        .arg(root_path.join("first_editable"))
+        .arg("--reinstall-package")
+        .arg("first-editable"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
 
-          hint: Packages were unavailable because the network was disabled
+    ----- stderr -----
+    Resolved 1 package in [TIME]
+    Uninstalled 1 package in [TIME]
+    Installed 1 package in [TIME]
+     - first-editable==0.0.1 (from file://[WORKSPACE]/scripts/packages/dependent_editables/first_editable)
+     + first-editable==0.0.1 (from file://[WORKSPACE]/scripts/packages/dependent_editables/first_editable)
     "###
     );
+}
 
-    // Populate the cache.
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+/// Install a local package that depends on a previously installed local package.
+#[test]
+fn already_installed_local_path_dependent() {
+    let context = TestContext::new("3.12");
+    let root_path = context
+        .workspace_root
+        .join("scripts/packages/dependent_locals");
+
+    // Install the first local
+    uv_snapshot!(context.filters(), context.install()
+        .arg(root_path.join("first_local")), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    black==23.10.1
-    click==8.1.7
-        # via black
-    mypy-extensions==1.0.0
-        # via black
-    packaging==23.2
-        # via black
-    pathspec==0.11.2
-        # via black
-    platformdirs==4.0.0
-        # via black
 
     ----- stderr -----
-    Resolved 6 packages in [TIME]
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + first-local==0.1.0 (from file://[WORKSPACE]/scripts/packages/dependent_locals/first_local)
     "###
     );
 
-    // Resolve with `--offline` with a populated cache.
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--offline"), @r###"
+    // Install the second local which depends on the first local
+    // The already installed first local package should satisfy the requirement
+    uv_snapshot!(context.filters(), context.install()
+        .arg(root_path.join("second_local"))
+        // Disable the index to guard this test against dependency confusion attacks
+        .arg("--no-index")
+        .arg("--find-links")
+        .arg(BUILD_VENDOR_LINKS_URL), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --offline
-    black==23.10.1
-    click==8.1.7
-        # via black
-    mypy-extensions==1.0.0
-        # via black
-    packaging==23.2
-        # via black
-    pathspec==0.11.2
-        # via black
-    platformdirs==4.0.0
-        # via black
 
     ----- stderr -----
-    Resolved 6 packages in [TIME]
+    Resolved 2 packages in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + second-local==0.1.0 (from file://[WORKSPACE]/scripts/packages/dependent_locals/second_local)
     "###
     );
 
-    Ok(())
-}
+    // Request install of the first local by full path again
+    // We should audit the installed package
+    uv_snapshot!(context.filters(), context.install()
+        .arg(root_path.join("first_local")), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
 
-/// Resolve without network access via the `--offline` flag, using `--find-links` for an HTML
-/// registry.
-#[test]
-fn offline_find_links() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("tqdm")?;
+    ----- stderr -----
+    Audited 1 package in [TIME]
+    "###
+    );
 
-    // Resolve with `--offline` and `--find-links`. We indicate that the network was disabled,
-    // since both the `--find-links` and the registry lookups fail (but, importantly, we don't error
-    // when failing to fetch the `--find-links` URL).
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--find-links")
-            .arg("https://download.pytorch.org/whl/torch_stable.html")
-            .arg("--offline"), @r###"
+    // Request reinstallation of the first package during install of the second
+    // It's not available on an index and the user has not specified the path so we fail
+    uv_snapshot!(context.filters(), context.install()
+        .arg(root_path.join("second_local"))
+        .arg("--reinstall-package")
+        .arg("first-local")
+        // Disable the index to guard this test against dependency confusion attacks
+        .arg("--no-index")
+        .arg("--find-links")
+        .arg(BUILD_VENDOR_LINKS_URL), @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because tqdm was not found in the cache and you require tqdm, we can
-          conclude that the requirements are unsatisfiable.
+      ╰─▶ Because first-local was not found in the provided package locations and second-local==0.1.0 depends on first-local, we can conclude that second-local==0.1.0 cannot be used.
+          And because only second-local==0.1.0 is available and you require second-local, we can conclude that the requirements are unsatisfiable.
+    "###
+    );
 
-          hint: Packages were unavailable because the network was disabled
+    // Request reinstallation of the first package
+    // We include it in the install command with a full path so we succeed
+    uv_snapshot!(context.filters(), context.install()
+        .arg(root_path.join("second_local"))
+        .arg(root_path.join("first_local"))
+        .arg("--reinstall-package")
+        .arg("first-local"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 2 packages in [TIME]
+    Uninstalled 1 package in [TIME]
+    Installed 1 package in [TIME]
+     - first-local==0.1.0 (from file://[WORKSPACE]/scripts/packages/dependent_locals/first_local)
+     + first-local==0.1.0 (from file://[WORKSPACE]/scripts/packages/dependent_locals/first_local)
     "###
     );
 
-    // Resolve with `--offline`, `--find-links`, and `--no-index`.
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--find-links")
-            .arg("https://download.pytorch.org/whl/torch_stable.html")
-            .arg("--no-index")
-            .arg("--offline"), @r###"
+    // Request upgrade of the first package
+    // It's not available on an index and the user has not specified the path so we fail
+    uv_snapshot!(context.filters(), context.install()
+        .arg(root_path.join("second_local"))
+        .arg("--upgrade-package")
+        .arg("first-local")
+        // Disable the index to guard this test against dependency confusion attacks
+        .arg("--no-index")
+        .arg("--find-links")
+        .arg(BUILD_VENDOR_LINKS_URL), @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because tqdm was not found in the cache and you require tqdm, we can
-          conclude that the requirements are unsatisfiable.
-
-          hint: Packages were unavailable because the network was disabled
+      ╰─▶ Because first-local was not found in the provided package locations and second-local==0.1.0 depends on first-local, we can conclude that second-local==0.1.0 cannot be used.
+          And because only second-local==0.1.0 is available and you require second-local, we can conclude that the requirements are unsatisfiable.
     "###
     );
 
-    Ok(())
+    // Request upgrade of the first package
+    // A full path is specified and there's nothing to upgrade to so we should just audit
+    uv_snapshot!(context.filters(), context.install()
+        .arg(root_path.join("first_local"))
+        .arg(root_path.join("second_local"))
+        .arg("--upgrade-package")
+        .arg("first-local")
+        // Disable the index to guard this test against dependency confusion attacks
+        .arg("--no-index")
+        .arg("--find-links")
+        .arg(BUILD_VENDOR_LINKS_URL), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 2 packages in [TIME]
+    Audited 2 packages in [TIME]
+    "###
+    );
 }
 
-/// Resolve nested `-r` requirements files with relative paths.
+/// A local version of a package shadowing a remote package is installed.
 #[test]
-fn compile_relative_subfile() -> Result<()> {
+fn already_installed_local_version_of_remote_package() {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("-r subdir/requirements.in")?;
+    let root_path = context.workspace_root.join("scripts/packages");
 
-    let subdir = context.temp_dir.child("subdir");
-    let requirements_in = subdir.child("requirements.in");
-    requirements_in.write_str("-r requirements-dev.in")?;
+    // Install the local anyio first
+    uv_snapshot!(context.filters(), context.install()
+        .arg(root_path.join("anyio_local")), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
 
-    let requirements_dev_in = subdir.child("requirements-dev.in");
-    requirements_dev_in.write_str("anyio")?;
+    ----- stderr -----
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + anyio==4.3.0+foo (from file://[WORKSPACE]/scripts/packages/anyio_local)
+    "###
+    );
 
-    uv_snapshot!(context
-        .compile()
-        .arg("requirements.in"), @r###"
+    // Install again without specifying a local path — this should not pull from the index
+    uv_snapshot!(context.filters(), context.install()
+        .arg("anyio"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    anyio==4.0.0
-    idna==3.4
-        # via anyio
-    sniffio==1.3.0
-        # via anyio
 
     ----- stderr -----
-    Resolved 3 packages in [TIME]
-    "###);
+    Audited 1 package in [TIME]
+    "###
+    );
 
-    Ok(())
-}
+    // Request install with a different version
+    // We should attempt to pull from the index since the installed version does not match
+    // but we disable it here to preserve this dependency for future tests
+    uv_snapshot!(context.filters(), context.install()
+        .arg("anyio==4.2.0")
+        .arg("--no-index"), @r###"
+    success: false
+    exit_code: 1
+    ----- stdout -----
 
-/// Resolve a package with an invalid extra named `.none`.
-#[test]
-fn compile_none_extra() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("entrypoints==0.3")?;
+    ----- stderr -----
+      × No solution found when resolving dependencies:
+      ╰─▶ Because anyio was not found in the provided package locations and you require anyio==4.2.0, we can conclude that the requirements are unsatisfiable.
+
+          hint: Packages were unavailable because index lookups were disabled and no additional package locations were provided (try: `--find-links <uri>`)
+    "###
+    );
 
-    uv_snapshot!(context
-        .compile()
-        .arg("requirements.in"), @r###"
+    // Request reinstallation with the local version segment — this should fail since it is not available
+    // in the index and the path was not provided
+    uv_snapshot!(context.filters(), context.install()
+        .arg("anyio==4.3.0+foo")
+        .arg("--reinstall"), @r###"
+    success: false
+    exit_code: 1
+    ----- stdout -----
+
+    ----- stderr -----
+      × No solution found when resolving dependencies:
+      ╰─▶ Because there is no version of anyio==4.3.0+foo and you require anyio==4.3.0+foo, we can conclude that the requirements are unsatisfiable.
+    "###
+    );
+
+    // Request reinstall with the full path, this should reinstall from the path
+    // and not pull from the index
+    uv_snapshot!(context.filters(), context.install()
+        .arg(root_path.join("anyio_local"))
+        .arg("--reinstall")
+        .arg("anyio"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    entrypoints==0.3
 
     ----- stderr -----
     Resolved 1 package in [TIME]
-    "###);
-
-    Ok(())
-}
+    Uninstalled 1 package in [TIME]
+    Installed 1 package in [TIME]
+     - anyio==4.3.0+foo (from file://[WORKSPACE]/scripts/packages/anyio_local)
+     + anyio==4.3.0+foo (from file://[WORKSPACE]/scripts/packages/anyio_local)
+    "###
+    );
 
-/// Resolve a package (`pytz`) with a preference that omits a trailing zero.
-///
-/// See: <https://github.com/astral-sh/uv/issues/1536>
-#[test]
-fn compile_types_pytz() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("types-pytz")?;
+    // Request reinstallation with just the name, this should pull from the index
+    // and replace the path dependency
+    uv_snapshot!(context.filters(), context.install()
+        .arg("anyio")
+        .arg("--reinstall"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
 
-    let requirements_txt = context.temp_dir.child("requirements.txt");
-    requirements_txt.write_str("types-pytz==2021.1")?;
+    ----- stderr -----
+    Resolved 3 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Uninstalled 1 package in [TIME]
+    Installed 3 packages in [TIME]
+     - anyio==4.3.0+foo (from file://[WORKSPACE]/scripts/packages/anyio_local)
+     + anyio==4.3.0
+     + idna==3.6
+     + sniffio==1.3.1
+    "###
+    );
 
-    uv_snapshot!(context
-        .compile()
-        .arg("requirements.in")
-        .arg("-o")
-        .arg("requirements.txt"), @r###"
+    // Install the local anyio again so we can test upgrades
+    uv_snapshot!(context.filters(), context.install()
+        .arg(root_path.join("anyio_local")), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 1 package in [TIME]
-    "###);
-
-    Ok(())
-}
-
-/// Resolve a package from a `requirements.in` file, with a `constraints.txt` pinning that package
-/// to a specific URL.
-#[test]
-fn compile_constraints_compatible_url() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("anyio>4")?;
-
-    let constraints_txt = context.temp_dir.child("constraints.txt");
-    constraints_txt.write_str("anyio @ https://files.pythonhosted.org/packages/bf/cd/d6d9bb1dadf73e7af02d18225cbd2c93f8552e13130484f1c8dcfece292b/anyio-4.2.0-py3-none-any.whl")?;
+    Uninstalled 1 package in [TIME]
+    Installed 1 package in [TIME]
+     - anyio==4.3.0
+     + anyio==4.3.0+foo (from file://[WORKSPACE]/scripts/packages/anyio_local)
+    "###
+    );
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--constraint")
-            .arg("constraints.txt"), @r###"
+    // Request upgrade with just the name
+    // We shouldn't pull from the index because the local version is "newer"
+    uv_snapshot!(context.filters(), context.install()
+        .arg("anyio")
+        .arg("--upgrade"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --constraint constraints.txt
-    anyio @ https://files.pythonhosted.org/packages/bf/cd/d6d9bb1dadf73e7af02d18225cbd2c93f8552e13130484f1c8dcfece292b/anyio-4.2.0-py3-none-any.whl
-    idna==3.4
-        # via anyio
-    sniffio==1.3.0
-        # via anyio
 
     ----- stderr -----
     Resolved 3 packages in [TIME]
+    Audited 3 packages in [TIME]
     "###
     );
 
-    Ok(())
+    // Install something that depends on anyio
+    // We shouldn't overwrite our local version with the remote anyio here
+    uv_snapshot!(context.filters(), context.install()
+        .arg("httpx"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 7 packages in [TIME]
+    Downloaded 4 packages in [TIME]
+    Installed 4 packages in [TIME]
+     + certifi==2024.2.2
+     + h11==0.14.0
+     + httpcore==1.0.4
+     + httpx==0.27.0
+    "###
+    );
 }
 
-/// Resolve a direct URL package from a `requirements.in` file, with a `constraints.txt` file
-/// pinning it to a specific version.
-#[test]
-fn compile_constraints_compatible_url_version() -> Result<()> {
+/// Install a package with multiple installed distributions in a virtual environment.
+#[test]
+#[cfg(unix)]
+fn already_installed_multiple_versions() -> Result<()> {
+    fn prepare(context: &TestContext) -> Result<()> {
+        use crate::common::copy_dir_all;
+
+        // Install into the base environment
+        context.install().arg("anyio==3.7.0").assert().success();
+
+        // Install another version into another environment
+        let context_duplicate = TestContext::new("3.12");
+        context_duplicate
+            .install()
+            .arg("anyio==4.0.0")
+            .assert()
+            .success();
+
+        // Copy the second version into the first environment
+        copy_dir_all(
+            context_duplicate
+                .site_packages()
+                .join("anyio-4.0.0.dist-info"),
+            context.site_packages().join("anyio-4.0.0.dist-info"),
+        )?;
+
+        Ok(())
+    }
+
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("anyio @ https://files.pythonhosted.org/packages/bf/cd/d6d9bb1dadf73e7af02d18225cbd2c93f8552e13130484f1c8dcfece292b/anyio-4.2.0-py3-none-any.whl")?;
 
-    let constraints_txt = context.temp_dir.child("constraints.txt");
-    constraints_txt.write_str("anyio>4")?;
+    prepare(&context)?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--constraint")
-            .arg("constraints.txt"), @r###"
+    // Request the second anyio version again
+    // Should remove both previous versions and reinstall the second one
+    uv_snapshot!(context.filters(), context.install().arg("anyio==4.0.0"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 3 packages in [TIME]
+    Downloaded 1 package in [TIME]
+    Uninstalled 2 packages in [TIME]
+    Installed 1 package in [TIME]
+     - anyio==3.7.0
+     - anyio==4.0.0
+     + anyio==4.0.0
+    "###
+    );
+
+    // Reset the test context
+    prepare(&context)?;
+
+    // Request the anyio without a version specifier
+    // This is loosely a regression test for the ordering of the installation preferences
+    // from existing site-packages
+    uv_snapshot!(context.filters(), context.install().arg("anyio"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --constraint constraints.txt
-    anyio @ https://files.pythonhosted.org/packages/bf/cd/d6d9bb1dadf73e7af02d18225cbd2c93f8552e13130484f1c8dcfece292b/anyio-4.2.0-py3-none-any.whl
-    idna==3.4
-        # via anyio
-    sniffio==1.3.0
-        # via anyio
 
     ----- stderr -----
     Resolved 3 packages in [TIME]
+    Uninstalled 2 packages in [TIME]
+    Installed 1 package in [TIME]
+     - anyio==3.7.0
+     - anyio==4.0.0
+     + anyio==4.0.0
     "###
     );
 
     Ok(())
 }
 
-/// Resolve a package from a `requirements.in` file, with a `constraints.txt` file pinning it to
-/// a specific URL with an incompatible version.
+/// Install a package from a remote URL
 #[test]
-fn compile_constraints_incompatible_url() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("anyio<4")?;
+#[cfg(feature = "git")]
+fn already_installed_remote_url() {
+    let context = TestContext::new("3.8");
 
-    let constraints_txt = context.temp_dir.child("constraints.txt");
-    constraints_txt.write_str("anyio @ https://files.pythonhosted.org/packages/bf/cd/d6d9bb1dadf73e7af02d18225cbd2c93f8552e13130484f1c8dcfece292b/anyio-4.2.0-py3-none-any.whl")?;
+    // First, install from the remote URL
+    uv_snapshot!(context.filters(), context.install().arg("uv-public-pypackage @ git+https://github.com/astral-test/uv-public-pypackage"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--constraint")
-            .arg("constraints.txt"), @r###"
+    ----- stderr -----
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + uv-public-pypackage==0.1.0 (from git+https://github.com/astral-test/uv-public-pypackage@b270df1a2fb5d012294e9aaf05e7e0bab1e6a389)
+    "###);
+
+    context.assert_installed("uv_public_pypackage", "0.1.0");
+
+    // Request installation again with a different URL, but the same _canonical_ URL. We should
+    // resolve the package (since we installed a specific commit, but are now requesting the default
+    // branch), but not reinstall the package.
+    uv_snapshot!(context.filters(), context.install().arg("uv-public-pypackage @ git+https://github.com/astral-test/uv-public-pypackage.git"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 1 package in [TIME]
+    Audited 1 package in [TIME]
+    "###);
+
+    // Request installation again with a different URL, but the same _canonical_ URL and the same
+    // commit. We should neither resolve nor reinstall the package, since it's already installed
+    // at this precise commit.
+    uv_snapshot!(context.filters(), context.install().arg("uv-public-pypackage @ git+https://github.com/astral-test/uv-public-pypackage.git@b270df1a2fb5d012294e9aaf05e7e0bab1e6a389"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Audited 1 package in [TIME]
+    "###);
+
+    // Request installation again with just the name
+    // We should just audit the URL package since it fulfills this requirement
+    uv_snapshot!(
+        context.install().arg("uv-public-pypackage"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Audited 1 package in [TIME]
+    "###);
+
+    // Request reinstallation
+    // We should fail since the URL was not provided
+    uv_snapshot!(
+        context.install()
+        .arg("uv-public-pypackage")
+        .arg("--no-index")
+        .arg("--reinstall"), @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because only anyio>=4 is available and you require anyio<4, we can
-          conclude that the requirements are unsatisfiable.
-    "###
-    );
+      ╰─▶ Because uv-public-pypackage was not found in the provided package locations and you require uv-public-pypackage, we can conclude that the requirements are unsatisfiable.
 
-    Ok(())
-}
+          hint: Packages were unavailable because index lookups were disabled and no additional package locations were provided (try: `--find-links <uri>`)
+    "###);
 
-/// Resolve a package from a `requirements.in` file, respecting the `--index-url` in a
-/// `requirements.in` file. The resolution should fail, since the package doesn't exist at the
-#[test]
-fn index_url_in_requirements() -> Result<()> {
-    let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("--index-url https://download.pytorch.org/whl\nanyio<4")?;
+    // Request installation again with just the full URL
+    // We should just audit the existing package
+    uv_snapshot!(
+        context.install().arg("uv-public-pypackage @ git+https://github.com/astral-test/uv-public-pypackage"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 1 package in [TIME]
+    Audited 1 package in [TIME]
+    "###);
+
+    // Request reinstallation with the full URL
+    // We should reinstall successfully
+    uv_snapshot!(
+        context.install()
+        .arg("uv-public-pypackage @ git+https://github.com/astral-test/uv-public-pypackage")
+        .arg("--reinstall"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
+    ----- stderr -----
+    Resolved 1 package in [TIME]
+    Uninstalled 1 package in [TIME]
+    Installed 1 package in [TIME]
+     - uv-public-pypackage==0.1.0 (from git+https://github.com/astral-test/uv-public-pypackage@b270df1a2fb5d012294e9aaf05e7e0bab1e6a389)
+     + uv-public-pypackage==0.1.0 (from git+https://github.com/astral-test/uv-public-pypackage@b270df1a2fb5d012294e9aaf05e7e0bab1e6a389)
+    "###);
+
+    // Request installation again with a different version
+    // We should attempt to pull from the index since the local version does not match
+    uv_snapshot!(
+        context.install().arg("uv-public-pypackage==0.2.0").arg("--no-index"), @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because anyio<4 was not found in the package registry and you require
-          anyio<4, we can conclude that the requirements are unsatisfiable.
-    "###
-    );
+      ╰─▶ Because uv-public-pypackage was not found in the provided package locations and you require uv-public-pypackage==0.2.0, we can conclude that the requirements are unsatisfiable.
 
-    Ok(())
+          hint: Packages were unavailable because index lookups were disabled and no additional package locations were provided (try: `--find-links <uri>`)
+    "###);
 }
 
-/// Resolve a package from a `requirements.in` file, respecting the `--index-url` passed via the
-/// command line over that in a `requirements.in` file.
+/// Sync using `--find-links` with a local directory.
 #[test]
-fn index_url_from_command_line() -> Result<()> {
+fn find_links() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("--index-url https://download.pytorch.org/whl\nanyio<4")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--index-url")
-            .arg("https://pypi.org/simple"), @r###"
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str(indoc! {r"
+        tqdm
+    "})?;
+
+    uv_snapshot!(context.filters(), context.install()
+        .arg("tqdm")
+        .arg("--find-links")
+        .arg(context.workspace_root.join("scripts/links/")), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in
-    anyio==3.7.1
-    idna==3.4
-        # via anyio
-    sniffio==1.3.0
-        # via anyio
 
     ----- stderr -----
-    Resolved 3 packages in [TIME]
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + tqdm==1000.0.0
     "###
     );
 
     Ok(())
 }
 
-/// Resolve a package from a `requirements.in` file with a dependency that uses an unsupported
-/// scheme.
+/// Sync using `--find-links` with a local directory, with wheels disabled.
 #[test]
-fn unsupported_scheme() -> Result<()> {
+fn find_links_no_binary() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("anyio @ bzr+https://example.com/anyio")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in"), @r###"
-    success: false
-    exit_code: 2
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str(indoc! {r"
+        tqdm
+    "})?;
+
+    uv_snapshot!(context.filters(), context.install()
+        .arg("tqdm")
+        .arg("--no-binary")
+        .arg(":all:")
+        .arg("--find-links")
+        .arg(context.workspace_root.join("scripts/links/")), @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    error: Unsupported scheme `bzr+https` on URL: bzr+https://example.com/anyio (Bazaar is not supported)
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + tqdm==999.0.0
     "###
     );
 
     Ok(())
 }
 
-/// Resolve a package with `--no-deps`, including a valid extra.
+/// Provide valid hashes for all dependencies with `--require-hashes`.
 #[test]
-fn no_deps_valid_extra() -> Result<()> {
+fn require_hashes() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("flask[dotenv]")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--no-deps"), @r###"
+    // Write to a requirements file.
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str(indoc::indoc! {r"
+        anyio==4.0.0 \
+            --hash=sha256:cfdb2b588b9fc25ede96d8db56ed50848b0b649dca3dd1df0b11f683bb9e0b5f \
+            --hash=sha256:f7ed51751b2c2add651e5747c891b47e26d2a21be5d32d9311dfe9692f3e5d7a
+        idna==3.6 \
+            --hash=sha256:9ecdbbd083b06798ae1e86adcbfe8ab1479cf864e4ee30fe4e46a003d12491ca \
+            --hash=sha256:c05567e9c24a6b9faaa835c4821bad0590fbb9d5779e7caa6e1cc4978e7eb24f
+            # via anyio
+        sniffio==1.3.1 \
+            --hash=sha256:2f6da418d1f1e0fddd844478f41680e794e6051915791a034ff65e5f100525a2 \
+            --hash=sha256:f4324edc670a0f49750a81b895f35c3adb843cca46f0530f79fc1babb23789dc
+            # via anyio
+    "})?;
+
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--require-hashes"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --no-deps
-    flask==3.0.0
 
     ----- stderr -----
-    Resolved 1 package in [TIME]
+    Resolved 3 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 3 packages in [TIME]
+     + anyio==4.0.0
+     + idna==3.6
+     + sniffio==1.3.1
     "###
     );
 
     Ok(())
 }
 
-/// Resolve a package with `--no-deps`, including an invalid extra.
+/// Omit hashes for dependencies with `--require-hashes`, which is allowed with `--no-deps`.
 #[test]
-fn no_deps_invalid_extra() -> Result<()> {
+fn require_hashes_no_deps() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("flask[empty]")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--no-deps"), @r###"
+    // Write to a requirements file.
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str(indoc::indoc! {r"
+        anyio==4.0.0 \
+            --hash=sha256:cfdb2b588b9fc25ede96d8db56ed50848b0b649dca3dd1df0b11f683bb9e0b5f \
+            --hash=sha256:f7ed51751b2c2add651e5747c891b47e26d2a21be5d32d9311dfe9692f3e5d7a
+    "})?;
+
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--no-deps")
+        .arg("--require-hashes"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --no-deps
-    flask==3.0.0
 
     ----- stderr -----
     Resolved 1 package in [TIME]
-    warning: The package `flask==3.0.0` does not have an extra named `empty`.
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + anyio==4.0.0
     "###
     );
 
     Ok(())
 }
 
-/// Resolve an editable package with an invalid extra.
+/// Provide the wrong hash with `--require-hashes`.
 #[test]
-fn editable_invalid_extra() -> Result<()> {
+fn require_hashes_mismatch() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("-e ../../scripts/editable-installs/black_editable[empty]")?;
 
-    let requirements_path = regex::escape(&requirements_in.normalized_display().to_string());
-    let filters: Vec<_> = [
-        (r" file://.*/", " file://[TEMP_DIR]/"),
-        (requirements_path.as_str(), "requirements.in"),
-    ]
-    .into_iter()
-    .chain(INSTA_FILTERS.to_vec())
-    .collect();
-
-    uv_snapshot!(filters, Command::new(get_bin())
-            .arg("pip")
-            .arg("compile")
-            .arg(requirements_in.path())
-            .arg("--cache-dir")
-            .arg(context.cache_dir.path())
-            .arg("--exclude-newer")
-            .arg(EXCLUDE_NEWER)
-            .env("VIRTUAL_ENV", context.venv.as_os_str()), @r###"
-    success: true
-    exit_code: 0
+    // Write to a requirements file.
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str(
+        "anyio==4.0.0 --hash=sha256:afdb2b588b9fc25ede96d8db56ed50848b0b649dca3dd1df0b11f683bb9e0b5f",
+    )?;
+
+    // Raise an error.
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--require-hashes"), @r###"
+    success: false
+    exit_code: 2
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile requirements.in --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z
-    -e ../../scripts/editable-installs/black_editable
 
     ----- stderr -----
-    Built 1 editable in [TIME]
-    Resolved 1 package in [TIME]
-    warning: The package `black @ file://[TEMP_DIR]/black_editable` does not have an extra named `empty`.
-    "###);
+    error: In `--require-hashes` mode, all requirements must be pinned upfront with `==`, but found: `idna`
+    "###
+    );
 
     Ok(())
 }
 
-/// Resolve a package from a `requirements.in` file, with a `constraints.txt` file pinning one of
-/// its transitive dependencies to a specific version.
+/// Omit a transitive dependency in `--require-hashes`.
 #[test]
-fn compile_constraints_compatible_version() -> Result<()> {
+fn require_hashes_missing_dependency() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("virtualenv")?;
 
-    let constraints_txt = context.temp_dir.child("constraints.txt");
-    constraints_txt.write_str("filelock==3.8.0")?;
+    // Write to a requirements file.
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str(
+        "anyio==4.0.0 --hash=sha256:cfdb2b588b9fc25ede96d8db56ed50848b0b649dca3dd1df0b11f683bb9e0b5f",
+    )?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--constraint")
-            .arg("constraints.txt"), @r###"
-    success: true
-    exit_code: 0
+    // Install without error when `--require-hashes` is omitted.
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--require-hashes"), @r###"
+    success: false
+    exit_code: 2
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --constraint constraints.txt
-    distlib==0.3.7
-        # via virtualenv
-    filelock==3.8.0
-        # via virtualenv
-    platformdirs==3.11.0
-        # via virtualenv
-    virtualenv==20.21.1
 
     ----- stderr -----
-    Resolved 4 packages in [TIME]
+    error: In `--require-hashes` mode, all requirements must be pinned upfront with `==`, but found: `idna`
     "###
     );
 
     Ok(())
 }
 
-/// Resolve a package from a `requirements.in` file, with a `constraints.txt` file pinning one of
-/// its direct dependencies to an incompatible version.
+/// We disallow `--require-hashes` for editables' dependencies.
 #[test]
-fn compile_constraints_incompatible_version() -> Result<()> {
+fn require_hashes_editable() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("filelock==1.0.0")?;
 
-    let constraints_txt = context.temp_dir.child("constraints.txt");
-    constraints_txt.write_str("filelock==3.8.0")?;
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str(&indoc::formatdoc! {r"
+        -e file://{workspace_root}/scripts/packages/black_editable[d]
+        ",
+        workspace_root = context.workspace_root.simplified_display(),
+    })?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--constraint")
-            .arg("constraints.txt"), @r###"
+    // Install the editable packages.
+    uv_snapshot!(context.filters(), context.install()
+        .arg("-r")
+        .arg(requirements_txt.path())
+        .arg("--require-hashes"), @r###"
     success: false
-    exit_code: 1
+    exit_code: 2
     ----- stdout -----
 
     ----- stderr -----
-      × No solution found when resolving dependencies:
-      ╰─▶ Because you require filelock==1.0.0 and you require filelock==3.8.0, we
-          can conclude that the requirements are unsatisfiable.
+    Built 1 editable in [TIME]
+    error: In `--require-hashes` mode, all requirements must be pinned upfront with `==`, but found: `aiohttp`
     "###
     );
 
     Ok(())
 }
 
-/// Resolve a package from a `requirements.in` file, with a `constraints.txt` file pinning one of
-/// its direct dependencies to an incompatible version.
+/// If a hash is only included as a constraint, that's not good enough for `--require-hashes`.
 #[test]
-fn conflicting_url_markers() -> Result<()> {
+fn require_hashes_constraint() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("filelock==1.0.0")?;
+
+    // Include the hash in the constraint file.
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str("anyio==4.0.0")?;
 
     let constraints_txt = context.temp_dir.child("constraints.txt");
-    constraints_txt.write_str("filelock==3.8.0")?;
+    constraints_txt.write_str("anyio==4.0.0 --hash=sha256:cfdb2b588b9fc25ede96d8db56ed50848b0b649dca3dd1df0b11f683bb9e0b5f")?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--constraint")
-            .arg("constraints.txt"), @r###"
+    // Install the editable packages.
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg(requirements_txt.path())
+        .arg("--require-hashes")
+        .arg("-c")
+        .arg(constraints_txt.path()), @r###"
     success: false
-    exit_code: 1
+    exit_code: 2
     ----- stdout -----
 
     ----- stderr -----
-      × No solution found when resolving dependencies:
-      ╰─▶ Because you require filelock==1.0.0 and you require filelock==3.8.0, we
-          can conclude that the requirements are unsatisfiable.
+    error: In `--require-hashes` mode, all requirement must have a hash, but none were provided for: anyio==4.0.0
+    "###
+    );
+
+    // Include the hash in the requirements file, but pin the version in the constraint file.
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str(
+        "anyio --hash=sha256:cfdb2b588b9fc25ede96d8db56ed50848b0b649dca3dd1df0b11f683bb9e0b5f",
+    )?;
+
+    let constraints_txt = context.temp_dir.child("constraints.txt");
+    constraints_txt.write_str("anyio==4.0.0")?;
+
+    // Install the editable packages.
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg(requirements_txt.path())
+        .arg("--require-hashes")
+        .arg("-c")
+        .arg(constraints_txt.path()), @r###"
+    success: false
+    exit_code: 2
+    ----- stdout -----
+
+    ----- stderr -----
+    error: In `--require-hashes` mode, all requirement must have their versions pinned with `==`, but found: anyio
     "###
     );
 
     Ok(())
 }
 
-/// Override a regular package with an editable.
-///
-/// At present, this incorrectly resolves to the regular package.
+/// We allow `--require-hashes` for unnamed URL dependencies.
 #[test]
-fn editable_override() -> Result<()> {
+fn require_hashes_unnamed() -> Result<()> {
     let context = TestContext::new("3.12");
 
-    // Add a non-editable requirement.
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("black")?;
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt
+        .write_str(indoc::indoc! {r"
+            https://files.pythonhosted.org/packages/36/55/ad4de788d84a630656ece71059665e01ca793c04294c463fd84132f40fe6/anyio-4.0.0-py3-none-any.whl --hash=sha256:cfdb2b588b9fc25ede96d8db56ed50848b0b649dca3dd1df0b11f683bb9e0b5f
+            idna==3.6 \
+                --hash=sha256:9ecdbbd083b06798ae1e86adcbfe8ab1479cf864e4ee30fe4e46a003d12491ca \
+                --hash=sha256:c05567e9c24a6b9faaa835c4821bad0590fbb9d5779e7caa6e1cc4978e7eb24f
+                # via anyio
+            sniffio==1.3.1 \
+                --hash=sha256:2f6da418d1f1e0fddd844478f41680e794e6051915791a034ff65e5f100525a2 \
+                --hash=sha256:f4324edc670a0f49750a81b895f35c3adb843cca46f0530f79fc1babb23789dc
+                # via anyio
+        "})?;
+
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--require-hashes"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
 
-    // Add an editable override.
-    let overrides_txt = context.temp_dir.child("overrides.txt");
-    overrides_txt.write_str("-e file://../../scripts/editable-installs/black_editable")?;
+    ----- stderr -----
+    Resolved 3 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 3 packages in [TIME]
+     + anyio==4.0.0 (from https://files.pythonhosted.org/packages/36/55/ad4de788d84a630656ece71059665e01ca793c04294c463fd84132f40fe6/anyio-4.0.0-py3-none-any.whl)
+     + idna==3.6
+     + sniffio==1.3.1
+    "###
+    );
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--override")
-            .arg("overrides.txt"), @r###"
+    Ok(())
+}
+
+/// We allow `--require-hashes` for unnamed URL dependencies. In this case, the unnamed URL is
+/// a repeat of a registered package.
+#[test]
+fn require_hashes_unnamed_repeated() -> Result<()> {
+    let context = TestContext::new("3.12");
+
+    // Re-run, but duplicate `anyio`.
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt
+        .write_str(indoc::indoc! {r"
+            anyio==4.0.0 \
+                --hash=sha256:cfdb2b588b9fc25ede96d8db56ed50848b0b649dca3dd1df0b11f683bb9e0b5f \
+                --hash=sha256:f7ed51751b2c2add651e5747c891b47e26d2a21be5d32d9311dfe9692f3e5d7a
+            https://files.pythonhosted.org/packages/36/55/ad4de788d84a630656ece71059665e01ca793c04294c463fd84132f40fe6/anyio-4.0.0-py3-none-any.whl --hash=sha256:cfdb2b588b9fc25ede96d8db56ed50848b0b649dca3dd1df0b11f683bb9e0b5f
+            idna==3.6 \
+                --hash=sha256:9ecdbbd083b06798ae1e86adcbfe8ab1479cf864e4ee30fe4e46a003d12491ca \
+                --hash=sha256:c05567e9c24a6b9faaa835c4821bad0590fbb9d5779e7caa6e1cc4978e7eb24f
+                # via anyio
+            sniffio==1.3.1 \
+                --hash=sha256:2f6da418d1f1e0fddd844478f41680e794e6051915791a034ff65e5f100525a2 \
+                --hash=sha256:f4324edc670a0f49750a81b895f35c3adb843cca46f0530f79fc1babb23789dc
+                # via anyio
+        "})?;
+
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg("requirements.txt")
+        .arg("--require-hashes"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --override overrides.txt
-    black==23.11.0
-    click==8.1.7
-        # via black
-    mypy-extensions==1.0.0
-        # via black
-    packaging==23.2
-        # via black
-    pathspec==0.11.2
-        # via black
-    platformdirs==4.0.0
-        # via black
 
     ----- stderr -----
-    Resolved 6 packages in [TIME]
+    Resolved 3 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 3 packages in [TIME]
+     + anyio==4.0.0 (from https://files.pythonhosted.org/packages/36/55/ad4de788d84a630656ece71059665e01ca793c04294c463fd84132f40fe6/anyio-4.0.0-py3-none-any.whl)
+     + idna==3.6
+     + sniffio==1.3.1
     "###
     );
 
     Ok(())
 }
 
-/// Override an editable with a regular package.
+/// If a hash is only included as a override, that's not good enough for `--require-hashes`.
 ///
-/// At present, this incorrectly resolves to the editable.
+/// TODO(charlie): This _should_ be allowed. It's a bug.
 #[test]
-fn override_editable() -> Result<()> {
+fn require_hashes_override() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("-e ../../scripts/editable-installs/black_editable")?;
+
+    // Include the hash in the override file.
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str("anyio==4.0.0")?;
 
     let overrides_txt = context.temp_dir.child("overrides.txt");
-    overrides_txt.write_str("black==23.10.1")?;
+    overrides_txt.write_str("anyio==4.0.0 --hash=sha256:cfdb2b588b9fc25ede96d8db56ed50848b0b649dca3dd1df0b11f683bb9e0b5f")?;
 
-    let requirements_path = regex::escape(&requirements_in.normalized_display().to_string());
-    let overrides_path = regex::escape(&overrides_txt.normalized_display().to_string());
-    let filters: Vec<_> = [
-        (requirements_path.as_str(), "requirements.in"),
-        (overrides_path.as_str(), "overrides.txt"),
-    ]
-    .into_iter()
-    .chain(INSTA_FILTERS.to_vec())
-    .collect();
-
-    uv_snapshot!(filters, Command::new(get_bin())
-            .arg("pip")
-            .arg("compile")
-            .arg(requirements_in.path())
-            .arg("--override")
-            .arg(overrides_txt.path())
-            .arg("--cache-dir")
-            .arg(context.cache_dir.path())
-            .arg("--exclude-newer")
-            .arg(EXCLUDE_NEWER)
-            .env("VIRTUAL_ENV", context.venv.as_os_str()), @r###"
-    success: true
-    exit_code: 0
+    // Install the editable packages.
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg(requirements_txt.path())
+        .arg("--require-hashes")
+        .arg("--override")
+        .arg(overrides_txt.path()), @r###"
+    success: false
+    exit_code: 2
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile requirements.in --override overrides.txt --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z
-    -e ../../scripts/editable-installs/black_editable
 
     ----- stderr -----
-    Built 1 editable in [TIME]
-    Resolved 1 package in [TIME]
-    "###);
+    error: In `--require-hashes` mode, all requirement must have a hash, but none were provided for: anyio==4.0.0
+    "###
+    );
+
+    // Include the hash in the requirements file, but pin the version in the override file.
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str(
+        "anyio --hash=sha256:cfdb2b588b9fc25ede96d8db56ed50848b0b649dca3dd1df0b11f683bb9e0b5f",
+    )?;
+
+    let overrides_txt = context.temp_dir.child("overrides.txt");
+    overrides_txt.write_str("anyio==4.0.0")?;
+
+    // Install the editable packages.
+    uv_snapshot!(context.install()
+        .arg("-r")
+        .arg(requirements_txt.path())
+        .arg("--require-hashes")
+        .arg("--override")
+        .arg(overrides_txt.path()), @r###"
+    success: false
+    exit_code: 2
+    ----- stdout -----
+
+    ----- stderr -----
+    error: In `--require-hashes` mode, all requirement must have their versions pinned with `==`, but found: anyio
+    "###
+    );
 
     Ok(())
 }
 
-/// Resolve a package with both a constraint _and_ an override. The override and the constraint are
-/// compatible, but resolve to exactly the same version.
 #[test]
-fn override_with_compatible_constraint() -> Result<()> {
+fn tool_uv_sources() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("anyio")?;
+    // Use a subdir to test path normalization.
+    let require_path = "some_dir/pyproject.toml";
+    let pyproject_toml = context.temp_dir.child(require_path);
+    pyproject_toml.write_str(indoc! {r#"
+        [project]
+        name = "foo"
+        version = "0.0.0"
+        dependencies = [
+          "tqdm>4,<=5",
+          "packaging @ git+https://github.com/pypa/packaging@32deafe8668a2130a3366b98154914d188f3718e",
+          "poetry_editable",
+          "urllib3 @ https://files.pythonhosted.org/packages/a2/73/a68704750a7679d0b6d3ad7aa8d4da8e14e151ae82e6fee774e6e0d05ec8/urllib3-2.2.1-py3-none-any.whl",
+          # Windows consistency
+          "colorama>0.4,<5",
+        ]
 
-    let constraints_txt = context.temp_dir.child("constraints.txt");
-    constraints_txt.write_str("anyio<=3.0.0")?;
+        [project.optional-dependencies]
+        utils = [
+            "boltons==24.0.0"
+        ]
+        dont_install_me = [
+            "broken @ https://example.org/does/not/exist"
+        ]
 
-    let overrides_txt = context.temp_dir.child("overrides.txt");
-    overrides_txt.write_str("anyio>=3.0.0")?;
+        [tool.uv.sources]
+        tqdm = { url = "https://files.pythonhosted.org/packages/a5/d6/502a859bac4ad5e274255576cd3e15ca273cdb91731bc39fb840dd422ee9/tqdm-4.66.0-py3-none-any.whl" }
+        boltons = { git = "https://github.com/mahmoud/boltons", rev = "57fbaa9b673ed85b32458b31baeeae230520e4a0" }
+        poetry_editable = { path = "../poetry_editable", editable = true }
+    "#})?;
+
+    let project_root = fs_err::canonicalize(std::env::current_dir()?.join("../.."))?;
+    fs_err::create_dir_all(context.temp_dir.join("poetry_editable/poetry_editable"))?;
+    fs_err::copy(
+        project_root.join("scripts/packages/poetry_editable/pyproject.toml"),
+        context.temp_dir.join("poetry_editable/pyproject.toml"),
+    )?;
+    fs_err::copy(
+        project_root.join("scripts/packages/poetry_editable/poetry_editable/__init__.py"),
+        context
+            .temp_dir
+            .join("poetry_editable/poetry_editable/__init__.py"),
+    )?;
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--constraint")
-            .arg("constraints.txt")
-            .arg("--override")
-            .arg("overrides.txt"), @r###"
+    // Install the editable packages.
+    uv_snapshot!(context.filters(), windows_filters=false, context.install()
+        .arg("--preview")
+        .arg("-r")
+        .arg(require_path)
+        .arg("--extra")
+        .arg("utils"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
-    # This file was autogenerated by uv via the following command:
-    #    uv pip compile --cache-dir [CACHE_DIR] --exclude-newer 2023-11-18T12:00:00Z requirements.in --constraint constraints.txt --override overrides.txt
-    anyio==3.0.0
-    idna==3.4
-        # via anyio
-    sniffio==1.3.0
-        # via anyio
 
     ----- stderr -----
-    Resolved 3 packages in [TIME]
+    Built 1 editable in [TIME]
+    Resolved 9 packages in [TIME]
+    Downloaded 8 packages in [TIME]
+    Installed 9 packages in [TIME]
+     + anyio==4.3.0
+     + boltons==24.0.1.dev0 (from git+https://github.com/mahmoud/boltons@57fbaa9b673ed85b32458b31baeeae230520e4a0)
+     + colorama==0.4.6
+     + idna==3.6
+     + packaging==24.1.dev0 (from git+https://github.com/pypa/packaging@32deafe8668a2130a3366b98154914d188f3718e)
+     + poetry-editable==0.1.0 (from file://[TEMP_DIR]/poetry_editable)
+     + sniffio==1.3.1
+     + tqdm==4.66.0 (from https://files.pythonhosted.org/packages/a5/d6/502a859bac4ad5e274255576cd3e15ca273cdb91731bc39fb840dd422ee9/tqdm-4.66.0-py3-none-any.whl)
+     + urllib3==2.2.1 (from https://files.pythonhosted.org/packages/a2/73/a68704750a7679d0b6d3ad7aa8d4da8e14e151ae82e6fee774e6e0d05ec8/urllib3-2.2.1-py3-none-any.whl)
     "###
     );
 
+    // Re-install the editable packages.
+    uv_snapshot!(context.filters(), windows_filters=false, context.install()
+        .arg("--preview")
+        .arg("-r")
+        .arg(require_path)
+        .arg("--extra")
+        .arg("utils"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Audited 6 packages in [TIME]
+    "###
+    );
     Ok(())
 }
 
-/// Resolve a package with both a constraint _and_ an override. The override and the constraint are
-/// incompatible, and so should error. (The correctness of this behavior is subject to debate.)
 #[test]
-fn override_with_incompatible_constraint() -> Result<()> {
+fn tool_uv_sources_is_in_preview() -> Result<()> {
     let context = TestContext::new("3.12");
-    let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("anyio")?;
-
-    let constraints_txt = context.temp_dir.child("constraints.txt");
-    constraints_txt.write_str("anyio<3.0.0")?;
-
-    let overrides_txt = context.temp_dir.child("overrides.txt");
-    overrides_txt.write_str("anyio>=3.0.0")?;
+    let pyproject_toml = context.temp_dir.child("pyproject.toml");
+    pyproject_toml.write_str(indoc! {r#"
+        [project]
+        name = "foo"
+        version = "0.0.0"
+        dependencies = [
+          "tqdm>4,<=5",
+        ]
 
-    uv_snapshot!(context.compile()
-            .arg("requirements.in")
-            .arg("--constraint")
-            .arg("constraints.txt")
-            .arg("--override")
-            .arg("overrides.txt"), @r###"
+        [tool.uv.sources]
+        tqdm = { url = "https://files.pythonhosted.org/packages/a5/d6/502a859bac4ad5e274255576cd3e15ca273cdb91731bc39fb840dd422ee9/tqdm-4.66.0-py3-none-any.whl" }
+    "#})?;
+
+    // Install the editable packages.
+    uv_snapshot!(context.filters(), windows_filters=false, context.install()
+        .arg("-r")
+        .arg("pyproject.toml")
+        .arg("--extra")
+        .arg("utils"), @r###"
     success: false
-    exit_code: 1
+    exit_code: 2
     ----- stdout -----
 
     ----- stderr -----
-      × No solution found when resolving dependencies:
-      ╰─▶ Because you require anyio>=3.0.0 and you require anyio<3.0.0, we can
-          conclude that the requirements are unsatisfiable.
+    error: Failed to parse `pyproject.toml`
+      Caused by: Failed to parse entry for: `tqdm`
+      Caused by: `tool.uv.sources` is a preview feature; use `--preview` or set `UV_PREVIEW=1` to enable it
     "###
     );
 
     Ok(())
 }
```

### Comparing `uv-0.1.9/crates/uv/tests/pip_compile_scenarios.rs` & `uv-0.2.0/crates/uv/tests/pip_compile_scenarios.rs`

 * *Files 12% similar despite different names*

```diff
@@ -1,482 +1,474 @@
 //! DO NOT EDIT
 //!
-//! Generated with ./scripts/scenarios/update.py
-//! Scenarios from <https://github.com/zanieb/packse/tree/de0bab473eeaa4445db5a8febd732c655fad3d52/scenarios>
+//! Generated with `./scripts/sync_scenarios.sh`
+//! Scenarios from <https://github.com/astral-sh/packse/tree/0.3.15/scenarios>
 //!
-#![cfg(all(feature = "python", feature = "pypi"))]
+#![cfg(all(feature = "python", feature = "pypi", unix))]
 
 use std::env;
 use std::process::Command;
 
 use anyhow::Result;
 use assert_cmd::assert::OutputAssertExt;
 use assert_fs::fixture::{FileWriteStr, PathChild};
 use predicates::prelude::predicate;
 
-use common::{create_bin_with_executables, get_bin, uv_snapshot, TestContext, INSTA_FILTERS};
+use common::{get_bin, python_path_with_versions, uv_snapshot, TestContext};
 
 mod common;
 
 /// Provision python binaries and return a `pip compile` command with options shared across all scenarios.
 fn command(context: &TestContext, python_versions: &[&str]) -> Command {
-    let bin = create_bin_with_executables(&context.temp_dir, python_versions)
-        .expect("Failed to create bin dir");
+    let python_path = python_path_with_versions(&context.temp_dir, python_versions)
+        .expect("Failed to create Python test path");
     let mut command = Command::new(get_bin());
     command
         .arg("pip")
         .arg("compile")
         .arg("requirements.in")
         .arg("--index-url")
-        .arg("https://test.pypi.org/simple")
+        .arg("https://astral-sh.github.io/packse/0.3.15/simple-html/")
         .arg("--find-links")
-        .arg("https://raw.githubusercontent.com/zanieb/packse/de0bab473eeaa4445db5a8febd732c655fad3d52/vendor/links.html")
+        .arg("https://raw.githubusercontent.com/astral-sh/packse/0.3.15/vendor/links.html")
         .arg("--cache-dir")
         .arg(context.cache_dir.path())
         .env("VIRTUAL_ENV", context.venv.as_os_str())
         .env("UV_NO_WRAP", "1")
-        .env("UV_TEST_PYTHON_PATH", bin)
+        .env("UV_TEST_PYTHON_PATH", python_path)
         .current_dir(&context.temp_dir);
 
     if cfg!(all(windows, debug_assertions)) {
         // TODO(konstin): Reduce stack usage in debug mode enough that the tests pass with the
         // default windows stack of 1MB
         command.env("UV_STACK_SIZE", (8 * 1024 * 1024).to_string());
     }
 
     command
 }
 
-/// requires-incompatible-python-version-compatible-override
-///
 /// The user requires a package which requires a Python version greater than the
 /// current version, but they use an alternative Python version for package
 /// resolution.
 ///
 /// ```text
-/// 3f4ac9b2
+/// incompatible-python-compatible-override
 /// ├── environment
 /// │   └── python3.9
 /// ├── root
 /// │   └── requires a==1.0.0
 /// │       └── satisfied by a-1.0.0
 /// └── a
 ///     └── a-1.0.0
 ///         └── requires python>=3.10 (incompatible with environment)
 /// ```
 #[test]
-fn requires_incompatible_python_version_compatible_override() -> Result<()> {
+fn incompatible_python_compatible_override() -> Result<()> {
     let context = TestContext::new("3.9");
     let python_versions = &[];
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-3f4ac9b2", "albatross"));
-    filters.push((r"-3f4ac9b2", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"incompatible-python-compatible-override-", "package-"));
 
     let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("a-3f4ac9b2==1.0.0")?;
+    requirements_in.write_str("incompatible-python-compatible-override-a==1.0.0")?;
 
     let output = uv_snapshot!(filters, command(&context, python_versions)
         .arg("--python-version=3.11")
         , @r###"
                  success: true
                  exit_code: 0
                  ----- stdout -----
                  # This file was autogenerated by uv via the following command:
-                 #    uv pip compile requirements.in --find-links https://raw.githubusercontent.com/zanieb/packse/de0bab473eeaa4445db5a8febd732c655fad3d52/vendor/links.html --cache-dir [CACHE_DIR] --python-version=3.11
-                 albatross==1.0.0
+                 #    uv pip compile requirements.in --cache-dir [CACHE_DIR] --python-version=3.11
+                 package-a==1.0.0
+                     # via -r requirements.in
 
                  ----- stderr -----
-                 warning: The requested Python version 3.11 is not available; 3.9.18 will be used to build dependencies instead.
+                 warning: The requested Python version 3.11 is not available; 3.9.[X] will be used to build dependencies instead.
                  Resolved 1 package in [TIME]
                  "###
     );
 
-    output
-        .assert()
-        .success()
-        .stdout(predicate::str::contains("a-3f4ac9b2==1.0.0"));
+    output.assert().success().stdout(predicate::str::contains(
+        "incompatible-python-compatible-override-a==1.0.0",
+    ));
 
     Ok(())
 }
 
-/// requires-compatible-python-version-incompatible-override
-///
 /// The user requires a package which requires a compatible Python version, but they
 /// request an incompatible Python version for package resolution.
 ///
 /// ```text
-/// fd6db412
+/// compatible-python-incompatible-override
 /// ├── environment
 /// │   └── python3.11
 /// ├── root
 /// │   └── requires a==1.0.0
 /// │       └── satisfied by a-1.0.0
 /// └── a
 ///     └── a-1.0.0
 ///         └── requires python>=3.10
 /// ```
 #[test]
-fn requires_compatible_python_version_incompatible_override() -> Result<()> {
+fn compatible_python_incompatible_override() -> Result<()> {
     let context = TestContext::new("3.11");
     let python_versions = &[];
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-fd6db412", "albatross"));
-    filters.push((r"-fd6db412", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"compatible-python-incompatible-override-", "package-"));
 
     let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("a-fd6db412==1.0.0")?;
+    requirements_in.write_str("compatible-python-incompatible-override-a==1.0.0")?;
 
     let output = uv_snapshot!(filters, command(&context, python_versions)
         .arg("--python-version=3.9")
         , @r###"
                  success: false
                  exit_code: 1
                  ----- stdout -----
 
                  ----- stderr -----
-                 warning: The requested Python version 3.9 is not available; 3.11.7 will be used to build dependencies instead.
+                 warning: The requested Python version 3.9 is not available; 3.11.[X] will be used to build dependencies instead.
                    × No solution found when resolving dependencies:
-                   ╰─▶ Because the requested Python version (3.9) does not satisfy Python>=3.10 and albatross==1.0.0 depends on Python>=3.10, we can conclude that albatross==1.0.0 cannot be used.
-                       And because you require albatross==1.0.0, we can conclude that the requirements are unsatisfiable.
+                   ╰─▶ Because the requested Python version (3.9) does not satisfy Python>=3.10 and package-a==1.0.0 depends on Python>=3.10, we can conclude that package-a==1.0.0 cannot be used.
+                       And because you require package-a==1.0.0, we can conclude that the requirements are unsatisfiable.
                  "###
     );
 
     output.assert().failure();
 
     Ok(())
 }
 
-/// requires-incompatible-python-version-compatible-override-no-wheels
-///
 /// The user requires a package which requires a incompatible Python version, but
 /// they request a compatible Python version for package resolution. There are only
 /// source distributions available for the package.
 ///
 /// ```text
-/// 3521037f
+/// incompatible-python-compatible-override-unavailable-no-wheels
 /// ├── environment
 /// │   └── python3.9
 /// ├── root
 /// │   └── requires a==1.0.0
 /// │       └── satisfied by a-1.0.0
 /// └── a
 ///     └── a-1.0.0
 ///         └── requires python>=3.10 (incompatible with environment)
 /// ```
 #[test]
-fn requires_incompatible_python_version_compatible_override_no_wheels() -> Result<()> {
+fn incompatible_python_compatible_override_unavailable_no_wheels() -> Result<()> {
     let context = TestContext::new("3.9");
     let python_versions = &[];
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-3521037f", "albatross"));
-    filters.push((r"-3521037f", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((
+        r"incompatible-python-compatible-override-unavailable-no-wheels-",
+        "package-",
+    ));
 
     let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("a-3521037f==1.0.0")?;
+    requirements_in
+        .write_str("incompatible-python-compatible-override-unavailable-no-wheels-a==1.0.0")?;
 
     // Since there are no wheels for the package and it is not compatible with the
     // local installation, we cannot build the source distribution to determine its
     // dependencies.
     let output = uv_snapshot!(filters, command(&context, python_versions)
         .arg("--python-version=3.11")
         , @r###"
                  success: false
                  exit_code: 1
                  ----- stdout -----
 
                  ----- stderr -----
-                 warning: The requested Python version 3.11 is not available; 3.9.18 will be used to build dependencies instead.
+                 warning: The requested Python version 3.11 is not available; 3.9.[X] will be used to build dependencies instead.
                    × No solution found when resolving dependencies:
-                   ╰─▶ Because the current Python version (3.9.18) does not satisfy Python>=3.10 and albatross==1.0.0 depends on Python>=3.10, we can conclude that albatross==1.0.0 cannot be used.
-                       And because you require albatross==1.0.0, we can conclude that the requirements are unsatisfiable.
+                   ╰─▶ Because the current Python version (3.9.[X]) does not satisfy Python>=3.10 and package-a==1.0.0 depends on Python>=3.10, we can conclude that package-a==1.0.0 cannot be used.
+                       And because you require package-a==1.0.0, we can conclude that the requirements are unsatisfiable.
                  "###
     );
 
     output.assert().failure();
 
     Ok(())
 }
 
-/// requires-incompatible-python-version-compatible-override-no-wheels-available-system
-///
 /// The user requires a package which requires a incompatible Python version, but
 /// they request a compatible Python version for package resolution. There are only
 /// source distributions available for the package. The user has a compatible Python
 /// version installed elsewhere on their system.
 ///
 /// ```text
-/// c68bcf5c
+/// incompatible-python-compatible-override-available-no-wheels
 /// ├── environment
 /// │   ├── python3.11
 /// │   └── python3.9 (active)
 /// ├── root
 /// │   └── requires a==1.0.0
 /// │       └── satisfied by a-1.0.0
 /// └── a
 ///     └── a-1.0.0
 ///         └── requires python>=3.10 (incompatible with environment)
 /// ```
 #[test]
-fn requires_incompatible_python_version_compatible_override_no_wheels_available_system(
-) -> Result<()> {
+fn incompatible_python_compatible_override_available_no_wheels() -> Result<()> {
     let context = TestContext::new("3.9");
     let python_versions = &["3.11"];
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-c68bcf5c", "albatross"));
-    filters.push((r"-c68bcf5c", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((
+        r"incompatible-python-compatible-override-available-no-wheels-",
+        "package-",
+    ));
 
     let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("a-c68bcf5c==1.0.0")?;
+    requirements_in
+        .write_str("incompatible-python-compatible-override-available-no-wheels-a==1.0.0")?;
 
     // Since there is a compatible Python version available on the system, it should be
     // used to build the source distributions.
     let output = uv_snapshot!(filters, command(&context, python_versions)
         .arg("--python-version=3.11")
         , @r###"
                  success: true
                  exit_code: 0
                  ----- stdout -----
                  # This file was autogenerated by uv via the following command:
-                 #    uv pip compile requirements.in --find-links https://raw.githubusercontent.com/zanieb/packse/de0bab473eeaa4445db5a8febd732c655fad3d52/vendor/links.html --cache-dir [CACHE_DIR] --python-version=3.11
-                 albatross==1.0.0
+                 #    uv pip compile requirements.in --cache-dir [CACHE_DIR] --python-version=3.11
+                 package-a==1.0.0
+                     # via -r requirements.in
 
                  ----- stderr -----
                  Resolved 1 package in [TIME]
                  "###
     );
 
-    output
-        .assert()
-        .success()
-        .stdout(predicate::str::contains("a-c68bcf5c==1.0.0"));
+    output.assert().success().stdout(predicate::str::contains(
+        "incompatible-python-compatible-override-available-no-wheels-a==1.0.0",
+    ));
 
     Ok(())
 }
 
-/// requires-incompatible-python-version-compatible-override-no-compatible-wheels
-///
 /// The user requires a package which requires a incompatible Python version, but
 /// they request a compatible Python version for package resolution. There is a
 /// wheel available for the package, but it does not have a compatible tag.
 ///
 /// ```text
-/// d7b25a2d
+/// incompatible-python-compatible-override-no-compatible-wheels
 /// ├── environment
 /// │   └── python3.9
 /// ├── root
 /// │   └── requires a==1.0.0
 /// │       └── satisfied by a-1.0.0
 /// └── a
 ///     └── a-1.0.0
 ///         └── requires python>=3.10 (incompatible with environment)
 /// ```
 #[test]
-fn requires_incompatible_python_version_compatible_override_no_compatible_wheels() -> Result<()> {
+fn incompatible_python_compatible_override_no_compatible_wheels() -> Result<()> {
     let context = TestContext::new("3.9");
     let python_versions = &[];
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-d7b25a2d", "albatross"));
-    filters.push((r"-d7b25a2d", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((
+        r"incompatible-python-compatible-override-no-compatible-wheels-",
+        "package-",
+    ));
 
     let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("a-d7b25a2d==1.0.0")?;
+    requirements_in
+        .write_str("incompatible-python-compatible-override-no-compatible-wheels-a==1.0.0")?;
 
     // Since there are no compatible wheels for the package and it is not compatible
     // with the local installation, we cannot build the source distribution to
     // determine its dependencies.
     let output = uv_snapshot!(filters, command(&context, python_versions)
         .arg("--python-version=3.11")
         , @r###"
                  success: false
                  exit_code: 1
                  ----- stdout -----
 
                  ----- stderr -----
-                 warning: The requested Python version 3.11 is not available; 3.9.18 will be used to build dependencies instead.
+                 warning: The requested Python version 3.11 is not available; 3.9.[X] will be used to build dependencies instead.
                    × No solution found when resolving dependencies:
-                   ╰─▶ Because the current Python version (3.9.18) does not satisfy Python>=3.10 and albatross==1.0.0 depends on Python>=3.10, we can conclude that albatross==1.0.0 cannot be used.
-                       And because you require albatross==1.0.0, we can conclude that the requirements are unsatisfiable.
+                   ╰─▶ Because the current Python version (3.9.[X]) does not satisfy Python>=3.10 and package-a==1.0.0 depends on Python>=3.10, we can conclude that package-a==1.0.0 cannot be used.
+                       And because you require package-a==1.0.0, we can conclude that the requirements are unsatisfiable.
                  "###
     );
 
     output.assert().failure();
 
     Ok(())
 }
 
-/// requires-incompatible-python-version-compatible-override-other-wheel
-///
 /// The user requires a package which requires a incompatible Python version, but
 /// they request a compatible Python version for package resolution. There are only
 /// source distributions available for the compatible version of the package, but
 /// there is an incompatible version with a wheel available.
 ///
 /// ```text
-/// a9179f0c
+/// incompatible-python-compatible-override-other-wheel
 /// ├── environment
 /// │   └── python3.9
 /// ├── root
 /// │   └── requires a
 /// │       ├── satisfied by a-1.0.0
 /// │       └── satisfied by a-2.0.0
 /// └── a
 ///     ├── a-1.0.0
 ///     │   └── requires python>=3.10 (incompatible with environment)
 ///     └── a-2.0.0
 ///         └── requires python>=3.12 (incompatible with environment)
 /// ```
 #[test]
-fn requires_incompatible_python_version_compatible_override_other_wheel() -> Result<()> {
+fn incompatible_python_compatible_override_other_wheel() -> Result<()> {
     let context = TestContext::new("3.9");
     let python_versions = &[];
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-a9179f0c", "albatross"));
-    filters.push((r"-a9179f0c", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((
+        r"incompatible-python-compatible-override-other-wheel-",
+        "package-",
+    ));
 
     let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("a-a9179f0c")?;
+    requirements_in.write_str("incompatible-python-compatible-override-other-wheel-a")?;
 
     // Since there are no wheels for the version of the package compatible with the
     // target and it is not compatible with the local installation, we cannot build the
     // source distribution to determine its dependencies. The other version has wheels
     // available, but is not compatible with the target version and cannot be used.
     let output = uv_snapshot!(filters, command(&context, python_versions)
         .arg("--python-version=3.11")
         , @r###"
                  success: false
                  exit_code: 1
                  ----- stdout -----
 
                  ----- stderr -----
-                 warning: The requested Python version 3.11 is not available; 3.9.18 will be used to build dependencies instead.
+                 warning: The requested Python version 3.11 is not available; 3.9.[X] will be used to build dependencies instead.
                    × No solution found when resolving dependencies:
-                   ╰─▶ Because the current Python version (3.9.18) does not satisfy Python>=3.10 and albatross==1.0.0 depends on Python>=3.10, we can conclude that albatross==1.0.0 cannot be used.
-                       And because only the following versions of albatross are available:
-                           albatross==1.0.0
-                           albatross==2.0.0
-                       we can conclude that albatross<2.0.0 cannot be used. (1)
-
-                       Because the requested Python version (3.11) does not satisfy Python>=3.12 and albatross==2.0.0 depends on Python>=3.12, we can conclude that albatross==2.0.0 cannot be used.
-                       And because we know from (1) that albatross<2.0.0 cannot be used, we can conclude that all versions of albatross cannot be used.
-                       And because you require albatross, we can conclude that the requirements are unsatisfiable.
+                   ╰─▶ Because the current Python version (3.9.[X]) does not satisfy Python>=3.10 and package-a==1.0.0 depends on Python>=3.10, we can conclude that package-a==1.0.0 cannot be used.
+                       And because only the following versions of package-a are available:
+                           package-a==1.0.0
+                           package-a==2.0.0
+                       we can conclude that package-a<2.0.0 cannot be used. (1)
+
+                       Because the requested Python version (3.11) does not satisfy Python>=3.12 and package-a==2.0.0 depends on Python>=3.12, we can conclude that package-a==2.0.0 cannot be used.
+                       And because we know from (1) that package-a<2.0.0 cannot be used, we can conclude that all versions of package-a cannot be used.
+                       And because you require package-a, we can conclude that the requirements are unsatisfiable.
                  "###
     );
 
     output.assert().failure();
 
     Ok(())
 }
 
-/// requires-python-patch-version-override-no-patch
-///
 /// The user requires a package which requires a Python version with a patch version
 /// and the user provides a target version without a patch version.
 ///
 /// ```text
-/// e1884826
+/// python-patch-override-no-patch
 /// ├── environment
 /// │   └── python3.8.18
 /// ├── root
 /// │   └── requires a==1.0.0
 /// │       └── satisfied by a-1.0.0
 /// └── a
 ///     └── a-1.0.0
 ///         └── requires python>=3.8.4
 /// ```
+#[cfg(feature = "python-patch")]
 #[test]
-fn requires_python_patch_version_override_no_patch() -> Result<()> {
+fn python_patch_override_no_patch() -> Result<()> {
     let context = TestContext::new("3.8.18");
     let python_versions = &[];
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-e1884826", "albatross"));
-    filters.push((r"-e1884826", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"python-patch-override-no-patch-", "package-"));
 
     let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("a-e1884826==1.0.0")?;
+    requirements_in.write_str("python-patch-override-no-patch-a==1.0.0")?;
 
     // Since the resolver is asked to solve with 3.8, the minimum compatible Python
     // requirement is treated as 3.8.0.
     let output = uv_snapshot!(filters, command(&context, python_versions)
         .arg("--python-version=3.8")
         , @r###"
                  success: false
                  exit_code: 1
                  ----- stdout -----
 
                  ----- stderr -----
                    × No solution found when resolving dependencies:
-                   ╰─▶ Because the requested Python version (3.8) does not satisfy Python>=3.8.4 and albatross==1.0.0 depends on Python>=3.8.4, we can conclude that albatross==1.0.0 cannot be used.
-                       And because you require albatross==1.0.0, we can conclude that the requirements are unsatisfiable.
+                   ╰─▶ Because the requested Python version (3.8) does not satisfy Python>=3.8.4 and package-a==1.0.0 depends on Python>=3.8.4, we can conclude that package-a==1.0.0 cannot be used.
+                       And because you require package-a==1.0.0, we can conclude that the requirements are unsatisfiable.
                  "###
     );
 
     output.assert().failure();
 
     Ok(())
 }
 
-/// requires-python-patch-version-override-patch-compatible
-///
 /// The user requires a package which requires a Python version with a patch version
 /// and the user provides a target version with a compatible patch version.
 ///
 /// ```text
-/// 91b4bcfc
+/// python-patch-override-patch-compatible
 /// ├── environment
 /// │   └── python3.8.18
 /// ├── root
 /// │   └── requires a==1.0.0
 /// │       └── satisfied by a-1.0.0
 /// └── a
 ///     └── a-1.0.0
 ///         └── requires python>=3.8.0
 /// ```
+#[cfg(feature = "python-patch")]
 #[test]
-fn requires_python_patch_version_override_patch_compatible() -> Result<()> {
+fn python_patch_override_patch_compatible() -> Result<()> {
     let context = TestContext::new("3.8.18");
     let python_versions = &[];
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-91b4bcfc", "albatross"));
-    filters.push((r"-91b4bcfc", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"python-patch-override-patch-compatible-", "package-"));
 
     let requirements_in = context.temp_dir.child("requirements.in");
-    requirements_in.write_str("a-91b4bcfc==1.0.0")?;
+    requirements_in.write_str("python-patch-override-patch-compatible-a==1.0.0")?;
 
     let output = uv_snapshot!(filters, command(&context, python_versions)
         .arg("--python-version=3.8.0")
         , @r###"
                  success: true
                  exit_code: 0
                  ----- stdout -----
                  # This file was autogenerated by uv via the following command:
-                 #    uv pip compile requirements.in --find-links https://raw.githubusercontent.com/zanieb/packse/de0bab473eeaa4445db5a8febd732c655fad3d52/vendor/links.html --cache-dir [CACHE_DIR] --python-version=3.8.0
-                 albatross==1.0.0
+                 #    uv pip compile requirements.in --cache-dir [CACHE_DIR] --python-version=3.8.0
+                 package-a==1.0.0
+                     # via -r requirements.in
 
                  ----- stderr -----
                  warning: The requested Python version 3.8.0 is not available; 3.8.18 will be used to build dependencies instead.
                  Resolved 1 package in [TIME]
                  "###
     );
 
-    output
-        .assert()
-        .success()
-        .stdout(predicate::str::contains("a-91b4bcfc==1.0.0"));
+    output.assert().success().stdout(predicate::str::contains(
+        "python-patch-override-patch-compatible-a==1.0.0",
+    ));
 
     Ok(())
 }
```

### Comparing `uv-0.1.9/crates/uv/tests/pip_install_scenarios.rs` & `uv-0.2.0/crates/uv/tests/pip_install_scenarios.rs`

 * *Files 26% similar despite different names*

```diff
@@ -1,21 +1,21 @@
 //! DO NOT EDIT
 //!
-//! Generated with ./scripts/scenarios/update.py
-//! Scenarios from <https://github.com/zanieb/packse/tree/de0bab473eeaa4445db5a8febd732c655fad3d52/scenarios>
+//! Generated with `./scripts/sync_scenarios.sh`
+//! Scenarios from <https://github.com/astral-sh/packse/tree/0.3.15/scenarios>
 //!
-#![cfg(all(feature = "python", feature = "pypi"))]
+#![cfg(all(feature = "python", feature = "pypi", unix))]
 
 use std::path::Path;
 use std::process::Command;
 
 use assert_cmd::assert::Assert;
 use assert_cmd::prelude::*;
 
-use common::{venv_to_interpreter, INSTA_FILTERS};
+use common::venv_to_interpreter;
 
 use crate::common::{get_bin, uv_snapshot, TestContext};
 
 mod common;
 
 fn assert_command(venv: &Path, command: &str, temp_dir: &Path) -> Assert {
     Command::new(venv_to_interpreter(venv))
@@ -42,17 +42,17 @@
 /// Create a `pip install` command with options shared across all scenarios.
 fn command(context: &TestContext) -> Command {
     let mut command = Command::new(get_bin());
     command
         .arg("pip")
         .arg("install")
         .arg("--index-url")
-        .arg("https://test.pypi.org/simple")
+        .arg("https://astral-sh.github.io/packse/0.3.15/simple-html/")
         .arg("--find-links")
-        .arg("https://raw.githubusercontent.com/zanieb/packse/de0bab473eeaa4445db5a8febd732c655fad3d52/vendor/links.html")
+        .arg("https://raw.githubusercontent.com/astral-sh/packse/0.3.15/vendor/links.html")
         .arg("--cache-dir")
         .arg(context.cache_dir.path())
         .env("VIRTUAL_ENV", context.venv.as_os_str())
         .env("UV_NO_WRAP", "1")
         .current_dir(&context.temp_dir);
 
     if cfg!(all(windows, debug_assertions)) {
@@ -60,259 +60,260 @@
         // default windows stack of 1MB
         command.env("UV_STACK_SIZE", (8 * 1024 * 1024).to_string());
     }
 
     command
 }
 
-/// requires-package-does-not-exist
-///
 /// The user requires any version of package `a` which does not exist.
 ///
 /// ```text
-/// 5a1a4a35
+/// requires-package-does-not-exist
 /// ├── environment
 /// │   └── python3.8
 /// └── root
 ///     └── requires a
 ///         └── unsatisfied: no versions for package
 /// ```
 #[test]
 fn requires_package_does_not_exist() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"-5a1a4a35", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"requires-package-does-not-exist-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-5a1a4a35")
+        .arg("requires-package-does-not-exist-a")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because a was not found in the package registry and you require a, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because package-a was not found in the package registry and you require package-a, we can conclude that the requirements are unsatisfiable.
     "###);
 
-    assert_not_installed(&context.venv, "a_5a1a4a35", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "requires_package_does_not_exist_a",
+        &context.temp_dir,
+    );
 }
 
-/// requires-exact-version-does-not-exist
-///
 /// The user requires an exact version of package `a` but only other versions exist
 ///
 /// ```text
-/// 7cff23d9
+/// requires-exact-version-does-not-exist
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a==2.0.0
 /// │       └── unsatisfied: no matching version
 /// └── a
 ///     └── a-1.0.0
 /// ```
 #[test]
 fn requires_exact_version_does_not_exist() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-7cff23d9", "albatross"));
-    filters.push((r"-7cff23d9", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"requires-exact-version-does-not-exist-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-7cff23d9==2.0.0")
+        .arg("requires-exact-version-does-not-exist-a==2.0.0")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because there is no version of albatross==2.0.0 and you require albatross==2.0.0, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because there is no version of package-a==2.0.0 and you require package-a==2.0.0, we can conclude that the requirements are unsatisfiable.
     "###);
 
-    assert_not_installed(&context.venv, "a_7cff23d9", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "requires_exact_version_does_not_exist_a",
+        &context.temp_dir,
+    );
 }
 
-/// requires-greater-version-does-not-exist
-///
 /// The user requires a version of `a` greater than `1.0.0` but only smaller or
 /// equal versions exist
 ///
 /// ```text
-/// 63569c9e
+/// requires-greater-version-does-not-exist
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a>1.0.0
 /// │       └── unsatisfied: no matching version
 /// └── a
 ///     ├── a-0.1.0
 ///     └── a-1.0.0
 /// ```
 #[test]
 fn requires_greater_version_does_not_exist() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-63569c9e", "albatross"));
-    filters.push((r"-63569c9e", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"requires-greater-version-does-not-exist-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-63569c9e>1.0.0")
+        .arg("requires-greater-version-does-not-exist-a>1.0.0")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because only albatross<=1.0.0 is available and you require albatross>1.0.0, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because only package-a<=1.0.0 is available and you require package-a>1.0.0, we can conclude that the requirements are unsatisfiable.
     "###);
 
-    assert_not_installed(&context.venv, "a_63569c9e", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "requires_greater_version_does_not_exist_a",
+        &context.temp_dir,
+    );
 }
 
-/// requires-less-version-does-not-exist
-///
 /// The user requires a version of `a` less than `1.0.0` but only larger versions
 /// exist
 ///
 /// ```text
-/// 2af6fa02
+/// requires-less-version-does-not-exist
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a<2.0.0
 /// │       └── unsatisfied: no matching version
 /// └── a
 ///     ├── a-2.0.0
 ///     ├── a-3.0.0
 ///     └── a-4.0.0
 /// ```
 #[test]
 fn requires_less_version_does_not_exist() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-2af6fa02", "albatross"));
-    filters.push((r"-2af6fa02", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"requires-less-version-does-not-exist-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-2af6fa02<2.0.0")
+        .arg("requires-less-version-does-not-exist-a<2.0.0")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because only albatross>=2.0.0 is available and you require albatross<2.0.0, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because only package-a>=2.0.0 is available and you require package-a<2.0.0, we can conclude that the requirements are unsatisfiable.
     "###);
 
-    assert_not_installed(&context.venv, "a_2af6fa02", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "requires_less_version_does_not_exist_a",
+        &context.temp_dir,
+    );
 }
 
-/// transitive-requires-package-does-not-exist
-///
 /// The user requires package `a` but `a` requires package `b` which does not exist
 ///
 /// ```text
-/// 64b04b2b
+/// transitive-requires-package-does-not-exist
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a
 /// │       └── satisfied by a-1.0.0
 /// └── a
 ///     └── a-1.0.0
 ///         └── requires b
 ///             └── unsatisfied: no versions for package
 /// ```
 #[test]
 fn transitive_requires_package_does_not_exist() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-64b04b2b", "albatross"));
-    filters.push((r"-64b04b2b", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"transitive-requires-package-does-not-exist-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-64b04b2b")
+        .arg("transitive-requires-package-does-not-exist-a")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because b was not found in the package registry and albatross==1.0.0 depends on b, we can conclude that albatross==1.0.0 cannot be used.
-          And because only albatross==1.0.0 is available and you require albatross, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because package-b was not found in the package registry and package-a==1.0.0 depends on package-b, we can conclude that package-a==1.0.0 cannot be used.
+          And because only package-a==1.0.0 is available and you require package-a, we can conclude that the requirements are unsatisfiable.
     "###);
 
-    assert_not_installed(&context.venv, "a_64b04b2b", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "transitive_requires_package_does_not_exist_a",
+        &context.temp_dir,
+    );
 }
 
-/// excluded-only-version
-///
 /// Only one version of the requested package is available, but the user has banned
 /// that version.
 ///
 /// ```text
-/// 72f0d052
+/// excluded-only-version
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a!=1.0.0
 /// │       └── unsatisfied: no matching version
 /// └── a
 ///     └── a-1.0.0
 /// ```
 #[test]
 fn excluded_only_version() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-72f0d052", "albatross"));
-    filters.push((r"-72f0d052", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"excluded-only-version-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-72f0d052!=1.0.0")
+        .arg("excluded-only-version-a!=1.0.0")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because only albatross==1.0.0 is available and you require one of:
-              albatross<1.0.0
-              albatross>1.0.0
+      ╰─▶ Because only package-a==1.0.0 is available and you require one of:
+              package-a<1.0.0
+              package-a>1.0.0
           we can conclude that the requirements are unsatisfiable.
     "###);
 
     // Only `a==1.0.0` is available but the user excluded it.
-    assert_not_installed(&context.venv, "a_72f0d052", &context.temp_dir);
+    assert_not_installed(&context.venv, "excluded_only_version_a", &context.temp_dir);
 }
 
-/// excluded-only-compatible-version
-///
 /// Only one version of the requested package `a` is compatible, but the user has
 /// banned that version.
 ///
 /// ```text
-/// d6ce69da
+/// excluded-only-compatible-version
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   ├── requires a!=2.0.0
 /// │   │   ├── satisfied by a-1.0.0
 /// │   │   └── satisfied by a-3.0.0
 /// │   └── requires b<3.0.0,>=2.0.0
@@ -332,62 +333,66 @@
 ///     ├── b-2.0.0
 ///     └── b-3.0.0
 /// ```
 #[test]
 fn excluded_only_compatible_version() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-d6ce69da", "albatross"));
-    filters.push((r"b-d6ce69da", "bluebird"));
-    filters.push((r"-d6ce69da", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"excluded-only-compatible-version-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-d6ce69da!=2.0.0")
-                .arg("b-d6ce69da<3.0.0,>=2.0.0")
+        .arg("excluded-only-compatible-version-a!=2.0.0")
+                .arg("excluded-only-compatible-version-b<3.0.0,>=2.0.0")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because only the following versions of albatross are available:
-              albatross==1.0.0
-              albatross==2.0.0
-              albatross==3.0.0
-          and albatross==1.0.0 depends on bluebird==1.0.0, we can conclude that albatross<2.0.0 depends on bluebird==1.0.0.
-          And because albatross==3.0.0 depends on bluebird==3.0.0, we can conclude that any of:
-              albatross<2.0.0
-              albatross>2.0.0
+      ╰─▶ Because only the following versions of package-a are available:
+              package-a==1.0.0
+              package-a==2.0.0
+              package-a==3.0.0
+          and package-a==1.0.0 depends on package-b==1.0.0, we can conclude that package-a<2.0.0 depends on package-b==1.0.0.
+          And because package-a==3.0.0 depends on package-b==3.0.0, we can conclude that any of:
+              package-a<2.0.0
+              package-a>2.0.0
           depends on one of:
-              bluebird==1.0.0
-              bluebird==3.0.0
+              package-b==1.0.0
+              package-b==3.0.0
 
           And because you require one of:
-              albatross<2.0.0
-              albatross>2.0.0
-          and you require bluebird>=2.0.0,<3.0.0, we can conclude that the requirements are unsatisfiable.
+              package-a<2.0.0
+              package-a>2.0.0
+          and package-b>=2.0.0,<3.0.0, we can conclude that the requirements are unsatisfiable.
     "###);
 
     // Only `a==1.2.0` is available since `a==1.0.0` and `a==3.0.0` require
     // incompatible versions of `b`. The user has excluded that version of `a` so
     // resolution fails.
-    assert_not_installed(&context.venv, "a_d6ce69da", &context.temp_dir);
-    assert_not_installed(&context.venv, "b_d6ce69da", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "excluded_only_compatible_version_a",
+        &context.temp_dir,
+    );
+    assert_not_installed(
+        &context.venv,
+        "excluded_only_compatible_version_b",
+        &context.temp_dir,
+    );
 }
 
-/// dependency-excludes-range-of-compatible-versions
-///
 /// There is a range of compatible versions for the requested package `a`, but
 /// another dependency `c` excludes that range.
 ///
 /// ```text
-/// 5824fb81
+/// dependency-excludes-range-of-compatible-versions
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   ├── requires a
 /// │   │   ├── satisfied by a-1.0.0
 /// │   │   ├── satisfied by a-2.0.0
 /// │   │   ├── satisfied by a-2.1.0
@@ -430,71 +435,81 @@
 ///         └── requires a>=3.0.0
 ///             └── satisfied by a-3.0.0
 /// ```
 #[test]
 fn dependency_excludes_range_of_compatible_versions() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-5824fb81", "albatross"));
-    filters.push((r"b-5824fb81", "bluebird"));
-    filters.push((r"c-5824fb81", "crow"));
-    filters.push((r"-5824fb81", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((
+        r"dependency-excludes-range-of-compatible-versions-",
+        "package-",
+    ));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-5824fb81")
-                .arg("b-5824fb81<3.0.0,>=2.0.0")
-                .arg("c-5824fb81")
+        .arg("dependency-excludes-range-of-compatible-versions-a")
+                .arg("dependency-excludes-range-of-compatible-versions-b<3.0.0,>=2.0.0")
+                .arg("dependency-excludes-range-of-compatible-versions-c")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because only the following versions of albatross are available:
-              albatross==1.0.0
-              albatross>=2.0.0,<=3.0.0
-          and albatross==1.0.0 depends on bluebird==1.0.0, we can conclude that albatross<2.0.0 depends on bluebird==1.0.0. (1)
-
-          Because only the following versions of crow are available:
-              crow==1.0.0
-              crow==2.0.0
-          and crow==1.0.0 depends on albatross<2.0.0, we can conclude that crow<2.0.0 depends on albatross<2.0.0.
-          And because crow==2.0.0 depends on albatross>=3.0.0, we can conclude that all versions of crow depend on one of:
-              albatross<2.0.0
-              albatross>=3.0.0
+      ╰─▶ Because package-a==1.0.0 depends on package-b==1.0.0 and only the following versions of package-a are available:
+              package-a==1.0.0
+              package-a>=2.0.0,<=3.0.0
+          we can conclude that package-a<2.0.0 depends on package-b==1.0.0. (1)
+
+          Because only the following versions of package-c are available:
+              package-c==1.0.0
+              package-c==2.0.0
+          and package-c==1.0.0 depends on package-a<2.0.0, we can conclude that package-c<2.0.0 depends on package-a<2.0.0.
+          And because package-c==2.0.0 depends on package-a>=3.0.0, we can conclude that all versions of package-c depend on one of:
+              package-a<2.0.0
+              package-a>=3.0.0
+
+          And because we know from (1) that package-a<2.0.0 depends on package-b==1.0.0, we can conclude that package-a!=3.0.0, package-b!=1.0.0, all versions of package-c are incompatible.
+          And because package-a==3.0.0 depends on package-b==3.0.0, we can conclude that all versions of package-c depend on one of:
+              package-b<=1.0.0
+              package-b>=3.0.0
 
-          And because we know from (1) that albatross<2.0.0 depends on bluebird==1.0.0, we can conclude that bluebird!=1.0.0, albatross!=3.0.0, all versions of crow are incompatible.
-          And because albatross==3.0.0 depends on bluebird==3.0.0, we can conclude that all versions of crow depend on one of:
-              bluebird<=1.0.0
-              bluebird>=3.0.0
-
-          And because you require bluebird>=2.0.0,<3.0.0 and you require crow, we can conclude that the requirements are unsatisfiable.
+          And because you require package-b>=2.0.0,<3.0.0 and package-c, we can conclude that the requirements are unsatisfiable.
     "###);
 
     // Only the `2.x` versions of `a` are available since `a==1.0.0` and `a==3.0.0`
     // require incompatible versions of `b`, but all available versions of `c` exclude
     // that range of `a` so resolution fails.
-    assert_not_installed(&context.venv, "a_5824fb81", &context.temp_dir);
-    assert_not_installed(&context.venv, "b_5824fb81", &context.temp_dir);
-    assert_not_installed(&context.venv, "c_5824fb81", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "dependency_excludes_range_of_compatible_versions_a",
+        &context.temp_dir,
+    );
+    assert_not_installed(
+        &context.venv,
+        "dependency_excludes_range_of_compatible_versions_b",
+        &context.temp_dir,
+    );
+    assert_not_installed(
+        &context.venv,
+        "dependency_excludes_range_of_compatible_versions_c",
+        &context.temp_dir,
+    );
 }
 
-/// dependency-excludes-non-contiguous-range-of-compatible-versions
-///
 /// There is a non-contiguous range of compatible versions for the requested package
 /// `a`, but another dependency `c` excludes the range. This is the same as
 /// `dependency-excludes-range-of-compatible-versions` but some of the versions of
 /// `a` are incompatible for another reason e.g. dependency on non-existent package
 /// `d`.
 ///
 /// ```text
-/// 119f929b
+/// dependency-excludes-non-contiguous-range-of-compatible-versions
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   ├── requires a
 /// │   │   ├── satisfied by a-1.0.0
 /// │   │   ├── satisfied by a-2.0.0
 /// │   │   ├── satisfied by a-2.1.0
@@ -545,67 +560,77 @@
 ///         └── requires a>=3.0.0
 ///             └── satisfied by a-3.0.0
 /// ```
 #[test]
 fn dependency_excludes_non_contiguous_range_of_compatible_versions() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-119f929b", "albatross"));
-    filters.push((r"b-119f929b", "bluebird"));
-    filters.push((r"c-119f929b", "crow"));
-    filters.push((r"-119f929b", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((
+        r"dependency-excludes-non-contiguous-range-of-compatible-versions-",
+        "package-",
+    ));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-119f929b")
-                .arg("b-119f929b<3.0.0,>=2.0.0")
-                .arg("c-119f929b")
+        .arg("dependency-excludes-non-contiguous-range-of-compatible-versions-a")
+                .arg("dependency-excludes-non-contiguous-range-of-compatible-versions-b<3.0.0,>=2.0.0")
+                .arg("dependency-excludes-non-contiguous-range-of-compatible-versions-c")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because only the following versions of albatross are available:
-              albatross==1.0.0
-              albatross>=2.0.0,<=3.0.0
-          and albatross==1.0.0 depends on bluebird==1.0.0, we can conclude that albatross<2.0.0 depends on bluebird==1.0.0. (1)
+      ╰─▶ Because package-a==1.0.0 depends on package-b==1.0.0 and only the following versions of package-a are available:
+              package-a==1.0.0
+              package-a>=2.0.0,<=3.0.0
+          we can conclude that package-a<2.0.0 depends on package-b==1.0.0. (1)
+
+          Because only the following versions of package-c are available:
+              package-c==1.0.0
+              package-c==2.0.0
+          and package-c==1.0.0 depends on package-a<2.0.0, we can conclude that package-c<2.0.0 depends on package-a<2.0.0.
+          And because package-c==2.0.0 depends on package-a>=3.0.0, we can conclude that all versions of package-c depend on one of:
+              package-a<2.0.0
+              package-a>=3.0.0
+
+          And because we know from (1) that package-a<2.0.0 depends on package-b==1.0.0, we can conclude that package-a!=3.0.0, package-b!=1.0.0, all versions of package-c are incompatible.
+          And because package-a==3.0.0 depends on package-b==3.0.0, we can conclude that all versions of package-c depend on one of:
+              package-b<=1.0.0
+              package-b>=3.0.0
 
-          Because only the following versions of crow are available:
-              crow==1.0.0
-              crow==2.0.0
-          and crow==1.0.0 depends on albatross<2.0.0, we can conclude that crow<2.0.0 depends on albatross<2.0.0.
-          And because crow==2.0.0 depends on albatross>=3.0.0, we can conclude that all versions of crow depend on one of:
-              albatross<2.0.0
-              albatross>=3.0.0
-
-          And because we know from (1) that albatross<2.0.0 depends on bluebird==1.0.0, we can conclude that bluebird!=1.0.0, all versions of crow, albatross!=3.0.0 are incompatible.
-          And because albatross==3.0.0 depends on bluebird==3.0.0, we can conclude that all versions of crow depend on one of:
-              bluebird<=1.0.0
-              bluebird>=3.0.0
-
-          And because you require bluebird>=2.0.0,<3.0.0 and you require crow, we can conclude that the requirements are unsatisfiable.
+          And because you require package-b>=2.0.0,<3.0.0 and package-c, we can conclude that the requirements are unsatisfiable.
     "###);
 
     // Only the `2.x` versions of `a` are available since `a==1.0.0` and `a==3.0.0`
     // require incompatible versions of `b`, but all available versions of `c` exclude
     // that range of `a` so resolution fails.
-    assert_not_installed(&context.venv, "a_119f929b", &context.temp_dir);
-    assert_not_installed(&context.venv, "b_119f929b", &context.temp_dir);
-    assert_not_installed(&context.venv, "c_119f929b", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "dependency_excludes_non_contiguous_range_of_compatible_versions_a",
+        &context.temp_dir,
+    );
+    assert_not_installed(
+        &context.venv,
+        "dependency_excludes_non_contiguous_range_of_compatible_versions_b",
+        &context.temp_dir,
+    );
+    assert_not_installed(
+        &context.venv,
+        "dependency_excludes_non_contiguous_range_of_compatible_versions_c",
+        &context.temp_dir,
+    );
 }
 
-/// extra-required
-///
 /// Optional dependencies are requested for the package.
 ///
 /// ```text
-/// c1e0ed38
+/// extra-required
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a[extra]
 /// │       ├── satisfied by a-1.0.0
 /// │       └── satisfied by a-1.0.0[extra]
 /// ├── a
@@ -616,87 +641,91 @@
 /// └── b
 ///     └── b-1.0.0
 /// ```
 #[test]
 fn extra_required() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-c1e0ed38", "albatross"));
-    filters.push((r"b-c1e0ed38", "bluebird"));
-    filters.push((r"-c1e0ed38", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"extra-required-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-c1e0ed38[extra]")
+        .arg("extra-required-a[extra]")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 2 packages in [TIME]
     Downloaded 2 packages in [TIME]
     Installed 2 packages in [TIME]
-     + albatross==1.0.0
-     + bluebird==1.0.0
+     + package-a==1.0.0
+     + package-b==1.0.0
     "###);
 
-    assert_installed(&context.venv, "a_c1e0ed38", "1.0.0", &context.temp_dir);
-    assert_installed(&context.venv, "b_c1e0ed38", "1.0.0", &context.temp_dir);
+    assert_installed(
+        &context.venv,
+        "extra_required_a",
+        "1.0.0",
+        &context.temp_dir,
+    );
+    assert_installed(
+        &context.venv,
+        "extra_required_b",
+        "1.0.0",
+        &context.temp_dir,
+    );
 }
 
-/// missing-extra
-///
 /// Optional dependencies are requested for the package, but the extra does not
 /// exist.
 ///
 /// ```text
-/// de25a6db
+/// missing-extra
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a[extra]
 /// │       └── satisfied by a-1.0.0
 /// └── a
 ///     └── a-1.0.0
 /// ```
 #[test]
 fn missing_extra() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-de25a6db", "albatross"));
-    filters.push((r"-de25a6db", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"missing-extra-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-de25a6db[extra]")
+        .arg("missing-extra-a[extra]")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 1 package in [TIME]
+    warning: The package `package-a==1.0.0` does not have an extra named `extra`.
     Downloaded 1 package in [TIME]
     Installed 1 package in [TIME]
-     + albatross==1.0.0
+     + package-a==1.0.0
     "###);
 
     // Missing extras are ignored during resolution.
-    assert_installed(&context.venv, "a_de25a6db", "1.0.0", &context.temp_dir);
+    assert_installed(&context.venv, "missing_extra_a", "1.0.0", &context.temp_dir);
 }
 
-/// multiple-extras-required
-///
 /// Multiple optional dependencies are requested for the package.
 ///
 /// ```text
-/// 502cbb59
+/// multiple-extras-required
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a[extra_b,extra_c]
 /// │       ├── satisfied by a-1.0.0
 /// │       ├── satisfied by a-1.0.0[extra_b]
 /// │       └── satisfied by a-1.0.0[extra_c]
@@ -713,48 +742,58 @@
 /// └── c
 ///     └── c-1.0.0
 /// ```
 #[test]
 fn multiple_extras_required() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-502cbb59", "albatross"));
-    filters.push((r"b-502cbb59", "bluebird"));
-    filters.push((r"c-502cbb59", "crow"));
-    filters.push((r"-502cbb59", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"multiple-extras-required-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-502cbb59[extra_b,extra_c]")
+        .arg("multiple-extras-required-a[extra_b,extra_c]")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 3 packages in [TIME]
     Downloaded 3 packages in [TIME]
     Installed 3 packages in [TIME]
-     + albatross==1.0.0
-     + bluebird==1.0.0
-     + crow==1.0.0
-    "###);
-
-    assert_installed(&context.venv, "a_502cbb59", "1.0.0", &context.temp_dir);
-    assert_installed(&context.venv, "b_502cbb59", "1.0.0", &context.temp_dir);
-    assert_installed(&context.venv, "c_502cbb59", "1.0.0", &context.temp_dir);
+     + package-a==1.0.0
+     + package-b==1.0.0
+     + package-c==1.0.0
+    "###);
+
+    assert_installed(
+        &context.venv,
+        "multiple_extras_required_a",
+        "1.0.0",
+        &context.temp_dir,
+    );
+    assert_installed(
+        &context.venv,
+        "multiple_extras_required_b",
+        "1.0.0",
+        &context.temp_dir,
+    );
+    assert_installed(
+        &context.venv,
+        "multiple_extras_required_c",
+        "1.0.0",
+        &context.temp_dir,
+    );
 }
 
-/// all-extras-required
-///
 /// Multiple optional dependencies are requested for the package via an 'all' extra.
 ///
 /// ```text
-/// 4cf56e90
+/// all-extras-required
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a[all]
 /// │       ├── satisfied by a-1.0.0
 /// │       ├── satisfied by a-1.0.0[all]
 /// │       ├── satisfied by a-1.0.0[extra_b]
@@ -783,49 +822,59 @@
 /// └── c
 ///     └── c-1.0.0
 /// ```
 #[test]
 fn all_extras_required() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-4cf56e90", "albatross"));
-    filters.push((r"b-4cf56e90", "bluebird"));
-    filters.push((r"c-4cf56e90", "crow"));
-    filters.push((r"-4cf56e90", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"all-extras-required-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-4cf56e90[all]")
+        .arg("all-extras-required-a[all]")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 3 packages in [TIME]
     Downloaded 3 packages in [TIME]
     Installed 3 packages in [TIME]
-     + albatross==1.0.0
-     + bluebird==1.0.0
-     + crow==1.0.0
-    "###);
-
-    assert_installed(&context.venv, "a_4cf56e90", "1.0.0", &context.temp_dir);
-    assert_installed(&context.venv, "b_4cf56e90", "1.0.0", &context.temp_dir);
-    assert_installed(&context.venv, "c_4cf56e90", "1.0.0", &context.temp_dir);
+     + package-a==1.0.0
+     + package-b==1.0.0
+     + package-c==1.0.0
+    "###);
+
+    assert_installed(
+        &context.venv,
+        "all_extras_required_a",
+        "1.0.0",
+        &context.temp_dir,
+    );
+    assert_installed(
+        &context.venv,
+        "all_extras_required_b",
+        "1.0.0",
+        &context.temp_dir,
+    );
+    assert_installed(
+        &context.venv,
+        "all_extras_required_c",
+        "1.0.0",
+        &context.temp_dir,
+    );
 }
 
-/// extra-incompatible-with-extra
-///
 /// Multiple optional dependencies are requested for the package, but they have
 /// conflicting requirements with each other.
 ///
 /// ```text
-/// a5547b80
+/// extra-incompatible-with-extra
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a[extra_b,extra_c]
 /// │       ├── satisfied by a-1.0.0
 /// │       ├── satisfied by a-1.0.0[extra_b]
 /// │       └── satisfied by a-1.0.0[extra_c]
@@ -841,45 +890,45 @@
 ///     ├── b-1.0.0
 ///     └── b-2.0.0
 /// ```
 #[test]
 fn extra_incompatible_with_extra() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-a5547b80", "albatross"));
-    filters.push((r"b-a5547b80", "bluebird"));
-    filters.push((r"-a5547b80", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"extra-incompatible-with-extra-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-a5547b80[extra_b,extra_c]")
+        .arg("extra-incompatible-with-extra-a[extra_b,extra_c]")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because only albatross[extra-c]==1.0.0 is available and albatross[extra-c]==1.0.0 depends on bluebird==2.0.0, we can conclude that all versions of albatross[extra-c] depend on bluebird==2.0.0.
-          And because albatross[extra-b]==1.0.0 depends on bluebird==1.0.0 and only albatross[extra-b]==1.0.0 is available, we can conclude that all versions of albatross[extra-c] and all versions of albatross[extra-b] are incompatible.
-          And because you require albatross[extra-b] and you require albatross[extra-c], we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because only package-a[extra-c]==1.0.0 is available and package-a[extra-c]==1.0.0 depends on package-b==2.0.0, we can conclude that all versions of package-a[extra-c] depend on package-b==2.0.0.
+          And because package-a[extra-b]==1.0.0 depends on package-b==1.0.0 and only package-a[extra-b]==1.0.0 is available, we can conclude that all versions of package-a[extra-b] and all versions of package-a[extra-c] are incompatible.
+          And because you require package-a[extra-b] and package-a[extra-c], we can conclude that the requirements are unsatisfiable.
     "###);
 
     // Because both `extra_b` and `extra_c` are requested and they require incompatible
     // versions of `b`, `a` cannot be installed.
-    assert_not_installed(&context.venv, "a_a5547b80", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "extra_incompatible_with_extra_a",
+        &context.temp_dir,
+    );
 }
 
-/// extra-incompatible-with-extra-not-requested
-///
 /// One of two incompatible optional dependencies are requested for the package.
 ///
 /// ```text
-/// 8bb31c23
+/// extra-incompatible-with-extra-not-requested
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a[extra_c]
 /// │       ├── satisfied by a-1.0.0
 /// │       ├── satisfied by a-1.0.0[extra_b]
 /// │       └── satisfied by a-1.0.0[extra_c]
@@ -895,48 +944,54 @@
 ///     ├── b-1.0.0
 ///     └── b-2.0.0
 /// ```
 #[test]
 fn extra_incompatible_with_extra_not_requested() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-8bb31c23", "albatross"));
-    filters.push((r"b-8bb31c23", "bluebird"));
-    filters.push((r"-8bb31c23", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"extra-incompatible-with-extra-not-requested-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-8bb31c23[extra_c]")
+        .arg("extra-incompatible-with-extra-not-requested-a[extra_c]")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 2 packages in [TIME]
     Downloaded 2 packages in [TIME]
     Installed 2 packages in [TIME]
-     + albatross==1.0.0
-     + bluebird==2.0.0
+     + package-a==1.0.0
+     + package-b==2.0.0
     "###);
 
     // Because the user does not request both extras, it is okay that one is
     // incompatible with the other.
-    assert_installed(&context.venv, "a_8bb31c23", "1.0.0", &context.temp_dir);
-    assert_installed(&context.venv, "b_8bb31c23", "2.0.0", &context.temp_dir);
+    assert_installed(
+        &context.venv,
+        "extra_incompatible_with_extra_not_requested_a",
+        "1.0.0",
+        &context.temp_dir,
+    );
+    assert_installed(
+        &context.venv,
+        "extra_incompatible_with_extra_not_requested_b",
+        "2.0.0",
+        &context.temp_dir,
+    );
 }
 
-/// extra-incompatible-with-root
-///
 /// Optional dependencies are requested for the package, but the extra is not
 /// compatible with other requested versions.
 ///
 /// ```text
-/// aca6971b
+/// extra-incompatible-with-root
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   ├── requires a[extra]
 /// │   │   ├── satisfied by a-1.0.0
 /// │   │   └── satisfied by a-1.0.0[extra]
 /// │   └── requires b==2.0.0
@@ -950,47 +1005,51 @@
 ///     ├── b-1.0.0
 ///     └── b-2.0.0
 /// ```
 #[test]
 fn extra_incompatible_with_root() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-aca6971b", "albatross"));
-    filters.push((r"b-aca6971b", "bluebird"));
-    filters.push((r"-aca6971b", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"extra-incompatible-with-root-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-aca6971b[extra]")
-                .arg("b-aca6971b==2.0.0")
+        .arg("extra-incompatible-with-root-a[extra]")
+                .arg("extra-incompatible-with-root-b==2.0.0")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because only albatross[extra]==1.0.0 is available and albatross[extra]==1.0.0 depends on bluebird==1.0.0, we can conclude that all versions of albatross[extra] depend on bluebird==1.0.0.
-          And because you require albatross[extra] and you require bluebird==2.0.0, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because only package-a[extra]==1.0.0 is available and package-a[extra]==1.0.0 depends on package-b==1.0.0, we can conclude that all versions of package-a[extra] depend on package-b==1.0.0.
+          And because you require package-a[extra] and package-b==2.0.0, we can conclude that the requirements are unsatisfiable.
     "###);
 
     // Because the user requested `b==2.0.0` but the requested extra requires
     // `b==1.0.0`, the dependencies cannot be satisfied.
-    assert_not_installed(&context.venv, "a_aca6971b", &context.temp_dir);
-    assert_not_installed(&context.venv, "b_aca6971b", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "extra_incompatible_with_root_a",
+        &context.temp_dir,
+    );
+    assert_not_installed(
+        &context.venv,
+        "extra_incompatible_with_root_b",
+        &context.temp_dir,
+    );
 }
 
-/// extra-does-not-exist-backtrack
-///
 /// Optional dependencies are requested for the package, the extra is only available
 /// on an older version.
 ///
 /// ```text
-/// c4307e58
+/// extra-does-not-exist-backtrack
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a[extra]
 /// │       ├── satisfied by a-1.0.0
 /// │       ├── satisfied by a-1.0.0[extra]
 /// │       ├── satisfied by a-2.0.0
@@ -1005,45 +1064,47 @@
 /// └── b
 ///     └── b-1.0.0
 /// ```
 #[test]
 fn extra_does_not_exist_backtrack() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-c4307e58", "albatross"));
-    filters.push((r"b-c4307e58", "bluebird"));
-    filters.push((r"-c4307e58", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"extra-does-not-exist-backtrack-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-c4307e58[extra]")
+        .arg("extra-does-not-exist-backtrack-a[extra]")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 1 package in [TIME]
+    warning: The package `package-a==3.0.0` does not have an extra named `extra`.
     Downloaded 1 package in [TIME]
     Installed 1 package in [TIME]
-     + albatross==3.0.0
+     + package-a==3.0.0
     "###);
 
     // The resolver should not backtrack to `a==1.0.0` because missing extras are
     // allowed during resolution. `b` should not be installed.
-    assert_installed(&context.venv, "a_c4307e58", "3.0.0", &context.temp_dir);
+    assert_installed(
+        &context.venv,
+        "extra_does_not_exist_backtrack_a",
+        "3.0.0",
+        &context.temp_dir,
+    );
 }
 
-/// direct-incompatible-versions
-///
 /// The user requires two incompatible, existing versions of package `a`
 ///
 /// ```text
-/// c0e7adfa
+/// direct-incompatible-versions
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   ├── requires a==1.0.0
 /// │   │   └── satisfied by a-1.0.0
 /// │   └── requires a==2.0.0
 /// │       └── satisfied by a-2.0.0
@@ -1051,43 +1112,48 @@
 ///     ├── a-1.0.0
 ///     └── a-2.0.0
 /// ```
 #[test]
 fn direct_incompatible_versions() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-c0e7adfa", "albatross"));
-    filters.push((r"-c0e7adfa", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"direct-incompatible-versions-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-c0e7adfa==1.0.0")
-                .arg("a-c0e7adfa==2.0.0")
+        .arg("direct-incompatible-versions-a==1.0.0")
+                .arg("direct-incompatible-versions-a==2.0.0")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because you require albatross==1.0.0 and you require albatross==2.0.0, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because you require package-a==1.0.0 and package-a==2.0.0, we can conclude that the requirements are unsatisfiable.
     "###);
 
-    assert_not_installed(&context.venv, "a_c0e7adfa", &context.temp_dir);
-    assert_not_installed(&context.venv, "a_c0e7adfa", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "direct_incompatible_versions_a",
+        &context.temp_dir,
+    );
+    assert_not_installed(
+        &context.venv,
+        "direct_incompatible_versions_a",
+        &context.temp_dir,
+    );
 }
 
-/// transitive-incompatible-with-root-version
-///
 /// The user requires packages `a` and `b` but `a` requires a different version of
 /// `b`
 ///
 /// ```text
-/// a13da883
+/// transitive-incompatible-with-root-version
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   ├── requires a
 /// │   │   └── satisfied by a-1.0.0
 /// │   └── requires b==1.0.0
 /// │       └── satisfied by b-1.0.0
@@ -1099,45 +1165,49 @@
 ///     ├── b-1.0.0
 ///     └── b-2.0.0
 /// ```
 #[test]
 fn transitive_incompatible_with_root_version() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-a13da883", "albatross"));
-    filters.push((r"b-a13da883", "bluebird"));
-    filters.push((r"-a13da883", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"transitive-incompatible-with-root-version-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-a13da883")
-                .arg("b-a13da883==1.0.0")
+        .arg("transitive-incompatible-with-root-version-a")
+                .arg("transitive-incompatible-with-root-version-b==1.0.0")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because only albatross==1.0.0 is available and albatross==1.0.0 depends on bluebird==2.0.0, we can conclude that all versions of albatross depend on bluebird==2.0.0.
-          And because you require albatross and you require bluebird==1.0.0, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because package-a==1.0.0 depends on package-b==2.0.0 and only package-a==1.0.0 is available, we can conclude that all versions of package-a depend on package-b==2.0.0.
+          And because you require package-a and package-b==1.0.0, we can conclude that the requirements are unsatisfiable.
     "###);
 
-    assert_not_installed(&context.venv, "a_a13da883", &context.temp_dir);
-    assert_not_installed(&context.venv, "b_a13da883", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "transitive_incompatible_with_root_version_a",
+        &context.temp_dir,
+    );
+    assert_not_installed(
+        &context.venv,
+        "transitive_incompatible_with_root_version_b",
+        &context.temp_dir,
+    );
 }
 
-/// transitive-incompatible-with-transitive
-///
 /// The user requires package `a` and `b`; `a` and `b` require different versions of
 /// `c`
 ///
 /// ```text
-/// ec82e315
+/// transitive-incompatible-with-transitive
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   ├── requires a
 /// │   │   └── satisfied by a-1.0.0
 /// │   └── requires b
 /// │       └── satisfied by b-1.0.0
@@ -1153,222 +1223,1540 @@
 ///     ├── c-1.0.0
 ///     └── c-2.0.0
 /// ```
 #[test]
 fn transitive_incompatible_with_transitive() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-ec82e315", "albatross"));
-    filters.push((r"b-ec82e315", "bluebird"));
-    filters.push((r"c-ec82e315", "crow"));
-    filters.push((r"-ec82e315", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"transitive-incompatible-with-transitive-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-ec82e315")
-                .arg("b-ec82e315")
+        .arg("transitive-incompatible-with-transitive-a")
+                .arg("transitive-incompatible-with-transitive-b")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because only albatross==1.0.0 is available and albatross==1.0.0 depends on crow==1.0.0, we can conclude that all versions of albatross depend on crow==1.0.0.
-          And because bluebird==1.0.0 depends on crow==2.0.0 and only bluebird==1.0.0 is available, we can conclude that all versions of bluebird and all versions of albatross are incompatible.
-          And because you require albatross and you require bluebird, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because only package-a==1.0.0 is available and package-a==1.0.0 depends on package-c==1.0.0, we can conclude that all versions of package-a depend on package-c==1.0.0.
+          And because package-b==1.0.0 depends on package-c==2.0.0 and only package-b==1.0.0 is available, we can conclude that all versions of package-a and all versions of package-b are incompatible.
+          And because you require package-a and package-b, we can conclude that the requirements are unsatisfiable.
     "###);
 
-    assert_not_installed(&context.venv, "a_ec82e315", &context.temp_dir);
-    assert_not_installed(&context.venv, "b_ec82e315", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "transitive_incompatible_with_transitive_a",
+        &context.temp_dir,
+    );
+    assert_not_installed(
+        &context.venv,
+        "transitive_incompatible_with_transitive_b",
+        &context.temp_dir,
+    );
 }
 
-/// package-only-prereleases
+/// The user requires `a`, which requires two incompatible, existing versions of
+/// package `b`
+///
+/// ```text
+/// transitive-incompatible-versions
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a==1.0.0
+/// │       └── satisfied by a-1.0.0
+/// └── a
+///     └── a-1.0.0
+///         ├── requires b==2.0.0
+///             └── unsatisfied: no versions for package
+///         └── requires b==1.0.0
+///             └── unsatisfied: no versions for package
+/// ```
+#[test]
+fn transitive_incompatible_versions() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"transitive-incompatible-versions-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("transitive-incompatible-versions-a==1.0.0")
+        , @r###"
+    success: false
+    exit_code: 1
+    ----- stdout -----
+
+    ----- stderr -----
+      × No solution found when resolving dependencies:
+      ╰─▶ Because package-a==1.0.0 depends on package-b==1.0.0 and package-b==2.0.0, we can conclude that package-a==1.0.0 cannot be used.
+          And because you require package-a==1.0.0, we can conclude that the requirements are unsatisfiable.
+    "###);
+
+    assert_not_installed(
+        &context.venv,
+        "transitive_incompatible_versions_a",
+        &context.temp_dir,
+    );
+}
+
+/// A simple version constraint should not exclude published versions with local
+/// segments.
+///
+/// ```text
+/// local-simple
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a==1.2.3
+/// │       └── satisfied by a-1.2.3+foo
+/// └── a
+///     └── a-1.2.3+foo
+/// ```
+#[test]
+fn local_simple() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"local-simple-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("local-simple-a==1.2.3")
+        , @r###"
+    success: false
+    exit_code: 1
+    ----- stdout -----
+
+    ----- stderr -----
+      × No solution found when resolving dependencies:
+      ╰─▶ Because there is no version of package-a==1.2.3 and you require package-a==1.2.3, we can conclude that the requirements are unsatisfiable.
+    "###);
+
+    // The version '1.2.3+foo' satisfies the constraint '==1.2.3'.
+    assert_not_installed(&context.venv, "local_simple_a", &context.temp_dir);
+}
+
+/// If there is a 1.2.3 version with an sdist published and no compatible wheels,
+/// then the sdist will be used.
+///
+/// ```text
+/// local-not-used-with-sdist
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a==1.2.3
+/// │       ├── satisfied by a-1.2.3
+/// │       └── satisfied by a-1.2.3+foo
+/// └── a
+///     ├── a-1.2.3
+///     └── a-1.2.3+foo
+/// ```
+#[test]
+fn local_not_used_with_sdist() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"local-not-used-with-sdist-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("local-not-used-with-sdist-a==1.2.3")
+        , @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + package-a==1.2.3
+    "###);
+
+    // The version '1.2.3' with an sdist satisfies the constraint '==1.2.3'.
+    assert_installed(
+        &context.venv,
+        "local_not_used_with_sdist_a",
+        "1.2.3",
+        &context.temp_dir,
+    );
+}
+
+/// Even if there is a 1.2.3 version published, if it is unavailable for some reason
+/// (no sdist and no compatible wheels in this case), a 1.2.3 version with a local
+/// segment should be usable instead.
+///
+/// ```text
+/// local-used-without-sdist
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a==1.2.3
+/// │       ├── satisfied by a-1.2.3
+/// │       └── satisfied by a-1.2.3+foo
+/// └── a
+///     ├── a-1.2.3
+///     └── a-1.2.3+foo
+/// ```
+#[test]
+fn local_used_without_sdist() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"local-used-without-sdist-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("local-used-without-sdist-a==1.2.3")
+        , @r###"
+    success: false
+    exit_code: 1
+    ----- stdout -----
+
+    ----- stderr -----
+      × No solution found when resolving dependencies:
+      ╰─▶ Because package-a==1.2.3 has no wheels are available with a matching Python ABI and you require package-a==1.2.3, we can conclude that the requirements are unsatisfiable.
+    "###);
+
+    // The version '1.2.3+foo' satisfies the constraint '==1.2.3'.
+    assert_not_installed(
+        &context.venv,
+        "local_used_without_sdist_a",
+        &context.temp_dir,
+    );
+}
+
+/// Tests that we can select an older version with a local segment when newer
+/// versions are incompatible.
 ///
+/// ```text
+/// local-not-latest
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a>=1
+/// │       ├── satisfied by a-1.2.3
+/// │       ├── satisfied by a-1.2.2+foo
+/// │       └── satisfied by a-1.2.1+foo
+/// └── a
+///     ├── a-1.2.3
+///     ├── a-1.2.2+foo
+///     └── a-1.2.1+foo
+/// ```
+#[test]
+fn local_not_latest() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"local-not-latest-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("local-not-latest-a>=1")
+        , @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + package-a==1.2.1+foo
+    "###);
+
+    assert_installed(
+        &context.venv,
+        "local_not_latest_a",
+        "1.2.1+foo",
+        &context.temp_dir,
+    );
+}
+
+/// A simple version constraint should not exclude published versions with local
+/// segments.
+///
+/// ```text
+/// local-transitive
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   ├── requires a
+/// │   │   └── satisfied by a-1.0.0
+/// │   └── requires b==2.0.0+foo
+/// │       └── satisfied by b-2.0.0+foo
+/// ├── a
+/// │   └── a-1.0.0
+/// │       └── requires b==2.0.0
+/// │           └── satisfied by b-2.0.0+foo
+/// └── b
+///     └── b-2.0.0+foo
+/// ```
+#[test]
+fn local_transitive() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"local-transitive-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("local-transitive-a")
+                .arg("local-transitive-b==2.0.0+foo")
+        , @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 2 packages in [TIME]
+    Downloaded 2 packages in [TIME]
+    Installed 2 packages in [TIME]
+     + package-a==1.0.0
+     + package-b==2.0.0+foo
+    "###);
+
+    // The version '2.0.0+foo' satisfies both ==2.0.0 and ==2.0.0+foo.
+    assert_installed(
+        &context.venv,
+        "local_transitive_a",
+        "1.0.0",
+        &context.temp_dir,
+    );
+    assert_installed(
+        &context.venv,
+        "local_transitive_b",
+        "2.0.0+foo",
+        &context.temp_dir,
+    );
+}
+
+/// A transitive constraint on a local version should not match an exclusive ordered
+/// operator.
+///
+/// ```text
+/// local-transitive-greater-than
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   ├── requires a
+/// │   │   └── satisfied by a-1.0.0
+/// │   └── requires b==2.0.0+foo
+/// │       └── satisfied by b-2.0.0+foo
+/// ├── a
+/// │   └── a-1.0.0
+/// │       └── requires b>2.0.0
+/// │           └── unsatisfied: no matching version
+/// └── b
+///     └── b-2.0.0+foo
+/// ```
+#[test]
+fn local_transitive_greater_than() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"local-transitive-greater-than-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("local-transitive-greater-than-a")
+                .arg("local-transitive-greater-than-b==2.0.0+foo")
+        , @r###"
+    success: false
+    exit_code: 1
+    ----- stdout -----
+
+    ----- stderr -----
+      × No solution found when resolving dependencies:
+      ╰─▶ Because package-a==1.0.0 depends on package-b>2.0.0 and only package-a==1.0.0 is available, we can conclude that all versions of package-a depend on package-b>2.0.0.
+          And because you require package-a and package-b==2.0.0+foo, we can conclude that the requirements are unsatisfiable.
+    "###);
+
+    assert_not_installed(
+        &context.venv,
+        "local_transitive_greater_than_a",
+        &context.temp_dir,
+    );
+    assert_not_installed(
+        &context.venv,
+        "local_transitive_greater_than_b",
+        &context.temp_dir,
+    );
+}
+
+/// A transitive constraint on a local version should match an inclusive ordered
+/// operator.
+///
+/// ```text
+/// local-transitive-greater-than-or-equal
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   ├── requires a
+/// │   │   └── satisfied by a-1.0.0
+/// │   └── requires b==2.0.0+foo
+/// │       └── satisfied by b-2.0.0+foo
+/// ├── a
+/// │   └── a-1.0.0
+/// │       └── requires b>=2.0.0
+/// │           └── satisfied by b-2.0.0+foo
+/// └── b
+///     └── b-2.0.0+foo
+/// ```
+#[test]
+fn local_transitive_greater_than_or_equal() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"local-transitive-greater-than-or-equal-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("local-transitive-greater-than-or-equal-a")
+                .arg("local-transitive-greater-than-or-equal-b==2.0.0+foo")
+        , @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 2 packages in [TIME]
+    Downloaded 2 packages in [TIME]
+    Installed 2 packages in [TIME]
+     + package-a==1.0.0
+     + package-b==2.0.0+foo
+    "###);
+
+    // The version '2.0.0+foo' satisfies both >=2.0.0 and ==2.0.0+foo.
+    assert_installed(
+        &context.venv,
+        "local_transitive_greater_than_or_equal_a",
+        "1.0.0",
+        &context.temp_dir,
+    );
+    assert_installed(
+        &context.venv,
+        "local_transitive_greater_than_or_equal_b",
+        "2.0.0+foo",
+        &context.temp_dir,
+    );
+}
+
+/// A transitive constraint on a local version should not match an exclusive ordered
+/// operator.
+///
+/// ```text
+/// local-transitive-less-than
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   ├── requires a
+/// │   │   └── satisfied by a-1.0.0
+/// │   └── requires b==2.0.0+foo
+/// │       └── satisfied by b-2.0.0+foo
+/// ├── a
+/// │   └── a-1.0.0
+/// │       └── requires b<2.0.0
+/// │           └── unsatisfied: no matching version
+/// └── b
+///     └── b-2.0.0+foo
+/// ```
+#[test]
+fn local_transitive_less_than() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"local-transitive-less-than-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("local-transitive-less-than-a")
+                .arg("local-transitive-less-than-b==2.0.0+foo")
+        , @r###"
+    success: false
+    exit_code: 1
+    ----- stdout -----
+
+    ----- stderr -----
+      × No solution found when resolving dependencies:
+      ╰─▶ Because package-a==1.0.0 depends on package-b<2.0.0 and only package-a==1.0.0 is available, we can conclude that all versions of package-a depend on package-b<2.0.0.
+          And because you require package-a and package-b==2.0.0+foo, we can conclude that the requirements are unsatisfiable.
+    "###);
+
+    assert_not_installed(
+        &context.venv,
+        "local_transitive_less_than_a",
+        &context.temp_dir,
+    );
+    assert_not_installed(
+        &context.venv,
+        "local_transitive_less_than_b",
+        &context.temp_dir,
+    );
+}
+
+/// A transitive constraint on a local version should match an inclusive ordered
+/// operator.
+///
+/// ```text
+/// local-transitive-less-than-or-equal
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   ├── requires a
+/// │   │   └── satisfied by a-1.0.0
+/// │   └── requires b==2.0.0+foo
+/// │       └── satisfied by b-2.0.0+foo
+/// ├── a
+/// │   └── a-1.0.0
+/// │       └── requires b<=2.0.0
+/// │           └── satisfied by b-2.0.0+foo
+/// └── b
+///     └── b-2.0.0+foo
+/// ```
+#[test]
+fn local_transitive_less_than_or_equal() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"local-transitive-less-than-or-equal-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("local-transitive-less-than-or-equal-a")
+                .arg("local-transitive-less-than-or-equal-b==2.0.0+foo")
+        , @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 2 packages in [TIME]
+    Downloaded 2 packages in [TIME]
+    Installed 2 packages in [TIME]
+     + package-a==1.0.0
+     + package-b==2.0.0+foo
+    "###);
+
+    // The version '2.0.0+foo' satisfies both <=2.0.0 and ==2.0.0+foo.
+    assert_installed(
+        &context.venv,
+        "local_transitive_less_than_or_equal_a",
+        "1.0.0",
+        &context.temp_dir,
+    );
+    assert_installed(
+        &context.venv,
+        "local_transitive_less_than_or_equal_b",
+        "2.0.0+foo",
+        &context.temp_dir,
+    );
+}
+
+/// A transitive dependency has both a non-local and local version published, but
+/// the non-local version is unusable.
+///
+/// ```text
+/// local-transitive-confounding
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a
+/// │       └── satisfied by a-1.0.0
+/// ├── a
+/// │   └── a-1.0.0
+/// │       └── requires b==2.0.0
+/// │           ├── satisfied by b-2.0.0
+/// │           └── satisfied by b-2.0.0+foo
+/// └── b
+///     ├── b-2.0.0
+///     └── b-2.0.0+foo
+/// ```
+#[test]
+fn local_transitive_confounding() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"local-transitive-confounding-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("local-transitive-confounding-a")
+        , @r###"
+    success: false
+    exit_code: 1
+    ----- stdout -----
+
+    ----- stderr -----
+      × No solution found when resolving dependencies:
+      ╰─▶ Because package-b==2.0.0 has no wheels are available with a matching Python ABI and package-a==1.0.0 depends on package-b==2.0.0, we can conclude that package-a==1.0.0 cannot be used.
+          And because only package-a==1.0.0 is available and you require package-a, we can conclude that the requirements are unsatisfiable.
+    "###);
+
+    // The version '2.0.0+foo' satisfies the constraint '==2.0.0'.
+    assert_not_installed(
+        &context.venv,
+        "local_transitive_confounding_a",
+        &context.temp_dir,
+    );
+}
+
+/// A dependency depends on a conflicting local version of a direct dependency.
+///
+/// ```text
+/// local-transitive-conflicting
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   ├── requires a
+/// │   │   └── satisfied by a-1.0.0
+/// │   └── requires b==2.0.0+foo
+/// │       └── satisfied by b-2.0.0+foo
+/// ├── a
+/// │   └── a-1.0.0
+/// │       └── requires b==2.0.0+bar
+/// │           └── satisfied by b-2.0.0+bar
+/// └── b
+///     ├── b-2.0.0+foo
+///     └── b-2.0.0+bar
+/// ```
+#[test]
+fn local_transitive_conflicting() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"local-transitive-conflicting-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("local-transitive-conflicting-a")
+                .arg("local-transitive-conflicting-b==2.0.0+foo")
+        , @r###"
+    success: false
+    exit_code: 1
+    ----- stdout -----
+
+    ----- stderr -----
+      × No solution found when resolving dependencies:
+      ╰─▶ Because package-a==1.0.0 depends on package-b==2.0.0+bar and only package-a==1.0.0 is available, we can conclude that all versions of package-a depend on package-b==2.0.0+bar.
+          And because you require package-a and package-b==2.0.0+foo, we can conclude that the requirements are unsatisfiable.
+    "###);
+
+    assert_not_installed(
+        &context.venv,
+        "local_transitive_conflicting_a",
+        &context.temp_dir,
+    );
+    assert_not_installed(
+        &context.venv,
+        "local_transitive_conflicting_b",
+        &context.temp_dir,
+    );
+}
+
+/// A dependency depends on a conflicting local version of a direct dependency, but
+/// we can backtrack to a compatible version.
+///
+/// ```text
+/// local-transitive-backtrack
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   ├── requires a
+/// │   │   ├── satisfied by a-1.0.0
+/// │   │   └── satisfied by a-2.0.0
+/// │   └── requires b==2.0.0+foo
+/// │       └── satisfied by b-2.0.0+foo
+/// ├── a
+/// │   ├── a-1.0.0
+/// │   │   └── requires b==2.0.0
+/// │   │       ├── satisfied by b-2.0.0+foo
+/// │   │       └── satisfied by b-2.0.0+bar
+/// │   └── a-2.0.0
+/// │       └── requires b==2.0.0+bar
+/// │           └── satisfied by b-2.0.0+bar
+/// └── b
+///     ├── b-2.0.0+foo
+///     └── b-2.0.0+bar
+/// ```
+#[test]
+fn local_transitive_backtrack() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"local-transitive-backtrack-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("local-transitive-backtrack-a")
+                .arg("local-transitive-backtrack-b==2.0.0+foo")
+        , @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 2 packages in [TIME]
+    Downloaded 2 packages in [TIME]
+    Installed 2 packages in [TIME]
+     + package-a==1.0.0
+     + package-b==2.0.0+foo
+    "###);
+
+    // Backtracking to '1.0.0' gives us compatible local versions of b.
+    assert_installed(
+        &context.venv,
+        "local_transitive_backtrack_a",
+        "1.0.0",
+        &context.temp_dir,
+    );
+    assert_installed(
+        &context.venv,
+        "local_transitive_backtrack_b",
+        "2.0.0+foo",
+        &context.temp_dir,
+    );
+}
+
+/// A local version should be excluded in exclusive ordered comparisons.
+///
+/// ```text
+/// local-greater-than
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a>1.2.3
+/// │       └── unsatisfied: no matching version
+/// └── a
+///     └── a-1.2.3+foo
+/// ```
+#[test]
+fn local_greater_than() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"local-greater-than-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("local-greater-than-a>1.2.3")
+        , @r###"
+    success: false
+    exit_code: 1
+    ----- stdout -----
+
+    ----- stderr -----
+      × No solution found when resolving dependencies:
+      ╰─▶ Because only package-a<=1.2.3 is available and you require package-a>1.2.3, we can conclude that the requirements are unsatisfiable.
+    "###);
+
+    assert_not_installed(&context.venv, "local_greater_than_a", &context.temp_dir);
+}
+
+/// A local version should be included in inclusive ordered comparisons.
+///
+/// ```text
+/// local-greater-than-or-equal
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a>=1.2.3
+/// │       └── satisfied by a-1.2.3+foo
+/// └── a
+///     └── a-1.2.3+foo
+/// ```
+#[test]
+fn local_greater_than_or_equal() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"local-greater-than-or-equal-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("local-greater-than-or-equal-a>=1.2.3")
+        , @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + package-a==1.2.3+foo
+    "###);
+
+    // The version '1.2.3+foo' satisfies the constraint '>=1.2.3'.
+    assert_installed(
+        &context.venv,
+        "local_greater_than_or_equal_a",
+        "1.2.3+foo",
+        &context.temp_dir,
+    );
+}
+
+/// A local version should be excluded in exclusive ordered comparisons.
+///
+/// ```text
+/// local-less-than
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a<1.2.3
+/// │       └── unsatisfied: no matching version
+/// └── a
+///     └── a-1.2.3+foo
+/// ```
+#[test]
+fn local_less_than() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"local-less-than-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("local-less-than-a<1.2.3")
+        , @r###"
+    success: false
+    exit_code: 1
+    ----- stdout -----
+
+    ----- stderr -----
+      × No solution found when resolving dependencies:
+      ╰─▶ Because only package-a>=1.2.3 is available and you require package-a<1.2.3, we can conclude that the requirements are unsatisfiable.
+    "###);
+
+    assert_not_installed(&context.venv, "local_less_than_a", &context.temp_dir);
+}
+
+/// A local version should be included in inclusive ordered comparisons.
+///
+/// ```text
+/// local-less-than-or-equal
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a<=1.2.3
+/// │       └── satisfied by a-1.2.3+foo
+/// └── a
+///     └── a-1.2.3+foo
+/// ```
+#[test]
+fn local_less_than_or_equal() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"local-less-than-or-equal-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("local-less-than-or-equal-a<=1.2.3")
+        , @r###"
+    success: false
+    exit_code: 1
+    ----- stdout -----
+
+    ----- stderr -----
+      × No solution found when resolving dependencies:
+      ╰─▶ Because only package-a>1.2.3 is available and you require package-a<=1.2.3, we can conclude that the requirements are unsatisfiable.
+    "###);
+
+    // The version '1.2.3+foo' satisfies the constraint '<=1.2.3'.
+    assert_not_installed(
+        &context.venv,
+        "local_less_than_or_equal_a",
+        &context.temp_dir,
+    );
+}
+
+/// A simple version constraint should not match a post-release version.
+///
+/// ```text
+/// post-simple
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a==1.2.3
+/// │       └── unsatisfied: no matching version
+/// └── a
+///     └── a-1.2.3.post1
+/// ```
+#[test]
+fn post_simple() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"post-simple-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("post-simple-a==1.2.3")
+        , @r###"
+    success: false
+    exit_code: 1
+    ----- stdout -----
+
+    ----- stderr -----
+      × No solution found when resolving dependencies:
+      ╰─▶ Because there is no version of package-a==1.2.3 and you require package-a==1.2.3, we can conclude that the requirements are unsatisfiable.
+    "###);
+
+    assert_not_installed(&context.venv, "post_simple_a", &context.temp_dir);
+}
+
+/// A greater-than-or-equal version constraint should match a post-release version.
+///
+/// ```text
+/// post-greater-than-or-equal
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a>=1.2.3
+/// │       └── satisfied by a-1.2.3.post1
+/// └── a
+///     └── a-1.2.3.post1
+/// ```
+#[test]
+fn post_greater_than_or_equal() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"post-greater-than-or-equal-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("post-greater-than-or-equal-a>=1.2.3")
+        , @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + package-a==1.2.3.post1
+    "###);
+
+    // The version '1.2.3.post1' satisfies the constraint '>=1.2.3'.
+    assert_installed(
+        &context.venv,
+        "post_greater_than_or_equal_a",
+        "1.2.3.post1",
+        &context.temp_dir,
+    );
+}
+
+/// A greater-than version constraint should not match a post-release version.
+///
+/// ```text
+/// post-greater-than
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a>1.2.3
+/// │       └── unsatisfied: no matching version
+/// └── a
+///     └── a-1.2.3.post1
+/// ```
+#[test]
+fn post_greater_than() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"post-greater-than-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("post-greater-than-a>1.2.3")
+        , @r###"
+    success: false
+    exit_code: 1
+    ----- stdout -----
+
+    ----- stderr -----
+      × No solution found when resolving dependencies:
+      ╰─▶ Because only package-a<=1.2.3 is available and you require package-a>1.2.3, we can conclude that the requirements are unsatisfiable.
+    "###);
+
+    assert_not_installed(&context.venv, "post_greater_than_a", &context.temp_dir);
+}
+
+/// A greater-than version constraint should match a post-release version if the
+/// constraint is itself a post-release version.
+///
+/// ```text
+/// post-greater-than-post
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a>1.2.3.post0
+/// │       └── satisfied by a-1.2.3.post1
+/// └── a
+///     ├── a-1.2.3.post0
+///     └── a-1.2.3.post1
+/// ```
+#[test]
+fn post_greater_than_post() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"post-greater-than-post-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("post-greater-than-post-a>1.2.3.post0")
+        , @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + package-a==1.2.3.post1
+    "###);
+
+    // The version '1.2.3.post1' satisfies the constraint '>1.2.3.post0'.
+    assert_installed(
+        &context.venv,
+        "post_greater_than_post_a",
+        "1.2.3.post1",
+        &context.temp_dir,
+    );
+}
+
+/// A greater-than-or-equal version constraint should match a post-release version
+/// if the constraint is itself a post-release version.
+///
+/// ```text
+/// post-greater-than-or-equal-post
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a>=1.2.3.post0
+/// │       ├── satisfied by a-1.2.3.post0
+/// │       └── satisfied by a-1.2.3.post1
+/// └── a
+///     ├── a-1.2.3.post0
+///     └── a-1.2.3.post1
+/// ```
+#[test]
+fn post_greater_than_or_equal_post() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"post-greater-than-or-equal-post-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("post-greater-than-or-equal-post-a>=1.2.3.post0")
+        , @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + package-a==1.2.3.post1
+    "###);
+
+    // The version '1.2.3.post1' satisfies the constraint '>=1.2.3.post0'.
+    assert_installed(
+        &context.venv,
+        "post_greater_than_or_equal_post_a",
+        "1.2.3.post1",
+        &context.temp_dir,
+    );
+}
+
+/// A less-than-or-equal version constraint should not match a post-release version.
+///
+/// ```text
+/// post-less-than-or-equal
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a<=1.2.3
+/// │       └── unsatisfied: no matching version
+/// └── a
+///     └── a-1.2.3.post1
+/// ```
+#[test]
+fn post_less_than_or_equal() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"post-less-than-or-equal-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("post-less-than-or-equal-a<=1.2.3")
+        , @r###"
+    success: false
+    exit_code: 1
+    ----- stdout -----
+
+    ----- stderr -----
+      × No solution found when resolving dependencies:
+      ╰─▶ Because only package-a>1.2.3 is available and you require package-a<=1.2.3, we can conclude that the requirements are unsatisfiable.
+    "###);
+
+    assert_not_installed(
+        &context.venv,
+        "post_less_than_or_equal_a",
+        &context.temp_dir,
+    );
+}
+
+/// A less-than version constraint should not match a post-release version.
+///
+/// ```text
+/// post-less-than
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a<1.2.3
+/// │       └── unsatisfied: no matching version
+/// └── a
+///     └── a-1.2.3.post1
+/// ```
+#[test]
+fn post_less_than() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"post-less-than-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("post-less-than-a<1.2.3")
+        , @r###"
+    success: false
+    exit_code: 1
+    ----- stdout -----
+
+    ----- stderr -----
+      × No solution found when resolving dependencies:
+      ╰─▶ Because only package-a>=1.2.3 is available and you require package-a<1.2.3, we can conclude that the requirements are unsatisfiable.
+    "###);
+
+    assert_not_installed(&context.venv, "post_less_than_a", &context.temp_dir);
+}
+
+/// A greater-than version constraint should not match a post-release version with a
+/// local version identifier.
+///
+/// ```text
+/// post-local-greater-than
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a>1.2.3
+/// │       └── unsatisfied: no matching version
+/// └── a
+///     ├── a-1.2.3.post1
+///     └── a-1.2.3.post1+local
+/// ```
+#[test]
+fn post_local_greater_than() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"post-local-greater-than-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("post-local-greater-than-a>1.2.3")
+        , @r###"
+    success: false
+    exit_code: 1
+    ----- stdout -----
+
+    ----- stderr -----
+      × No solution found when resolving dependencies:
+      ╰─▶ Because only package-a<=1.2.3 is available and you require package-a>1.2.3, we can conclude that the requirements are unsatisfiable.
+    "###);
+
+    assert_not_installed(
+        &context.venv,
+        "post_local_greater_than_a",
+        &context.temp_dir,
+    );
+}
+
+/// A greater-than version constraint should not match a post-release version with a
+/// local version identifier.
+///
+/// ```text
+/// post-local-greater-than-post
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a>1.2.3.post1
+/// │       └── unsatisfied: no matching version
+/// └── a
+///     ├── a-1.2.3.post1
+///     └── a-1.2.3.post1+local
+/// ```
+#[test]
+fn post_local_greater_than_post() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"post-local-greater-than-post-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("post-local-greater-than-post-a>1.2.3.post1")
+        , @r###"
+    success: false
+    exit_code: 1
+    ----- stdout -----
+
+    ----- stderr -----
+      × No solution found when resolving dependencies:
+      ╰─▶ Because only package-a<1.2.3.post2 is available and you require package-a>=1.2.3.post2, we can conclude that the requirements are unsatisfiable.
+    "###);
+
+    assert_not_installed(
+        &context.venv,
+        "post_local_greater_than_post_a",
+        &context.temp_dir,
+    );
+}
+
+/// An equal version constraint should not match a post-release version if the post-
+/// release version is not available.
+///
+/// ```text
+/// post-equal-not-available
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a==1.2.3.post0
+/// │       └── unsatisfied: no matching version
+/// └── a
+///     ├── a-1.2.3
+///     └── a-1.2.3.post1
+/// ```
+#[test]
+fn post_equal_not_available() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"post-equal-not-available-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("post-equal-not-available-a==1.2.3.post0")
+        , @r###"
+    success: false
+    exit_code: 1
+    ----- stdout -----
+
+    ----- stderr -----
+      × No solution found when resolving dependencies:
+      ╰─▶ Because there is no version of package-a==1.2.3.post0 and you require package-a==1.2.3.post0, we can conclude that the requirements are unsatisfiable.
+    "###);
+
+    assert_not_installed(
+        &context.venv,
+        "post_equal_not_available_a",
+        &context.temp_dir,
+    );
+}
+
+/// An equal version constraint should match a post-release version if the post-
+/// release version is available.
+///
+/// ```text
+/// post-equal-available
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a==1.2.3.post0
+/// │       └── satisfied by a-1.2.3.post0
+/// └── a
+///     ├── a-1.2.3
+///     └── a-1.2.3.post0
+/// ```
+#[test]
+fn post_equal_available() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"post-equal-available-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("post-equal-available-a==1.2.3.post0")
+        , @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + package-a==1.2.3.post0
+    "###);
+
+    // The version '1.2.3.post0' satisfies the constraint '==1.2.3.post0'.
+    assert_installed(
+        &context.venv,
+        "post_equal_available_a",
+        "1.2.3.post0",
+        &context.temp_dir,
+    );
+}
+
+/// A greater-than version constraint should not match a post-release version if the
+/// post-release version is not available.
+///
+/// ```text
+/// post-greater-than-post-not-available
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a>1.2.3.post2
+/// │       └── unsatisfied: no matching version
+/// └── a
+///     ├── a-1.2.3
+///     ├── a-1.2.3.post0
+///     └── a-1.2.3.post1
+/// ```
+#[test]
+fn post_greater_than_post_not_available() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"post-greater-than-post-not-available-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("post-greater-than-post-not-available-a>1.2.3.post2")
+        , @r###"
+    success: false
+    exit_code: 1
+    ----- stdout -----
+
+    ----- stderr -----
+      × No solution found when resolving dependencies:
+      ╰─▶ Because only package-a<1.2.3.post3 is available and you require package-a>=1.2.3.post3, we can conclude that the requirements are unsatisfiable.
+    "###);
+
+    assert_not_installed(
+        &context.venv,
+        "post_greater_than_post_not_available_a",
+        &context.temp_dir,
+    );
+}
+
 /// The user requires any version of package `a` which only has prerelease versions
 /// available.
 ///
 /// ```text
-/// 472fcc7e
+/// package-only-prereleases
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a
 /// │       └── unsatisfied: no matching version
 /// └── a
 ///     └── a-1.0.0a1
 /// ```
 #[test]
 fn package_only_prereleases() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-472fcc7e", "albatross"));
-    filters.push((r"-472fcc7e", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"package-only-prereleases-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-472fcc7e")
+        .arg("package-only-prereleases-a")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 1 package in [TIME]
     Downloaded 1 package in [TIME]
     Installed 1 package in [TIME]
-     + albatross==1.0.0a1
+     + package-a==1.0.0a1
     "###);
 
     // Since there are only prerelease versions of `a` available, it should be
     // installed even though the user did not include a prerelease specifier.
-    assert_installed(&context.venv, "a_472fcc7e", "1.0.0a1", &context.temp_dir);
+    assert_installed(
+        &context.venv,
+        "package_only_prereleases_a",
+        "1.0.0a1",
+        &context.temp_dir,
+    );
 }
 
-/// package-only-prereleases-in-range
-///
 /// The user requires a version of package `a` which only matches prerelease
 /// versions but they did not include a prerelease specifier.
 ///
 /// ```text
-/// 1017748b
+/// package-only-prereleases-in-range
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a>0.1.0
 /// │       └── unsatisfied: no matching version
 /// └── a
 ///     ├── a-0.1.0
 ///     └── a-1.0.0a1
 /// ```
 #[test]
 fn package_only_prereleases_in_range() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-1017748b", "albatross"));
-    filters.push((r"-1017748b", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"package-only-prereleases-in-range-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-1017748b>0.1.0")
+        .arg("package-only-prereleases-in-range-a>0.1.0")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because only albatross<=0.1.0 is available and you require albatross>0.1.0, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because only package-a<=0.1.0 is available and you require package-a>0.1.0, we can conclude that the requirements are unsatisfiable.
 
-          hint: Pre-releases are available for albatross in the requested range (e.g., 1.0.0a1), but pre-releases weren't enabled (try: `--prerelease=allow`)
+          hint: Pre-releases are available for package-a in the requested range (e.g., 1.0.0a1), but pre-releases weren't enabled (try: `--prerelease=allow`)
     "###);
 
     // Since there are stable versions of `a` available, prerelease versions should not
     // be selected without explicit opt-in.
-    assert_not_installed(&context.venv, "a_1017748b", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "package_only_prereleases_in_range_a",
+        &context.temp_dir,
+    );
 }
 
-/// requires-package-only-prereleases-in-range-global-opt-in
-///
 /// The user requires a version of package `a` which only matches prerelease
 /// versions. They did not include a prerelease specifier for the package, but they
 /// opted into prereleases globally.
 ///
 /// ```text
-/// 95140069
+/// requires-package-only-prereleases-in-range-global-opt-in
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a>0.1.0
 /// │       └── unsatisfied: no matching version
 /// └── a
 ///     ├── a-0.1.0
 ///     └── a-1.0.0a1
 /// ```
 #[test]
 fn requires_package_only_prereleases_in_range_global_opt_in() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-95140069", "albatross"));
-    filters.push((r"-95140069", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((
+        r"requires-package-only-prereleases-in-range-global-opt-in-",
+        "package-",
+    ));
 
     uv_snapshot!(filters, command(&context)
         .arg("--prerelease=allow")
-        .arg("a-95140069>0.1.0")
+        .arg("requires-package-only-prereleases-in-range-global-opt-in-a>0.1.0")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 1 package in [TIME]
     Downloaded 1 package in [TIME]
     Installed 1 package in [TIME]
-     + albatross==1.0.0a1
+     + package-a==1.0.0a1
     "###);
 
-    assert_installed(&context.venv, "a_95140069", "1.0.0a1", &context.temp_dir);
+    assert_installed(
+        &context.venv,
+        "requires_package_only_prereleases_in_range_global_opt_in_a",
+        "1.0.0a1",
+        &context.temp_dir,
+    );
 }
 
-/// requires-package-prerelease-and-final-any
-///
 /// The user requires any version of package `a` has a prerelease version available
 /// and an older non-prerelease version.
 ///
 /// ```text
-/// 909975d8
+/// requires-package-prerelease-and-final-any
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a
 /// │       └── satisfied by a-0.1.0
 /// └── a
 ///     ├── a-0.1.0
 ///     └── a-1.0.0a1
 /// ```
 #[test]
 fn requires_package_prerelease_and_final_any() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-909975d8", "albatross"));
-    filters.push((r"-909975d8", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"requires-package-prerelease-and-final-any-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-909975d8")
+        .arg("requires-package-prerelease-and-final-any-a")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 1 package in [TIME]
     Downloaded 1 package in [TIME]
     Installed 1 package in [TIME]
-     + albatross==0.1.0
+     + package-a==0.1.0
     "###);
 
     // Since the user did not provide a prerelease specifier, the older stable version
     // should be selected.
-    assert_installed(&context.venv, "a_909975d8", "0.1.0", &context.temp_dir);
+    assert_installed(
+        &context.venv,
+        "requires_package_prerelease_and_final_any_a",
+        "0.1.0",
+        &context.temp_dir,
+    );
 }
 
-/// package-prerelease-specified-only-final-available
-///
 /// The user requires a version of `a` with a prerelease specifier and only stable
 /// releases are available.
 ///
 /// ```text
-/// 6f8bea9f
+/// package-prerelease-specified-only-final-available
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a>=0.1.0a1
 /// │       ├── satisfied by a-0.1.0
 /// │       ├── satisfied by a-0.2.0
 /// │       └── satisfied by a-0.3.0
@@ -1377,44 +2765,49 @@
 ///     ├── a-0.2.0
 ///     └── a-0.3.0
 /// ```
 #[test]
 fn package_prerelease_specified_only_final_available() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-6f8bea9f", "albatross"));
-    filters.push((r"-6f8bea9f", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((
+        r"package-prerelease-specified-only-final-available-",
+        "package-",
+    ));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-6f8bea9f>=0.1.0a1")
+        .arg("package-prerelease-specified-only-final-available-a>=0.1.0a1")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 1 package in [TIME]
     Downloaded 1 package in [TIME]
     Installed 1 package in [TIME]
-     + albatross==0.3.0
+     + package-a==0.3.0
     "###);
 
     // The latest stable version should be selected.
-    assert_installed(&context.venv, "a_6f8bea9f", "0.3.0", &context.temp_dir);
+    assert_installed(
+        &context.venv,
+        "package_prerelease_specified_only_final_available_a",
+        "0.3.0",
+        &context.temp_dir,
+    );
 }
 
-/// package-prerelease-specified-only-prerelease-available
-///
 /// The user requires a version of `a` with a prerelease specifier and only
 /// prerelease releases are available.
 ///
 /// ```text
-/// 48d4bba0
+/// package-prerelease-specified-only-prerelease-available
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a>=0.1.0a1
 /// │       ├── satisfied by a-0.1.0a1
 /// │       ├── satisfied by a-0.2.0a1
 /// │       └── satisfied by a-0.3.0a1
@@ -1423,44 +2816,49 @@
 ///     ├── a-0.2.0a1
 ///     └── a-0.3.0a1
 /// ```
 #[test]
 fn package_prerelease_specified_only_prerelease_available() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-48d4bba0", "albatross"));
-    filters.push((r"-48d4bba0", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((
+        r"package-prerelease-specified-only-prerelease-available-",
+        "package-",
+    ));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-48d4bba0>=0.1.0a1")
+        .arg("package-prerelease-specified-only-prerelease-available-a>=0.1.0a1")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 1 package in [TIME]
     Downloaded 1 package in [TIME]
     Installed 1 package in [TIME]
-     + albatross==0.3.0a1
+     + package-a==0.3.0a1
     "###);
 
     // The latest prerelease version should be selected.
-    assert_installed(&context.venv, "a_48d4bba0", "0.3.0a1", &context.temp_dir);
+    assert_installed(
+        &context.venv,
+        "package_prerelease_specified_only_prerelease_available_a",
+        "0.3.0a1",
+        &context.temp_dir,
+    );
 }
 
-/// package-prerelease-specified-mixed-available
-///
 /// The user requires a version of `a` with a prerelease specifier and both
 /// prerelease and stable releases are available.
 ///
 /// ```text
-/// 2b1193a7
+/// package-prerelease-specified-mixed-available
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a>=0.1.0a1
 /// │       ├── satisfied by a-0.1.0
 /// │       ├── satisfied by a-0.2.0a1
 /// │       ├── satisfied by a-0.3.0
@@ -1471,45 +2869,47 @@
 ///     ├── a-0.3.0
 ///     └── a-1.0.0a1
 /// ```
 #[test]
 fn package_prerelease_specified_mixed_available() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-2b1193a7", "albatross"));
-    filters.push((r"-2b1193a7", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"package-prerelease-specified-mixed-available-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-2b1193a7>=0.1.0a1")
+        .arg("package-prerelease-specified-mixed-available-a>=0.1.0a1")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 1 package in [TIME]
     Downloaded 1 package in [TIME]
     Installed 1 package in [TIME]
-     + albatross==1.0.0a1
+     + package-a==1.0.0a1
     "###);
 
     // Since the user provided a prerelease specifier, the latest prerelease version
     // should be selected.
-    assert_installed(&context.venv, "a_2b1193a7", "1.0.0a1", &context.temp_dir);
+    assert_installed(
+        &context.venv,
+        "package_prerelease_specified_mixed_available_a",
+        "1.0.0a1",
+        &context.temp_dir,
+    );
 }
 
-/// package-multiple-prereleases-kinds
-///
 /// The user requires `a` which has multiple prereleases available with different
 /// labels.
 ///
 /// ```text
-/// 72919cf7
+/// package-multiple-prereleases-kinds
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a>=1.0.0a1
 /// │       ├── satisfied by a-1.0.0a1
 /// │       ├── satisfied by a-1.0.0b1
 /// │       └── satisfied by a-1.0.0rc1
@@ -1518,43 +2918,45 @@
 ///     ├── a-1.0.0b1
 ///     └── a-1.0.0rc1
 /// ```
 #[test]
 fn package_multiple_prereleases_kinds() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-72919cf7", "albatross"));
-    filters.push((r"-72919cf7", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"package-multiple-prereleases-kinds-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-72919cf7>=1.0.0a1")
+        .arg("package-multiple-prereleases-kinds-a>=1.0.0a1")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 1 package in [TIME]
     Downloaded 1 package in [TIME]
     Installed 1 package in [TIME]
-     + albatross==1.0.0rc1
+     + package-a==1.0.0rc1
     "###);
 
     // Release candidates should be the highest precedence prerelease kind.
-    assert_installed(&context.venv, "a_72919cf7", "1.0.0rc1", &context.temp_dir);
+    assert_installed(
+        &context.venv,
+        "package_multiple_prereleases_kinds_a",
+        "1.0.0rc1",
+        &context.temp_dir,
+    );
 }
 
-/// package-multiple-prereleases-numbers
-///
 /// The user requires `a` which has multiple alphas available.
 ///
 /// ```text
-/// cecdb92d
+/// package-multiple-prereleases-numbers
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a>=1.0.0a1
 /// │       ├── satisfied by a-1.0.0a1
 /// │       ├── satisfied by a-1.0.0a2
 /// │       └── satisfied by a-1.0.0a3
@@ -1563,44 +2965,46 @@
 ///     ├── a-1.0.0a2
 ///     └── a-1.0.0a3
 /// ```
 #[test]
 fn package_multiple_prereleases_numbers() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-cecdb92d", "albatross"));
-    filters.push((r"-cecdb92d", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"package-multiple-prereleases-numbers-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-cecdb92d>=1.0.0a1")
+        .arg("package-multiple-prereleases-numbers-a>=1.0.0a1")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 1 package in [TIME]
     Downloaded 1 package in [TIME]
     Installed 1 package in [TIME]
-     + albatross==1.0.0a3
+     + package-a==1.0.0a3
     "###);
 
     // The latest alpha version should be selected.
-    assert_installed(&context.venv, "a_cecdb92d", "1.0.0a3", &context.temp_dir);
+    assert_installed(
+        &context.venv,
+        "package_multiple_prereleases_numbers_a",
+        "1.0.0a3",
+        &context.temp_dir,
+    );
 }
 
-/// transitive-package-only-prereleases
-///
 /// The user requires any version of package `a` which requires `b` which only has
 /// prerelease versions available.
 ///
 /// ```text
-/// e3c94488
+/// transitive-package-only-prereleases
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a
 /// │       └── satisfied by a-0.1.0
 /// ├── a
 /// │   └── a-0.1.0
@@ -1609,48 +3013,54 @@
 /// └── b
 ///     └── b-1.0.0a1
 /// ```
 #[test]
 fn transitive_package_only_prereleases() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-e3c94488", "albatross"));
-    filters.push((r"b-e3c94488", "bluebird"));
-    filters.push((r"-e3c94488", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"transitive-package-only-prereleases-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-e3c94488")
+        .arg("transitive-package-only-prereleases-a")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 2 packages in [TIME]
     Downloaded 2 packages in [TIME]
     Installed 2 packages in [TIME]
-     + albatross==0.1.0
-     + bluebird==1.0.0a1
+     + package-a==0.1.0
+     + package-b==1.0.0a1
     "###);
 
     // Since there are only prerelease versions of `b` available, it should be selected
     // even though the user did not opt-in to prereleases.
-    assert_installed(&context.venv, "a_e3c94488", "0.1.0", &context.temp_dir);
-    assert_installed(&context.venv, "b_e3c94488", "1.0.0a1", &context.temp_dir);
+    assert_installed(
+        &context.venv,
+        "transitive_package_only_prereleases_a",
+        "0.1.0",
+        &context.temp_dir,
+    );
+    assert_installed(
+        &context.venv,
+        "transitive_package_only_prereleases_b",
+        "1.0.0a1",
+        &context.temp_dir,
+    );
 }
 
-/// transitive-package-only-prereleases-in-range
-///
 /// The user requires package `a` which has a dependency on a package which only
 /// matches prerelease versions but they did not include a prerelease specifier.
 ///
 /// ```text
-/// 20238f1b
+/// transitive-package-only-prereleases-in-range
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a
 /// │       └── satisfied by a-0.1.0
 /// ├── a
 /// │   └── a-0.1.0
@@ -1660,49 +3070,49 @@
 ///     ├── b-0.1.0
 ///     └── b-1.0.0a1
 /// ```
 #[test]
 fn transitive_package_only_prereleases_in_range() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-20238f1b", "albatross"));
-    filters.push((r"b-20238f1b", "bluebird"));
-    filters.push((r"-20238f1b", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"transitive-package-only-prereleases-in-range-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-20238f1b")
+        .arg("transitive-package-only-prereleases-in-range-a")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because only bluebird<=0.1 is available and albatross==0.1.0 depends on bluebird>0.1, we can conclude that albatross==0.1.0 cannot be used.
-          And because only albatross==0.1.0 is available and you require albatross, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because only package-b<=0.1 is available and package-a==0.1.0 depends on package-b>0.1, we can conclude that package-a==0.1.0 cannot be used.
+          And because only package-a==0.1.0 is available and you require package-a, we can conclude that the requirements are unsatisfiable.
 
-          hint: Pre-releases are available for bluebird in the requested range (e.g., 1.0.0a1), but pre-releases weren't enabled (try: `--prerelease=allow`)
+          hint: Pre-releases are available for package-b in the requested range (e.g., 1.0.0a1), but pre-releases weren't enabled (try: `--prerelease=allow`)
     "###);
 
     // Since there are stable versions of `b` available, the prerelease version should
     // not be selected without explicit opt-in. The available version is excluded by
     // the range requested by the user.
-    assert_not_installed(&context.venv, "a_20238f1b", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "transitive_package_only_prereleases_in_range_a",
+        &context.temp_dir,
+    );
 }
 
-/// transitive-package-only-prereleases-in-range-opt-in
-///
 /// The user requires package `a` which has a dependency on a package which only
 /// matches prerelease versions; the user has opted into allowing prereleases in `b`
 /// explicitly.
 ///
 /// ```text
-/// d65d5fdf
+/// transitive-package-only-prereleases-in-range-opt-in
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   ├── requires a
 /// │   │   └── satisfied by a-0.1.0
 /// │   └── requires b>0.0.0a1
 /// │       └── satisfied by b-0.1.0
@@ -1714,49 +3124,58 @@
 ///     ├── b-0.1.0
 ///     └── b-1.0.0a1
 /// ```
 #[test]
 fn transitive_package_only_prereleases_in_range_opt_in() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-d65d5fdf", "albatross"));
-    filters.push((r"b-d65d5fdf", "bluebird"));
-    filters.push((r"-d65d5fdf", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((
+        r"transitive-package-only-prereleases-in-range-opt-in-",
+        "package-",
+    ));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-d65d5fdf")
-                .arg("b-d65d5fdf>0.0.0a1")
+        .arg("transitive-package-only-prereleases-in-range-opt-in-a")
+                .arg("transitive-package-only-prereleases-in-range-opt-in-b>0.0.0a1")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 2 packages in [TIME]
     Downloaded 2 packages in [TIME]
     Installed 2 packages in [TIME]
-     + albatross==0.1.0
-     + bluebird==1.0.0a1
+     + package-a==0.1.0
+     + package-b==1.0.0a1
     "###);
 
     // Since the user included a dependency on `b` with a prerelease specifier, a
     // prerelease version can be selected.
-    assert_installed(&context.venv, "a_d65d5fdf", "0.1.0", &context.temp_dir);
-    assert_installed(&context.venv, "b_d65d5fdf", "1.0.0a1", &context.temp_dir);
+    assert_installed(
+        &context.venv,
+        "transitive_package_only_prereleases_in_range_opt_in_a",
+        "0.1.0",
+        &context.temp_dir,
+    );
+    assert_installed(
+        &context.venv,
+        "transitive_package_only_prereleases_in_range_opt_in_b",
+        "1.0.0a1",
+        &context.temp_dir,
+    );
 }
 
-/// transitive-prerelease-and-stable-dependency
-///
 /// A transitive dependency has both a prerelease and a stable selector, but can
 /// only be satisfied by a prerelease
 ///
 /// ```text
-/// d62255d0
+/// transitive-prerelease-and-stable-dependency
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   ├── requires a
 /// │   │   └── satisfied by a-1.0.0
 /// │   └── requires b
 /// │       └── satisfied by b-1.0.0
@@ -1772,50 +3191,53 @@
 ///     ├── c-1.0.0
 ///     └── c-2.0.0b1
 /// ```
 #[test]
 fn transitive_prerelease_and_stable_dependency() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-d62255d0", "albatross"));
-    filters.push((r"b-d62255d0", "bluebird"));
-    filters.push((r"c-d62255d0", "crow"));
-    filters.push((r"-d62255d0", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"transitive-prerelease-and-stable-dependency-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-d62255d0")
-                .arg("b-d62255d0")
+        .arg("transitive-prerelease-and-stable-dependency-a")
+                .arg("transitive-prerelease-and-stable-dependency-b")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because there is no version of crow==2.0.0b1 and albatross==1.0.0 depends on crow==2.0.0b1, we can conclude that albatross==1.0.0 cannot be used.
-          And because only albatross==1.0.0 is available and you require albatross, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because there is no version of package-c==2.0.0b1 and package-a==1.0.0 depends on package-c==2.0.0b1, we can conclude that package-a==1.0.0 cannot be used.
+          And because only package-a==1.0.0 is available and you require package-a, we can conclude that the requirements are unsatisfiable.
 
-          hint: crow was requested with a pre-release marker (e.g., crow==2.0.0b1), but pre-releases weren't enabled (try: `--prerelease=allow`)
+          hint: package-c was requested with a pre-release marker (e.g., package-c==2.0.0b1), but pre-releases weren't enabled (try: `--prerelease=allow`)
     "###);
 
     // Since the user did not explicitly opt-in to a prerelease, it cannot be selected.
-    assert_not_installed(&context.venv, "a_d62255d0", &context.temp_dir);
-    assert_not_installed(&context.venv, "b_d62255d0", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "transitive_prerelease_and_stable_dependency_a",
+        &context.temp_dir,
+    );
+    assert_not_installed(
+        &context.venv,
+        "transitive_prerelease_and_stable_dependency_b",
+        &context.temp_dir,
+    );
 }
 
-/// transitive-prerelease-and-stable-dependency-opt-in
-///
 /// A transitive dependency has both a prerelease and a stable selector, but can
 /// only be satisfied by a prerelease. The user includes an opt-in to prereleases of
 /// the transitive dependency.
 ///
 /// ```text
-/// 0778b0eb
+/// transitive-prerelease-and-stable-dependency-opt-in
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   ├── requires a
 /// │   │   └── satisfied by a-1.0.0
 /// │   ├── requires b
 /// │   │   └── satisfied by b-1.0.0
@@ -1834,52 +3256,65 @@
 ///     ├── c-1.0.0
 ///     └── c-2.0.0b1
 /// ```
 #[test]
 fn transitive_prerelease_and_stable_dependency_opt_in() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-0778b0eb", "albatross"));
-    filters.push((r"b-0778b0eb", "bluebird"));
-    filters.push((r"c-0778b0eb", "crow"));
-    filters.push((r"-0778b0eb", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((
+        r"transitive-prerelease-and-stable-dependency-opt-in-",
+        "package-",
+    ));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-0778b0eb")
-                .arg("b-0778b0eb")
-                .arg("c-0778b0eb>=0.0.0a1")
+        .arg("transitive-prerelease-and-stable-dependency-opt-in-a")
+                .arg("transitive-prerelease-and-stable-dependency-opt-in-b")
+                .arg("transitive-prerelease-and-stable-dependency-opt-in-c>=0.0.0a1")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 3 packages in [TIME]
     Downloaded 3 packages in [TIME]
     Installed 3 packages in [TIME]
-     + albatross==1.0.0
-     + bluebird==1.0.0
-     + crow==2.0.0b1
+     + package-a==1.0.0
+     + package-b==1.0.0
+     + package-c==2.0.0b1
     "###);
 
     // Since the user explicitly opted-in to a prerelease for `c`, it can be installed.
-    assert_installed(&context.venv, "a_0778b0eb", "1.0.0", &context.temp_dir);
-    assert_installed(&context.venv, "b_0778b0eb", "1.0.0", &context.temp_dir);
-    assert_installed(&context.venv, "c_0778b0eb", "2.0.0b1", &context.temp_dir);
+    assert_installed(
+        &context.venv,
+        "transitive_prerelease_and_stable_dependency_opt_in_a",
+        "1.0.0",
+        &context.temp_dir,
+    );
+    assert_installed(
+        &context.venv,
+        "transitive_prerelease_and_stable_dependency_opt_in_b",
+        "1.0.0",
+        &context.temp_dir,
+    );
+    assert_installed(
+        &context.venv,
+        "transitive_prerelease_and_stable_dependency_opt_in_c",
+        "2.0.0b1",
+        &context.temp_dir,
+    );
 }
 
-/// transitive-prerelease-and-stable-dependency-many-versions
-///
 /// A transitive dependency has both a prerelease and a stable selector, but can
 /// only be satisfied by a prerelease. There are many prerelease versions.
 ///
 /// ```text
-/// cc6a6eac
+/// transitive-prerelease-and-stable-dependency-many-versions
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   ├── requires a
 /// │   │   └── satisfied by a-1.0.0
 /// │   └── requires b
 /// │       └── satisfied by b-1.0.0
@@ -1920,52 +3355,58 @@
 ///     ├── c-2.0.0b8
 ///     └── c-2.0.0b9
 /// ```
 #[test]
 fn transitive_prerelease_and_stable_dependency_many_versions() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-cc6a6eac", "albatross"));
-    filters.push((r"b-cc6a6eac", "bluebird"));
-    filters.push((r"c-cc6a6eac", "crow"));
-    filters.push((r"-cc6a6eac", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((
+        r"transitive-prerelease-and-stable-dependency-many-versions-",
+        "package-",
+    ));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-cc6a6eac")
-                .arg("b-cc6a6eac")
+        .arg("transitive-prerelease-and-stable-dependency-many-versions-a")
+                .arg("transitive-prerelease-and-stable-dependency-many-versions-b")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because only albatross==1.0.0 is available and albatross==1.0.0 depends on crow>=2.0.0b1, we can conclude that all versions of albatross depend on crow>=2.0.0b1.
-          And because only crow<2.0.0b1 is available, we can conclude that all versions of albatross depend on crow>3.0.0.
-          And because bluebird==1.0.0 depends on crow and only bluebird==1.0.0 is available, we can conclude that all versions of albatross and all versions of bluebird are incompatible.
-          And because you require albatross and you require bluebird, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because only package-a==1.0.0 is available and package-a==1.0.0 depends on package-c>=2.0.0b1, we can conclude that all versions of package-a depend on package-c>=2.0.0b1.
+          And because only package-c<2.0.0b1 is available, we can conclude that all versions of package-a depend on package-c>3.0.0.
+          And because package-b==1.0.0 depends on package-c and only package-b==1.0.0 is available, we can conclude that all versions of package-a and all versions of package-b are incompatible.
+          And because you require package-a and package-b, we can conclude that the requirements are unsatisfiable.
 
-          hint: crow was requested with a pre-release marker (e.g., crow>=2.0.0b1), but pre-releases weren't enabled (try: `--prerelease=allow`)
+          hint: package-c was requested with a pre-release marker (e.g., package-c>=2.0.0b1), but pre-releases weren't enabled (try: `--prerelease=allow`)
     "###);
 
     // Since the user did not explicitly opt-in to a prerelease, it cannot be selected.
-    assert_not_installed(&context.venv, "a_cc6a6eac", &context.temp_dir);
-    assert_not_installed(&context.venv, "b_cc6a6eac", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "transitive_prerelease_and_stable_dependency_many_versions_a",
+        &context.temp_dir,
+    );
+    assert_not_installed(
+        &context.venv,
+        "transitive_prerelease_and_stable_dependency_many_versions_b",
+        &context.temp_dir,
+    );
 }
 
-/// transitive-prerelease-and-stable-dependency-many-versions-holes
-///
 /// A transitive dependency has both a prerelease and a stable selector, but can
 /// only be satisfied by a prerelease. There are many prerelease versions and some
 /// are excluded.
 ///
 /// ```text
-/// 041e36bc
+/// transitive-prerelease-and-stable-dependency-many-versions-holes
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   ├── requires a
 /// │   │   └── satisfied by a-1.0.0
 /// │   └── requires b
 /// │       └── satisfied by b-1.0.0
@@ -1998,225 +3439,430 @@
 ///     ├── c-2.0.0b8
 ///     └── c-2.0.0b9
 /// ```
 #[test]
 fn transitive_prerelease_and_stable_dependency_many_versions_holes() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-041e36bc", "albatross"));
-    filters.push((r"b-041e36bc", "bluebird"));
-    filters.push((r"c-041e36bc", "crow"));
-    filters.push((r"-041e36bc", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((
+        r"transitive-prerelease-and-stable-dependency-many-versions-holes-",
+        "package-",
+    ));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-041e36bc")
-                .arg("b-041e36bc")
+        .arg("transitive-prerelease-and-stable-dependency-many-versions-holes-a")
+                .arg("transitive-prerelease-and-stable-dependency-many-versions-holes-b")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because only the following versions of crow are available:
-              crow<=1.0.0
-              crow>=2.0.0a5,<=2.0.0a7
-              crow==2.0.0b1
-              crow>=2.0.0b5
-          and albatross==1.0.0 depends on one of:
-              crow>1.0.0,<2.0.0a5
-              crow>2.0.0a7,<2.0.0b1
-              crow>2.0.0b1,<2.0.0b5
-          we can conclude that albatross==1.0.0 cannot be used.
-          And because only albatross==1.0.0 is available and you require albatross, we can conclude that the requirements are unsatisfiable.
-
-          hint: crow was requested with a pre-release marker (e.g., any of:
-              crow>1.0.0,<2.0.0a5
-              crow>2.0.0a7,<2.0.0b1
-              crow>2.0.0b1,<2.0.0b5
+      ╰─▶ Because only the following versions of package-c are available:
+              package-c<=1.0.0
+              package-c>=2.0.0a5,<=2.0.0a7
+              package-c==2.0.0b1
+              package-c>=2.0.0b5
+          and package-a==1.0.0 depends on one of:
+              package-c>1.0.0,<2.0.0a5
+              package-c>2.0.0a7,<2.0.0b1
+              package-c>2.0.0b1,<2.0.0b5
+          we can conclude that package-a==1.0.0 cannot be used.
+          And because only package-a==1.0.0 is available and you require package-a, we can conclude that the requirements are unsatisfiable.
+
+          hint: package-c was requested with a pre-release marker (e.g., any of:
+              package-c>1.0.0,<2.0.0a5
+              package-c>2.0.0a7,<2.0.0b1
+              package-c>2.0.0b1,<2.0.0b5
           ), but pre-releases weren't enabled (try: `--prerelease=allow`)
     "###);
 
     // Since the user did not explicitly opt-in to a prerelease, it cannot be selected.
-    assert_not_installed(&context.venv, "a_041e36bc", &context.temp_dir);
-    assert_not_installed(&context.venv, "b_041e36bc", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "transitive_prerelease_and_stable_dependency_many_versions_holes_a",
+        &context.temp_dir,
+    );
+    assert_not_installed(
+        &context.venv,
+        "transitive_prerelease_and_stable_dependency_many_versions_holes_b",
+        &context.temp_dir,
+    );
 }
 
-/// requires-python-version-does-not-exist
+/// The user requires a non-prerelease version of `a` which only has prerelease
+/// versions available. There are pre-releases on the boundary of their range.
 ///
+/// ```text
+/// package-only-prereleases-boundary
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a<0.2.0
+/// │       └── unsatisfied: no matching version
+/// └── a
+///     ├── a-0.1.0a1
+///     ├── a-0.2.0a1
+///     └── a-0.3.0a1
+/// ```
+#[test]
+fn package_only_prereleases_boundary() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"package-only-prereleases-boundary-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("package-only-prereleases-boundary-a<0.2.0")
+        , @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + package-a==0.1.0a1
+    "###);
+
+    // Since there are only prerelease versions of `a` available, a prerelease is
+    // allowed. Since the user did not explicitly request a pre-release, pre-releases at
+    // the boundary should not be selected.
+    assert_installed(
+        &context.venv,
+        "package_only_prereleases_boundary_a",
+        "0.1.0a1",
+        &context.temp_dir,
+    );
+}
+
+/// The user requires a non-prerelease version of `a` but has enabled pre-releases.
+/// There are pre-releases on the boundary of their range.
+///
+/// ```text
+/// package-prereleases-boundary
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a<0.2.0
+/// │       └── satisfied by a-0.1.0
+/// └── a
+///     ├── a-0.1.0
+///     ├── a-0.2.0a1
+///     └── a-0.3.0
+/// ```
+#[test]
+fn package_prereleases_boundary() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"package-prereleases-boundary-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("--prerelease=allow")
+        .arg("package-prereleases-boundary-a<0.2.0")
+        , @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + package-a==0.1.0
+    "###);
+
+    // Since the user did not use a pre-release specifier, pre-releases at the boundary
+    // should not be selected even though pre-releases are allowed.
+    assert_installed(
+        &context.venv,
+        "package_prereleases_boundary_a",
+        "0.1.0",
+        &context.temp_dir,
+    );
+}
+
+/// The user requires a non-prerelease version of `a` but has enabled pre-releases.
+/// There are pre-releases on the boundary of their range.
+///
+/// ```text
+/// package-prereleases-global-boundary
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a<0.2.0
+/// │       └── satisfied by a-0.1.0
+/// └── a
+///     ├── a-0.1.0
+///     ├── a-0.2.0a1
+///     └── a-0.3.0
+/// ```
+#[test]
+fn package_prereleases_global_boundary() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"package-prereleases-global-boundary-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("--prerelease=allow")
+        .arg("package-prereleases-global-boundary-a<0.2.0")
+        , @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + package-a==0.1.0
+    "###);
+
+    // Since the user did not use a pre-release specifier, pre-releases at the boundary
+    // should not be selected even though pre-releases are allowed.
+    assert_installed(
+        &context.venv,
+        "package_prereleases_global_boundary_a",
+        "0.1.0",
+        &context.temp_dir,
+    );
+}
+
+/// The user requires a prerelease version of `a`. There are pre-releases on the
+/// boundary of their range.
+///
+/// ```text
+/// package-prereleases-specifier-boundary
+/// ├── environment
+/// │   └── python3.8
+/// ├── root
+/// │   └── requires a<0.2.0a2
+/// │       └── satisfied by a-0.1.0
+/// └── a
+///     ├── a-0.1.0
+///     ├── a-0.2.0
+///     ├── a-0.2.0a1
+///     ├── a-0.2.0a2
+///     ├── a-0.2.0a3
+///     └── a-0.3.0
+/// ```
+#[test]
+fn package_prereleases_specifier_boundary() {
+    let context = TestContext::new("3.8");
+
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"package-prereleases-specifier-boundary-", "package-"));
+
+    uv_snapshot!(filters, command(&context)
+        .arg("package-prereleases-specifier-boundary-a<0.2.0a2")
+        , @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + package-a==0.2.0a1
+    "###);
+
+    // Since the user used a pre-release specifier, pre-releases at the boundary should
+    // be selected.
+    assert_installed(
+        &context.venv,
+        "package_prereleases_specifier_boundary_a",
+        "0.2.0a1",
+        &context.temp_dir,
+    );
+}
+
 /// The user requires a package which requires a Python version that does not exist
 ///
 /// ```text
-/// 4486c0e5
+/// python-version-does-not-exist
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a==1.0.0
 /// │       └── satisfied by a-1.0.0
 /// └── a
 ///     └── a-1.0.0
-///         └── requires python>=4.0 (incompatible with environment)
+///         └── requires python>=3.30 (incompatible with environment)
 /// ```
 #[test]
-fn requires_python_version_does_not_exist() {
+fn python_version_does_not_exist() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-4486c0e5", "albatross"));
-    filters.push((r"-4486c0e5", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"python-version-does-not-exist-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-4486c0e5==1.0.0")
+        .arg("python-version-does-not-exist-a==1.0.0")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because the current Python version (3.8.18) does not satisfy Python>=4.0 and albatross==1.0.0 depends on Python>=4.0, we can conclude that albatross==1.0.0 cannot be used.
-          And because you require albatross==1.0.0, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because the current Python version (3.8.[X]) does not satisfy Python>=3.30 and package-a==1.0.0 depends on Python>=3.30, we can conclude that package-a==1.0.0 cannot be used.
+          And because you require package-a==1.0.0, we can conclude that the requirements are unsatisfiable.
     "###);
 
-    assert_not_installed(&context.venv, "a_4486c0e5", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "python_version_does_not_exist_a",
+        &context.temp_dir,
+    );
 }
 
-/// requires-python-version-less-than-current
-///
 /// The user requires a package which requires a Python version less than the
 /// current version
 ///
 /// ```text
-/// d4ea58de
+/// python-less-than-current
 /// ├── environment
 /// │   └── python3.9
 /// ├── root
 /// │   └── requires a==1.0.0
 /// │       └── satisfied by a-1.0.0
 /// └── a
 ///     └── a-1.0.0
 ///         └── requires python<=3.8 (incompatible with environment)
 /// ```
 #[test]
-fn requires_python_version_less_than_current() {
+fn python_less_than_current() {
     let context = TestContext::new("3.9");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-d4ea58de", "albatross"));
-    filters.push((r"-d4ea58de", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"python-less-than-current-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-d4ea58de==1.0.0")
+        .arg("python-less-than-current-a==1.0.0")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because the current Python version (3.9.18) does not satisfy Python<=3.8 and albatross==1.0.0 depends on Python<=3.8, we can conclude that albatross==1.0.0 cannot be used.
-          And because you require albatross==1.0.0, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because the current Python version (3.9.[X]) does not satisfy Python<=3.8 and package-a==1.0.0 depends on Python<=3.8, we can conclude that package-a==1.0.0 cannot be used.
+          And because you require package-a==1.0.0, we can conclude that the requirements are unsatisfiable.
     "###);
 
-    assert_not_installed(&context.venv, "a_d4ea58de", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "python_less_than_current_a",
+        &context.temp_dir,
+    );
 }
 
-/// requires-python-version-greater-than-current
-///
 /// The user requires a package which requires a Python version greater than the
 /// current version
 ///
 /// ```text
-/// 741c8854
+/// python-greater-than-current
 /// ├── environment
 /// │   └── python3.9
 /// ├── root
 /// │   └── requires a==1.0.0
 /// │       └── satisfied by a-1.0.0
 /// └── a
 ///     └── a-1.0.0
 ///         └── requires python>=3.10 (incompatible with environment)
 /// ```
 #[test]
-fn requires_python_version_greater_than_current() {
+fn python_greater_than_current() {
     let context = TestContext::new("3.9");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-741c8854", "albatross"));
-    filters.push((r"-741c8854", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"python-greater-than-current-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-741c8854==1.0.0")
+        .arg("python-greater-than-current-a==1.0.0")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because the current Python version (3.9.18) does not satisfy Python>=3.10 and albatross==1.0.0 depends on Python>=3.10, we can conclude that albatross==1.0.0 cannot be used.
-          And because you require albatross==1.0.0, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because the current Python version (3.9.[X]) does not satisfy Python>=3.10 and package-a==1.0.0 depends on Python>=3.10, we can conclude that package-a==1.0.0 cannot be used.
+          And because you require package-a==1.0.0, we can conclude that the requirements are unsatisfiable.
     "###);
 
-    assert_not_installed(&context.venv, "a_741c8854", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "python_greater_than_current_a",
+        &context.temp_dir,
+    );
 }
 
-/// requires-python-version-greater-than-current-patch
-///
 /// The user requires a package which requires a Python version with a patch version
 /// greater than the current patch version
 ///
 /// ```text
-/// 0044ac94
+/// python-greater-than-current-patch
 /// ├── environment
 /// │   └── python3.8.12
 /// ├── root
 /// │   └── requires a==1.0.0
 /// │       └── satisfied by a-1.0.0
 /// └── a
 ///     └── a-1.0.0
 ///         └── requires python>=3.8.14 (incompatible with environment)
 /// ```
+#[cfg(feature = "python-patch")]
 #[test]
-fn requires_python_version_greater_than_current_patch() {
+fn python_greater_than_current_patch() {
     let context = TestContext::new("3.8.12");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-0044ac94", "albatross"));
-    filters.push((r"-0044ac94", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"python-greater-than-current-patch-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-0044ac94==1.0.0")
+        .arg("python-greater-than-current-patch-a==1.0.0")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because the current Python version (3.8.12) does not satisfy Python>=3.8.14 and albatross==1.0.0 depends on Python>=3.8.14, we can conclude that albatross==1.0.0 cannot be used.
-          And because you require albatross==1.0.0, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because the current Python version (3.8.12) does not satisfy Python>=3.8.14 and package-a==1.0.0 depends on Python>=3.8.14, we can conclude that package-a==1.0.0 cannot be used.
+          And because you require package-a==1.0.0, we can conclude that the requirements are unsatisfiable.
     "###);
 
-    assert_not_installed(&context.venv, "a_0044ac94", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "python_greater_than_current_patch_a",
+        &context.temp_dir,
+    );
 }
 
-/// requires-python-version-greater-than-current-many
-///
 /// The user requires a package which has many versions which all require a Python
 /// version greater than the current version
 ///
 /// ```text
-/// da5bd150
+/// python-greater-than-current-many
 /// ├── environment
 /// │   └── python3.9
 /// ├── root
 /// │   └── requires a==1.0.0
 /// │       └── unsatisfied: no matching version
 /// └── a
 ///     ├── a-2.0.0
@@ -2241,44 +3887,45 @@
 ///     │   └── requires python>=3.11 (incompatible with environment)
 ///     ├── a-3.4.0
 ///     │   └── requires python>=3.11 (incompatible with environment)
 ///     └── a-3.5.0
 ///         └── requires python>=3.11 (incompatible with environment)
 /// ```
 #[test]
-fn requires_python_version_greater_than_current_many() {
+fn python_greater_than_current_many() {
     let context = TestContext::new("3.9");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-da5bd150", "albatross"));
-    filters.push((r"-da5bd150", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"python-greater-than-current-many-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-da5bd150==1.0.0")
+        .arg("python-greater-than-current-many-a==1.0.0")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because there is no version of albatross==1.0.0 and you require albatross==1.0.0, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because there is no version of package-a==1.0.0 and you require package-a==1.0.0, we can conclude that the requirements are unsatisfiable.
     "###);
 
-    assert_not_installed(&context.venv, "a_da5bd150", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "python_greater_than_current_many_a",
+        &context.temp_dir,
+    );
 }
 
-/// requires-python-version-greater-than-current-backtrack
-///
 /// The user requires a package where recent versions require a Python version
 /// greater than the current version, but an older version is compatible.
 ///
 /// ```text
-/// 3204bc0a
+/// python-greater-than-current-backtrack
 /// ├── environment
 /// │   └── python3.9
 /// ├── root
 /// │   └── requires a
 /// │       ├── satisfied by a-1.0.0
 /// │       ├── satisfied by a-2.0.0
 /// │       ├── satisfied by a-3.0.0
@@ -2289,46 +3936,48 @@
 ///     │   └── requires python>=3.10 (incompatible with environment)
 ///     ├── a-3.0.0
 ///     │   └── requires python>=3.11 (incompatible with environment)
 ///     └── a-4.0.0
 ///         └── requires python>=3.12 (incompatible with environment)
 /// ```
 #[test]
-fn requires_python_version_greater_than_current_backtrack() {
+fn python_greater_than_current_backtrack() {
     let context = TestContext::new("3.9");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-3204bc0a", "albatross"));
-    filters.push((r"-3204bc0a", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"python-greater-than-current-backtrack-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-3204bc0a")
+        .arg("python-greater-than-current-backtrack-a")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 1 package in [TIME]
     Downloaded 1 package in [TIME]
     Installed 1 package in [TIME]
-     + albatross==1.0.0
+     + package-a==1.0.0
     "###);
 
-    assert_installed(&context.venv, "a_3204bc0a", "1.0.0", &context.temp_dir);
+    assert_installed(
+        &context.venv,
+        "python_greater_than_current_backtrack_a",
+        "1.0.0",
+        &context.temp_dir,
+    );
 }
 
-/// requires-python-version-greater-than-current-excluded
-///
 /// The user requires a package where recent versions require a Python version
 /// greater than the current version, but an excluded older version is compatible.
 ///
 /// ```text
-/// 874cae6d
+/// python-greater-than-current-excluded
 /// ├── environment
 /// │   └── python3.9
 /// ├── root
 /// │   └── requires a>=2.0.0
 /// │       ├── satisfied by a-2.0.0
 /// │       ├── satisfied by a-3.0.0
 /// │       └── satisfied by a-4.0.0
@@ -2338,632 +3987,613 @@
 ///     │   └── requires python>=3.10 (incompatible with environment)
 ///     ├── a-3.0.0
 ///     │   └── requires python>=3.11 (incompatible with environment)
 ///     └── a-4.0.0
 ///         └── requires python>=3.12 (incompatible with environment)
 /// ```
 #[test]
-fn requires_python_version_greater_than_current_excluded() {
+fn python_greater_than_current_excluded() {
     let context = TestContext::new("3.9");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-874cae6d", "albatross"));
-    filters.push((r"-874cae6d", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"python-greater-than-current-excluded-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-874cae6d>=2.0.0")
+        .arg("python-greater-than-current-excluded-a>=2.0.0")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because the current Python version (3.9.18) does not satisfy Python>=3.10,<3.11 and the current Python version (3.9.18) does not satisfy Python>=3.12, we can conclude that any of:
+      ╰─▶ Because the current Python version (3.9.[X]) does not satisfy Python>=3.10,<3.11 and the current Python version (3.9.[X]) does not satisfy Python>=3.12, we can conclude that any of:
               Python>=3.10,<3.11
               Python>=3.12
            are incompatible.
-          And because the current Python version (3.9.18) does not satisfy Python>=3.11,<3.12, we can conclude that Python>=3.10 are incompatible.
-          And because albatross==2.0.0 depends on Python>=3.10 and only the following versions of albatross are available:
-              albatross<=2.0.0
-              albatross==3.0.0
-              albatross==4.0.0
-          we can conclude that albatross>=2.0.0,<3.0.0 cannot be used. (1)
-
-          Because the current Python version (3.9.18) does not satisfy Python>=3.11,<3.12 and the current Python version (3.9.18) does not satisfy Python>=3.12, we can conclude that Python>=3.11 are incompatible.
-          And because albatross==3.0.0 depends on Python>=3.11, we can conclude that albatross==3.0.0 cannot be used.
-          And because we know from (1) that albatross>=2.0.0,<3.0.0 cannot be used, we can conclude that albatross>=2.0.0,<4.0.0 cannot be used. (2)
-
-          Because the current Python version (3.9.18) does not satisfy Python>=3.12 and albatross==4.0.0 depends on Python>=3.12, we can conclude that albatross==4.0.0 cannot be used.
-          And because we know from (2) that albatross>=2.0.0,<4.0.0 cannot be used, we can conclude that albatross>=2.0.0 cannot be used.
-          And because you require albatross>=2.0.0, we can conclude that the requirements are unsatisfiable.
-    "###);
-
-    assert_not_installed(&context.venv, "a_874cae6d", &context.temp_dir);
+          And because the current Python version (3.9.[X]) does not satisfy Python>=3.11,<3.12, we can conclude that Python>=3.10 are incompatible.
+          And because package-a==2.0.0 depends on Python>=3.10 and only the following versions of package-a are available:
+              package-a<=2.0.0
+              package-a==3.0.0
+              package-a==4.0.0
+          we can conclude that package-a>=2.0.0,<3.0.0 cannot be used. (1)
+
+          Because the current Python version (3.9.[X]) does not satisfy Python>=3.11,<3.12 and the current Python version (3.9.[X]) does not satisfy Python>=3.12, we can conclude that Python>=3.11 are incompatible.
+          And because package-a==3.0.0 depends on Python>=3.11, we can conclude that package-a==3.0.0 cannot be used.
+          And because we know from (1) that package-a>=2.0.0,<3.0.0 cannot be used, we can conclude that package-a>=2.0.0,<4.0.0 cannot be used. (2)
+
+          Because the current Python version (3.9.[X]) does not satisfy Python>=3.12 and package-a==4.0.0 depends on Python>=3.12, we can conclude that package-a==4.0.0 cannot be used.
+          And because we know from (2) that package-a>=2.0.0,<4.0.0 cannot be used, we can conclude that package-a>=2.0.0 cannot be used.
+          And because you require package-a>=2.0.0, we can conclude that the requirements are unsatisfiable.
+    "###);
+
+    assert_not_installed(
+        &context.venv,
+        "python_greater_than_current_excluded_a",
+        &context.temp_dir,
+    );
 }
 
-/// specific-tag-and-default
-///
 /// A wheel for a specific platform is available alongside the default.
 ///
 /// ```text
-/// 8f7a81f1
+/// specific-tag-and-default
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a
 /// │       └── satisfied by a-1.0.0
 /// └── a
 ///     └── a-1.0.0
 /// ```
 #[test]
 fn specific_tag_and_default() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-8f7a81f1", "albatross"));
-    filters.push((r"-8f7a81f1", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"specific-tag-and-default-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-8f7a81f1")
+        .arg("specific-tag-and-default-a")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 1 package in [TIME]
     Downloaded 1 package in [TIME]
     Installed 1 package in [TIME]
-     + albatross==1.0.0
+     + package-a==1.0.0
     "###);
 }
 
-/// only-wheels
-///
 /// No source distributions are available, only wheels.
 ///
 /// ```text
-/// a874f41e
+/// only-wheels
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a
 /// │       └── satisfied by a-1.0.0
 /// └── a
 ///     └── a-1.0.0
 /// ```
 #[test]
 fn only_wheels() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-a874f41e", "albatross"));
-    filters.push((r"-a874f41e", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"only-wheels-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-a874f41e")
+        .arg("only-wheels-a")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 1 package in [TIME]
     Downloaded 1 package in [TIME]
     Installed 1 package in [TIME]
-     + albatross==1.0.0
+     + package-a==1.0.0
     "###);
 }
 
-/// no-wheels
-///
 /// No wheels are available, only source distributions.
 ///
 /// ```text
-/// 0278f343
+/// no-wheels
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a
 /// │       └── satisfied by a-1.0.0
 /// └── a
 ///     └── a-1.0.0
 /// ```
 #[test]
 fn no_wheels() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-0278f343", "albatross"));
-    filters.push((r"-0278f343", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"no-wheels-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-0278f343")
+        .arg("no-wheels-a")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 1 package in [TIME]
     Downloaded 1 package in [TIME]
     Installed 1 package in [TIME]
-     + albatross==1.0.0
+     + package-a==1.0.0
     "###);
 }
 
-/// no-wheels-with-matching-platform
-///
 /// No wheels with matching platform tags are available, just source distributions.
 ///
 /// ```text
-/// f1a1f15c
+/// no-wheels-with-matching-platform
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a
 /// │       └── satisfied by a-1.0.0
 /// └── a
 ///     └── a-1.0.0
 /// ```
 #[test]
 fn no_wheels_with_matching_platform() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-f1a1f15c", "albatross"));
-    filters.push((r"-f1a1f15c", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"no-wheels-with-matching-platform-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-f1a1f15c")
+        .arg("no-wheels-with-matching-platform-a")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 1 package in [TIME]
     Downloaded 1 package in [TIME]
     Installed 1 package in [TIME]
-     + albatross==1.0.0
+     + package-a==1.0.0
     "###);
 }
 
-/// no-sdist-no-wheels-with-matching-platform
-///
 /// No wheels with matching platform tags are available, nor are any source
 /// distributions available
 ///
 /// ```text
-/// 94e293e5
+/// no-sdist-no-wheels-with-matching-platform
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a
 /// │       └── satisfied by a-1.0.0
 /// └── a
 ///     └── a-1.0.0
 /// ```
 #[test]
 fn no_sdist_no_wheels_with_matching_platform() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-94e293e5", "albatross"));
-    filters.push((r"-94e293e5", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"no-sdist-no-wheels-with-matching-platform-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-94e293e5")
+        .arg("no-sdist-no-wheels-with-matching-platform-a")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because only albatross==1.0.0 is available and albatross==1.0.0 is unusable because no wheels are available with a matching platform, we can conclude that all versions of albatross cannot be used.
-          And because you require albatross, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because only package-a==1.0.0 is available and package-a==1.0.0 has no wheels are available with a matching platform, we can conclude that all versions of package-a cannot be used.
+          And because you require package-a, we can conclude that the requirements are unsatisfiable.
     "###);
 
-    assert_not_installed(&context.venv, "a_94e293e5", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "no_sdist_no_wheels_with_matching_platform_a",
+        &context.temp_dir,
+    );
 }
 
-/// no-sdist-no-wheels-with-matching-python
-///
 /// No wheels with matching Python tags are available, nor are any source
 /// distributions available
 ///
 /// ```text
-/// 40fe677d
+/// no-sdist-no-wheels-with-matching-python
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a
 /// │       └── satisfied by a-1.0.0
 /// └── a
 ///     └── a-1.0.0
 /// ```
 #[test]
 fn no_sdist_no_wheels_with_matching_python() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-40fe677d", "albatross"));
-    filters.push((r"-40fe677d", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"no-sdist-no-wheels-with-matching-python-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-40fe677d")
+        .arg("no-sdist-no-wheels-with-matching-python-a")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because only albatross==1.0.0 is available and albatross==1.0.0 is unusable because no wheels are available with a matching Python implementation, we can conclude that all versions of albatross cannot be used.
-          And because you require albatross, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because only package-a==1.0.0 is available and package-a==1.0.0 has no wheels are available with a matching Python implementation, we can conclude that all versions of package-a cannot be used.
+          And because you require package-a, we can conclude that the requirements are unsatisfiable.
     "###);
 
-    assert_not_installed(&context.venv, "a_40fe677d", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "no_sdist_no_wheels_with_matching_python_a",
+        &context.temp_dir,
+    );
 }
 
-/// no-sdist-no-wheels-with-matching-abi
-///
 /// No wheels with matching ABI tags are available, nor are any source distributions
 /// available
 ///
 /// ```text
-/// 8727a9b9
+/// no-sdist-no-wheels-with-matching-abi
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a
 /// │       └── satisfied by a-1.0.0
 /// └── a
 ///     └── a-1.0.0
 /// ```
 #[test]
 fn no_sdist_no_wheels_with_matching_abi() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-8727a9b9", "albatross"));
-    filters.push((r"-8727a9b9", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"no-sdist-no-wheels-with-matching-abi-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-8727a9b9")
+        .arg("no-sdist-no-wheels-with-matching-abi-a")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because only albatross==1.0.0 is available and albatross==1.0.0 is unusable because no wheels are available with a matching Python ABI, we can conclude that all versions of albatross cannot be used.
-          And because you require albatross, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because only package-a==1.0.0 is available and package-a==1.0.0 has no wheels are available with a matching Python ABI, we can conclude that all versions of package-a cannot be used.
+          And because you require package-a, we can conclude that the requirements are unsatisfiable.
     "###);
 
-    assert_not_installed(&context.venv, "a_8727a9b9", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "no_sdist_no_wheels_with_matching_abi_a",
+        &context.temp_dir,
+    );
 }
 
-/// no-wheels-no-build
-///
 /// No wheels are available, only source distributions but the user has disabled
 /// builds.
 ///
 /// ```text
-/// 662cbd94
+/// no-wheels-no-build
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a
 /// │       └── satisfied by a-1.0.0
 /// └── a
 ///     └── a-1.0.0
 /// ```
 #[test]
 fn no_wheels_no_build() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-662cbd94", "albatross"));
-    filters.push((r"-662cbd94", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"no-wheels-no-build-", "package-"));
 
     uv_snapshot!(filters, command(&context)
         .arg("--only-binary")
-        .arg("a-662cbd94")
-        .arg("a-662cbd94")
+        .arg("no-wheels-no-build-a")
+        .arg("no-wheels-no-build-a")
         , @r###"
     success: false
-    exit_code: 2
+    exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
-    error: Failed to download and build: albatross==1.0.0
-      Caused by: Building source distributions is disabled
+      × No solution found when resolving dependencies:
+      ╰─▶ Because only package-a==1.0.0 is available and package-a==1.0.0 has no usable wheels and building from source is disabled, we can conclude that all versions of package-a cannot be used.
+          And because you require package-a, we can conclude that the requirements are unsatisfiable.
     "###);
 
-    assert_not_installed(&context.venv, "a_662cbd94", &context.temp_dir);
+    assert_not_installed(&context.venv, "no_wheels_no_build_a", &context.temp_dir);
 }
 
-/// only-wheels-no-binary
-///
 /// No source distributions are available, only wheels but the user has disabled
 /// using pre-built binaries.
 ///
 /// ```text
-/// dd137625
+/// only-wheels-no-binary
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a
 /// │       └── satisfied by a-1.0.0
 /// └── a
 ///     └── a-1.0.0
 /// ```
 #[test]
 fn only_wheels_no_binary() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-dd137625", "albatross"));
-    filters.push((r"-dd137625", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"only-wheels-no-binary-", "package-"));
 
     uv_snapshot!(filters, command(&context)
         .arg("--no-binary")
-        .arg("a-dd137625")
-        .arg("a-dd137625")
+        .arg("only-wheels-no-binary-a")
+        .arg("only-wheels-no-binary-a")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because only albatross==1.0.0 is available and albatross==1.0.0 is unusable because no source distribution is available and using wheels is disabled, we can conclude that all versions of albatross cannot be used.
-          And because you require albatross, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because only package-a==1.0.0 is available and package-a==1.0.0 has no available source distribution and using wheels is disabled, we can conclude that all versions of package-a cannot be used.
+          And because you require package-a, we can conclude that the requirements are unsatisfiable.
     "###);
 
-    assert_not_installed(&context.venv, "a_dd137625", &context.temp_dir);
+    assert_not_installed(&context.venv, "only_wheels_no_binary_a", &context.temp_dir);
 }
 
-/// no-build
-///
 /// Both wheels and source distributions are available, and the user has disabled
 /// builds.
 ///
 /// ```text
-/// 9ff1e173
+/// no-build
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a
 /// │       └── satisfied by a-1.0.0
 /// └── a
 ///     └── a-1.0.0
 /// ```
 #[test]
 fn no_build() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-9ff1e173", "albatross"));
-    filters.push((r"-9ff1e173", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"no-build-", "package-"));
 
     uv_snapshot!(filters, command(&context)
         .arg("--only-binary")
-        .arg("a-9ff1e173")
-        .arg("a-9ff1e173")
+        .arg("no-build-a")
+        .arg("no-build-a")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 1 package in [TIME]
     Downloaded 1 package in [TIME]
     Installed 1 package in [TIME]
-     + albatross==1.0.0
+     + package-a==1.0.0
     "###);
 
     // The wheel should be used for install
 }
 
-/// no-binary
-///
 /// Both wheels and source distributions are available, and the user has disabled
 /// binaries.
 ///
 /// ```text
-/// 10e961b8
+/// no-binary
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a
 /// │       └── satisfied by a-1.0.0
 /// └── a
 ///     └── a-1.0.0
 /// ```
 #[test]
 fn no_binary() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-10e961b8", "albatross"));
-    filters.push((r"-10e961b8", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"no-binary-", "package-"));
 
     uv_snapshot!(filters, command(&context)
         .arg("--no-binary")
-        .arg("a-10e961b8")
-        .arg("a-10e961b8")
+        .arg("no-binary-a")
+        .arg("no-binary-a")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 1 package in [TIME]
     Downloaded 1 package in [TIME]
     Installed 1 package in [TIME]
-     + albatross==1.0.0
+     + package-a==1.0.0
     "###);
 
     // The source distribution should be used for install
 }
 
-/// package-only-yanked
-///
 /// The user requires any version of package `a` which only has yanked versions
 /// available.
 ///
 /// ```text
-/// e3de7eb4
+/// package-only-yanked
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a
 /// │       └── unsatisfied: no matching version
 /// └── a
 ///     └── a-1.0.0 (yanked)
 /// ```
 #[test]
 fn package_only_yanked() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-e3de7eb4", "albatross"));
-    filters.push((r"-e3de7eb4", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"package-only-yanked-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-e3de7eb4")
+        .arg("package-only-yanked-a")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because only albatross==1.0.0 is available and albatross==1.0.0 is unusable because it was yanked, we can conclude that all versions of albatross cannot be used.
-          And because you require albatross, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because only package-a==1.0.0 is available and package-a==1.0.0 was yanked (reason: Yanked for testing), we can conclude that all versions of package-a cannot be used.
+          And because you require package-a, we can conclude that the requirements are unsatisfiable.
     "###);
 
     // Yanked versions should not be installed, even if they are the only one
     // available.
-    assert_not_installed(&context.venv, "a_e3de7eb4", &context.temp_dir);
+    assert_not_installed(&context.venv, "package_only_yanked_a", &context.temp_dir);
 }
 
-/// package-only-yanked-in-range
-///
 /// The user requires a version of package `a` which only matches yanked versions.
 ///
 /// ```text
-/// 84b3720e
+/// package-only-yanked-in-range
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a>0.1.0
 /// │       └── unsatisfied: no matching version
 /// └── a
 ///     ├── a-0.1.0
 ///     └── a-1.0.0 (yanked)
 /// ```
 #[test]
 fn package_only_yanked_in_range() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-84b3720e", "albatross"));
-    filters.push((r"-84b3720e", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"package-only-yanked-in-range-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-84b3720e>0.1.0")
+        .arg("package-only-yanked-in-range-a>0.1.0")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because only the following versions of albatross are available:
-              albatross<=0.1.0
-              albatross==1.0.0
-          and albatross==1.0.0 is unusable because it was yanked, we can conclude that albatross>0.1.0 cannot be used.
-          And because you require albatross>0.1.0, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because only the following versions of package-a are available:
+              package-a<=0.1.0
+              package-a==1.0.0
+          and package-a==1.0.0 was yanked (reason: Yanked for testing), we can conclude that package-a>0.1.0 cannot be used.
+          And because you require package-a>0.1.0, we can conclude that the requirements are unsatisfiable.
     "###);
 
     // Since there are other versions of `a` available, yanked versions should not be
     // selected without explicit opt-in.
-    assert_not_installed(&context.venv, "a_84b3720e", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "package_only_yanked_in_range_a",
+        &context.temp_dir,
+    );
 }
 
-/// requires-package-yanked-and-unyanked-any
-///
 /// The user requires any version of package `a` has a yanked version available and
 /// an older unyanked version.
 ///
 /// ```text
-/// 93eac6d7
+/// requires-package-yanked-and-unyanked-any
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a
 /// │       └── satisfied by a-0.1.0
 /// └── a
 ///     ├── a-0.1.0
 ///     └── a-1.0.0 (yanked)
 /// ```
 #[test]
 fn requires_package_yanked_and_unyanked_any() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-93eac6d7", "albatross"));
-    filters.push((r"-93eac6d7", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"requires-package-yanked-and-unyanked-any-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-93eac6d7")
+        .arg("requires-package-yanked-and-unyanked-any-a")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 1 package in [TIME]
     Downloaded 1 package in [TIME]
     Installed 1 package in [TIME]
-     + albatross==0.1.0
+     + package-a==0.1.0
     "###);
 
     // The unyanked version should be selected.
-    assert_installed(&context.venv, "a_93eac6d7", "0.1.0", &context.temp_dir);
+    assert_installed(
+        &context.venv,
+        "requires_package_yanked_and_unyanked_any_a",
+        "0.1.0",
+        &context.temp_dir,
+    );
 }
 
-/// package-yanked-specified-mixed-available
-///
 /// The user requires any version of `a` and both yanked and unyanked releases are
 /// available.
 ///
 /// ```text
-/// 3325916e
+/// package-yanked-specified-mixed-available
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a>=0.1.0
 /// │       ├── satisfied by a-0.1.0
 /// │       └── satisfied by a-0.3.0
 /// └── a
@@ -2972,44 +4602,46 @@
 ///     ├── a-0.3.0
 ///     └── a-1.0.0 (yanked)
 /// ```
 #[test]
 fn package_yanked_specified_mixed_available() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-3325916e", "albatross"));
-    filters.push((r"-3325916e", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"package-yanked-specified-mixed-available-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-3325916e>=0.1.0")
+        .arg("package-yanked-specified-mixed-available-a>=0.1.0")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 1 package in [TIME]
     Downloaded 1 package in [TIME]
     Installed 1 package in [TIME]
-     + albatross==0.3.0
+     + package-a==0.3.0
     "###);
 
     // The latest unyanked version should be selected.
-    assert_installed(&context.venv, "a_3325916e", "0.3.0", &context.temp_dir);
+    assert_installed(
+        &context.venv,
+        "package_yanked_specified_mixed_available_a",
+        "0.3.0",
+        &context.temp_dir,
+    );
 }
 
-/// transitive-package-only-yanked
-///
 /// The user requires any version of package `a` which requires `b` which only has
 /// yanked versions available.
 ///
 /// ```text
-/// 9ec30fe2
+/// transitive-package-only-yanked
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a
 /// │       └── satisfied by a-0.1.0
 /// ├── a
 /// │   └── a-0.1.0
@@ -3018,46 +4650,46 @@
 /// └── b
 ///     └── b-1.0.0 (yanked)
 /// ```
 #[test]
 fn transitive_package_only_yanked() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-9ec30fe2", "albatross"));
-    filters.push((r"b-9ec30fe2", "bluebird"));
-    filters.push((r"-9ec30fe2", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"transitive-package-only-yanked-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-9ec30fe2")
+        .arg("transitive-package-only-yanked-a")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because only bluebird==1.0.0 is available and bluebird==1.0.0 is unusable because it was yanked, we can conclude that all versions of bluebird cannot be used.
-          And because albatross==0.1.0 depends on bluebird, we can conclude that albatross==0.1.0 cannot be used.
-          And because only albatross==0.1.0 is available and you require albatross, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because only package-b==1.0.0 is available and package-b==1.0.0 was yanked (reason: Yanked for testing), we can conclude that all versions of package-b cannot be used.
+          And because package-a==0.1.0 depends on package-b, we can conclude that package-a==0.1.0 cannot be used.
+          And because only package-a==0.1.0 is available and you require package-a, we can conclude that the requirements are unsatisfiable.
     "###);
 
     // Yanked versions should not be installed, even if they are the only one
     // available.
-    assert_not_installed(&context.venv, "a_9ec30fe2", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "transitive_package_only_yanked_a",
+        &context.temp_dir,
+    );
 }
 
-/// transitive-package-only-yanked-in-range
-///
 /// The user requires package `a` which has a dependency on a package which only
 /// matches yanked versions.
 ///
 /// ```text
-/// 872d714e
+/// transitive-package-only-yanked-in-range
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   └── requires a
 /// │       └── satisfied by a-0.1.0
 /// ├── a
 /// │   └── a-0.1.0
@@ -3067,50 +4699,50 @@
 ///     ├── b-0.1.0
 ///     └── b-1.0.0 (yanked)
 /// ```
 #[test]
 fn transitive_package_only_yanked_in_range() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-872d714e", "albatross"));
-    filters.push((r"b-872d714e", "bluebird"));
-    filters.push((r"-872d714e", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"transitive-package-only-yanked-in-range-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-872d714e")
+        .arg("transitive-package-only-yanked-in-range-a")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because only the following versions of bluebird are available:
-              bluebird<=0.1
-              bluebird==1.0.0
-          and bluebird==1.0.0 is unusable because it was yanked, we can conclude that bluebird>0.1 cannot be used.
-          And because albatross==0.1.0 depends on bluebird>0.1, we can conclude that albatross==0.1.0 cannot be used.
-          And because only albatross==0.1.0 is available and you require albatross, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because only the following versions of package-b are available:
+              package-b<=0.1
+              package-b==1.0.0
+          and package-b==1.0.0 was yanked (reason: Yanked for testing), we can conclude that package-b>0.1 cannot be used.
+          And because package-a==0.1.0 depends on package-b>0.1, we can conclude that package-a==0.1.0 cannot be used.
+          And because only package-a==0.1.0 is available and you require package-a, we can conclude that the requirements are unsatisfiable.
     "###);
 
     // Yanked versions should not be installed, even if they are the only valid version
     // in a range.
-    assert_not_installed(&context.venv, "a_872d714e", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "transitive_package_only_yanked_in_range_a",
+        &context.temp_dir,
+    );
 }
 
-/// transitive-package-only-yanked-in-range-opt-in
-///
 /// The user requires package `a` which has a dependency on a package which only
 /// matches yanked versions; the user has opted into allowing the yanked version of
 /// `b` explicitly.
 ///
 /// ```text
-/// 1bbd5d1b
+/// transitive-package-only-yanked-in-range-opt-in
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   ├── requires a
 /// │   │   └── satisfied by a-0.1.0
 /// │   └── requires b==1.0.0
 /// │       └── unsatisfied: no matching version
@@ -3122,50 +4754,59 @@
 ///     ├── b-0.1.0
 ///     └── b-1.0.0 (yanked)
 /// ```
 #[test]
 fn transitive_package_only_yanked_in_range_opt_in() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-1bbd5d1b", "albatross"));
-    filters.push((r"b-1bbd5d1b", "bluebird"));
-    filters.push((r"-1bbd5d1b", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((
+        r"transitive-package-only-yanked-in-range-opt-in-",
+        "package-",
+    ));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-1bbd5d1b")
-                .arg("b-1bbd5d1b==1.0.0")
+        .arg("transitive-package-only-yanked-in-range-opt-in-a")
+                .arg("transitive-package-only-yanked-in-range-opt-in-b==1.0.0")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 2 packages in [TIME]
     Downloaded 2 packages in [TIME]
     Installed 2 packages in [TIME]
-     + albatross==0.1.0
-     + bluebird==1.0.0
-    warning: bluebird==1.0.0 is yanked.
+     + package-a==0.1.0
+     + package-b==1.0.0
+    warning: package-b==1.0.0 is yanked (reason: "Yanked for testing").
     "###);
 
     // Since the user included a dependency on `b` with an exact specifier, the yanked
     // version can be selected.
-    assert_installed(&context.venv, "a_1bbd5d1b", "0.1.0", &context.temp_dir);
-    assert_installed(&context.venv, "b_1bbd5d1b", "1.0.0", &context.temp_dir);
+    assert_installed(
+        &context.venv,
+        "transitive_package_only_yanked_in_range_opt_in_a",
+        "0.1.0",
+        &context.temp_dir,
+    );
+    assert_installed(
+        &context.venv,
+        "transitive_package_only_yanked_in_range_opt_in_b",
+        "1.0.0",
+        &context.temp_dir,
+    );
 }
 
-/// transitive-yanked-and-unyanked-dependency
-///
 /// A transitive dependency has both a yanked and an unyanked version, but can only
 /// be satisfied by a yanked version
 ///
 /// ```text
-/// eb1ba5f5
+/// transitive-yanked-and-unyanked-dependency
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   ├── requires a
 /// │   │   └── satisfied by a-1.0.0
 /// │   └── requires b
 /// │       └── satisfied by b-1.0.0
@@ -3181,48 +4822,51 @@
 ///     ├── c-1.0.0
 ///     └── c-2.0.0 (yanked)
 /// ```
 #[test]
 fn transitive_yanked_and_unyanked_dependency() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-eb1ba5f5", "albatross"));
-    filters.push((r"b-eb1ba5f5", "bluebird"));
-    filters.push((r"c-eb1ba5f5", "crow"));
-    filters.push((r"-eb1ba5f5", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((r"transitive-yanked-and-unyanked-dependency-", "package-"));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-eb1ba5f5")
-                .arg("b-eb1ba5f5")
+        .arg("transitive-yanked-and-unyanked-dependency-a")
+                .arg("transitive-yanked-and-unyanked-dependency-b")
         , @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
       × No solution found when resolving dependencies:
-      ╰─▶ Because crow==2.0.0 is unusable because it was yanked and albatross==1.0.0 depends on crow==2.0.0, we can conclude that albatross==1.0.0 cannot be used.
-          And because only albatross==1.0.0 is available and you require albatross, we can conclude that the requirements are unsatisfiable.
+      ╰─▶ Because package-c==2.0.0 was yanked (reason: Yanked for testing) and package-a==1.0.0 depends on package-c==2.0.0, we can conclude that package-a==1.0.0 cannot be used.
+          And because only package-a==1.0.0 is available and you require package-a, we can conclude that the requirements are unsatisfiable.
     "###);
 
     // Since the user did not explicitly select the yanked version, it cannot be used.
-    assert_not_installed(&context.venv, "a_eb1ba5f5", &context.temp_dir);
-    assert_not_installed(&context.venv, "b_eb1ba5f5", &context.temp_dir);
+    assert_not_installed(
+        &context.venv,
+        "transitive_yanked_and_unyanked_dependency_a",
+        &context.temp_dir,
+    );
+    assert_not_installed(
+        &context.venv,
+        "transitive_yanked_and_unyanked_dependency_b",
+        &context.temp_dir,
+    );
 }
 
-/// transitive-yanked-and-unyanked-dependency-opt-in
-///
 /// A transitive dependency has both a yanked and an unyanked version, but can only
 /// be satisfied by a yanked. The user includes an opt-in to the yanked version of
 /// the transitive dependency.
 ///
 /// ```text
-/// f0760ee9
+/// transitive-yanked-and-unyanked-dependency-opt-in
 /// ├── environment
 /// │   └── python3.8
 /// ├── root
 /// │   ├── requires a
 /// │   │   └── satisfied by a-1.0.0
 /// │   ├── requires b
 /// │   │   └── satisfied by b-1.0.0
@@ -3240,39 +4884,54 @@
 ///     ├── c-1.0.0
 ///     └── c-2.0.0 (yanked)
 /// ```
 #[test]
 fn transitive_yanked_and_unyanked_dependency_opt_in() {
     let context = TestContext::new("3.8");
 
-    // In addition to the standard filters, swap out package names for more realistic messages
-    let mut filters = INSTA_FILTERS.to_vec();
-    filters.push((r"a-f0760ee9", "albatross"));
-    filters.push((r"b-f0760ee9", "bluebird"));
-    filters.push((r"c-f0760ee9", "crow"));
-    filters.push((r"-f0760ee9", ""));
+    // In addition to the standard filters, swap out package names for shorter messages
+    let mut filters = context.filters();
+    filters.push((
+        r"transitive-yanked-and-unyanked-dependency-opt-in-",
+        "package-",
+    ));
 
     uv_snapshot!(filters, command(&context)
-        .arg("a-f0760ee9")
-                .arg("b-f0760ee9")
-                .arg("c-f0760ee9==2.0.0")
+        .arg("transitive-yanked-and-unyanked-dependency-opt-in-a")
+                .arg("transitive-yanked-and-unyanked-dependency-opt-in-b")
+                .arg("transitive-yanked-and-unyanked-dependency-opt-in-c==2.0.0")
         , @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     Resolved 3 packages in [TIME]
     Downloaded 3 packages in [TIME]
     Installed 3 packages in [TIME]
-     + albatross==1.0.0
-     + bluebird==1.0.0
-     + crow==2.0.0
-    warning: crow==2.0.0 is yanked.
+     + package-a==1.0.0
+     + package-b==1.0.0
+     + package-c==2.0.0
+    warning: package-c==2.0.0 is yanked (reason: "Yanked for testing").
     "###);
 
     // Since the user explicitly selected the yanked version of `c`, it can be
     // installed.
-    assert_installed(&context.venv, "a_f0760ee9", "1.0.0", &context.temp_dir);
-    assert_installed(&context.venv, "b_f0760ee9", "1.0.0", &context.temp_dir);
-    assert_installed(&context.venv, "c_f0760ee9", "2.0.0", &context.temp_dir);
+    assert_installed(
+        &context.venv,
+        "transitive_yanked_and_unyanked_dependency_opt_in_a",
+        "1.0.0",
+        &context.temp_dir,
+    );
+    assert_installed(
+        &context.venv,
+        "transitive_yanked_and_unyanked_dependency_opt_in_b",
+        "1.0.0",
+        &context.temp_dir,
+    );
+    assert_installed(
+        &context.venv,
+        "transitive_yanked_and_unyanked_dependency_opt_in_c",
+        "2.0.0",
+        &context.temp_dir,
+    );
 }
```

### Comparing `uv-0.1.9/crates/uv/tests/pip_uninstall.rs` & `uv-0.2.0/crates/uv/tests/pip_list.rs`

 * *Files 26% similar despite different names*

```diff
@@ -1,554 +1,727 @@
 use std::process::Command;
 
 use anyhow::Result;
-use assert_cmd::prelude::*;
+use assert_fs::fixture::ChildPath;
+use assert_fs::fixture::FileWriteStr;
+use assert_fs::fixture::PathChild;
 use assert_fs::prelude::*;
-use common::{uv_snapshot, INSTA_FILTERS};
-use url::Url;
-use uv_fs::Normalized;
 
-use crate::common::{get_bin, venv_to_interpreter, TestContext};
+use common::uv_snapshot;
+
+use crate::common::{get_bin, TestContext, EXCLUDE_NEWER};
 
 mod common;
 
+/// Create a `pip install` command with options shared across scenarios.
+fn install_command(context: &TestContext) -> Command {
+    let mut command = Command::new(get_bin());
+    command
+        .arg("pip")
+        .arg("install")
+        .arg("--cache-dir")
+        .arg(context.cache_dir.path())
+        .arg("--exclude-newer")
+        .arg(EXCLUDE_NEWER)
+        .env("VIRTUAL_ENV", context.venv.as_os_str())
+        .env("UV_NO_WRAP", "1")
+        .current_dir(&context.temp_dir);
+
+    if cfg!(all(windows, debug_assertions)) {
+        // TODO(konstin): Reduce stack usage in debug mode enough that the tests pass with the
+        // default windows stack of 1MB
+        command.env("UV_STACK_SIZE", (2 * 1024 * 1024).to_string());
+    }
+
+    command
+}
+
 #[test]
-fn no_arguments() -> Result<()> {
-    let temp_dir = assert_fs::TempDir::new()?;
+fn list_empty_columns() {
+    let context = TestContext::new("3.12");
 
     uv_snapshot!(Command::new(get_bin())
         .arg("pip")
-        .arg("uninstall")
-        .current_dir(&temp_dir), @r###"
-    success: false
-    exit_code: 2
+        .arg("list")
+        .arg("--format")
+        .arg("columns")
+        .arg("--cache-dir")
+        .arg(context.cache_dir.path())
+        .env("VIRTUAL_ENV", context.venv.as_os_str())
+        .env("UV_NO_WRAP", "1")
+        .current_dir(&context.temp_dir), @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    error: the following required arguments were not provided:
-      <PACKAGE|--requirement <REQUIREMENT>|--editable <EDITABLE>>
-
-    Usage: uv pip uninstall <PACKAGE|--requirement <REQUIREMENT>|--editable <EDITABLE>>
-
-    For more information, try '--help'.
     "###
     );
-
-    Ok(())
 }
 
 #[test]
-fn invalid_requirement() -> Result<()> {
-    let temp_dir = assert_fs::TempDir::new()?;
+fn list_empty_freeze() {
+    let context = TestContext::new("3.12");
 
     uv_snapshot!(Command::new(get_bin())
         .arg("pip")
-        .arg("uninstall")
-        .arg("flask==1.0.x")
-        .current_dir(&temp_dir), @r###"
-    success: false
-    exit_code: 2
+        .arg("list")
+        .arg("--format")
+        .arg("freeze")
+        .arg("--cache-dir")
+        .arg(context.cache_dir.path())
+        .env("VIRTUAL_ENV", context.venv.as_os_str())
+        .env("UV_NO_WRAP", "1")
+        .current_dir(&context.temp_dir), @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    error: Failed to parse `flask==1.0.x`
-      Caused by: after parsing 1.0, found ".x" after it, which is not part of a valid version
-    flask==1.0.x
-         ^^^^^^^
-    "###);
-
-    Ok(())
+    "###
+    );
 }
 
 #[test]
-fn missing_requirements_txt() -> Result<()> {
-    let temp_dir = assert_fs::TempDir::new()?;
+fn list_empty_json() {
+    let context = TestContext::new("3.12");
 
     uv_snapshot!(Command::new(get_bin())
         .arg("pip")
-        .arg("uninstall")
-        .arg("-r")
-        .arg("requirements.txt")
-        .current_dir(&temp_dir), @r###"
-    success: false
-    exit_code: 2
+        .arg("list")
+        .arg("--format")
+        .arg("json")
+        .arg("--cache-dir")
+        .arg(context.cache_dir.path())
+        .env("VIRTUAL_ENV", context.venv.as_os_str())
+        .env("UV_NO_WRAP", "1")
+        .current_dir(&context.temp_dir), @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
+    []
 
     ----- stderr -----
-    error: failed to open file `requirements.txt`
-      Caused by: No such file or directory (os error 2)
     "###
     );
-
-    Ok(())
 }
 
 #[test]
-fn invalid_requirements_txt_requirement() -> Result<()> {
-    let temp_dir = assert_fs::TempDir::new()?;
-    let requirements_txt = temp_dir.child("requirements.txt");
-    requirements_txt.touch()?;
-    requirements_txt.write_str("flask==1.0.x")?;
+fn list_single_no_editable() -> Result<()> {
+    let context = TestContext::new("3.12");
 
-    uv_snapshot!(Command::new(get_bin())
-        .arg("pip")
-        .arg("uninstall")
+    let requirements_txt = context.temp_dir.child("requirements.txt");
+    requirements_txt.write_str("MarkupSafe==2.1.3")?;
+
+    uv_snapshot!(install_command(&context)
         .arg("-r")
         .arg("requirements.txt")
-        .current_dir(&temp_dir), @r###"
-    success: false
-    exit_code: 2
+        .arg("--strict"), @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    error: Couldn't parse requirement in `requirements.txt` at position 0
-      Caused by: after parsing 1.0, found ".x" after it, which is not part of a valid version
-    flask==1.0.x
-         ^^^^^^^
-    "###);
-
-    Ok(())
-}
+    Resolved 1 package in [TIME]
+    Downloaded 1 package in [TIME]
+    Installed 1 package in [TIME]
+     + markupsafe==2.1.3
+    "###
+    );
 
-#[test]
-fn missing_pyproject_toml() -> Result<()> {
-    let temp_dir = assert_fs::TempDir::new()?;
+    context.assert_command("import markupsafe").success();
 
     uv_snapshot!(Command::new(get_bin())
         .arg("pip")
-        .arg("uninstall")
-        .arg("-r")
-        .arg("pyproject.toml")
-        .current_dir(&temp_dir), @r###"
-    success: false
-    exit_code: 2
+        .arg("list")
+        .arg("--cache-dir")
+        .arg(context.cache_dir.path())
+        .env("VIRTUAL_ENV", context.venv.as_os_str())
+        .env("UV_NO_WRAP", "1")
+        .current_dir(&context.temp_dir), @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
+    Package    Version
+    ---------- -------
+    markupsafe 2.1.3
 
     ----- stderr -----
-    error: failed to open file `pyproject.toml`
-      Caused by: No such file or directory (os error 2)
     "###
     );
 
     Ok(())
 }
 
 #[test]
-fn invalid_pyproject_toml_syntax() -> Result<()> {
-    let temp_dir = assert_fs::TempDir::new()?;
-    let pyproject_toml = temp_dir.child("pyproject.toml");
-    pyproject_toml.touch()?;
-    pyproject_toml.write_str("123 - 456")?;
+fn list_editable() {
+    let context = TestContext::new("3.12");
 
-    uv_snapshot!(Command::new(get_bin())
-        .arg("pip")
-        .arg("uninstall")
-        .arg("-r")
-        .arg("pyproject.toml")
-        .current_dir(&temp_dir), @r###"
-    success: false
-    exit_code: 2
+    // Install the editable package.
+    uv_snapshot!(context.filters(), install_command(&context)
+        .arg("-e")
+        .arg(context.workspace_root.join("scripts/packages/poetry_editable")), @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    error: Failed to parse `pyproject.toml`
-      Caused by: TOML parse error at line 1, column 5
-      |
-    1 | 123 - 456
-      |     ^
-    expected `.`, `=`
-
-    "###);
-
-    Ok(())
-}
+    Built 1 editable in [TIME]
+    Resolved 4 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 4 packages in [TIME]
+     + anyio==4.3.0
+     + idna==3.6
+     + poetry-editable==0.1.0 (from file://[WORKSPACE]/scripts/packages/poetry_editable)
+     + sniffio==1.3.1
+    "###
+    );
 
-#[test]
-fn invalid_pyproject_toml_schema() -> Result<()> {
-    let temp_dir = assert_fs::TempDir::new()?;
-    let pyproject_toml = temp_dir.child("pyproject.toml");
-    pyproject_toml.touch()?;
-    pyproject_toml.write_str("[project]")?;
+    let filters = context
+        .filters()
+        .into_iter()
+        .chain(vec![(r"\-\-\-\-\-\-+.*", "[UNDERLINE]"), ("  +", " ")])
+        .collect::<Vec<_>>();
 
-    uv_snapshot!(Command::new(get_bin())
+    uv_snapshot!(filters, Command::new(get_bin())
         .arg("pip")
-        .arg("uninstall")
-        .arg("-r")
-        .arg("pyproject.toml")
-        .current_dir(&temp_dir), @r###"
-    success: false
-    exit_code: 2
+        .arg("list")
+        .arg("--cache-dir")
+        .arg(context.cache_dir.path())
+        .env("VIRTUAL_ENV", context.venv.as_os_str())
+        .env("UV_NO_WRAP", "1")
+        .current_dir(&context.temp_dir), @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
+    Package Version Editable project location
+    [UNDERLINE]
+    anyio 4.3.0
+    idna 3.6
+    poetry-editable 0.1.0 [WORKSPACE]/scripts/packages/poetry_editable
+    sniffio 1.3.1
 
     ----- stderr -----
-    error: Failed to parse `pyproject.toml`
-      Caused by: TOML parse error at line 1, column 1
-      |
-    1 | [project]
-      | ^^^^^^^^^
-    missing field `name`
-
-    "###);
-
-    Ok(())
+    "###
+    );
 }
 
 #[test]
-fn invalid_pyproject_toml_requirement() -> Result<()> {
-    let temp_dir = assert_fs::TempDir::new()?;
-    let pyproject_toml = temp_dir.child("pyproject.toml");
-    pyproject_toml.touch()?;
-    pyproject_toml.write_str(
-        r#"[project]
-name = "project"
-dependencies = ["flask==1.0.x"]
-"#,
-    )?;
+fn list_editable_only() {
+    let context = TestContext::new("3.12");
 
-    uv_snapshot!(Command::new(get_bin())
-        .arg("pip")
-        .arg("uninstall")
-        .arg("-r")
-        .arg("pyproject.toml")
-        .current_dir(&temp_dir), @r###"
-    success: false
-    exit_code: 2
+    // Install the editable package.
+    uv_snapshot!(context.filters(), install_command(&context)
+        .arg("-e")
+        .arg(context.workspace_root.join("scripts/packages/poetry_editable")), @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    error: Failed to parse `pyproject.toml`
-      Caused by: TOML parse error at line 3, column 16
-      |
-    3 | dependencies = ["flask==1.0.x"]
-      |                ^^^^^^^^^^^^^^^^
-    after parsing 1.0, found ".x" after it, which is not part of a valid version
-    flask==1.0.x
-         ^^^^^^^
-
-    "###);
-
-    Ok(())
-}
-
-#[test]
-fn uninstall() -> Result<()> {
-    let context = TestContext::new("3.12");
+    Built 1 editable in [TIME]
+    Resolved 4 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 4 packages in [TIME]
+     + anyio==4.3.0
+     + idna==3.6
+     + poetry-editable==0.1.0 (from file://[WORKSPACE]/scripts/packages/poetry_editable)
+     + sniffio==1.3.1
+    "###
+    );
 
-    let requirements_txt = context.temp_dir.child("requirements.txt");
-    requirements_txt.touch()?;
-    requirements_txt.write_str("MarkupSafe==2.1.3")?;
+    let filters = context
+        .filters()
+        .into_iter()
+        .chain(vec![(r"\-\-\-\-\-\-+.*", "[UNDERLINE]"), ("  +", " ")])
+        .collect::<Vec<_>>();
 
-    Command::new(get_bin())
+    uv_snapshot!(filters, Command::new(get_bin())
         .arg("pip")
-        .arg("sync")
-        .arg("requirements.txt")
+        .arg("list")
+        .arg("--editable")
         .arg("--cache-dir")
         .arg(context.cache_dir.path())
         .env("VIRTUAL_ENV", context.venv.as_os_str())
-        .current_dir(&context.temp_dir)
-        .assert()
-        .success();
-
-    Command::new(venv_to_interpreter(&context.venv))
-        .arg("-c")
-        .arg("import markupsafe")
-        .current_dir(&context.temp_dir)
-        .assert()
-        .success();
+        .env("UV_NO_WRAP", "1")
+        .current_dir(&context.temp_dir), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+    Package Version Editable project location
+    [UNDERLINE]
+    poetry-editable 0.1.0 [WORKSPACE]/scripts/packages/poetry_editable
 
-    uv_snapshot!(Command::new(get_bin())
+    ----- stderr -----
+    "###
+    );
+
+    uv_snapshot!(filters, Command::new(get_bin())
         .arg("pip")
-        .arg("uninstall")
-        .arg("MarkupSafe")
+        .arg("list")
+        .arg("--exclude-editable")
         .arg("--cache-dir")
         .arg(context.cache_dir.path())
         .env("VIRTUAL_ENV", context.venv.as_os_str())
+        .env("UV_NO_WRAP", "1")
         .current_dir(&context.temp_dir), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
+    Package Version
+    [UNDERLINE]
+    anyio 4.3.0
+    idna 3.6
+    sniffio 1.3.1
 
     ----- stderr -----
-    Uninstalled 1 package in [TIME]
-     - markupsafe==2.1.3
     "###
     );
 
-    Command::new(venv_to_interpreter(&context.venv))
-        .arg("-c")
-        .arg("import markupsafe")
-        .current_dir(&context.temp_dir)
-        .assert()
-        .failure();
+    uv_snapshot!(filters, Command::new(get_bin())
+        .arg("pip")
+        .arg("list")
+        .arg("--editable")
+        .arg("--exclude-editable")
+        .arg("--cache-dir")
+        .arg(context.cache_dir.path())
+        .env("VIRTUAL_ENV", context.venv.as_os_str())
+        .env("UV_NO_WRAP", "1")
+        .current_dir(&context.temp_dir), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
 
-    Ok(())
+    ----- stderr -----
+    "###
+    );
 }
 
 #[test]
-fn missing_record() -> Result<()> {
+fn list_exclude() {
     let context = TestContext::new("3.12");
 
-    let requirements_txt = context.temp_dir.child("requirements.txt");
-    requirements_txt.touch()?;
-    requirements_txt.write_str("MarkupSafe==2.1.3")?;
+    // Install the editable package.
+    uv_snapshot!(context.filters(), install_command(&context)
+        .arg("-e")
+        .arg(context.workspace_root.join("scripts/packages/poetry_editable")), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
 
-    Command::new(get_bin())
-        .arg("pip")
-        .arg("sync")
-        .arg("requirements.txt")
-        .arg("--cache-dir")
-        .arg(context.cache_dir.path())
-        .env("VIRTUAL_ENV", context.venv.as_os_str())
-        .current_dir(&context.temp_dir)
-        .assert()
-        .success();
-
-    Command::new(venv_to_interpreter(&context.venv))
-        .arg("-c")
-        .arg("import markupsafe")
-        .current_dir(&context.temp_dir)
-        .assert()
-        .success();
-
-    // Delete the RECORD file.
-    let dist_info = fs_err::canonicalize(if cfg!(unix) {
-        context
-            .venv
-            .join("lib")
-            .join("python3.12")
-            .join("site-packages")
-            .join("MarkupSafe-2.1.3.dist-info")
-    } else if cfg!(windows) {
-        context
-            .venv
-            .join("Lib")
-            .join("site-packages")
-            .join("MarkupSafe-2.1.3.dist-info")
-    } else {
-        unimplemented!("Only Windows and Unix are supported")
-    })
-    .unwrap();
-    std::fs::remove_file(dist_info.join("RECORD"))?;
-
-    let dist_info_str = regex::escape(&format!(
-        "RECORD file not found at: {}",
-        dist_info.normalized_display()
-    ));
-    let filters: Vec<_> = [(
-        dist_info_str.as_str(),
-        "RECORD file not found at: [DIST_INFO]",
-    )]
-    .into_iter()
-    .chain(INSTA_FILTERS.to_vec())
-    .collect();
+    ----- stderr -----
+    Built 1 editable in [TIME]
+    Resolved 4 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 4 packages in [TIME]
+     + anyio==4.3.0
+     + idna==3.6
+     + poetry-editable==0.1.0 (from file://[WORKSPACE]/scripts/packages/poetry_editable)
+     + sniffio==1.3.1
+    "###
+    );
+
+    let filters = context
+        .filters()
+        .into_iter()
+        .chain(vec![(r"\-\-\-\-\-\-+.*", "[UNDERLINE]"), ("  +", " ")])
+        .collect::<Vec<_>>();
 
     uv_snapshot!(filters, Command::new(get_bin())
-        .arg("pip")
-        .arg("uninstall")
-        .arg("MarkupSafe")
-        .arg("--cache-dir")
-        .arg(context.cache_dir.path())
-        .env("VIRTUAL_ENV", context.venv.as_os_str())
-        .current_dir(&context.temp_dir), @r###"
-    success: false
-    exit_code: 2
+    .arg("pip")
+    .arg("list")
+    .arg("--exclude")
+    .arg("numpy")
+    .arg("--cache-dir")
+    .arg(context.cache_dir.path())
+    .env("VIRTUAL_ENV", context.venv.as_os_str())
+    .env("UV_NO_WRAP", "1")
+    .current_dir(&context.temp_dir), @r###"
+    success: true
+    exit_code: 0
     ----- stdout -----
+    Package Version Editable project location
+    [UNDERLINE]
+    anyio 4.3.0
+    idna 3.6
+    poetry-editable 0.1.0 [WORKSPACE]/scripts/packages/poetry_editable
+    sniffio 1.3.1
 
     ----- stderr -----
-    error: Cannot uninstall package; RECORD file not found at: [DIST_INFO]/RECORD
     "###
     );
 
-    Ok(())
+    uv_snapshot!(filters, Command::new(get_bin())
+    .arg("pip")
+    .arg("list")
+    .arg("--exclude")
+    .arg("poetry-editable")
+    .arg("--cache-dir")
+    .arg(context.cache_dir.path())
+    .env("VIRTUAL_ENV", context.venv.as_os_str())
+    .env("UV_NO_WRAP", "1")
+    .current_dir(&context.temp_dir), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+    Package Version
+    [UNDERLINE]
+    anyio 4.3.0
+    idna 3.6
+    sniffio 1.3.1
+
+    ----- stderr -----
+    "###
+    );
+
+    uv_snapshot!(filters, Command::new(get_bin())
+    .arg("pip")
+    .arg("list")
+    .arg("--exclude")
+    .arg("numpy")
+    .arg("--exclude")
+    .arg("poetry-editable")
+    .arg("--cache-dir")
+    .arg(context.cache_dir.path())
+    .env("VIRTUAL_ENV", context.venv.as_os_str())
+    .env("UV_NO_WRAP", "1")
+    .current_dir(&context.temp_dir), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+    Package Version
+    [UNDERLINE]
+    anyio 4.3.0
+    idna 3.6
+    sniffio 1.3.1
+
+    ----- stderr -----
+    "###
+    );
 }
 
 #[test]
-fn uninstall_editable_by_name() -> Result<()> {
+#[cfg(not(windows))]
+fn list_format_json() {
     let context = TestContext::new("3.12");
 
-    let current_dir = std::env::current_dir()?;
-    let workspace_dir = regex::escape(
-        Url::from_directory_path(current_dir.join("..").join("..").canonicalize()?)
-            .unwrap()
-            .as_str(),
+    // Install the editable package.
+    uv_snapshot!(context.filters(), install_command(&context)
+        .arg("-e")
+        .arg(context.workspace_root.join("scripts/packages/poetry_editable")), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Built 1 editable in [TIME]
+    Resolved 4 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 4 packages in [TIME]
+     + anyio==4.3.0
+     + idna==3.6
+     + poetry-editable==0.1.0 (from file://[WORKSPACE]/scripts/packages/poetry_editable)
+     + sniffio==1.3.1
+    "###
     );
 
-    let filters: Vec<_> = [(workspace_dir.as_str(), "file://[WORKSPACE_DIR]/")]
+    let filters: Vec<_> = context
+        .filters()
         .into_iter()
-        .chain(INSTA_FILTERS.to_vec())
+        .chain(vec![(r"\-\-\-\-\-\-+.*", "[UNDERLINE]"), ("  +", " ")])
         .collect();
 
-    let requirements_txt = context.temp_dir.child("requirements.txt");
-    requirements_txt.touch()?;
-    requirements_txt.write_str("-e ../../scripts/editable-installs/poetry_editable")?;
+    uv_snapshot!(filters, Command::new(get_bin())
+    .arg("pip")
+    .arg("list")
+    .arg("--format=json")
+    .arg("--cache-dir")
+    .arg(context.cache_dir.path())
+    .env("VIRTUAL_ENV", context.venv.as_os_str())
+    .env("UV_NO_WRAP", "1")
+    .current_dir(&context.temp_dir), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+    [{"name":"anyio","version":"4.3.0"},{"name":"idna","version":"3.6"},{"name":"poetry-editable","version":"0.1.0","editable_project_location":"[WORKSPACE]/scripts/packages/poetry_editable"},{"name":"sniffio","version":"1.3.1"}]
 
-    Command::new(get_bin())
-        .arg("pip")
-        .arg("sync")
-        .arg(requirements_txt.path())
-        .arg("--cache-dir")
-        .arg(context.cache_dir.path())
-        .env("VIRTUAL_ENV", context.venv.as_os_str())
-        .assert()
-        .success();
+    ----- stderr -----
+    "###
+    );
 
-    Command::new(venv_to_interpreter(&context.venv))
-        .arg("-c")
-        .arg("import poetry_editable")
-        .assert()
-        .success();
+    uv_snapshot!(filters, Command::new(get_bin())
+    .arg("pip")
+    .arg("list")
+    .arg("--format=json")
+    .arg("--editable")
+    .arg("--cache-dir")
+    .arg(context.cache_dir.path())
+    .env("VIRTUAL_ENV", context.venv.as_os_str())
+    .env("UV_NO_WRAP", "1")
+    .current_dir(&context.temp_dir), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+    [{"name":"poetry-editable","version":"0.1.0","editable_project_location":"[WORKSPACE]/scripts/packages/poetry_editable"}]
+
+    ----- stderr -----
+    "###
+    );
 
-    // Uninstall the editable by name.
     uv_snapshot!(filters, Command::new(get_bin())
-        .arg("pip")
-        .arg("uninstall")
-        .arg("poetry-editable")
-        .arg("--cache-dir")
-        .arg(context.cache_dir.path())
-        .env("VIRTUAL_ENV", context.venv.as_os_str()), @r###"
+    .arg("pip")
+    .arg("list")
+    .arg("--format=json")
+    .arg("--exclude-editable")
+    .arg("--cache-dir")
+    .arg(context.cache_dir.path())
+    .env("VIRTUAL_ENV", context.venv.as_os_str())
+    .env("UV_NO_WRAP", "1")
+    .current_dir(&context.temp_dir), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
+    [{"name":"anyio","version":"4.3.0"},{"name":"idna","version":"3.6"},{"name":"sniffio","version":"1.3.1"}]
 
     ----- stderr -----
-    Uninstalled 1 package in [TIME]
-     - poetry-editable==0.1.0 (from file://[WORKSPACE_DIR]/scripts/editable-installs/poetry_editable)
     "###
     );
 
-    Command::new(venv_to_interpreter(&context.venv))
-        .arg("-c")
-        .arg("import poetry_editable")
-        .assert()
-        .failure();
+    uv_snapshot!(filters, Command::new(get_bin())
+    .arg("pip")
+    .arg("list")
+    .arg("--format=json")
+    .arg("--editable")
+    .arg("--exclude-editable")
+    .arg("--cache-dir")
+    .arg(context.cache_dir.path())
+    .env("VIRTUAL_ENV", context.venv.as_os_str())
+    .env("UV_NO_WRAP", "1")
+    .current_dir(&context.temp_dir), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+    []
 
-    Ok(())
+    ----- stderr -----
+    "###
+    );
 }
 
 #[test]
-fn uninstall_editable_by_path() -> Result<()> {
+fn list_format_freeze() {
     let context = TestContext::new("3.12");
 
-    let current_dir = std::env::current_dir()?;
-    let workspace_dir = regex::escape(
-        Url::from_directory_path(current_dir.join("..").join("..").canonicalize()?)
-            .unwrap()
-            .as_str(),
+    // Install the editable package.
+    uv_snapshot!(context.filters(), install_command(&context)
+        .arg("-e")
+        .arg(context.workspace_root.join("scripts/packages/poetry_editable")), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Built 1 editable in [TIME]
+    Resolved 4 packages in [TIME]
+    Downloaded 3 packages in [TIME]
+    Installed 4 packages in [TIME]
+     + anyio==4.3.0
+     + idna==3.6
+     + poetry-editable==0.1.0 (from file://[WORKSPACE]/scripts/packages/poetry_editable)
+     + sniffio==1.3.1
+    "###
     );
 
-    let filters: Vec<_> = [(workspace_dir.as_str(), "file://[WORKSPACE_DIR]/")]
+    let filters = context
+        .filters()
         .into_iter()
-        .chain(INSTA_FILTERS.to_vec())
-        .collect();
+        .chain(vec![(r"\-\-\-\-\-\-+.*", "[UNDERLINE]"), ("  +", " ")])
+        .collect::<Vec<_>>();
 
-    let requirements_txt = context.temp_dir.child("requirements.txt");
-    requirements_txt.touch()?;
-    requirements_txt.write_str("-e ../../scripts/editable-installs/poetry_editable")?;
+    uv_snapshot!(filters, Command::new(get_bin())
+    .arg("pip")
+    .arg("list")
+    .arg("--format=freeze")
+    .arg("--cache-dir")
+    .arg(context.cache_dir.path())
+    .env("VIRTUAL_ENV", context.venv.as_os_str())
+    .env("UV_NO_WRAP", "1")
+    .current_dir(&context.temp_dir), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+    anyio==4.3.0
+    idna==3.6
+    poetry-editable==0.1.0
+    sniffio==1.3.1
 
-    Command::new(get_bin())
-        .arg("pip")
-        .arg("sync")
-        .arg(requirements_txt.path())
-        .arg("--cache-dir")
-        .arg(context.cache_dir.path())
-        .env("VIRTUAL_ENV", context.venv.as_os_str())
-        .assert()
-        .success();
+    ----- stderr -----
+    "###
+    );
 
-    Command::new(venv_to_interpreter(&context.venv))
-        .arg("-c")
-        .arg("import poetry_editable")
-        .assert()
-        .success();
+    uv_snapshot!(filters, Command::new(get_bin())
+    .arg("pip")
+    .arg("list")
+    .arg("--format=freeze")
+    .arg("--editable")
+    .arg("--cache-dir")
+    .arg(context.cache_dir.path())
+    .env("VIRTUAL_ENV", context.venv.as_os_str())
+    .env("UV_NO_WRAP", "1")
+    .current_dir(&context.temp_dir), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+    poetry-editable==0.1.0
+
+    ----- stderr -----
+    "###
+    );
 
-    // Uninstall the editable by path.
     uv_snapshot!(filters, Command::new(get_bin())
-        .arg("pip")
-        .arg("uninstall")
-        .arg("-e")
-        .arg("../../scripts/editable-installs/poetry_editable")
-        .arg("--cache-dir")
-        .arg(context.cache_dir.path())
-        .env("VIRTUAL_ENV", context.venv.as_os_str()), @r###"
+    .arg("pip")
+    .arg("list")
+    .arg("--format=freeze")
+    .arg("--exclude-editable")
+    .arg("--cache-dir")
+    .arg(context.cache_dir.path())
+    .env("VIRTUAL_ENV", context.venv.as_os_str())
+    .env("UV_NO_WRAP", "1")
+    .current_dir(&context.temp_dir), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
+    anyio==4.3.0
+    idna==3.6
+    sniffio==1.3.1
 
     ----- stderr -----
-    Uninstalled 1 package in [TIME]
-     - poetry-editable==0.1.0 (from file://[WORKSPACE_DIR]/scripts/editable-installs/poetry_editable)
     "###
     );
 
-    Command::new(venv_to_interpreter(&context.venv))
-        .arg("-c")
-        .arg("import poetry_editable")
-        .assert()
-        .failure();
+    uv_snapshot!(filters, Command::new(get_bin())
+    .arg("pip")
+    .arg("list")
+    .arg("--format=freeze")
+    .arg("--editable")
+    .arg("--exclude-editable")
+    .arg("--cache-dir")
+    .arg(context.cache_dir.path())
+    .env("VIRTUAL_ENV", context.venv.as_os_str())
+    .env("UV_NO_WRAP", "1")
+    .current_dir(&context.temp_dir), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
 
-    Ok(())
+    ----- stderr -----
+    "###
+    );
 }
 
 #[test]
-fn uninstall_duplicate_editable() -> Result<()> {
+fn list_legacy_editable() -> Result<()> {
     let context = TestContext::new("3.12");
 
-    let current_dir = std::env::current_dir()?;
-    let workspace_dir = regex::escape(
-        Url::from_directory_path(current_dir.join("..").join("..").canonicalize()?)
-            .unwrap()
-            .as_str(),
-    );
+    let site_packages = ChildPath::new(context.site_packages());
 
-    let filters: Vec<_> = [(workspace_dir.as_str(), "file://[WORKSPACE_DIR]/")]
-        .into_iter()
-        .chain(INSTA_FILTERS.to_vec())
-        .collect();
+    let target = context.temp_dir.child("zstandard_project");
+    target.child("zstd").create_dir_all()?;
+    target.child("zstd").child("__init__.py").write_str("")?;
+
+    target.child("zstandard.egg-info").create_dir_all()?;
+    target
+        .child("zstandard.egg-info")
+        .child("PKG-INFO")
+        .write_str(
+            "Metadata-Version: 2.1
+Name: zstandard
+Version: 0.22.0
+",
+        )?;
+
+    site_packages
+        .child("zstandard.egg-link")
+        .write_str(target.path().to_str().unwrap())?;
+
+    site_packages.child("easy-install.pth").write_str(&format!(
+        "something\n{}\nanother thing\n",
+        target.path().to_str().unwrap()
+    ))?;
 
-    let requirements_txt = context.temp_dir.child("requirements.txt");
-    requirements_txt.touch()?;
-    requirements_txt.write_str("-e ../../scripts/editable-installs/poetry_editable")?;
+    let filters = context
+        .filters()
+        .into_iter()
+        .chain(vec![(r"\-\-\-\-\-\-+.*", "[UNDERLINE]"), ("  +", " ")])
+        .collect::<Vec<_>>();
 
-    Command::new(get_bin())
+    uv_snapshot!(filters, Command::new(get_bin())
         .arg("pip")
-        .arg("sync")
-        .arg(requirements_txt.path())
+        .arg("list")
+        .arg("--editable")
         .arg("--cache-dir")
         .arg(context.cache_dir.path())
         .env("VIRTUAL_ENV", context.venv.as_os_str())
-        .assert()
-        .success();
+        .env("UV_NO_WRAP", "1")
+        .current_dir(&context.temp_dir), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+    Package Version Editable project location
+    [UNDERLINE]
+    zstandard 0.22.0 [TEMP_DIR]/zstandard_project
+
+    ----- stderr -----
+    "###
+    );
+
+    Ok(())
+}
 
-    Command::new(venv_to_interpreter(&context.venv))
-        .arg("-c")
-        .arg("import poetry_editable")
-        .assert()
-        .success();
+#[test]
+fn list_legacy_editable_invalid_version() -> Result<()> {
+    let context = TestContext::new("3.12");
+
+    let site_packages = ChildPath::new(context.site_packages());
+
+    let target = context.temp_dir.child("paramiko_project");
+    target.child("paramiko.egg-info").create_dir_all()?;
+    target
+        .child("paramiko.egg-info")
+        .child("PKG-INFO")
+        .write_str(
+            "Metadata-Version: 1.0
+Name: paramiko
+Version: 0.1-bulbasaur
+",
+        )?;
+    site_packages
+        .child("paramiko.egg-link")
+        .write_str(target.path().to_str().unwrap())?;
+
+    let filters = context
+        .filters()
+        .into_iter()
+        .chain(vec![(r"\-\-\-\-\-\-+.*", "[UNDERLINE]"), ("  +", " ")])
+        .collect::<Vec<_>>();
 
-    // Uninstall the editable by both path and name.
     uv_snapshot!(filters, Command::new(get_bin())
         .arg("pip")
-        .arg("uninstall")
-        .arg("poetry-editable")
-        .arg("-e")
-        .arg("../../scripts/editable-installs/poetry_editable")
+        .arg("list")
+        .arg("--editable")
         .arg("--cache-dir")
         .arg(context.cache_dir.path())
-        .env("VIRTUAL_ENV", context.venv.as_os_str()), @r###"
-    success: true
-    exit_code: 0
+        .env("VIRTUAL_ENV", context.venv.as_os_str())
+        .env("UV_NO_WRAP", "1")
+        .current_dir(&context.temp_dir), @r###"
+    success: false
+    exit_code: 2
     ----- stdout -----
 
     ----- stderr -----
-    Uninstalled 1 package in [TIME]
-     - poetry-editable==0.1.0 (from file://[WORKSPACE_DIR]/scripts/editable-installs/poetry_editable)
+    error: Failed to read metadata: from [SITE_PACKAGES]/paramiko.egg-link
+     Caused by: after parsing '0.1-b', found 'ulbasaur', which is not part of a valid version
     "###
     );
 
-    Command::new(venv_to_interpreter(&context.venv))
-        .arg("-c")
-        .arg("import poetry_editable")
-        .assert()
-        .failure();
-
     Ok(())
 }
```

### Comparing `uv-0.1.9/crates/uv/tests/venv.rs` & `uv-0.2.0/crates/uv/tests/venv.rs`

 * *Files 26% similar despite different names*

```diff
@@ -1,654 +1,533 @@
 #![cfg(feature = "python")]
 
 use std::process::Command;
+use std::{ffi::OsString, str::FromStr};
 
 use anyhow::Result;
+use assert_cmd::prelude::*;
+use assert_fs::fixture::ChildPath;
 use assert_fs::prelude::*;
+use fs_err::PathExt;
+use uv_fs::Simplified;
+use uv_interpreter::PythonVersion;
 
-use uv_fs::Normalized;
-
-use crate::common::{
-    create_bin_with_executables, get_bin, uv_snapshot, TestContext, EXCLUDE_NEWER,
-};
+use crate::common::{get_bin, python_path_with_versions, uv_snapshot, TestContext, EXCLUDE_NEWER};
 
 mod common;
 
+struct VenvTestContext {
+    cache_dir: assert_fs::TempDir,
+    temp_dir: assert_fs::TempDir,
+    venv: ChildPath,
+    python_path: OsString,
+    python_versions: Vec<PythonVersion>,
+}
+
+impl VenvTestContext {
+    fn new(python_versions: &[&str]) -> Self {
+        let temp_dir = assert_fs::TempDir::new().unwrap();
+        let python_path = python_path_with_versions(&temp_dir, python_versions)
+            .expect("Failed to create Python test path");
+        let venv = temp_dir.child(".venv");
+        let python_versions = python_versions
+            .iter()
+            .map(|version| {
+                PythonVersion::from_str(version).expect("Tests should use valid Python versions")
+            })
+            .collect::<Vec<_>>();
+        Self {
+            cache_dir: assert_fs::TempDir::new().unwrap(),
+            temp_dir,
+            venv,
+            python_path,
+            python_versions,
+        }
+    }
+
+    fn venv_command(&self) -> Command {
+        let mut command = Command::new(get_bin());
+        command
+            .arg("venv")
+            .arg("--cache-dir")
+            .arg(self.cache_dir.path())
+            .arg("--exclude-newer")
+            .arg(EXCLUDE_NEWER)
+            .env("UV_TEST_PYTHON_PATH", self.python_path.clone())
+            .env("UV_NO_WRAP", "1")
+            .env("UV_STACK_SIZE", (2 * 1024 * 1024).to_string())
+            .current_dir(self.temp_dir.path());
+        command
+    }
+
+    fn filters(&self) -> Vec<(String, String)> {
+        // On windows, a directory can have multiple names (https://superuser.com/a/1666770), e.g.
+        // `C:\Users\KONSTA~1` and `C:\Users\Konstantin` are the same.
+        let venv_full = regex::escape(&self.venv.display().to_string());
+        let mut filters = vec![(venv_full, ".venv".to_string())];
+
+        // For mac, otherwise it shows some /var/folders/ path.
+        if let Ok(canonicalized) = self.venv.path().fs_err_canonicalize() {
+            let venv_full = regex::escape(&canonicalized.simplified_display().to_string());
+            filters.push((venv_full, ".venv".to_string()));
+        }
+
+        filters.push((
+            r"interpreter at: .+".to_string(),
+            "interpreter at: [PATH]".to_string(),
+        ));
+        filters.push((
+            r"Activate with: (?:.*)\\Scripts\\activate".to_string(),
+            "Activate with: source .venv/bin/activate".to_string(),
+        ));
+
+        // Add Python patch version filtering unless one was explicitly requested to ensure
+        // snapshots are patch version agnostic when it is not a part of the test.
+        if self
+            .python_versions
+            .iter()
+            .all(|version| version.patch().is_none())
+        {
+            for python_version in &self.python_versions {
+                filters.push((
+                    format!(
+                        r"({})\.\d+",
+                        regex::escape(python_version.to_string().as_str())
+                    ),
+                    "$1.[X]".to_string(),
+                ));
+            }
+        }
+
+        filters
+    }
+}
+
 #[test]
-fn create_venv() -> Result<()> {
-    let temp_dir = assert_fs::TempDir::new()?;
-    let cache_dir = assert_fs::TempDir::new()?;
-    let bin = create_bin_with_executables(&temp_dir, &["3.12"]).expect("Failed to create bin dir");
-    let venv = temp_dir.child(".venv");
+fn create_venv() {
+    let context = VenvTestContext::new(&["3.12"]);
 
     // Create a virtual environment at `.venv`.
-    let filter_venv = regex::escape(&venv.normalized_display().to_string());
-    let filter_prompt = r"Activate with: (?:.*)\\Scripts\\activate";
-    let filters = &[
-        (
-            r"Using Python 3\.\d+\.\d+ interpreter at .+",
-            "Using Python [VERSION] interpreter at [PATH]",
-        ),
-        (&filter_venv, "/home/ferris/project/.venv"),
-        (
-            &filter_prompt,
-            "Activate with: source /home/ferris/project/.venv/bin/activate",
-        ),
-    ];
-    uv_snapshot!(filters, Command::new(get_bin())
-        .arg("venv")
-        .arg(venv.as_os_str())
+    uv_snapshot!(context.filters(), context.venv_command()
+        .arg(context.venv.as_os_str())
         .arg("--python")
-        .arg("3.12")
-        .arg("--cache-dir")
-        .arg(cache_dir.path())
-        .arg("--exclude-newer")
-        .arg(EXCLUDE_NEWER)
-        .env("UV_TEST_PYTHON_PATH", bin.clone())
-        .current_dir(&temp_dir), @r###"
+        .arg("3.12"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    Using Python [VERSION] interpreter at [PATH]
-    Creating virtualenv at: /home/ferris/project/.venv
-    Activate with: source /home/ferris/project/.venv/bin/activate
+    Using Python 3.12.[X] interpreter at: [PATH]
+    Creating virtualenv at: .venv
+    Activate with: source .venv/bin/activate
     "###
     );
 
-    venv.assert(predicates::path::is_dir());
+    context.venv.assert(predicates::path::is_dir());
 
     // Create a virtual environment at the same location, which should replace it.
-    let filter_venv = regex::escape(&venv.normalized_display().to_string());
-    let filter_prompt = r"Activate with: (?:.*)\\Scripts\\activate";
-    let filters = &[
-        (
-            r"Using Python 3\.\d+\.\d+ interpreter at .+",
-            "Using Python [VERSION] interpreter at [PATH]",
-        ),
-        (&filter_venv, "/home/ferris/project/.venv"),
-        (
-            &filter_prompt,
-            "Activate with: source /home/ferris/project/.venv/bin/activate",
-        ),
-    ];
-    uv_snapshot!(filters, Command::new(get_bin())
-        .arg("venv")
-        .arg(venv.as_os_str())
+    uv_snapshot!(context.filters(), context.venv_command()
+        .arg(context.venv.as_os_str())
         .arg("--python")
-        .arg("3.12")
-        .arg("--cache-dir")
-        .arg(cache_dir.path())
-        .arg("--exclude-newer")
-        .arg(EXCLUDE_NEWER)
-        .env("UV_NO_WRAP", "1")
-        .env("UV_TEST_PYTHON_PATH", bin)
-        .current_dir(&temp_dir), @r###"
+        .arg("3.12"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    Using Python [VERSION] interpreter at [PATH]
-    Creating virtualenv at: /home/ferris/project/.venv
-    Activate with: source /home/ferris/project/.venv/bin/activate
+    Using Python 3.12.[X] interpreter at: [PATH]
+    Creating virtualenv at: .venv
+    Activate with: source .venv/bin/activate
     "###
     );
 
-    venv.assert(predicates::path::is_dir());
-
-    Ok(())
+    context.venv.assert(predicates::path::is_dir());
 }
 
 #[test]
-fn create_venv_defaults_to_cwd() -> Result<()> {
-    let temp_dir = assert_fs::TempDir::new()?;
-    let cache_dir = assert_fs::TempDir::new()?;
-    let bin = create_bin_with_executables(&temp_dir, &["3.12"]).expect("Failed to create bin dir");
-    let venv = temp_dir.child(".venv");
-
-    let filter_venv = regex::escape(&venv.normalized_display().to_string());
-    let filter_prompt = r"Activate with: (?:.*)\\Scripts\\activate";
-    let filters = &[
-        (
-            r"Using Python 3\.\d+\.\d+ interpreter at .+",
-            "Using Python [VERSION] interpreter at [PATH]",
-        ),
-        (&filter_venv, "/home/ferris/project/.venv"),
-        (&filter_prompt, "Activate with: source .venv/bin/activate"),
-    ];
-    uv_snapshot!(filters, Command::new(get_bin())
-        .arg("venv")
+fn create_venv_defaults_to_cwd() {
+    let context = VenvTestContext::new(&["3.12"]);
+    uv_snapshot!(context.filters(), context.venv_command()
         .arg("--python")
-        .arg("3.12")
-        .arg("--cache-dir")
-        .arg(cache_dir.path())
-        .arg("--exclude-newer")
-        .arg(EXCLUDE_NEWER)
-        .env("UV_NO_WRAP", "1")
-        .env("UV_TEST_PYTHON_PATH", bin)
-        .current_dir(&temp_dir), @r###"
+        .arg("3.12"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    Using Python [VERSION] interpreter at [PATH]
+    Using Python 3.12.[X] interpreter at: [PATH]
     Creating virtualenv at: .venv
     Activate with: source .venv/bin/activate
     "###
     );
 
-    venv.assert(predicates::path::is_dir());
-
-    Ok(())
+    context.venv.assert(predicates::path::is_dir());
 }
 
 #[test]
-fn seed() -> Result<()> {
-    let temp_dir = assert_fs::TempDir::new()?;
-    let cache_dir = assert_fs::TempDir::new()?;
-    let bin = create_bin_with_executables(&temp_dir, &["3.12"]).expect("Failed to create bin dir");
-    let venv = temp_dir.child(".venv");
-
-    let filter_venv = regex::escape(&venv.normalized_display().to_string());
-    let filter_prompt = r"Activate with: (?:.*)\\Scripts\\activate";
-    let filters = &[
-        (
-            r"Using Python 3\.\d+\.\d+ interpreter at .+",
-            "Using Python [VERSION] interpreter at [PATH]",
-        ),
-        (&filter_venv, "/home/ferris/project/.venv"),
-        (
-            &filter_prompt,
-            "Activate with: source /home/ferris/project/.venv/bin/activate",
-        ),
-    ];
-    uv_snapshot!(filters, Command::new(get_bin())
-        .arg("venv")
-        .arg(venv.as_os_str())
+fn seed() {
+    let context = VenvTestContext::new(&["3.12"]);
+    uv_snapshot!(context.filters(), context.venv_command()
+        .arg(context.venv.as_os_str())
         .arg("--seed")
         .arg("--python")
-        .arg("3.12")
-        .arg("--cache-dir")
-        .arg(cache_dir.path())
-        .arg("--exclude-newer")
-        .arg(EXCLUDE_NEWER)
-        .env("UV_NO_WRAP", "1")
-        .env("UV_TEST_PYTHON_PATH", bin)
-        .current_dir(&temp_dir), @r###"
+        .arg("3.12"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    Using Python [VERSION] interpreter at [PATH]
-    Creating virtualenv at: /home/ferris/project/.venv
-     + pip==23.3.1
-    Activate with: source /home/ferris/project/.venv/bin/activate
+    Using Python 3.12.[X] interpreter at: [PATH]
+    Creating virtualenv at: .venv
+     + pip==24.0
+    Activate with: source .venv/bin/activate
     "###
     );
 
-    venv.assert(predicates::path::is_dir());
-
-    Ok(())
+    context.venv.assert(predicates::path::is_dir());
 }
 
 #[test]
-fn seed_older_python_version() -> Result<()> {
-    let temp_dir = assert_fs::TempDir::new()?;
-    let cache_dir = assert_fs::TempDir::new()?;
-    let bin = create_bin_with_executables(&temp_dir, &["3.10"]).expect("Failed to create bin dir");
-    let venv = temp_dir.child(".venv");
-
-    let filter_venv = regex::escape(&venv.normalized_display().to_string());
-    let filter_prompt = r"Activate with: (?:.*)\\Scripts\\activate";
-    let filters = &[
-        (
-            r"Using Python 3\.\d+\.\d+ interpreter at .+",
-            "Using Python [VERSION] interpreter at [PATH]",
-        ),
-        (&filter_venv, "/home/ferris/project/.venv"),
-        (
-            &filter_prompt,
-            "Activate with: source /home/ferris/project/.venv/bin/activate",
-        ),
-    ];
-    uv_snapshot!(filters, Command::new(get_bin())
-        .arg("venv")
-        .arg(venv.as_os_str())
+fn seed_older_python_version() {
+    let context = VenvTestContext::new(&["3.10"]);
+    uv_snapshot!(context.filters(), context.venv_command()
+        .arg(context.venv.as_os_str())
         .arg("--seed")
         .arg("--python")
-        .arg("3.10")
-        .arg("--cache-dir")
-        .arg(cache_dir.path())
-        .arg("--exclude-newer")
-        .arg(EXCLUDE_NEWER)
-        .env("UV_NO_WRAP", "1")
-        .env("UV_TEST_PYTHON_PATH", bin)
-        .current_dir(&temp_dir), @r###"
+        .arg("3.10"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    Using Python [VERSION] interpreter at [PATH]
-    Creating virtualenv at: /home/ferris/project/.venv
-     + pip==23.3.1
-     + setuptools==68.2.2
-     + wheel==0.41.3
-    Activate with: source /home/ferris/project/.venv/bin/activate
+    Using Python 3.10.[X] interpreter at: [PATH]
+    Creating virtualenv at: .venv
+     + pip==24.0
+     + setuptools==69.2.0
+     + wheel==0.43.0
+    Activate with: source .venv/bin/activate
     "###
     );
 
-    venv.assert(predicates::path::is_dir());
-
-    Ok(())
+    context.venv.assert(predicates::path::is_dir());
 }
 
 #[test]
-fn create_venv_unknown_python_minor() -> Result<()> {
-    let temp_dir = assert_fs::TempDir::new()?;
-    let cache_dir = assert_fs::TempDir::new()?;
-    let bin = create_bin_with_executables(&temp_dir, &["3.12"]).expect("Failed to create bin dir");
-    let venv = temp_dir.child(".venv");
+fn create_venv_unknown_python_minor() {
+    let context = VenvTestContext::new(&["3.12"]);
 
-    let mut command = Command::new(get_bin());
+    let mut command = context.venv_command();
     command
-        .arg("venv")
-        .arg(venv.as_os_str())
+        .arg(context.venv.as_os_str())
+        // Request a version we know we'll never see
         .arg("--python")
-        .arg("3.15")
-        .arg("--cache-dir")
-        .arg(cache_dir.path())
-        .arg("--exclude-newer")
-        .arg(EXCLUDE_NEWER)
-        .env("UV_NO_WRAP", "1")
-        .env("UV_TEST_PYTHON_PATH", bin)
-        .current_dir(&temp_dir);
+        .arg("3.100")
+        // Unset this variable to force what the user would see
+        .env_remove("UV_TEST_PYTHON_PATH");
+
     if cfg!(windows) {
         uv_snapshot!(&mut command, @r###"
         success: false
         exit_code: 1
         ----- stdout -----
 
         ----- stderr -----
-          × No Python 3.15 found through `py --list-paths` or in `PATH`. Is Python 3.15 installed?
+          × No interpreter found for Python 3.100 in search path or `py` launcher output
         "###
         );
     } else {
         uv_snapshot!(&mut command, @r###"
         success: false
         exit_code: 1
         ----- stdout -----
 
         ----- stderr -----
-          × No Python 3.15 In `PATH`. Is Python 3.15 installed?
+          × No interpreter found for Python 3.100 in search path
         "###
         );
     }
 
-    venv.assert(predicates::path::missing());
-
-    Ok(())
+    context.venv.assert(predicates::path::missing());
 }
 
 #[test]
-fn create_venv_unknown_python_patch() -> Result<()> {
-    let temp_dir = assert_fs::TempDir::new()?;
-    let cache_dir = assert_fs::TempDir::new()?;
-    let bin = create_bin_with_executables(&temp_dir, &["3.12"]).expect("Failed to create bin dir");
-    let venv = temp_dir.child(".venv");
-
-    let filter_venv = regex::escape(&venv.normalized_display().to_string());
-    let filters = &[
-        (
-            r"Using Python 3\.\d+\.\d+ interpreter at .+",
-            "Using Python [VERSION] interpreter at [PATH]",
-        ),
-        (
-            r"No Python 3\.8\.0 found through `py --list-paths` or in `PATH`\. Is Python 3\.8\.0 installed\?",
-            "No Python 3.8.0 In `PATH`. Is Python 3.8.0 installed?",
-        ),
-        (&filter_venv, "/home/ferris/project/.venv"),
-    ];
-    uv_snapshot!(filters, Command::new(get_bin())
-        .arg("venv")
-        .arg(venv.as_os_str())
+fn create_venv_unknown_python_patch() {
+    let context = VenvTestContext::new(&["3.12"]);
+
+    let mut command = context.venv_command();
+    command
+        .arg(context.venv.as_os_str())
+        // Request a version we know we'll never see
         .arg("--python")
-        .arg("3.8.0")
-        .arg("--cache-dir")
-        .arg(cache_dir.path())
-        .arg("--exclude-newer")
-        .arg(EXCLUDE_NEWER)
-        .env("UV_NO_WRAP", "1")
-        .env("UV_TEST_PYTHON_PATH", bin)
-        .current_dir(&temp_dir), @r###"
-    success: false
-    exit_code: 1
-    ----- stdout -----
+        .arg("3.12.100")
+        // Unset this variable to force what the user would see
+        .env_remove("UV_TEST_PYTHON_PATH");
 
-    ----- stderr -----
-      × No Python 3.8.0 In `PATH`. Is Python 3.8.0 installed?
-    "###
-    );
+    if cfg!(windows) {
+        uv_snapshot!(&mut command, @r###"
+        success: false
+        exit_code: 1
+        ----- stdout -----
 
-    venv.assert(predicates::path::missing());
+        ----- stderr -----
+          × No interpreter found for Python 3.12.100 in search path or `py` launcher output
+        "###
+        );
+    } else {
+        uv_snapshot!(&mut command, @r###"
+        success: false
+        exit_code: 1
+        ----- stdout -----
 
-    Ok(())
+        ----- stderr -----
+          × No interpreter found for Python 3.12.100 in search path
+        "###
+        );
+    }
+
+    context.venv.assert(predicates::path::missing());
 }
 
+#[cfg(feature = "python-patch")]
 #[test]
-fn create_venv_python_patch() -> Result<()> {
-    let temp_dir = assert_fs::TempDir::new()?;
-    let cache_dir = assert_fs::TempDir::new()?;
-    let bin =
-        create_bin_with_executables(&temp_dir, &["3.12.1"]).expect("Failed to create bin dir");
-    let venv = temp_dir.child(".venv");
-
-    let filter_venv = regex::escape(&venv.normalized_display().to_string());
-    let filter_prompt = r"Activate with: (?:.*)\\Scripts\\activate";
-    let filters = &[
-        (r"interpreter at .+", "interpreter at [PATH]"),
-        (&filter_venv, "/home/ferris/project/.venv"),
-        (
-            &filter_prompt,
-            "Activate with: source /home/ferris/project/.venv/bin/activate",
-        ),
-    ];
-    uv_snapshot!(filters, Command::new(get_bin())
-        .arg("venv")
-        .arg(venv.as_os_str())
+fn create_venv_python_patch() {
+    let context = VenvTestContext::new(&["3.12.1"]);
+
+    uv_snapshot!(context.filters(), context.venv_command()
+        .arg(context.venv.as_os_str())
         .arg("--python")
-        .arg("3.12.1")
-        .arg("--cache-dir")
-        .arg(cache_dir.path())
-        .arg("--exclude-newer")
-        .arg(EXCLUDE_NEWER)
-        .env("UV_NO_WRAP", "1")
-        .env("UV_TEST_PYTHON_PATH", bin)
-        .current_dir(&temp_dir), @r###"
+        .arg("3.12.1"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    Using Python 3.12.1 interpreter at [PATH]
-    Creating virtualenv at: /home/ferris/project/.venv
-    Activate with: source /home/ferris/project/.venv/bin/activate
+    Using Python 3.12.1 interpreter at: [PATH]
+    Creating virtualenv at: .venv
+    Activate with: source .venv/bin/activate
     "###
     );
 
-    venv.assert(predicates::path::is_dir());
-
-    Ok(())
+    context.venv.assert(predicates::path::is_dir());
 }
 
 #[test]
 fn file_exists() -> Result<()> {
-    let temp_dir = assert_fs::TempDir::new()?;
-    let cache_dir = assert_fs::TempDir::new()?;
-    let bin = create_bin_with_executables(&temp_dir, &["3.12"]).expect("Failed to create bin dir");
-    let venv = temp_dir.child(".venv");
+    let context = VenvTestContext::new(&["3.12"]);
 
     // Create a file at `.venv`. Creating a virtualenv at the same path should fail.
-    venv.touch()?;
+    context.venv.touch()?;
 
-    let filter_venv = regex::escape(&venv.normalized_display().to_string());
-    let filters = &[
-        (
-            r"Using Python 3\.\d+\.\d+ interpreter at .+",
-            "Using Python [VERSION] interpreter at [PATH]",
-        ),
-        (&filter_venv, "/home/ferris/project/.venv"),
-    ];
-    uv_snapshot!(filters, Command::new(get_bin())
-        .arg("venv")
-        .arg(venv.as_os_str())
+    uv_snapshot!(context.filters(), context.venv_command()
+        .arg(context.venv.as_os_str())
         .arg("--python")
-        .arg("3.12")
-        .arg("--cache-dir")
-        .arg(cache_dir.path())
-        .arg("--exclude-newer")
-        .arg(EXCLUDE_NEWER)
-        .env("UV_NO_WRAP", "1")
-        .env("UV_TEST_PYTHON_PATH", bin)
-        .current_dir(&temp_dir), @r###"
+        .arg("3.12"), @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
-    Using Python [VERSION] interpreter at [PATH]
-    Creating virtualenv at: /home/ferris/project/.venv
+    Using Python 3.12.[X] interpreter at: [PATH]
+    Creating virtualenv at: .venv
     uv::venv::creation
 
       × Failed to create virtualenv
-      ╰─▶ File exists at `/home/ferris/project/.venv`
+      ╰─▶ File exists at `.venv`
     "###
     );
 
     Ok(())
 }
 
 #[test]
 fn empty_dir_exists() -> Result<()> {
-    let temp_dir = assert_fs::TempDir::new()?;
-    let cache_dir = assert_fs::TempDir::new()?;
-    let bin = create_bin_with_executables(&temp_dir, &["3.12"]).expect("Failed to create bin dir");
-    let venv = temp_dir.child(".venv");
+    let context = VenvTestContext::new(&["3.12"]);
 
     // Create an empty directory at `.venv`. Creating a virtualenv at the same path should succeed.
-    venv.create_dir_all()?;
-
-    let filter_venv = regex::escape(&venv.normalized_display().to_string());
-    let filter_prompt = r"Activate with: (?:.*)\\Scripts\\activate";
-    let filters = &[
-        (
-            r"Using Python 3\.\d+\.\d+ interpreter at .+",
-            "Using Python [VERSION] interpreter at [PATH]",
-        ),
-        (&filter_venv, "/home/ferris/project/.venv"),
-        (
-            &filter_prompt,
-            "Activate with: source /home/ferris/project/.venv/bin/activate",
-        ),
-    ];
-    uv_snapshot!(filters, Command::new(get_bin())
-        .arg("venv")
-        .arg(venv.as_os_str())
+    context.venv.create_dir_all()?;
+    uv_snapshot!(context.filters(), context.venv_command()
+        .arg(context.venv.as_os_str())
         .arg("--python")
-        .arg("3.12")
-        .arg("--cache-dir")
-        .arg(cache_dir.path())
-        .arg("--exclude-newer")
-        .arg(EXCLUDE_NEWER)
-        .env("UV_NO_WRAP", "1")
-        .env("UV_TEST_PYTHON_PATH", bin)
-        .current_dir(&temp_dir), @r###"
+        .arg("3.12"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
-    Using Python [VERSION] interpreter at [PATH]
-    Creating virtualenv at: /home/ferris/project/.venv
-    Activate with: source /home/ferris/project/.venv/bin/activate
+    Using Python 3.12.[X] interpreter at: [PATH]
+    Creating virtualenv at: .venv
+    Activate with: source .venv/bin/activate
     "###
     );
 
-    venv.assert(predicates::path::is_dir());
+    context.venv.assert(predicates::path::is_dir());
 
     Ok(())
 }
 
 #[test]
 fn non_empty_dir_exists() -> Result<()> {
-    let temp_dir = assert_fs::TempDir::new()?;
-    let cache_dir = assert_fs::TempDir::new()?;
-    let bin = create_bin_with_executables(&temp_dir, &["3.12"]).expect("Failed to create bin dir");
-    let venv = temp_dir.child(".venv");
+    let context = VenvTestContext::new(&["3.12"]);
 
     // Create a non-empty directory at `.venv`. Creating a virtualenv at the same path should fail.
-    venv.create_dir_all()?;
-    venv.child("file").touch()?;
+    context.venv.create_dir_all()?;
+    context.venv.child("file").touch()?;
 
-    let filter_venv = regex::escape(&venv.normalized_display().to_string());
-    let filters = &[
-        (
-            r"Using Python 3\.\d+\.\d+ interpreter at .+",
-            "Using Python [VERSION] interpreter at [PATH]",
-        ),
-        (&filter_venv, "/home/ferris/project/.venv"),
-    ];
-    uv_snapshot!(filters, Command::new(get_bin())
-        .arg("venv")
-        .arg(venv.as_os_str())
+    uv_snapshot!(context.filters(), context.venv_command()
+        .arg(context.venv.as_os_str())
         .arg("--python")
-        .arg("3.12")
-        .arg("--cache-dir")
-        .arg(cache_dir.path())
-        .arg("--exclude-newer")
-        .arg(EXCLUDE_NEWER)
-        .env("UV_NO_WRAP", "1")
-        .env("UV_TEST_PYTHON_PATH", bin)
-        .current_dir(&temp_dir), @r###"
+        .arg("3.12"), @r###"
     success: false
     exit_code: 1
     ----- stdout -----
 
     ----- stderr -----
-    Using Python [VERSION] interpreter at [PATH]
-    Creating virtualenv at: /home/ferris/project/.venv
+    Using Python 3.12.[X] interpreter at: [PATH]
+    Creating virtualenv at: .venv
     uv::venv::creation
 
       × Failed to create virtualenv
-      ╰─▶ The directory `/home/ferris/project/.venv` exists, but it's not a virtualenv
+      ╰─▶ The directory `.venv` exists, but it's not a virtualenv
+    "###
+    );
+
+    Ok(())
+}
+
+#[test]
+fn non_empty_dir_exists_allow_existing() -> Result<()> {
+    let context = VenvTestContext::new(&["3.12"]);
+
+    // Create a non-empty directory at `.venv`. Creating a virtualenv at the same path should
+    // succeed when `--allow-existing` is specified, but fail when it is not.
+    context.venv.create_dir_all()?;
+    context.venv.child("file").touch()?;
+
+    uv_snapshot!(context.filters(), context.venv_command()
+        .arg(context.venv.as_os_str())
+        .arg("--python")
+        .arg("3.12"), @r###"
+    success: false
+    exit_code: 1
+    ----- stdout -----
+
+    ----- stderr -----
+    Using Python 3.12.[X] interpreter at: [PATH]
+    Creating virtualenv at: .venv
+    uv::venv::creation
+
+      × Failed to create virtualenv
+      ╰─▶ The directory `.venv` exists, but it's not a virtualenv
+    "###
+    );
+
+    uv_snapshot!(context.filters(), context.venv_command()
+        .arg(context.venv.as_os_str())
+        .arg("--allow-existing")
+        .arg("--python")
+        .arg("3.12"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Using Python 3.12.[X] interpreter at: [PATH]
+    Creating virtualenv at: .venv
+    Activate with: source .venv/bin/activate
+    "###
+    );
+
+    // Running again should _also_ succeed, overwriting existing symlinks and respecting existing
+    // directories.
+    uv_snapshot!(context.filters(), context.venv_command()
+        .arg(context.venv.as_os_str())
+        .arg("--allow-existing")
+        .arg("--python")
+        .arg("3.12"), @r###"
+    success: true
+    exit_code: 0
+    ----- stdout -----
+
+    ----- stderr -----
+    Using Python 3.12.[X] interpreter at: [PATH]
+    Creating virtualenv at: .venv
+    Activate with: source .venv/bin/activate
     "###
     );
 
     Ok(())
 }
 
 #[test]
 #[cfg(windows)]
 fn windows_shims() -> Result<()> {
-    let temp_dir = assert_fs::TempDir::new()?;
-    let cache_dir = assert_fs::TempDir::new()?;
-    let bin =
-        create_bin_with_executables(&temp_dir, &["3.8", "3.9"]).expect("Failed to create bin dir");
-    let venv = temp_dir.child(".venv");
-    let shim_path = temp_dir.child("shim");
+    let context = VenvTestContext::new(&["3.9", "3.8"]);
+    let shim_path = context.temp_dir.child("shim");
 
-    let py38 = std::env::split_paths(&bin)
+    let py38 = std::env::split_paths(&context.python_path)
         .last()
-        .expect("create_bin_with_executables to set up the python versions");
+        .expect("python_path_with_versions to set up the python versions");
     // We want 3.8 and the first version should be 3.9.
     // Picking the last is necessary to prove that shims work because the python version selects
     // the python version from the first path segment by default, so we take the last to prove it's not
     // returning that version.
     assert!(py38.to_str().unwrap().contains("3.8"));
 
     // Write the shim script that forwards the arguments to the python3.8 installation.
-    std::fs::create_dir(&shim_path)?;
-    std::fs::write(
+    fs_err::create_dir(&shim_path)?;
+    fs_err::write(
         shim_path.child("python.bat"),
         format!("@echo off\r\n{}/python.exe %*", py38.display()),
     )?;
 
     // Create a virtual environment at `.venv`, passing the redundant `--clear` flag.
-    let filter_venv = regex::escape(&venv.normalized_display().to_string());
-    let filter_prompt = r"Activate with: (?:.*)\\Scripts\\activate";
-    let filters = &[
-        (
-            r"Using Python 3\.8.\d+ interpreter at .+",
-            "Using Python 3.8.x interpreter at [PATH]",
-        ),
-        (&filter_venv, "/home/ferris/project/.venv"),
-        (
-            &filter_prompt,
-            "Activate with: source /home/ferris/project/.venv/bin/activate",
-        ),
-    ];
-    uv_snapshot!(filters, Command::new(get_bin())
-        .arg("venv")
-        .arg(venv.as_os_str())
+    uv_snapshot!(context.filters(), context.venv_command()
+        .arg(context.venv.as_os_str())
         .arg("--clear")
-        .arg("--cache-dir")
-        .arg(cache_dir.path())
-        .arg("--exclude-newer")
-        .arg(EXCLUDE_NEWER)
-        .env("UV_TEST_PYTHON_PATH", format!("{};{}", shim_path.display(), bin.normalized_display()))
-        .current_dir(&temp_dir), @r###"
+        .env("UV_TEST_PYTHON_PATH", format!("{};{}", shim_path.display(), context.python_path.simplified_display())), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     warning: virtualenv's `--clear` has no effect (uv always clears the virtual environment).
-    Using Python 3.8.x interpreter at [PATH]
-    Creating virtualenv at: /home/ferris/project/.venv
-    Activate with: source /home/ferris/project/.venv/bin/activate
+    Using Python 3.8.[X] interpreter at: [PATH]
+    Creating virtualenv at: .venv
+    Activate with: source .venv/bin/activate
     "###
     );
 
-    venv.assert(predicates::path::is_dir());
+    context.venv.assert(predicates::path::is_dir());
 
     Ok(())
 }
 
 #[test]
-fn virtualenv_compatibility() -> Result<()> {
-    let temp_dir = assert_fs::TempDir::new()?;
-    let cache_dir = assert_fs::TempDir::new()?;
-    let bin = create_bin_with_executables(&temp_dir, &["3.12"]).expect("Failed to create bin dir");
-    let venv = temp_dir.child(".venv");
+fn virtualenv_compatibility() {
+    let context = VenvTestContext::new(&["3.12"]);
 
     // Create a virtual environment at `.venv`, passing the redundant `--clear` flag.
-    let filter_venv = regex::escape(&venv.normalized_display().to_string());
-    let filter_prompt = r"Activate with: (?:.*)\\Scripts\\activate";
-    let filters = &[
-        (
-            r"Using Python 3\.\d+\.\d+ interpreter at .+",
-            "Using Python [VERSION] interpreter at [PATH]",
-        ),
-        (&filter_venv, "/home/ferris/project/.venv"),
-        (
-            &filter_prompt,
-            "Activate with: source /home/ferris/project/.venv/bin/activate",
-        ),
-    ];
-    uv_snapshot!(filters, Command::new(get_bin())
-        .arg("venv")
-        .arg(venv.as_os_str())
+    uv_snapshot!(context.filters(), context.venv_command()
+        .arg(context.venv.as_os_str())
         .arg("--clear")
         .arg("--python")
-        .arg("3.12")
-        .arg("--cache-dir")
-        .arg(cache_dir.path())
-        .arg("--exclude-newer")
-        .arg(EXCLUDE_NEWER)
-        .env("UV_TEST_PYTHON_PATH", bin.clone())
-        .current_dir(&temp_dir), @r###"
+        .arg("3.12"), @r###"
     success: true
     exit_code: 0
     ----- stdout -----
 
     ----- stderr -----
     warning: virtualenv's `--clear` has no effect (uv always clears the virtual environment).
-    Using Python [VERSION] interpreter at [PATH]
-    Creating virtualenv at: /home/ferris/project/.venv
-    Activate with: source /home/ferris/project/.venv/bin/activate
+    Using Python 3.12.[X] interpreter at: [PATH]
+    Creating virtualenv at: .venv
+    Activate with: source .venv/bin/activate
     "###
     );
 
-    venv.assert(predicates::path::is_dir());
-
-    Ok(())
+    context.venv.assert(predicates::path::is_dir());
 }
 
 #[test]
 fn verify_pyvenv_cfg() {
     let context = TestContext::new("3.12");
     let venv = context.temp_dir.child(".venv");
     let pyvenv_cfg = venv.child("pyvenv.cfg");
@@ -659,7 +538,90 @@
     pyvenv_cfg.assert(predicates::path::is_file());
 
     // Check if "uv = version" is present in the file
     let version = env!("CARGO_PKG_VERSION").to_string();
     let search_string = format!("uv = {version}");
     pyvenv_cfg.assert(predicates::str::contains(search_string));
 }
+
+/// Ensure that a nested virtual environment uses the same `home` directory as the parent.
+#[test]
+fn verify_nested_pyvenv_cfg() -> Result<()> {
+    let context = VenvTestContext::new(&["3.12"]);
+
+    // Create a virtual environment at `.venv`.
+    context
+        .venv_command()
+        .arg(context.venv.as_os_str())
+        .arg("--python")
+        .arg("3.12")
+        .assert()
+        .success();
+
+    let pyvenv_cfg = context.venv.child("pyvenv.cfg");
+
+    // Check pyvenv.cfg exists
+    pyvenv_cfg.assert(predicates::path::is_file());
+
+    // Extract the "home" line from the pyvenv.cfg file.
+    let contents = fs_err::read_to_string(pyvenv_cfg.path())?;
+    let venv_home = contents
+        .lines()
+        .find(|line| line.starts_with("home"))
+        .expect("home line not found");
+
+    // Now, create a virtual environment from within the virtual environment.
+    let subvenv = context.temp_dir.child(".subvenv");
+    context
+        .venv_command()
+        .arg(subvenv.as_os_str())
+        .arg("--python")
+        .arg("3.12")
+        .env("VIRTUAL_ENV", context.venv.as_os_str())
+        .assert()
+        .success();
+
+    let sub_pyvenv_cfg = subvenv.child("pyvenv.cfg");
+
+    // Extract the "home" line from the pyvenv.cfg file.
+    let contents = fs_err::read_to_string(sub_pyvenv_cfg.path())?;
+    let sub_venv_home = contents
+        .lines()
+        .find(|line| line.starts_with("home"))
+        .expect("home line not found");
+
+    // Check that both directories point to the same home.
+    assert_eq!(sub_venv_home, venv_home);
+
+    Ok(())
+}
+
+/// See <https://github.com/astral-sh/uv/issues/3280>
+#[test]
+#[cfg(windows)]
+fn path_with_trailing_space_gives_proper_error() {
+    let context = VenvTestContext::new(&["3.12"]);
+
+    let mut filters = context.filters();
+    filters.push((
+        regex::escape(&context.cache_dir.path().display().to_string()).to_string(),
+        r"C:\Path\to\Cache\dir".to_string(),
+    ));
+    // Create a virtual environment at `.venv`.
+    uv_snapshot!(filters, Command::new(get_bin())
+        .arg("venv")
+        .arg(context.venv.as_os_str())
+        .arg("--python")
+        .arg("3.12")
+        .env("UV_CACHE_DIR", format!("{} ", context.cache_dir.path().display()))
+        .env("UV_TEST_PYTHON_PATH", context.python_path.clone())
+        .current_dir(context.temp_dir.path()), @r###"
+    success: false
+    exit_code: 2
+    ----- stdout -----
+
+    ----- stderr -----
+    error: failed to open file `C:\Path\to\Cache\dir \CACHEDIR.TAG`
+      Caused by: The system cannot find the path specified. (os error 3)
+    "###
+    );
+}
```

### Comparing `uv-0.1.9/Cargo.lock` & `uv-0.2.0/Cargo.lock`

 * *Files 9% similar despite different names*

```diff
@@ -26,17 +26,17 @@
  "getrandom",
  "once_cell",
  "version_check",
 ]
 
 [[package]]
 name = "aho-corasick"
-version = "1.1.2"
+version = "1.1.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "b2969dcb958b36655471fc61f7e416fa76033bdd4bfed0678d8fee1e2d07a1f0"
+checksum = "8e60d3430d3a69478ad0993f19238d2df97c507009a52b3c10addcd7f6bcb916"
 dependencies = [
  "memchr",
 ]
 
 [[package]]
 name = "alloc-no-stdlib"
 version = "2.0.4"
@@ -71,89 +71,100 @@
 name = "anes"
 version = "0.1.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "4b46cbb362ab8752921c97e041f5e366ee6297bd428a31275b9fcf1e380f7299"
 
 [[package]]
 name = "anstream"
-version = "0.6.11"
+version = "0.6.14"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "6e2e1ebcb11de5c03c67de28a7df593d32191b44939c482e97702baaaa6ab6a5"
+checksum = "418c75fa768af9c03be99d17643f93f79bbba589895012a80e3452a19ddda15b"
 dependencies = [
  "anstyle",
  "anstyle-parse",
  "anstyle-query",
  "anstyle-wincon",
  "colorchoice",
+ "is_terminal_polyfill",
  "utf8parse",
 ]
 
 [[package]]
 name = "anstyle"
-version = "1.0.6"
+version = "1.0.7"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "8901269c6307e8d93993578286ac0edf7f195079ffff5ebdeea6a59ffb7e36bc"
+checksum = "038dfcf04a5feb68e9c60b21c9625a54c2c0616e79b72b0fd87075a056ae1d1b"
 
 [[package]]
 name = "anstyle-parse"
-version = "0.2.3"
+version = "0.2.4"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "c75ac65da39e5fe5ab759307499ddad880d724eed2f6ce5b5e8a26f4f387928c"
+checksum = "c03a11a9034d92058ceb6ee011ce58af4a9bf61491aa7e1e59ecd24bd40d22d4"
 dependencies = [
  "utf8parse",
 ]
 
 [[package]]
 name = "anstyle-query"
-version = "1.0.2"
+version = "1.0.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "e28923312444cdd728e4738b3f9c9cac739500909bb3d3c94b43551b16517648"
+checksum = "a64c907d4e79225ac72e2a354c9ce84d50ebb4586dee56c82b3ee73004f537f5"
 dependencies = [
  "windows-sys 0.52.0",
 ]
 
 [[package]]
 name = "anstyle-wincon"
-version = "3.0.2"
+version = "3.0.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "1cd54b81ec8d6180e24654d0b371ad22fc3dd083b6ff8ba325b72e00c87660a7"
+checksum = "61a38449feb7068f52bb06c12759005cf459ee52bb4adc1d5a7c4322d716fb19"
 dependencies = [
  "anstyle",
  "windows-sys 0.52.0",
 ]
 
 [[package]]
 name = "anyhow"
-version = "1.0.79"
+version = "1.0.86"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "080e9890a082662b09c1ad45f567faeeb47f22b5fb23895fbe1e651e718e25ca"
+checksum = "b3d1d046238990b9cf5bcde22a3fb3584ee5cf65fb2765f454ed428c7a0063da"
 
 [[package]]
 name = "arc-swap"
-version = "1.6.0"
+version = "1.7.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "bddcadddf5e9015d310179a59bb28c4d4b9920ad0f11e8e14dbadf654890c9a6"
+checksum = "69f7f8c3906b62b754cd5326047894316021dcfe5a194c8ea52bdd94934a3457"
 
 [[package]]
 name = "arrayref"
 version = "0.3.7"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "6b4930d2cb77ce62f89ee5d5289b4ac049559b1c45539271f5ed4fdc7db34545"
 
 [[package]]
 name = "arrayvec"
 version = "0.7.4"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "96d30a06541fbafbc7f82ed10c06164cfbd2c401138f6addd8404629c4b16711"
 
 [[package]]
+name = "assert-json-diff"
+version = "2.0.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "47e4f2b81832e72834d7518d8487a0396a28cc408186a2e8854c0f98011faf12"
+dependencies = [
+ "serde",
+ "serde_json",
+]
+
+[[package]]
 name = "assert_cmd"
-version = "2.0.13"
+version = "2.0.14"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "00ad3f3a942eee60335ab4342358c161ee296829e0d16ff42fc1d6cb07815467"
+checksum = "ed72493ac66d5804837f480ab3766c72bdfab91a65e565fc54fa9e42db0073a8"
 dependencies = [
  "anstyle",
  "bstr",
  "doc-comment",
  "predicates",
  "predicates-core",
  "predicates-tree",
@@ -172,44 +183,59 @@
  "predicates",
  "predicates-core",
  "predicates-tree",
  "tempfile",
 ]
 
 [[package]]
+name = "async-channel"
+version = "2.3.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "89b47800b0be77592da0afd425cc03468052844aff33b84e33cc696f64e77b6a"
+dependencies = [
+ "concurrent-queue",
+ "event-listener-strategy",
+ "futures-core",
+ "pin-project-lite",
+]
+
+[[package]]
 name = "async-compression"
-version = "0.4.6"
+version = "0.4.10"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "a116f46a969224200a0a97f29cfd4c50e7534e4b4826bd23ea2c3c533039c82c"
+checksum = "9c90a406b4495d129f00461241616194cb8a032c8d1c53c657f0961d5f8e0498"
 dependencies = [
  "brotli",
+ "bzip2",
  "flate2",
  "futures-core",
  "futures-io",
  "memchr",
  "pin-project-lite",
  "tokio",
+ "zstd",
+ "zstd-safe",
 ]
 
 [[package]]
 name = "async-trait"
-version = "0.1.77"
+version = "0.1.80"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "c980ee35e870bd1a4d2c8294d4c04d0499e67bca1e4b5cefcc693c2fa00caea9"
+checksum = "c6fa2087f2753a7da8cc1c0dbfcf89579dd57458e36769de5ac750b4671737ca"
 dependencies = [
  "proc-macro2",
  "quote",
- "syn 2.0.48",
+ "syn 2.0.64",
 ]
 
 [[package]]
 name = "async_http_range_reader"
-version = "0.6.1"
+version = "0.8.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "5143aaae4ec035a5d7cfda666eab896fe5428a2a8ab09ca651a2dce3a8f06912"
+checksum = "f1a0e0571c5d724d17fbe0b608d31a91e94938722c141877d3a2982216b084c2"
 dependencies = [
  "bisection",
  "futures",
  "http-content-range",
  "itertools 0.12.1",
  "memmap2 0.9.4",
  "reqwest",
@@ -219,37 +245,114 @@
  "tokio-stream",
  "tokio-util",
  "tracing",
 ]
 
 [[package]]
 name = "async_zip"
-version = "0.0.16"
-source = "git+https://github.com/charliermarsh/rs-async-zip?rev=d76801da0943de985254fc6255c0e476b57c5836#d76801da0943de985254fc6255c0e476b57c5836"
+version = "0.0.17"
+source = "git+https://github.com/charliermarsh/rs-async-zip?rev=1dcb40cfe1bf5325a6fd4bfcf9894db40241f585#1dcb40cfe1bf5325a6fd4bfcf9894db40241f585"
 dependencies = [
  "async-compression",
  "crc32fast",
  "futures-lite",
  "pin-project",
  "thiserror",
  "tokio",
  "tokio-util",
 ]
 
 [[package]]
+name = "atomic-waker"
+version = "1.1.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "1505bd5d3d116872e7271a6d4e16d81d0c8570876c8de68093a09ac269d8aac0"
+
+[[package]]
 name = "autocfg"
-version = "1.1.0"
+version = "1.3.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "0c4b4d0bd25bd0b74681c0ad21497610ce1b7c91b1022cd21c80c6fbdd9476b0"
+
+[[package]]
+name = "axoasset"
+version = "0.9.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "d468802bab17cbc0cc575e9b053f41e72aa36bfa6b7f55e3529ffa43161b97fa"
+checksum = "6d492e2a60fbacf2154ee58fd4bc3dd7385a5febf10fef6145924fd3117cd920"
+dependencies = [
+ "camino",
+ "miette",
+ "mime",
+ "serde",
+ "serde_json",
+ "thiserror",
+ "url",
+ "walkdir",
+]
+
+[[package]]
+name = "axoprocess"
+version = "0.2.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "4de46920588aef95658797996130bacd542436aee090084646521260a74bda7d"
+dependencies = [
+ "miette",
+ "thiserror",
+ "tracing",
+]
+
+[[package]]
+name = "axotag"
+version = "0.2.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "d888fac0b73e64cbdf36a743fc5a25af5ae955c357535cb420b389bf1e1a6c54"
+dependencies = [
+ "miette",
+ "semver",
+ "thiserror",
+]
+
+[[package]]
+name = "axoupdater"
+version = "0.6.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "7d6bf8aaa32e7d33071ed9a339fc34ac30b0a5190f82069fbd12a1266bda9068"
+dependencies = [
+ "axoasset",
+ "axoprocess",
+ "axotag",
+ "camino",
+ "homedir",
+ "miette",
+ "reqwest",
+ "serde",
+ "temp-dir",
+ "thiserror",
+ "tokio",
+]
+
+[[package]]
+name = "backoff"
+version = "0.4.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "b62ddb9cb1ec0a098ad4bbf9344d0713fa193ae1a80af55febcff2627b6a00c1"
+dependencies = [
+ "futures-core",
+ "getrandom",
+ "instant",
+ "pin-project-lite",
+ "rand",
+ "tokio",
+]
 
 [[package]]
 name = "backtrace"
-version = "0.3.69"
+version = "0.3.71"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "2089b7e3f35b9dd2d0ed921ead4f6d318c27680d4a5bd167b3ee120edb105837"
+checksum = "26b05800d2e817c8b3b4b54abd461726265fa9789ae34330622f2db9ee696f9d"
 dependencies = [
  "addr2line",
  "cc",
  "cfg-if",
  "libc",
  "miniz_oxide",
  "object",
@@ -274,20 +377,39 @@
 [[package]]
 name = "base64"
 version = "0.21.7"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "9d297deb1925b89f2ccc13d7635fa0714f12c87adce1c75356b39ca9b7178567"
 
 [[package]]
+name = "base64"
+version = "0.22.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "72b3254f16251a8381aa12e40e3c4d2f0199f8c6508fbecb9d91f575e0fbb8c6"
+
+[[package]]
 name = "bench"
 version = "0.0.0"
 dependencies = [
+ "anyhow",
+ "codspeed-criterion-compat",
  "criterion",
  "distribution-filename",
+ "distribution-types",
+ "once_cell",
+ "pep508_rs",
  "platform-tags",
+ "tokio",
+ "uv-cache",
+ "uv-client",
+ "uv-configuration",
+ "uv-distribution",
+ "uv-interpreter",
+ "uv-resolver",
+ "uv-types",
 ]
 
 [[package]]
 name = "bisection"
 version = "0.1.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "021e079a1bab0ecce6cf4b4b74c0c37afa4a697136eb3b127875c84a8f04a8c3"
@@ -296,17 +418,17 @@
 name = "bitflags"
 version = "1.3.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "bef38d45163c2f1dde094a7dfd33ccf595c92905c8f8f4fdc18d06fb1037718a"
 
 [[package]]
 name = "bitflags"
-version = "2.4.2"
+version = "2.5.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "ed570934406eb16438a4e976b1b4500774099c13b8cb96eec99f620f05090ddf"
+checksum = "cf4b9d6a944f767f8e5e0db018570623c85f3d925ac718db4e06d0187adb21c1"
 
 [[package]]
 name = "bitvec"
 version = "1.0.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "1bc2832c24239b0141d5674bb9174f9d68a8b5b3f2753311927c172ca46f7e9c"
 dependencies = [
@@ -323,49 +445,49 @@
 checksum = "3078c7629b62d3f0439517fa394996acacc5cbc91c5a20d8c658e77abd503a71"
 dependencies = [
  "generic-array",
 ]
 
 [[package]]
 name = "brotli"
-version = "3.4.0"
+version = "6.0.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "516074a47ef4bce09577a3b379392300159ce5b1ba2e501ff1c819950066100f"
+checksum = "74f7971dbd9326d58187408ab83117d8ac1bb9c17b085fdacd1cf2f598719b6b"
 dependencies = [
  "alloc-no-stdlib",
  "alloc-stdlib",
  "brotli-decompressor",
 ]
 
 [[package]]
 name = "brotli-decompressor"
-version = "2.5.1"
+version = "4.0.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "4e2e4afe60d7dd600fdd3de8d0f08c2b7ec039712e3b6137ff98b7004e82de4f"
+checksum = "e6221fe77a248b9117d431ad93761222e1cf8ff282d9d1d5d9f53d6299a1cf76"
 dependencies = [
  "alloc-no-stdlib",
  "alloc-stdlib",
 ]
 
 [[package]]
 name = "bstr"
-version = "1.9.0"
+version = "1.9.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "c48f0051a4b4c5e0b6d365cd04af53aeaa209e3cc15ec2cdb69e73cc87fbd0dc"
+checksum = "05efc5cfd9110c8416e471df0e96702d58690178e206e61b7173706673c93706"
 dependencies = [
  "memchr",
- "regex-automata 0.4.5",
+ "regex-automata 0.4.6",
  "serde",
 ]
 
 [[package]]
 name = "bumpalo"
-version = "3.14.0"
+version = "3.16.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "7f30e7476521f6f8af1a1c4c0b8cc94f0bee37d91763d0ca2665f299b6cd8aec"
+checksum = "79296716171880943b8470b5f8d03aa55eb2e645a4874bdbb28adb49162e012c"
 
 [[package]]
 name = "bytecheck"
 version = "0.6.12"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "23cdc57ce23ac53c931e88a43d06d070a6fd142f2617be5855eb75efc9beb1c2"
 dependencies = [
@@ -383,29 +505,50 @@
  "proc-macro2",
  "quote",
  "syn 1.0.109",
 ]
 
 [[package]]
 name = "bytemuck"
-version = "1.14.3"
+version = "1.16.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "a2ef034f05691a48569bd920a96c81b9d91bbad1ab5ac7c4616c1f6ef36cb79f"
+checksum = "78834c15cb5d5efe3452d58b1e8ba890dd62d21907f867f383358198e56ebca5"
 
 [[package]]
 name = "byteorder"
 version = "1.5.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "1fd0f2584146f6f2ef48085050886acf353beff7305ebd1ae69500e27c67f64b"
 
 [[package]]
 name = "bytes"
-version = "1.5.0"
+version = "1.6.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "514de17de45fdb8dc022b1a7975556c53c86f9f0aa5f534b98977b171857c2c9"
+
+[[package]]
+name = "bzip2"
+version = "0.4.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "bdb116a6ef3f6c3698828873ad02c3014b3c85cadb88496095628e3ef1e347f8"
+dependencies = [
+ "bzip2-sys",
+ "libc",
+]
+
+[[package]]
+name = "bzip2-sys"
+version = "0.1.11+1.0.8"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "a2bd12c1caf447e69cd4528f47f94d203fd2582878ecb9e9465484c4148a8223"
+checksum = "736a955f3fa7875102d57c82b8cac37ec45224a07fd32d58f9f7a186b6cd4cdc"
+dependencies = [
+ "cc",
+ "libc",
+ "pkg-config",
+]
 
 [[package]]
 name = "cache-key"
 version = "0.0.1"
 dependencies = [
  "hex",
  "seahash",
@@ -419,26 +562,26 @@
 checksum = "4703f3937077db8fa35bee3c8789343c1aec2585f0146f09d658d4ccc0e8d873"
 dependencies = [
  "tempfile",
 ]
 
 [[package]]
 name = "camino"
-version = "1.1.6"
+version = "1.1.7"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "c59e92b5a388f549b863a7bea62612c09f24c8393560709a54558a9abdfb3b9c"
+checksum = "e0ec6b951b160caa93cc0c7b209e5a3bff7aae9062213451ac99493cd844c239"
 dependencies = [
  "serde",
 ]
 
 [[package]]
 name = "cargo-util"
-version = "0.2.9"
+version = "0.2.11"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "74862c3c6e53a1c1f8f0178f9d38ab41e49746cd3a7cafc239b3d0248fd4e342"
+checksum = "f6e977de2867ec90a1654882ff95ca5849a526e893bab588f84664cfcdb11c0a"
 dependencies = [
  "anyhow",
  "core-foundation",
  "filetime",
  "hex",
  "ignore",
  "jobserver",
@@ -457,51 +600,58 @@
 name = "cast"
 version = "0.3.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "37b2a672a2cb129a2e41c10b1224bb368f9f37a2b16b612598138befd7b37eb5"
 
 [[package]]
 name = "cc"
-version = "1.0.83"
+version = "1.0.97"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "f1174fb0b6ec23863f8b971027804a42614e347eafb0a95bf0b12cdae21fc4d0"
+checksum = "099a5357d84c4c61eb35fc8eafa9a79a902c2f76911e5747ced4e032edd8d9b4"
 dependencies = [
  "jobserver",
  "libc",
+ "once_cell",
 ]
 
 [[package]]
 name = "cfg-if"
 version = "1.0.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "baf1de4339761588bc0619e3cbc0120ee582ebb74b53b4efbf79117bd2da40fd"
 
 [[package]]
+name = "cfg_aliases"
+version = "0.1.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "fd16c4719339c4530435d38e511904438d07cce7950afa3718a84ac36c10e89e"
+
+[[package]]
 name = "charset"
 version = "0.1.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "18e9079d1a12a2cc2bffb5db039c43661836ead4082120d5844f02555aca2d46"
 dependencies = [
  "base64 0.13.1",
  "encoding_rs",
 ]
 
 [[package]]
 name = "chrono"
-version = "0.4.34"
+version = "0.4.38"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "5bc015644b92d5890fab7489e49d21f879d5c990186827d42ec511919404f38b"
+checksum = "a21f936df1771bf62b77f047b726c4625ff2e8aa607c01ec06e5a05bd8463401"
 dependencies = [
  "android-tzdata",
  "iana-time-zone",
  "js-sys",
  "num-traits",
  "serde",
  "wasm-bindgen",
- "windows-targets 0.52.0",
+ "windows-targets 0.52.5",
 ]
 
 [[package]]
 name = "ciborium"
 version = "0.2.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "42e69ffd6f0917f5c029256a24d0161db17cea3997d185db0d35926308770f0e"
@@ -525,39 +675,40 @@
 dependencies = [
  "ciborium-io",
  "half",
 ]
 
 [[package]]
 name = "clap"
-version = "4.5.0"
+version = "4.5.4"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "80c21025abd42669a92efc996ef13cfb2c5c627858421ea58d5c3b331a6c134f"
+checksum = "90bc066a67923782aa8515dbaea16946c5bcc5addbd668bb80af688e53e548a0"
 dependencies = [
  "clap_builder",
  "clap_derive",
 ]
 
 [[package]]
 name = "clap_builder"
-version = "4.5.0"
+version = "4.5.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "458bf1f341769dfcf849846f65dffdf9146daa56bcd2a47cb4e1de9915567c99"
+checksum = "ae129e2e766ae0ec03484e609954119f123cc1fe650337e155d03b022f24f7b4"
 dependencies = [
  "anstream",
  "anstyle",
  "clap_lex",
  "strsim",
+ "terminal_size",
 ]
 
 [[package]]
 name = "clap_complete"
-version = "4.5.1"
+version = "4.5.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "885e4d7d5af40bfb99ae6f9433e292feac98d452dcb3ec3d25dfe7552b77da8c"
+checksum = "dd79504325bf38b10165b02e89b4347300f855f273c4cb30c4a3209e6583275e"
 dependencies = [
  "clap",
 ]
 
 [[package]]
 name = "clap_complete_command"
 version = "0.5.1"
@@ -588,22 +739,22 @@
 dependencies = [
  "clap",
  "clap_complete",
 ]
 
 [[package]]
 name = "clap_derive"
-version = "4.5.0"
+version = "4.5.4"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "307bc0538d5f0f83b8248db3087aa92fe504e4691294d0c96c0eabc33f47ba47"
+checksum = "528131438037fd55894f62d6e9f068b8f45ac57ffa77517819645d10aed04f64"
 dependencies = [
- "heck",
+ "heck 0.5.0",
  "proc-macro2",
  "quote",
- "syn 2.0.48",
+ "syn 2.0.64",
 ]
 
 [[package]]
 name = "clap_lex"
 version = "0.7.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "98cc8fbded0c607b7ba9dd60cd98df59af97e84d24e49c8557331cfc26d301ce"
@@ -614,24 +765,65 @@
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "a31c789563b815f77f4250caee12365734369f942439b7defd71e18a48197130"
 dependencies = [
  "cc",
 ]
 
 [[package]]
+name = "codspeed"
+version = "2.6.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "3a104ac948e0188b921eb3fcbdd55dcf62e542df4c7ab7e660623f6288302089"
+dependencies = [
+ "colored",
+ "libc",
+ "serde_json",
+]
+
+[[package]]
+name = "codspeed-criterion-compat"
+version = "2.6.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "722c36bdc62d9436d027256ce2627af81ac7a596dfc7d13d849d0d212448d7fe"
+dependencies = [
+ "codspeed",
+ "colored",
+ "criterion",
+]
+
+[[package]]
 name = "color_quant"
 version = "1.1.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "3d7b894f5411737b7867f4827955924d7c254fc9f4d91a6aad6b097804b1018b"
 
 [[package]]
 name = "colorchoice"
-version = "1.0.0"
+version = "1.0.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "acbf1af155f9b9ef647e42cdc158db4b64a1b61f743629225fde6f3e0be2a7c7"
+checksum = "0b6a852b24ab71dffc585bcb46eaf7959d175cb865a7152e35b348d1b2960422"
+
+[[package]]
+name = "colored"
+version = "2.1.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "cbf2150cce219b664a8a70df7a1f933836724b503f8a413af9365b4dcc4d90b8"
+dependencies = [
+ "lazy_static",
+ "windows-sys 0.48.0",
+]
+
+[[package]]
+name = "concurrent-queue"
+version = "2.5.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "4ca0197aee26d1ae37445ee532fefce43251d24cc7c166799f4d46817f1d3973"
+dependencies = [
+ "crossbeam-utils",
+]
 
 [[package]]
 name = "configparser"
 version = "3.0.4"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "4ec6d3da8e550377a85339063af6e3735f4b1d9392108da4e083a1b3b9820288"
 
@@ -689,24 +881,26 @@
 checksum = "f2b12d017a929603d80db1831cd3a24082f8137ce19c69e6447f54f5fc8d692f"
 dependencies = [
  "anes",
  "cast",
  "ciborium",
  "clap",
  "criterion-plot",
+ "futures",
  "is-terminal",
  "itertools 0.10.5",
  "num-traits",
  "once_cell",
  "oorandom",
  "regex",
  "serde",
  "serde_derive",
  "serde_json",
  "tinytemplate",
+ "tokio",
  "walkdir",
 ]
 
 [[package]]
 name = "criterion-plot"
 version = "0.5.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
@@ -776,68 +970,83 @@
 checksum = "5efa2b3d7902f4b634a20cae3c9c4e6209dc4779feb6863329607560143efa70"
 dependencies = [
  "memchr",
 ]
 
 [[package]]
 name = "ctrlc"
-version = "3.4.2"
+version = "3.4.4"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "b467862cc8610ca6fc9a1532d7777cee0804e678ab45410897b9396495994a0b"
+checksum = "672465ae37dc1bc6380a6547a8883d5dd397b0f1faaad4f265726cc7042a5345"
 dependencies = [
- "nix",
+ "nix 0.28.0",
  "windows-sys 0.52.0",
 ]
 
 [[package]]
 name = "dashmap"
 version = "5.5.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "978747c1d849a7d2ee5e8adc0159961c48fb7e5db2f06af6723b80123bb53856"
 dependencies = [
  "cfg-if",
- "hashbrown 0.14.3",
+ "hashbrown 0.14.5",
  "lock_api",
  "once_cell",
- "parking_lot_core 0.9.9",
+ "parking_lot_core 0.9.10",
 ]
 
 [[package]]
 name = "data-encoding"
-version = "2.5.0"
+version = "2.6.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "7e962a19be5cfc3f3bf6dd8f61eb50107f356ad6270fbb3ed41476571db78be5"
+checksum = "e8566979429cf69b49a5c740c60791108e86440e8be149bbea4fe54d2c32d6e2"
 
 [[package]]
 name = "data-url"
 version = "0.2.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "8d7439c3735f405729d52c3fbbe4de140eaf938a1fe47d227c27f8254d4302a5"
 
 [[package]]
-name = "deranged"
-version = "0.3.11"
+name = "deadpool"
+version = "0.10.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "b42b6fa04a440b495c8b04d0e71b707c585f83cb9cb28cf8cd0d976c315e31b4"
+checksum = "fb84100978c1c7b37f09ed3ce3e5f843af02c2a2c431bae5b19230dad2c1b490"
 dependencies = [
- "powerfmt",
+ "async-trait",
+ "deadpool-runtime",
+ "num_cpus",
+ "tokio",
 ]
 
 [[package]]
+name = "deadpool-runtime"
+version = "0.1.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "092966b41edc516079bdf31ec78a2e0588d1d0c08f78b91d8307215928642b2b"
+
+[[package]]
 name = "derivative"
 version = "2.2.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "fcc3dd5e9e9c0b295d6e1e4d811fb6f157d5ffd784b8d202fc62eac8035a770b"
 dependencies = [
  "proc-macro2",
  "quote",
  "syn 1.0.109",
 ]
 
 [[package]]
+name = "diff"
+version = "0.1.13"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "56254986775e3233ffa9c4d7d3faaf6d36a2c09d30b20687e9f88bc8bafc16c8"
+
+[[package]]
 name = "difflib"
 version = "0.4.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "6184e33543162437515c2e2b48714794e37845ec9851711914eec9d308f6ebe8"
 
 [[package]]
 name = "digest"
@@ -887,33 +1096,33 @@
 
 [[package]]
 name = "distribution-types"
 version = "0.0.1"
 dependencies = [
  "anyhow",
  "cache-key",
- "data-encoding",
  "distribution-filename",
  "fs-err",
- "itertools 0.12.1",
+ "git2",
+ "indexmap",
+ "itertools 0.13.0",
  "once_cell",
  "pep440_rs",
  "pep508_rs",
  "platform-tags",
  "pypi-types",
  "rkyv",
  "rustc-hash",
+ "schemars",
  "serde",
  "serde_json",
- "sha2",
  "thiserror",
  "tracing",
  "url",
  "urlencoding",
- "uv-auth",
  "uv-fs",
  "uv-git",
  "uv-normalize",
 ]
 
 [[package]]
 name = "doc-comment"
@@ -924,55 +1133,91 @@
 [[package]]
 name = "dunce"
 version = "1.0.4"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "56ce8c6da7551ec6c462cbaf3bfbc75131ebbfa1c944aeaa9dab51ca1c5f0c3b"
 
 [[package]]
+name = "dyn-clone"
+version = "1.0.17"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "0d6ef0072f8a535281e4876be788938b528e9a1d43900b82c2569af7da799125"
+
+[[package]]
 name = "either"
-version = "1.10.0"
+version = "1.12.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "11157ac094ffbdde99aa67b23417ebdd801842852b500e395a45a9c0aac03e4a"
+checksum = "3dca9240753cf90908d7e4aac30f630662b02aebaa1b58a3cadabdb23385b58b"
 
 [[package]]
 name = "encode_unicode"
 version = "0.3.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "a357d28ed41a50f9c765dbfe56cbc04a64e53e5fc58ba79fbc34c10ef3df831f"
 
 [[package]]
 name = "encoding_rs"
-version = "0.8.33"
+version = "0.8.34"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "7268b386296a025e474d5140678f75d6de9493ae55a5d709eeb9dd08149945e1"
+checksum = "b45de904aa0b010bce2ab45264d0631681847fa7b6f2eaa7dab7619943bc4f59"
 dependencies = [
  "cfg-if",
 ]
 
 [[package]]
+name = "encoding_rs_io"
+version = "0.1.7"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "1cc3c5651fb62ab8aa3103998dade57efdd028544bd300516baa31840c252a83"
+dependencies = [
+ "encoding_rs",
+]
+
+[[package]]
 name = "equivalent"
 version = "1.0.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "5443807d6dff69373d433ab9ef5378ad8df50ca6298caf15de6e52e24aaf54d5"
 
 [[package]]
 name = "errno"
-version = "0.3.8"
+version = "0.3.9"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "a258e46cdc063eb8519c00b9fc845fc47bcfca4130e2f08e88665ceda8474245"
+checksum = "534c5cf6194dfab3db3242765c03bbe257cf92f22b38f6bc0c58d59108a820ba"
 dependencies = [
  "libc",
  "windows-sys 0.52.0",
 ]
 
 [[package]]
+name = "event-listener"
+version = "5.3.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "6d9944b8ca13534cdfb2800775f8dd4902ff3fc75a50101466decadfdf322a24"
+dependencies = [
+ "concurrent-queue",
+ "parking",
+ "pin-project-lite",
+]
+
+[[package]]
+name = "event-listener-strategy"
+version = "0.5.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "0f214dc438f977e6d4e3500aaa277f5ad94ca83fbbd9b1a15713ce2344ccc5a1"
+dependencies = [
+ "event-listener",
+ "pin-project-lite",
+]
+
+[[package]]
 name = "fastrand"
-version = "2.0.1"
+version = "2.1.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "25cbce373ec4653f1a01a31e8a5e5ec0c622dc27ff9c4e6606eefef5cbbed4a5"
+checksum = "9fc0510504f03c51ada170672ac806f1f105a88aa97a5281117e1ddc3368e51a"
 
 [[package]]
 name = "fdeflate"
 version = "0.3.4"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "4f9bfee30e4dedf0ab8b422f03af778d9612b63f502710fc500a334ebe2de645"
 dependencies = [
@@ -995,17 +1240,17 @@
 name = "fixedbitset"
 version = "0.4.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "0ce7134b9999ecaf8bcd65542e436736ef32ddca1b3e06094cb6ec5755203b80"
 
 [[package]]
 name = "flate2"
-version = "1.0.28"
+version = "1.0.30"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "46303f565772937ffe1d394a4fac6f411c6013172fadde9dcdb1e147a086940e"
+checksum = "5f54427cfd1c7829e2a139fcefea601bf088ebca651d2bf53ebc600eac295dae"
 dependencies = [
  "crc32fast",
  "libz-ng-sys",
  "miniz_oxide",
 ]
 
 [[package]]
@@ -1125,17 +1370,17 @@
 name = "futures-io"
 version = "0.3.30"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "a44623e20b9681a318efdd71c299b6b222ed6f231972bfe2f224ebad6311f0c1"
 
 [[package]]
 name = "futures-lite"
-version = "2.2.0"
+version = "2.3.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "445ba825b27408685aaecefd65178908c36c6e96aaf6d8599419d46e624192ba"
+checksum = "52527eb5074e35e9339c6b4e8d12600c7128b68fb25dcb9fa9dec18f7c25f3a5"
 dependencies = [
  "fastrand",
  "futures-core",
  "futures-io",
  "parking",
  "pin-project-lite",
 ]
@@ -1144,15 +1389,15 @@
 name = "futures-macro"
 version = "0.3.30"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "87750cf4b7a4c0625b1529e4c543c2182106e4dedc60a2a6455e00d212c489ac"
 dependencies = [
  "proc-macro2",
  "quote",
- "syn 2.0.48",
+ "syn 2.0.64",
 ]
 
 [[package]]
 name = "futures-sink"
 version = "0.3.30"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "9fb8e00e87438d937621c1c6269e53f536c14d3fbd6a042bb24879e57d474fb5"
@@ -1189,17 +1434,17 @@
 dependencies = [
  "typenum",
  "version_check",
 ]
 
 [[package]]
 name = "getrandom"
-version = "0.2.12"
+version = "0.2.14"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "190092ea657667030ac6a35e305e62fc4dd69fd98ac98631e5d3a2b1575a12b5"
+checksum = "94b22e06ecb0110981051723910cbf0b5f5e09a2062dd7663334ee79a9d1286c"
 dependencies = [
  "cfg-if",
  "js-sys",
  "libc",
  "wasi",
  "wasm-bindgen",
 ]
@@ -1218,19 +1463,19 @@
 name = "gimli"
 version = "0.28.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "4271d37baee1b8c7e4b708028c57d816cf9d2434acb33a549475f78c181f6253"
 
 [[package]]
 name = "git2"
-version = "0.18.2"
+version = "0.18.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "1b3ba52851e73b46a4c3df1d89343741112003f0f6f13beb0dfac9e457c3fdcd"
+checksum = "232e6a7bfe35766bf715e55a88b39a700596c0ccfd88cd3680b4cdb40d66ef70"
 dependencies = [
- "bitflags 2.4.2",
+ "bitflags 2.5.0",
  "libc",
  "libgit2-sys",
  "log",
  "openssl-probe",
  "openssl-sys",
  "url",
 ]
@@ -1246,87 +1491,53 @@
 version = "0.4.14"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "57da3b9b5b85bd66f31093f8c408b90a74431672542466497dcbdfdc02034be1"
 dependencies = [
  "aho-corasick",
  "bstr",
  "log",
- "regex-automata 0.4.5",
- "regex-syntax 0.8.2",
+ "regex-automata 0.4.6",
+ "regex-syntax 0.8.3",
 ]
 
 [[package]]
 name = "globwalk"
 version = "0.9.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "0bf760ebf69878d9fd8f110c89703d90ce35095324d1f1edcb595c63945ee757"
 dependencies = [
- "bitflags 2.4.2",
+ "bitflags 2.5.0",
  "ignore",
  "walkdir",
 ]
 
 [[package]]
-name = "goblin"
-version = "0.8.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "bb07a4ffed2093b118a525b1d8f5204ae274faed5604537caf7135d0f18d9887"
-dependencies = [
- "log",
- "plain",
- "scroll",
-]
-
-[[package]]
-name = "gourgeist"
-version = "0.0.4"
-dependencies = [
- "anstream",
- "cachedir",
- "camino",
- "clap",
- "directories",
- "fs-err",
- "platform-host",
- "serde",
- "serde_json",
- "tempfile",
- "thiserror",
- "tracing",
- "tracing-subscriber",
- "uv-cache",
- "uv-fs",
- "uv-interpreter",
- "which",
-]
-
-[[package]]
 name = "h2"
-version = "0.3.24"
+version = "0.4.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "bb2c4422095b67ee78da96fbb51a4cc413b3b25883c7717ff7ca1ab31022c9c9"
+checksum = "fa82e28a107a8cc405f0839610bdc9b15f1e25ec7d696aa5cf173edbcb1486ab"
 dependencies = [
+ "atomic-waker",
  "bytes",
  "fnv",
  "futures-core",
  "futures-sink",
- "futures-util",
  "http",
- "indexmap 2.2.3",
+ "indexmap",
  "slab",
  "tokio",
  "tokio-util",
  "tracing",
 ]
 
 [[package]]
 name = "half"
-version = "2.3.1"
+version = "2.4.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "bc52e53916c08643f1b56ec082790d1e86a32e58dc5268f897f313fbae7b4872"
+checksum = "6dd08c532ae367adf81c312a4580bc67f1d0fe8bc9c460520283f4c0ff277888"
 dependencies = [
  "cfg-if",
  "crunchy",
 ]
 
 [[package]]
 name = "hashbrown"
@@ -1335,29 +1546,35 @@
 checksum = "8a9ee70c43aaf417c914396645a0fa852624801b24ebb7ae78fe8272889ac888"
 dependencies = [
  "ahash",
 ]
 
 [[package]]
 name = "hashbrown"
-version = "0.14.3"
+version = "0.14.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "290f1a1d9242c78d09ce40a5e87e7554ee637af1351968159f4952f028f75604"
+checksum = "e5274423e17b7c9fc20b6e7e208532f9b19825d82dfd615708b70edd83df41f1"
 
 [[package]]
 name = "heck"
 version = "0.4.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "95505c38b4572b2d910cecb0281560f54b440a19336cbbcb27bf6ce6adc6f5a8"
 
 [[package]]
+name = "heck"
+version = "0.5.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "2304e00983f87ffb38b55b444b5e3b60a884b5d30c0fca7d82fe33449bbe55ea"
+
+[[package]]
 name = "hermit-abi"
-version = "0.3.6"
+version = "0.3.9"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "bd5256b483761cd23699d0da46cc6fd2ee3be420bbe6d020ae4a091e70b7e9fd"
+checksum = "d231dfb89cfffdbc30e7fc41579ed6066ad03abda9e567ccafae602b97ec5024"
 
 [[package]]
 name = "hex"
 version = "0.4.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "7f24254aa9a54b5c858eaee2f5bccdb46aaf0e486a595ed5fd8f86ba55232a70"
 
@@ -1376,41 +1593,67 @@
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "e3d1354bf6b7235cb4a0576c2619fd4ed18183f689b12b006a0ee7329eeff9a5"
 dependencies = [
  "windows-sys 0.52.0",
 ]
 
 [[package]]
+name = "homedir"
+version = "0.2.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "22074da8bba2ef26fc1737ae6c777b5baab5524c2dc403b5c6a76166766ccda5"
+dependencies = [
+ "cfg-if",
+ "nix 0.26.4",
+ "serde",
+ "widestring",
+ "windows-sys 0.48.0",
+ "wmi",
+]
+
+[[package]]
 name = "html-escape"
 version = "0.2.13"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "6d1ad449764d627e22bfd7cd5e8868264fc9236e07c752972b4080cd351cb476"
 dependencies = [
  "utf8-width",
 ]
 
 [[package]]
 name = "http"
-version = "0.2.11"
+version = "1.1.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "8947b1a6fad4393052c7ba1f4cd97bed3e953a95c79c92ad9b051a04611d9fbb"
+checksum = "21b9ddb458710bc376481b842f5da65cdf31522de232c1ca8146abce2a358258"
 dependencies = [
  "bytes",
  "fnv",
  "itoa",
 ]
 
 [[package]]
 name = "http-body"
-version = "0.4.6"
+version = "1.0.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "7ceab25649e9960c0311ea418d17bee82c0dcec1bd053b5f9a66e265a693bed2"
+checksum = "1cac85db508abc24a2e48553ba12a996e87244a0395ce011e62b37158745d643"
 dependencies = [
  "bytes",
  "http",
+]
+
+[[package]]
+name = "http-body-util"
+version = "0.1.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "0475f8b2ac86659c21b64320d5d653f9efe42acd2a4e560073ec61a155a34f1d"
+dependencies = [
+ "bytes",
+ "futures-core",
+ "http",
+ "http-body",
  "pin-project-lite",
 ]
 
 [[package]]
 name = "http-content-range"
 version = "0.1.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
@@ -1426,62 +1669,82 @@
 name = "httpdate"
 version = "1.0.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "df3b46402a9d5adb4c86a0cf463f42e19994e3ee891101b1841f30a545cb49a9"
 
 [[package]]
 name = "hyper"
-version = "0.14.28"
+version = "1.3.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "bf96e135eb83a2a8ddf766e426a841d8ddd7449d5f00d34ea02b41d2f19eef80"
+checksum = "fe575dd17d0862a9a33781c8c4696a55c320909004a67a00fb286ba8b1bc496d"
 dependencies = [
  "bytes",
  "futures-channel",
- "futures-core",
  "futures-util",
  "h2",
  "http",
  "http-body",
  "httparse",
  "httpdate",
  "itoa",
  "pin-project-lite",
- "socket2",
+ "smallvec",
  "tokio",
- "tower-service",
- "tracing",
  "want",
 ]
 
 [[package]]
 name = "hyper-rustls"
-version = "0.24.2"
+version = "0.26.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "ec3efd23720e2049821a693cbc7e65ea87c72f1c58ff2f9522ff332b1491e590"
+checksum = "a0bea761b46ae2b24eb4aef630d8d1c398157b6fc29e6350ecf090a0b70c952c"
 dependencies = [
  "futures-util",
  "http",
  "hyper",
+ "hyper-util",
  "rustls",
+ "rustls-pki-types",
  "tokio",
  "tokio-rustls",
+ "tower-service",
+]
+
+[[package]]
+name = "hyper-util"
+version = "0.1.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "ca38ef113da30126bbff9cd1705f9273e15d45498615d138b0c20279ac7a76aa"
+dependencies = [
+ "bytes",
+ "futures-channel",
+ "futures-util",
+ "http",
+ "http-body",
+ "hyper",
+ "pin-project-lite",
+ "socket2",
+ "tokio",
+ "tower",
+ "tower-service",
+ "tracing",
 ]
 
 [[package]]
 name = "iana-time-zone"
 version = "0.1.60"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "e7ffbb5a1b541ea2561f8c41c087286cc091e21e556a4f09a8f6cbf17b69b141"
 dependencies = [
  "android_system_properties",
  "core-foundation-sys",
  "iana-time-zone-haiku",
  "js-sys",
  "wasm-bindgen",
- "windows-core",
+ "windows-core 0.52.0",
 ]
 
 [[package]]
 name = "iana-time-zone-haiku"
 version = "0.1.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "f31827a206f56af32e590ba56d5d2d085f558508192593743f16b2306495269f"
@@ -1505,44 +1768,34 @@
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "b46810df39e66e925525d6e38ce1e7f6e1d208f72dc39757880fcb66e2c58af1"
 dependencies = [
  "crossbeam-deque",
  "globset",
  "log",
  "memchr",
- "regex-automata 0.4.5",
+ "regex-automata 0.4.6",
  "same-file",
  "walkdir",
  "winapi-util",
 ]
 
 [[package]]
 name = "imagesize"
 version = "0.11.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "b72ad49b554c1728b1e83254a1b1565aea4161e28dabbfa171fc15fe62299caf"
 
 [[package]]
 name = "indexmap"
-version = "1.9.3"
+version = "2.2.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "bd070e393353796e801d209ad339e89596eb4c8d430d18ede6a1cced8fafbd99"
-dependencies = [
- "autocfg",
- "hashbrown 0.12.3",
-]
-
-[[package]]
-name = "indexmap"
-version = "2.2.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "233cf39063f058ea2caae4091bf4a3ef70a653afbc026f5c4a4135d114e3c177"
+checksum = "168fb715dda47215e360912c096649d23d58bf392ac62f73919e831745e40f26"
 dependencies = [
  "equivalent",
- "hashbrown 0.14.3",
+ "hashbrown 0.14.5",
  "serde",
 ]
 
 [[package]]
 name = "indicatif"
 version = "0.17.8"
 source = "registry+https://github.com/rust-lang/crates.io-index"
@@ -1554,66 +1807,62 @@
  "portable-atomic",
  "unicode-width",
  "vt100",
 ]
 
 [[package]]
 name = "indoc"
-version = "2.0.4"
+version = "2.0.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "1e186cfbae8084e513daff4240b4797e342f988cecda4fb6c939150f96315fd8"
+checksum = "b248f5224d1d606005e02c97f5aa4e88eeb230488bcc03bc9ca4d7991399f2b5"
 
 [[package]]
 name = "insta"
-version = "1.34.0"
+version = "1.39.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "5d64600be34b2fcfc267740a243fa7744441bb4947a619ac4e5bb6507f35fbfc"
+checksum = "810ae6042d48e2c9e9215043563a58a80b877bc863228a74cf10c49d4620a6f5"
 dependencies = [
  "console",
  "lazy_static",
  "linked-hash-map",
+ "pest",
+ "pest_derive",
  "regex",
+ "serde",
  "similar",
- "yaml-rust",
 ]
 
 [[package]]
 name = "install-wheel-rs"
 version = "0.0.1"
 dependencies = [
  "clap",
  "configparser",
  "csv",
  "data-encoding",
  "distribution-filename",
  "fs-err",
- "fs2",
- "goblin",
  "indoc",
  "mailparse",
  "once_cell",
+ "pathdiff",
  "pep440_rs",
- "platform-host",
  "platform-info",
- "plist",
- "pyo3",
+ "platform-tags",
  "pypi-types",
- "rayon",
  "reflink-copy",
  "regex",
  "rustc-hash",
+ "schemars",
  "serde",
  "serde_json",
  "sha2",
- "target-lexicon",
  "tempfile",
  "thiserror",
  "tracing",
- "tracing-subscriber",
- "url",
  "uv-fs",
  "uv-normalize",
  "walkdir",
  "zip",
 ]
 
 [[package]]
@@ -1648,14 +1897,20 @@
 [[package]]
 name = "is_ci"
 version = "1.2.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "7655c9839580ee829dfacba1d1278c2b7883e50a277ff7541299489d6bdfdc45"
 
 [[package]]
+name = "is_terminal_polyfill"
+version = "1.70.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "f8478577c03552c21db0e2724ffb8986a5ce7af88107e6be5d2ee6e158c12800"
+
+[[package]]
 name = "itertools"
 version = "0.10.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "b0fd2260e829bddf4cb6ea802289de2f86d6a7a690192fbe91b3f46e0f2c8473"
 dependencies = [
  "either",
 ]
@@ -1666,51 +1921,60 @@
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "ba291022dbbd398a455acf126c1e341954079855bc60dfdda641363bd6922569"
 dependencies = [
  "either",
 ]
 
 [[package]]
+name = "itertools"
+version = "0.13.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "413ee7dfc52ee1a4949ceeb7dbc8a33f2d6c088194d9f922fb8318faf1f01186"
+dependencies = [
+ "either",
+]
+
+[[package]]
 name = "itoa"
-version = "1.0.10"
+version = "1.0.11"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "b1a46d1a171d865aa5f83f92695765caa047a9b4cbae2cbf37dbd613a793fd4c"
+checksum = "49f1f14873335454500d59611f1cf4a4b0f786f9ac11f4312a78e4cf2566695b"
 
 [[package]]
 name = "jobserver"
-version = "0.1.28"
+version = "0.1.31"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "ab46a6e9526ddef3ae7f787c06f0f2600639ba80ea3eade3d8e670a2230f51d6"
+checksum = "d2b099aaa34a9751c5bf0878add70444e1ed2dd73f347be99003d4577277de6e"
 dependencies = [
  "libc",
 ]
 
 [[package]]
 name = "jpeg-decoder"
 version = "0.3.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "f5d4a7da358eff58addd2877a45865158f0d78c911d43a5784ceb7bbf52833b0"
 
 [[package]]
 name = "js-sys"
-version = "0.3.68"
+version = "0.3.69"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "406cda4b368d531c842222cf9d2600a9a4acce8d29423695379c6868a143a9ee"
+checksum = "29c15563dc2726973df627357ce0c9ddddbea194836909d655df6a75d2cf296d"
 dependencies = [
  "wasm-bindgen",
 ]
 
 [[package]]
 name = "junction"
-version = "1.0.0"
+version = "1.1.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "ca39ef0d69b18e6a2fd14c2f0a1d593200f4a4ed949b240b5917ab51fac754cb"
+checksum = "1c9c415a9b7b1e86cd5738f39d34c9e78c765da7fb1756dbd7d31b3b0d2e7afa"
 dependencies = [
  "scopeguard",
- "winapi",
+ "windows-sys 0.52.0",
 ]
 
 [[package]]
 name = "kurbo"
 version = "0.8.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "7a53776d271cfb873b17c618af0298445c88afc52837f3e948fa3fafd131f449"
@@ -1751,31 +2015,30 @@
  "libz-sys",
  "openssl-sys",
  "pkg-config",
 ]
 
 [[package]]
 name = "libmimalloc-sys"
-version = "0.1.35"
+version = "0.1.37"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "3979b5c37ece694f1f5e51e7ecc871fdb0f517ed04ee45f88d15d6d553cb9664"
+checksum = "81eb4061c0582dedea1cbc7aff2240300dd6982e0239d1c99e65c1dbf4a30ba7"
 dependencies = [
  "cc",
  "libc",
 ]
 
 [[package]]
 name = "libredox"
-version = "0.0.1"
+version = "0.1.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "85c833ca1e66078851dba29046874e38f08b2c883700aa29a03ddd3b23814ee8"
+checksum = "c0ff37bd590ca25063e35af745c343cb7a0271906fb7b37e4813e8f79f00268d"
 dependencies = [
- "bitflags 2.4.2",
+ "bitflags 2.5.0",
  "libc",
- "redox_syscall 0.4.1",
 ]
 
 [[package]]
 name = "libssh2-sys"
 version = "0.3.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "2dc8a030b787e2119a731f1951d6a773e2280c660f8ec4b0f5e1505a386e71ee"
@@ -1796,66 +2059,57 @@
 dependencies = [
  "cmake",
  "libc",
 ]
 
 [[package]]
 name = "libz-sys"
-version = "1.1.15"
+version = "1.1.16"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "037731f5d3aaa87a5675e895b63ddff1a87624bc29f77004ea829809654e48f6"
+checksum = "5e143b5e666b2695d28f6bca6497720813f699c9602dd7f5cac91008b8ada7f9"
 dependencies = [
  "cc",
  "libc",
  "pkg-config",
  "vcpkg",
 ]
 
 [[package]]
-name = "line-wrap"
-version = "0.1.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "f30344350a2a51da54c1d53be93fade8a237e545dbcc4bdbe635413f2117cab9"
-dependencies = [
- "safemem",
-]
-
-[[package]]
 name = "linked-hash-map"
 version = "0.5.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "0717cef1bc8b636c6e1c1bbdefc09e6322da8a9321966e8928ef80d20f7f770f"
 
 [[package]]
 name = "linux-raw-sys"
-version = "0.4.13"
+version = "0.4.14"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "01cda141df6706de531b6c46c3a33ecca755538219bd484262fa09410c13539c"
+checksum = "78b3ae25bc7c8c38cec158d1f2757ee79e9b3740fbc7ccf0e59e4b08d793fa89"
 
 [[package]]
 name = "lock_api"
-version = "0.4.11"
+version = "0.4.12"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "3c168f8615b12bc01f9c17e2eb0cc07dcae1940121185446edc3744920e8ef45"
+checksum = "07af8b9cdd281b7915f413fa73f29ebd5d55d0d3f0155584dade1ff18cea1b17"
 dependencies = [
  "autocfg",
  "scopeguard",
 ]
 
 [[package]]
 name = "log"
-version = "0.4.20"
+version = "0.4.21"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "b5e6163cb8c49088c2c36f57875e58ccd8c87c7427f7fbd50ea6710b2f3f2e8f"
+checksum = "90ed8c1e510134f979dbc4f070f87d4313098b704861a105fe34231c70a3901c"
 
 [[package]]
 name = "mailparse"
-version = "0.14.1"
+version = "0.15.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "2d096594926cab442e054e047eb8c1402f7d5b2272573b97ba68aa40629f9757"
+checksum = "3da03d5980411a724e8aaf7b61a7b5e386ec55a7fb49ee3d0ff79efc7e5e7c7e"
 dependencies = [
  "charset",
  "data-encoding",
  "quoted_printable",
 ]
 
 [[package]]
@@ -1864,18 +2118,28 @@
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "8263075bb86c5a1b1427b5ae862e8889656f126e9f77c484496e8b47cf5c5558"
 dependencies = [
  "regex-automata 0.1.10",
 ]
 
 [[package]]
+name = "md-5"
+version = "0.10.6"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "d89e7ee0cfbedfc4da3340218492196241d89eefb6dab27de5df917a6d2e78cf"
+dependencies = [
+ "cfg-if",
+ "digest",
+]
+
+[[package]]
 name = "memchr"
-version = "2.7.1"
+version = "2.7.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "523dc4f511e55ab87b694dc30d0f820d60906ef06413f93d4d7a1385599cc149"
+checksum = "6c8640c5d730cb13ebd907d8d04b52f55ac9a2eec55b440c8892f40d56c76c1d"
 
 [[package]]
 name = "memmap2"
 version = "0.5.10"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "83faa42c0a078c393f6b29d5db232d8be22776a891f8f56e5284faee4a20b327"
 dependencies = [
@@ -1889,91 +2153,91 @@
 checksum = "fe751422e4a8caa417e13c3ea66452215d7d63e19e604f4980461212f3ae1322"
 dependencies = [
  "libc",
 ]
 
 [[package]]
 name = "memoffset"
-version = "0.9.0"
+version = "0.7.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "5de893c32cde5f383baa4c04c5d6dbdd735cfd4a794b0debdb2bb1b421da5ff4"
+dependencies = [
+ "autocfg",
+]
+
+[[package]]
+name = "memoffset"
+version = "0.9.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "5a634b1c61a95585bd15607c6ab0c4e5b226e695ff2800ba0cdccddf208c406c"
+checksum = "488016bfae457b036d996092f6cb448677611ce4449e970ceaf42695203f218a"
 dependencies = [
  "autocfg",
 ]
 
 [[package]]
 name = "miette"
-version = "6.0.1"
+version = "7.2.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "337e1043bbc086dac9d9674983bef52ac991ce150e09b5b8e35c5a73dd83f66c"
+checksum = "4edc8853320c2a0dab800fbda86253c8938f6ea88510dc92c5f1ed20e794afc1"
 dependencies = [
  "backtrace",
  "backtrace-ext",
+ "cfg-if",
  "miette-derive",
- "owo-colors 3.5.0",
+ "owo-colors",
  "supports-color",
  "supports-hyperlinks",
  "supports-unicode",
  "terminal_size",
  "textwrap",
  "thiserror",
  "unicode-width",
 ]
 
 [[package]]
 name = "miette-derive"
-version = "6.0.1"
+version = "7.2.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "71e622f2a0dd84cbca79bc6c3c33f4fd7dc69faf992216516aacc1d136102800"
+checksum = "dcf09caffaac8068c346b6df2a7fc27a177fd20b39421a39ce0a211bde679a6c"
 dependencies = [
  "proc-macro2",
  "quote",
- "syn 2.0.48",
+ "syn 2.0.64",
 ]
 
 [[package]]
 name = "mimalloc"
-version = "0.1.39"
+version = "0.1.41"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "fa01922b5ea280a911e323e4d2fd24b7fe5cc4042e0d2cda3c40775cdc4bdc9c"
+checksum = "9f41a2280ded0da56c8cf898babb86e8f10651a34adcfff190ae9a1159c6908d"
 dependencies = [
  "libmimalloc-sys",
 ]
 
 [[package]]
 name = "mime"
 version = "0.3.17"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "6877bb514081ee2a7ff5ef9de3281f14a4dd4bceac4c09388074a6b5df8a139a"
 
 [[package]]
-name = "mime_guess"
-version = "2.0.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "4192263c238a5f0d0c6bfd21f336a313a4ce1c450542449ca191bb657b4642ef"
-dependencies = [
- "mime",
- "unicase",
-]
-
-[[package]]
 name = "miniz_oxide"
 version = "0.7.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "9d811f3e15f28568be3407c8e7fdb6514c1cda3cb30683f15b6a1a1dc4ea14a7"
 dependencies = [
  "adler",
  "simd-adler32",
 ]
 
 [[package]]
 name = "mio"
-version = "0.8.10"
+version = "0.8.11"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "8f3d0b296e374a4e6f3c7b0a1f5a51d748a0d34c85e7dc48fc3fa9a87657fe09"
+checksum = "a4a650543ca06a924e8b371db273b2756685faae30f8487da1b56505a8f78b0c"
 dependencies = [
  "libc",
  "wasi",
  "windows-sys 0.48.0",
 ]
 
 [[package]]
@@ -1992,20 +2256,34 @@
 checksum = "3ffa00dec017b5b1a8b7cf5e2c008bfda1aa7e0697ac1508b491fdf2622fb4d8"
 dependencies = [
  "rand",
 ]
 
 [[package]]
 name = "nix"
-version = "0.27.1"
+version = "0.26.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "598beaf3cc6fdd9a5dfb1630c2800c7acd31df7aaf0f565796fba2b53ca1af1b"
+dependencies = [
+ "bitflags 1.3.2",
+ "cfg-if",
+ "libc",
+ "memoffset 0.7.1",
+ "pin-utils",
+]
+
+[[package]]
+name = "nix"
+version = "0.28.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "2eb04e9c688eff1c89d72b407f168cf79bb9e867a9d3323ed6c01519eb9cc053"
+checksum = "ab2156c4fce2f8df6c499cc1c763e4394b7482525bf2a9701c9d79d215f519e4"
 dependencies = [
- "bitflags 2.4.2",
+ "bitflags 2.5.0",
  "cfg-if",
+ "cfg_aliases",
  "libc",
 ]
 
 [[package]]
 name = "normalize-line-endings"
 version = "0.3.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
@@ -2027,24 +2305,18 @@
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "c073d3c1930d0751774acf49e66653acecb416c3a54c6ec095a9b11caddb5a68"
 dependencies = [
  "windows-sys 0.48.0",
 ]
 
 [[package]]
-name = "num-conv"
-version = "0.1.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "51d515d32fb182ee37cda2ccdcb92950d6a3c2893aa280e540671c2cd0f3b1d9"
-
-[[package]]
 name = "num-traits"
-version = "0.2.18"
+version = "0.2.19"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "da0df0e5185db44f69b44f26786fe401b6c293d1907744beaa7fa62b2e5a517a"
+checksum = "071dfc062690e90b734c0b2273ce72ad0ffa95f0c74596bc250dcfd960262841"
 dependencies = [
  "autocfg",
 ]
 
 [[package]]
 name = "num_cpus"
 version = "1.16.0"
@@ -2071,14 +2343,15 @@
 ]
 
 [[package]]
 name = "once-map"
 version = "0.0.1"
 dependencies = [
  "dashmap",
+ "futures",
  "tokio",
 ]
 
 [[package]]
 name = "once_cell"
 version = "1.19.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
@@ -2103,17 +2376,17 @@
 checksum = "5cff92b6f71555b61bb9315f7c64da3ca43d87531622120fea0195fc761b4843"
 dependencies = [
  "cc",
 ]
 
 [[package]]
 name = "openssl-sys"
-version = "0.9.99"
+version = "0.9.102"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "22e1bf214306098e4832460f797824c05d25aacdf896f64a985fb0fd992454ae"
+checksum = "c597637d56fbc83893a35eb0dd04b2b8e7a50c91e64e9493e398b5df4fb45fa2"
 dependencies = [
  "cc",
  "libc",
  "openssl-src",
  "pkg-config",
  "vcpkg",
 ]
@@ -2128,20 +2401,14 @@
 name = "overload"
 version = "0.1.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "b15813163c1d831bf4a13c3610c05c0d03b39feb07f7e09fa234dac9b15aaf39"
 
 [[package]]
 name = "owo-colors"
-version = "3.5.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "c1b04fb49957986fdce4d6ee7a65027d55d4b6d2265e5848bbb507b58ccfdb6f"
-
-[[package]]
-name = "owo-colors"
 version = "4.0.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "caff54706df99d2a78a5a4e3455ff45448d81ef1bb63c22cd14052ca0e993a3f"
 
 [[package]]
 name = "parking"
 version = "2.2.0"
@@ -2156,14 +2423,24 @@
 dependencies = [
  "instant",
  "lock_api",
  "parking_lot_core 0.8.6",
 ]
 
 [[package]]
+name = "parking_lot"
+version = "0.12.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "7e4af0ca4f6caed20e900d564c242b8e5d4903fdacf31d3daf527b66fe6f42fb"
+dependencies = [
+ "lock_api",
+ "parking_lot_core 0.9.10",
+]
+
+[[package]]
 name = "parking_lot_core"
 version = "0.8.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "60a2cfe6f0ad2bfc16aefa463b497d5c7a5ecd44a23efa72aa342d90177356dc"
 dependencies = [
  "cfg-if",
  "instant",
@@ -2171,59 +2448,81 @@
  "redox_syscall 0.2.16",
  "smallvec",
  "winapi",
 ]
 
 [[package]]
 name = "parking_lot_core"
-version = "0.9.9"
+version = "0.9.10"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "4c42a9226546d68acdd9c0a280d17ce19bfe27a46bf68784e4066115788d008e"
+checksum = "1e401f977ab385c9e4e3ab30627d6f26d00e2c73eef317493c4ec6d468726cf8"
 dependencies = [
  "cfg-if",
  "libc",
- "redox_syscall 0.4.1",
+ "redox_syscall 0.5.1",
  "smallvec",
- "windows-targets 0.48.5",
+ "windows-targets 0.52.5",
 ]
 
 [[package]]
 name = "paste"
-version = "1.0.14"
+version = "1.0.15"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "de3145af08024dea9fa9914f381a17b8fc6034dfb00f3a84013f7ff43f29ed4c"
+checksum = "57c0d7b74b563b49d38dae00a0c37d4d6de9b432382b2892f0574ddcae73fd0a"
+
+[[package]]
+name = "path-absolutize"
+version = "3.1.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "e4af381fe79fa195b4909485d99f73a80792331df0625188e707854f0b3383f5"
+dependencies = [
+ "path-dedot",
+]
+
+[[package]]
+name = "path-dedot"
+version = "3.1.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "07ba0ad7e047712414213ff67533e6dd477af0a4e1d14fb52343e53d30ea9397"
+dependencies = [
+ "once_cell",
+]
+
+[[package]]
+name = "pathdiff"
+version = "0.2.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "8835116a5c179084a830efb3adc117ab007512b535bc1a21c991d3b32a6b44dd"
 
 [[package]]
 name = "pep440_rs"
 version = "0.5.0"
 dependencies = [
  "indoc",
  "once_cell",
- "pubgrub",
  "pyo3",
  "rkyv",
  "serde",
  "tracing",
  "unicode-width",
  "unscanny",
 ]
 
 [[package]]
 name = "pep508_rs"
 version = "0.4.2"
 dependencies = [
  "derivative",
- "indoc",
+ "insta",
  "log",
  "once_cell",
  "pep440_rs",
  "pyo3",
  "pyo3-log",
  "regex",
- "rkyv",
  "serde",
  "serde_json",
  "testing_logger",
  "thiserror",
  "tracing",
  "unicode-width",
  "url",
@@ -2234,127 +2533,137 @@
 [[package]]
 name = "percent-encoding"
 version = "2.3.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "e3148f5046208a5d56bcfc03053e3ca6334e51da8dfb19b6cdc8b306fae3283e"
 
 [[package]]
+name = "pest"
+version = "2.7.10"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "560131c633294438da9f7c4b08189194b20946c8274c6b9e38881a7874dc8ee8"
+dependencies = [
+ "memchr",
+ "thiserror",
+ "ucd-trie",
+]
+
+[[package]]
+name = "pest_derive"
+version = "2.7.10"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "26293c9193fbca7b1a3bf9b79dc1e388e927e6cacaa78b4a3ab705a1d3d41459"
+dependencies = [
+ "pest",
+ "pest_generator",
+]
+
+[[package]]
+name = "pest_generator"
+version = "2.7.10"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "3ec22af7d3fb470a85dd2ca96b7c577a1eb4ef6f1683a9fe9a8c16e136c04687"
+dependencies = [
+ "pest",
+ "pest_meta",
+ "proc-macro2",
+ "quote",
+ "syn 2.0.64",
+]
+
+[[package]]
+name = "pest_meta"
+version = "2.7.10"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "d7a240022f37c361ec1878d646fc5b7d7c4d28d5946e1a80ad5a7a4f4ca0bdcd"
+dependencies = [
+ "once_cell",
+ "pest",
+ "sha2",
+]
+
+[[package]]
 name = "petgraph"
-version = "0.6.4"
+version = "0.6.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "e1d3afd2628e69da2be385eb6f2fd57c8ac7977ceeff6dc166ff1657b0e386a9"
+checksum = "b4c5cc86750666a3ed20bdaf5ca2a0344f9c67674cae0515bec2da16fbaa47db"
 dependencies = [
  "fixedbitset",
- "indexmap 2.2.3",
+ "indexmap",
 ]
 
 [[package]]
 name = "pico-args"
 version = "0.5.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "5be167a7af36ee22fe3115051bc51f6e6c7054c9348e28deb4f49bd6f705a315"
 
 [[package]]
 name = "pin-project"
-version = "1.1.4"
+version = "1.1.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "0302c4a0442c456bd56f841aee5c3bfd17967563f6fadc9ceb9f9c23cf3807e0"
+checksum = "b6bf43b791c5b9e34c3d182969b4abb522f9343702850a2e57f460d00d09b4b3"
 dependencies = [
  "pin-project-internal",
 ]
 
 [[package]]
 name = "pin-project-internal"
-version = "1.1.4"
+version = "1.1.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "266c042b60c9c76b8d53061e52b2e0d1116abc57cefc8c5cd671619a56ac3690"
+checksum = "2f38a4412a78282e09a2cf38d195ea5420d15ba0602cb375210efbc877243965"
 dependencies = [
  "proc-macro2",
  "quote",
- "syn 2.0.48",
+ "syn 2.0.64",
 ]
 
 [[package]]
 name = "pin-project-lite"
-version = "0.2.13"
+version = "0.2.14"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "8afb450f006bf6385ca15ef45d71d2288452bc3683ce2e2cacc0d18e4be60b58"
+checksum = "bda66fc9667c18cb2758a2ac84d1167245054bcf85d5d1aaa6923f45801bdd02"
 
 [[package]]
 name = "pin-utils"
 version = "0.1.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "8b870d8c151b6f2fb93e84a13146138f05d02ed11c7e7c54f8826aaaf7c9f184"
 
 [[package]]
 name = "pkg-config"
 version = "0.3.30"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "d231b230927b5e4ad203db57bbcbee2802f6bce620b1e4a9024a07d94e2907ec"
 
 [[package]]
-name = "plain"
-version = "0.2.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "b4596b6d070b27117e987119b4dac604f3c58cfb0b191112e24771b2faeac1a6"
-
-[[package]]
-name = "platform-host"
-version = "0.0.1"
-dependencies = [
- "fs-err",
- "goblin",
- "once_cell",
- "platform-info",
- "plist",
- "regex",
- "serde",
- "target-lexicon",
- "thiserror",
- "tracing",
-]
-
-[[package]]
 name = "platform-info"
-version = "2.0.2"
+version = "2.0.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "d6259c4860e53bf665016f1b2f46a8859cadfa717581dc9d597ae4069de6300f"
+checksum = "d5ff316b9c4642feda973c18f0decd6c8b0919d4722566f6e4337cce0dd88217"
 dependencies = [
  "libc",
  "winapi",
 ]
 
 [[package]]
 name = "platform-tags"
 version = "0.0.1"
 dependencies = [
- "platform-host",
+ "insta",
  "rustc-hash",
- "thiserror",
-]
-
-[[package]]
-name = "plist"
-version = "1.6.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "e5699cc8a63d1aa2b1ee8e12b9ad70ac790d65788cd36101fa37f87ea46c4cef"
-dependencies = [
- "base64 0.21.7",
- "indexmap 2.2.3",
- "line-wrap",
- "quick-xml",
  "serde",
- "time",
+ "thiserror",
 ]
 
 [[package]]
 name = "png"
-version = "0.17.11"
+version = "0.17.13"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "1f6c3c3e617595665b8ea2ff95a86066be38fb121ff920a9c0eb282abcd1da5a"
+checksum = "06e4b0d3d1312775e782c86c91a111aa1f910cbb65e1337f9975b5f9a554b5e1"
 dependencies = [
  "bitflags 1.3.2",
  "crc32fast",
  "fdeflate",
  "flate2",
  "miniz_oxide",
 ]
@@ -2371,20 +2680,14 @@
 [[package]]
 name = "portable-atomic"
 version = "1.6.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "7170ef9988bc169ba16dd36a7fa041e5c4cbeb6a35b76d4c03daded371eae7c0"
 
 [[package]]
-name = "powerfmt"
-version = "0.2.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "439ee305def115ba05938db6eb1644ff94165c5ab5e9420d1c1bcedbba909391"
-
-[[package]]
 name = "ppv-lite86"
 version = "0.2.17"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "5b40af805b3121feab8a3c29f04d8ad262fa8e0561883e7653e024ae4479e6de"
 
 [[package]]
 name = "predicates"
@@ -2413,28 +2716,39 @@
 checksum = "368ba315fb8c5052ab692e68a0eefec6ec57b23a36959c14496f0b0df2c0cecf"
 dependencies = [
  "predicates-core",
  "termtree",
 ]
 
 [[package]]
-name = "priority-queue"
+name = "pretty_assertions"
 version = "1.4.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "a0bda9164fe05bc9225752d54aae413343c36f684380005398a6a8fde95fe785"
+checksum = "af7cee1a6c8a5b9208b3cb1061f10c0cb689087b3d8ce85fb9d2dd7a29b6ba66"
+dependencies = [
+ "diff",
+ "yansi",
+]
+
+[[package]]
+name = "priority-queue"
+version = "2.0.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "509354d8a769e8d0b567d6821b84495c60213162761a732d68ce87c964bd347f"
 dependencies = [
  "autocfg",
- "indexmap 1.9.3",
+ "equivalent",
+ "indexmap",
 ]
 
 [[package]]
 name = "proc-macro2"
-version = "1.0.78"
+version = "1.0.82"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "e2422ad645d89c99f8f3e6b88a9fdeca7fabeac836b1002371c4367c8f984aae"
+checksum = "8ad3d49ab951a01fbaafe34f2ec74122942fe18a3f9814c3268f1bb72042131b"
 dependencies = [
  "unicode-ident",
 ]
 
 [[package]]
 name = "ptr_meta"
 version = "0.1.4"
@@ -2454,145 +2768,122 @@
  "quote",
  "syn 1.0.109",
 ]
 
 [[package]]
 name = "pubgrub"
 version = "0.2.1"
-source = "git+https://github.com/zanieb/pubgrub?rev=aab132a3d4d444dd8dd41d8c4e605abd69dacfe1#aab132a3d4d444dd8dd41d8c4e605abd69dacfe1"
+source = "git+https://github.com/astral-sh/pubgrub?rev=0e684a874c9fb8f74738cd8875524c80e3d4820b#0e684a874c9fb8f74738cd8875524c80e3d4820b"
 dependencies = [
- "indexmap 2.2.3",
+ "indexmap",
  "log",
  "priority-queue",
  "rustc-hash",
  "thiserror",
 ]
 
 [[package]]
 name = "pyo3"
-version = "0.20.2"
+version = "0.21.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "9a89dc7a5850d0e983be1ec2a463a171d20990487c3cfcd68b5363f1ee3d6fe0"
+checksum = "a5e00b96a521718e08e03b1a622f01c8a8deb50719335de3f60b3b3950f069d8"
 dependencies = [
  "cfg-if",
  "indoc",
  "libc",
- "memoffset",
- "parking_lot",
+ "memoffset 0.9.1",
+ "parking_lot 0.12.2",
+ "portable-atomic",
  "pyo3-build-config",
  "pyo3-ffi",
  "pyo3-macros",
  "unindent",
 ]
 
 [[package]]
 name = "pyo3-build-config"
-version = "0.20.2"
+version = "0.21.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "07426f0d8fe5a601f26293f300afd1a7b1ed5e78b2a705870c5f30893c5163be"
+checksum = "7883df5835fafdad87c0d888b266c8ec0f4c9ca48a5bed6bbb592e8dedee1b50"
 dependencies = [
  "once_cell",
  "target-lexicon",
 ]
 
 [[package]]
 name = "pyo3-ffi"
-version = "0.20.2"
+version = "0.21.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "dbb7dec17e17766b46bca4f1a4215a85006b4c2ecde122076c562dd058da6cf1"
+checksum = "01be5843dc60b916ab4dad1dca6d20b9b4e6ddc8e15f50c47fe6d85f1fb97403"
 dependencies = [
  "libc",
  "pyo3-build-config",
 ]
 
 [[package]]
 name = "pyo3-log"
-version = "0.9.0"
+version = "0.10.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "4c10808ee7250403bedb24bc30c32493e93875fef7ba3e4292226fe924f398bd"
+checksum = "2af49834b8d2ecd555177e63b273b708dea75150abc6f5341d0a6e1a9623976c"
 dependencies = [
  "arc-swap",
  "log",
  "pyo3",
 ]
 
 [[package]]
 name = "pyo3-macros"
-version = "0.20.2"
+version = "0.21.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "05f738b4e40d50b5711957f142878cfa0f28e054aa0ebdfc3fd137a843f74ed3"
+checksum = "77b34069fc0682e11b31dbd10321cbf94808394c56fd996796ce45217dfac53c"
 dependencies = [
  "proc-macro2",
  "pyo3-macros-backend",
  "quote",
- "syn 2.0.48",
+ "syn 2.0.64",
 ]
 
 [[package]]
 name = "pyo3-macros-backend"
-version = "0.20.2"
+version = "0.21.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "0fc910d4851847827daf9d6cdd4a823fbdaab5b8818325c5e97a86da79e8881f"
+checksum = "08260721f32db5e1a5beae69a55553f56b99bd0e1c3e6e0a5e8851a9d0f5a85c"
 dependencies = [
- "heck",
+ "heck 0.4.1",
  "proc-macro2",
+ "pyo3-build-config",
  "quote",
- "syn 2.0.48",
+ "syn 2.0.64",
 ]
 
 [[package]]
 name = "pypi-types"
 version = "0.0.1"
 dependencies = [
  "chrono",
- "indoc",
- "insta",
+ "indexmap",
  "mailparse",
  "once_cell",
  "pep440_rs",
  "pep508_rs",
  "regex",
  "rkyv",
  "serde",
- "serde_json",
- "tempfile",
- "test-case",
  "thiserror",
+ "toml",
  "tracing",
  "url",
  "uv-normalize",
 ]
 
 [[package]]
-name = "pyproject-toml"
-version = "0.10.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "3b80f889b6d413c3f8963a2c7db03f95dd6e1d85e1074137cb2013ea2faa8898"
-dependencies = [
- "indexmap 2.2.3",
- "pep440_rs",
- "pep508_rs",
- "serde",
- "toml",
-]
-
-[[package]]
-name = "quick-xml"
-version = "0.31.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "1004a344b30a54e2ee58d66a71b32d2db2feb0a31f9a2d302bf0536f15de2a33"
-dependencies = [
- "memchr",
-]
-
-[[package]]
 name = "quote"
-version = "1.0.35"
+version = "1.0.36"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "291ec9ab5efd934aaf503a6466c5d5251535d108ee747472c3977cc5acc868ef"
+checksum = "0fa76aaf39101c457836aec0ce2316dbdc3ab723cdda1c6bd4e6ad4208acaca7"
 dependencies = [
  "proc-macro2",
 ]
 
 [[package]]
 name = "quoted_printable"
 version = "0.5.0"
@@ -2633,17 +2924,17 @@
 checksum = "ec0be4795e2f6a28069bec0b5ff3e2ac9bafc99e6a9a7dc3547996c5c816922c"
 dependencies = [
  "getrandom",
 ]
 
 [[package]]
 name = "rayon"
-version = "1.8.1"
+version = "1.10.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "fa7237101a77a10773db45d62004a272517633fbcc3df19d96455ede1122e051"
+checksum = "b418a60154510ca1a002a752ca9714984e21e4241e804d32555251faf8b78ffa"
 dependencies = [
  "either",
  "rayon-core",
 ]
 
 [[package]]
 name = "rayon-core"
@@ -2685,78 +2976,87 @@
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "4722d768eff46b75989dd134e5c353f0d6296e5aaa3132e776cbdb56be7731aa"
 dependencies = [
  "bitflags 1.3.2",
 ]
 
 [[package]]
+name = "redox_syscall"
+version = "0.5.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "469052894dcb553421e483e4209ee581a45100d31b4018de03e5a7ad86374a7e"
+dependencies = [
+ "bitflags 2.5.0",
+]
+
+[[package]]
 name = "redox_users"
-version = "0.4.4"
+version = "0.4.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "a18479200779601e498ada4e8c1e1f50e3ee19deb0259c25825a98b5603b2cb4"
+checksum = "bd283d9651eeda4b2a83a43c1c91b266c40fd76ecd39a50a8c630ae69dc72891"
 dependencies = [
  "getrandom",
  "libredox",
  "thiserror",
 ]
 
 [[package]]
 name = "reflink-copy"
-version = "0.1.14"
+version = "0.1.17"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "767be24c0da52e7448d495b8d162506a9aa125426651d547d545d6c2b4b65b62"
+checksum = "7c3138c30c59ed9b8572f82bed97ea591ecd7e45012566046cc39e72679cff22"
 dependencies = [
  "cfg-if",
  "rustix",
- "windows",
+ "windows 0.56.0",
 ]
 
 [[package]]
 name = "regex"
-version = "1.10.3"
+version = "1.10.4"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "b62dbe01f0b06f9d8dc7d49e05a0785f153b00b2c227856282f671e0318c9b15"
+checksum = "c117dbdfde9c8308975b6a18d71f3f385c89461f7b3fb054288ecf2a2058ba4c"
 dependencies = [
  "aho-corasick",
  "memchr",
- "regex-automata 0.4.5",
- "regex-syntax 0.8.2",
+ "regex-automata 0.4.6",
+ "regex-syntax 0.8.3",
 ]
 
 [[package]]
 name = "regex-automata"
 version = "0.1.10"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "6c230d73fb8d8c1b9c0b3135c5142a8acee3a0558fb8db5cf1cb65f8d7862132"
 dependencies = [
  "regex-syntax 0.6.29",
 ]
 
 [[package]]
 name = "regex-automata"
-version = "0.4.5"
+version = "0.4.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "5bb987efffd3c6d0d8f5f89510bb458559eab11e4f869acb20bf845e016259cd"
+checksum = "86b83b8b9847f9bf95ef68afb0b8e6cdb80f498442f5179a29fad448fcc1eaea"
 dependencies = [
  "aho-corasick",
  "memchr",
- "regex-syntax 0.8.2",
+ "regex-syntax 0.8.3",
 ]
 
 [[package]]
 name = "regex-syntax"
 version = "0.6.29"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "f162c6dd7b008981e4d40210aca20b4bd0f9b60ca9271061b07f78537722f2e1"
 
 [[package]]
 name = "regex-syntax"
-version = "0.8.2"
+version = "0.8.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "c08c74e62047bb2de4ff487b251e4a92e24f48745648451635cec7d591162d9f"
+checksum = "adad44e29e4c806119491a7f06f03de4d1af22c3a680dd47f1e6e179439d1f56"
 
 [[package]]
 name = "rend"
 version = "0.4.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "71fe3824f5629716b1589be05dacd749f6aa084c87e00e016714a8cdfccc997c"
 dependencies = [
@@ -2765,113 +3065,115 @@
 
 [[package]]
 name = "requirements-txt"
 version = "0.0.1"
 dependencies = [
  "anyhow",
  "assert_fs",
+ "distribution-types",
  "fs-err",
  "indoc",
  "insta",
- "itertools 0.10.5",
- "once_cell",
- "pep440_rs",
+ "itertools 0.13.0",
  "pep508_rs",
  "regex",
- "serde",
- "serde_json",
+ "reqwest",
+ "reqwest-middleware",
  "tempfile",
  "test-case",
  "thiserror",
+ "tokio",
  "tracing",
  "unscanny",
  "url",
+ "uv-client",
+ "uv-configuration",
  "uv-fs",
  "uv-normalize",
  "uv-warnings",
 ]
 
 [[package]]
 name = "reqwest"
-version = "0.11.24"
+version = "0.12.4"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "c6920094eb85afde5e4a138be3f2de8bbdf28000f0029e72c45025a56b042251"
+checksum = "566cafdd92868e0939d3fb961bd0dc25fcfaaed179291093b3d43e6b3150ea10"
 dependencies = [
  "async-compression",
- "base64 0.21.7",
+ "base64 0.22.1",
  "bytes",
- "encoding_rs",
+ "futures-channel",
  "futures-core",
  "futures-util",
- "h2",
  "http",
  "http-body",
+ "http-body-util",
  "hyper",
  "hyper-rustls",
+ "hyper-util",
  "ipnet",
  "js-sys",
  "log",
  "mime",
- "mime_guess",
  "once_cell",
  "percent-encoding",
  "pin-project-lite",
  "rustls",
  "rustls-native-certs",
  "rustls-pemfile",
+ "rustls-pki-types",
  "serde",
  "serde_json",
  "serde_urlencoded",
  "sync_wrapper",
- "system-configuration",
  "tokio",
  "tokio-rustls",
  "tokio-util",
  "tower-service",
  "url",
  "wasm-bindgen",
  "wasm-bindgen-futures",
  "wasm-streams",
  "web-sys",
+ "webpki-roots",
  "winreg",
 ]
 
 [[package]]
 name = "reqwest-middleware"
-version = "0.2.4"
+version = "0.3.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "88a3e86aa6053e59030e7ce2d2a3b258dd08fc2d337d52f73f6cb480f5858690"
+checksum = "a45d100244a467870f6cb763c4484d010a6bed6bd610b3676e3825d93fb4cfbd"
 dependencies = [
  "anyhow",
  "async-trait",
  "http",
  "reqwest",
  "serde",
- "task-local-extensions",
  "thiserror",
+ "tower-service",
 ]
 
 [[package]]
 name = "reqwest-retry"
-version = "0.3.0"
+version = "0.5.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "9af20b65c2ee9746cc575acb6bd28a05ffc0d15e25c992a8f4462d8686aacb4f"
+checksum = "40f342894422862af74c50e1e9601cf0931accc9c6981e5eb413c46603b616b5"
 dependencies = [
  "anyhow",
  "async-trait",
  "chrono",
  "futures",
  "getrandom",
  "http",
  "hyper",
- "parking_lot",
+ "parking_lot 0.11.2",
  "reqwest",
  "reqwest-middleware",
  "retry-policies",
- "task-local-extensions",
  "tokio",
  "tracing",
  "wasm-timer",
 ]
 
 [[package]]
 name = "resvg"
@@ -2890,17 +3192,17 @@
  "tiny-skia",
  "usvg",
  "usvg-text-layout",
 ]
 
 [[package]]
 name = "retry-policies"
-version = "0.2.1"
+version = "0.3.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "17dd00bff1d737c40dbcd47d4375281bf4c17933f9eef0a185fc7bacca23ecbd"
+checksum = "493b4243e32d6eedd29f9a398896e35c6943a123b55eec97dcaee98310d25810"
 dependencies = [
  "anyhow",
  "chrono",
  "rand",
 ]
 
 [[package]]
@@ -2910,24 +3212,25 @@
 checksum = "05aaa8004b64fd573fc9d002f4e632d51ad4f026c2b5ba95fcb6c2f32c2c47d8"
 dependencies = [
  "bytemuck",
 ]
 
 [[package]]
 name = "ring"
-version = "0.17.7"
+version = "0.17.8"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "688c63d65483050968b2a8937f7995f443e27041a0f7700aa59b0822aedebb74"
+checksum = "c17fa4cb658e3583423e915b9f3acc01cceaee1860e33d59ebae66adc3a2dc0d"
 dependencies = [
  "cc",
+ "cfg-if",
  "getrandom",
  "libc",
  "spin",
  "untrusted",
- "windows-sys 0.48.0",
+ "windows-sys 0.52.0",
 ]
 
 [[package]]
 name = "rkyv"
 version = "0.7.44"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "5cba464629b3394fc4dbc6f940ff8f5b4ff5c7aef40f29166fd4ad12acbc99c0"
@@ -2953,28 +3256,28 @@
  "proc-macro2",
  "quote",
  "syn 1.0.109",
 ]
 
 [[package]]
 name = "rmp"
-version = "0.8.12"
+version = "0.8.14"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "7f9860a6cc38ed1da53456442089b4dfa35e7cedaa326df63017af88385e6b20"
+checksum = "228ed7c16fa39782c3b3468e974aec2795e9089153cd08ee2e9aefb3613334c4"
 dependencies = [
  "byteorder",
  "num-traits",
  "paste",
 ]
 
 [[package]]
 name = "rmp-serde"
-version = "1.1.2"
+version = "1.3.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "bffea85eea980d8a74453e5d02a8d93028f3c34725de143085a844ebe953258a"
+checksum = "52e599a477cf9840e92f2cde9a7189e67b42c57532749bf90aea6ec10facd4db"
 dependencies = [
  "byteorder",
  "rmp",
  "serde",
 ]
 
 [[package]]
@@ -3002,78 +3305,98 @@
 [[package]]
 name = "roxmltree"
 version = "0.19.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "3cd14fd5e3b777a7422cca79358c57a8f6e3a703d9ac187448d0daf220c2407f"
 
 [[package]]
+name = "rust-netrc"
+version = "0.1.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "32662f97cbfdbad9d5f78f1338116f06871e7dae4fd37e9f59a0f57cf2044868"
+dependencies = [
+ "thiserror",
+]
+
+[[package]]
 name = "rustc-demangle"
-version = "0.1.23"
+version = "0.1.24"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "d626bb9dae77e28219937af045c257c28bfd3f69333c512553507f5f9798cb76"
+checksum = "719b953e2095829ee67db738b3bfa9fa368c94900df327b3f07fe6e794d2fe1f"
 
 [[package]]
 name = "rustc-hash"
 version = "1.1.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "08d43f7aa6b08d49f382cde6a7982047c3426db949b1424bc4b7ec9ae12c6ce2"
 
 [[package]]
 name = "rustix"
-version = "0.38.31"
+version = "0.38.34"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "6ea3e1a662af26cd7a3ba09c0297a31af215563ecf42817c98df621387f4e949"
+checksum = "70dc5ec042f7a43c4a73241207cecc9873a06d45debb38b329f8541d85c2730f"
 dependencies = [
- "bitflags 2.4.2",
+ "bitflags 2.5.0",
  "errno",
  "libc",
  "linux-raw-sys",
  "windows-sys 0.52.0",
 ]
 
 [[package]]
 name = "rustls"
-version = "0.21.10"
+version = "0.22.4"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "f9d5a6813c0759e4609cd494e8e725babae6a2ca7b62a5536a13daaec6fcb7ba"
+checksum = "bf4ef73721ac7bcd79b2b315da7779d8fc09718c6b3d2d1b2d94850eb8c18432"
 dependencies = [
  "log",
  "ring",
+ "rustls-pki-types",
  "rustls-webpki",
- "sct",
+ "subtle",
+ "zeroize",
 ]
 
 [[package]]
 name = "rustls-native-certs"
-version = "0.6.3"
+version = "0.7.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "a9aace74cb666635c918e9c12bc0d348266037aa8eb599b5cba565709a8dff00"
+checksum = "8f1fb85efa936c42c6d5fc28d2629bb51e4b2f4b8a5211e297d599cc5a093792"
 dependencies = [
  "openssl-probe",
  "rustls-pemfile",
+ "rustls-pki-types",
  "schannel",
  "security-framework",
 ]
 
 [[package]]
 name = "rustls-pemfile"
-version = "1.0.4"
+version = "2.1.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "1c74cae0a4cf6ccbbf5f359f08efdf8ee7e1dc532573bf0db71968cb56b1448c"
+checksum = "29993a25686778eb88d4189742cd713c9bce943bc54251a33509dc63cbacf73d"
 dependencies = [
- "base64 0.21.7",
+ "base64 0.22.1",
+ "rustls-pki-types",
 ]
 
 [[package]]
+name = "rustls-pki-types"
+version = "1.7.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "976295e77ce332211c0d24d92c0e83e50f5c5f046d11082cea19f3df13a3562d"
+
+[[package]]
 name = "rustls-webpki"
-version = "0.101.7"
+version = "0.102.4"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "8b6275d1ee7a1cd780b64aca7726599a1dbc893b1e64144529e55c3c2f745765"
+checksum = "ff448f7e92e913c4b7d4c6d8e4540a1724b319b4152b8aef6d4cf8339712b33e"
 dependencies = [
  "ring",
+ "rustls-pki-types",
  "untrusted",
 ]
 
 [[package]]
 name = "rustybuzz"
 version = "0.7.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
@@ -3087,23 +3410,17 @@
  "unicode-ccc",
  "unicode-general-category",
  "unicode-script",
 ]
 
 [[package]]
 name = "ryu"
-version = "1.0.16"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "f98d2aa92eebf49b69786be48e4477826b256916e84a57ff2a4f21923b48eb4c"
-
-[[package]]
-name = "safemem"
-version = "0.3.3"
+version = "1.0.18"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "ef703b7cb59335eae2eb93ceb664c0eb7ea6bf567079d843e09420219668e072"
+checksum = "f3cb5ba0dc43242ce17de99c180e96db90b235b8a9fdc9543c96d2209116bd9f"
 
 [[package]]
 name = "same-file"
 version = "1.0.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "93fc1dc3aaa9bfed95e02e6eadabb4baf7e3078b0bd1b4d7b6b0b68378900502"
 dependencies = [
@@ -3116,114 +3433,126 @@
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "fbc91545643bcf3a0bbb6569265615222618bdf33ce4ffbbd13c4bbd4c093534"
 dependencies = [
  "windows-sys 0.52.0",
 ]
 
 [[package]]
-name = "scopeguard"
-version = "1.2.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "94143f37725109f92c262ed2cf5e59bce7498c01bcc1502d7b9afe439a4e9f49"
-
-[[package]]
-name = "scroll"
-version = "0.12.0"
+name = "schemars"
+version = "0.8.20"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "6ab8598aa408498679922eff7fa985c25d58a90771bd6be794434c5277eab1a6"
+checksum = "b0218ceea14babe24a4a5836f86ade86c1effbc198164e619194cb5069187e29"
 dependencies = [
- "scroll_derive",
+ "dyn-clone",
+ "schemars_derive",
+ "serde",
+ "serde_json",
+ "url",
 ]
 
 [[package]]
-name = "scroll_derive"
-version = "0.12.0"
+name = "schemars_derive"
+version = "0.8.20"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "7f81c2fde025af7e69b1d1420531c8a8811ca898919db177141a85313b1cb932"
+checksum = "3ed5a1ccce8ff962e31a165d41f6e2a2dd1245099dc4d594f5574a86cd90f4d3"
 dependencies = [
  "proc-macro2",
  "quote",
- "syn 2.0.48",
+ "serde_derive_internals",
+ "syn 2.0.64",
 ]
 
 [[package]]
-name = "sct"
-version = "0.7.1"
+name = "scopeguard"
+version = "1.2.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "da046153aa2352493d6cb7da4b6e5c0c057d8a1d0a9aa8560baffdd945acd414"
-dependencies = [
- "ring",
- "untrusted",
-]
+checksum = "94143f37725109f92c262ed2cf5e59bce7498c01bcc1502d7b9afe439a4e9f49"
 
 [[package]]
 name = "seahash"
 version = "4.1.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "1c107b6f4780854c8b126e228ea8869f4d7b71260f962fefb57b996b8959ba6b"
 
 [[package]]
 name = "security-framework"
-version = "2.9.2"
+version = "2.11.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "05b64fb303737d99b81884b2c63433e9ae28abebe5eb5045dcdd175dc2ecf4de"
+checksum = "c627723fd09706bacdb5cf41499e95098555af3c3c29d014dc3c458ef6be11c0"
 dependencies = [
- "bitflags 1.3.2",
+ "bitflags 2.5.0",
  "core-foundation",
  "core-foundation-sys",
  "libc",
  "security-framework-sys",
 ]
 
 [[package]]
 name = "security-framework-sys"
-version = "2.9.1"
+version = "2.11.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "e932934257d3b408ed8f30db49d85ea163bfe74961f017f405b025af298f0c7a"
+checksum = "317936bbbd05227752583946b9e66d7ce3b489f84e11a94a510b4437fef407d7"
 dependencies = [
  "core-foundation-sys",
  "libc",
 ]
 
 [[package]]
+name = "semver"
+version = "1.0.23"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "61697e0a1c7e512e84a621326239844a24d8207b4669b41bc18b32ea5cbf988b"
+
+[[package]]
 name = "serde"
-version = "1.0.196"
+version = "1.0.202"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "870026e60fa08c69f064aa766c10f10b1d62db9ccd4d0abb206472bee0ce3b32"
+checksum = "226b61a0d411b2ba5ff6d7f73a476ac4f8bb900373459cd00fab8512828ba395"
 dependencies = [
  "serde_derive",
 ]
 
 [[package]]
 name = "serde_derive"
-version = "1.0.196"
+version = "1.0.202"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "33c85360c95e7d137454dc81d9a4ed2b8efd8fbe19cee57357b32b9771fccb67"
+checksum = "6048858004bcff69094cd972ed40a32500f153bd3be9f716b2eed2e8217c4838"
 dependencies = [
  "proc-macro2",
  "quote",
- "syn 2.0.48",
+ "syn 2.0.64",
+]
+
+[[package]]
+name = "serde_derive_internals"
+version = "0.29.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "18d26a20a969b9e3fdf2fc2d9f21eda6c40e2de84c9408bb5d3b05d499aae711"
+dependencies = [
+ "proc-macro2",
+ "quote",
+ "syn 2.0.64",
 ]
 
 [[package]]
 name = "serde_json"
-version = "1.0.113"
+version = "1.0.117"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "69801b70b1c3dac963ecb03a364ba0ceda9cf60c71cfe475e99864759c8b8a79"
+checksum = "455182ea6142b14f93f4bc5320a2b31c1f266b66a4a5c858b013302a5d8cbfc3"
 dependencies = [
  "itoa",
  "ryu",
  "serde",
 ]
 
 [[package]]
 name = "serde_spanned"
-version = "0.6.5"
+version = "0.6.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "eb3622f419d1296904700073ea6cc23ad690adbd66f13ea683df73298736f0c1"
+checksum = "79e674e01f999af37c49f70a6ede167a8a60b2503e56c5599532a65baa5969a0"
 dependencies = [
  "serde",
 ]
 
 [[package]]
 name = "serde_urlencoded"
 version = "0.7.1"
@@ -3271,17 +3600,17 @@
 name = "shell-escape"
 version = "0.1.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "45bb67a18fa91266cc7807181f62f9178a6873bfad7dc788c42e6430db40184f"
 
 [[package]]
 name = "signal-hook-registry"
-version = "1.4.1"
+version = "1.4.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "d8229b473baa5980ac72ef434c4415e70c4b5e71b423043adb4ba059f89c99a1"
+checksum = "a9e9e0b4211b72e7b8b6e85c807d36c212bdb33ea8587f7569562a84df5465b1"
 dependencies = [
  "libc",
 ]
 
 [[package]]
 name = "simd-adler32"
 version = "0.3.7"
@@ -3292,17 +3621,17 @@
 name = "simdutf8"
 version = "0.1.4"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "f27f6278552951f1f2b8cf9da965d10969b2efdea95a6ec47987ab46edfe263a"
 
 [[package]]
 name = "similar"
-version = "2.4.0"
+version = "2.5.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "32fea41aca09ee824cc9724996433064c89f7777e60762749a4170a14abbfa21"
+checksum = "fa42c91313f1d05da9b26f267f931cf178d4aba455b4c4622dd7355eb80c6640"
 
 [[package]]
 name = "simplecss"
 version = "0.2.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "a11be7c62927d9427e9f40f3444d5499d868648e2edbc4e2116de69e7ec0e89d"
 dependencies = [
@@ -3322,32 +3651,32 @@
 checksum = "8f92a496fb766b417c996b9c5e57daf2f7ad3b0bebe1ccfca4856390e3d3bb67"
 dependencies = [
  "autocfg",
 ]
 
 [[package]]
 name = "smallvec"
-version = "1.13.1"
+version = "1.13.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "e6ecd384b10a64542d77071bd64bd7b231f4ed5940fba55e98c3de13824cf3d7"
+checksum = "3c5e1a9a646d36c3599cd173a41282daf47c44583ad367b8e6837255952e5c67"
 
 [[package]]
 name = "smawk"
 version = "0.3.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "b7c388c1b5e93756d0c740965c41e8822f866621d41acbdf6336a6a168f8840c"
 
 [[package]]
 name = "socket2"
-version = "0.5.5"
+version = "0.5.7"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "7b5fac59a5cb5dd637972e5fca70daf0523c9067fcdc4842f053dae04a18f8e9"
+checksum = "ce305eb0b4296696835b71df73eb912e0f1ffd2556a501fcede6e0c50349191c"
 dependencies = [
  "libc",
- "windows-sys 0.48.0",
+ "windows-sys 0.52.0",
 ]
 
 [[package]]
 name = "spin"
 version = "0.9.8"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "6980e8d7511241f8acf4aebddbb1ff938df5eebe98691418c4468d0b72a96a67"
@@ -3359,17 +3688,17 @@
 checksum = "6637bab7722d379c8b41ba849228d680cc12d0a45ba1fa2b48f2a30577a06731"
 dependencies = [
  "float-cmp",
 ]
 
 [[package]]
 name = "strsim"
-version = "0.11.0"
+version = "0.11.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "5ee073c9e4cd00e28217186dbe12796d692868f432bf2e97ee73bed0c56dfa01"
+checksum = "7da8b5736845d9f2fcb837ea5d9e2628564b3b043a70948a3f0b778838c5fb4f"
 
 [[package]]
 name = "subtle"
 version = "2.5.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "81cdd64d312baedb58e21336b31bc043b77e01cc99033ce76ef539f78e965ebc"
 
@@ -3392,17 +3721,17 @@
 name = "supports-unicode"
 version = "3.0.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "b7401a30af6cb5818bb64852270bb722533397edcfc7344954a38f420819ece2"
 
 [[package]]
 name = "svg"
-version = "0.15.0"
+version = "0.15.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "2198f991cd549041203080de947415bae45220eab7253c220b87e3188d19f21a"
+checksum = "683eed9bd9a2b078f92f87d166db38292e8114ab16d4cf23787ad4eecd1bb6e5"
 
 [[package]]
 name = "svgfilters"
 version = "0.4.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "639abcebc15fdc2df179f37d6f5463d660c1c79cd552c12343a4600827a04bce"
 dependencies = [
@@ -3439,47 +3768,36 @@
  "proc-macro2",
  "quote",
  "unicode-ident",
 ]
 
 [[package]]
 name = "syn"
-version = "2.0.48"
+version = "2.0.64"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "0f3531638e407dfc0814761abb7c00a5b54992b849452a0646b7f65c9f770f3f"
+checksum = "7ad3dee41f36859875573074334c200d1add8e4a87bb37113ebd31d926b7b11f"
 dependencies = [
  "proc-macro2",
  "quote",
  "unicode-ident",
 ]
 
 [[package]]
 name = "sync_wrapper"
 version = "0.1.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "2047c6ded9c721764247e62cd3b03c09ffc529b2ba5b10ec482ae507a4a70160"
 
 [[package]]
-name = "system-configuration"
-version = "0.5.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "ba3a3adc5c275d719af8cb4272ea1c4a6d668a777f37e115f6d11ddbc1c8e0e7"
-dependencies = [
- "bitflags 1.3.2",
- "core-foundation",
- "system-configuration-sys",
-]
-
-[[package]]
-name = "system-configuration-sys"
-version = "0.5.0"
+name = "sys-info"
+version = "0.9.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "a75fb188eb626b924683e3b95e3a48e63551fcfb51949de2f06a9d91dbee93c9"
+checksum = "0b3a0d0aba8bf96a0e1ddfdc352fc53b3df7f39318c71854910c3c4b024ae52c"
 dependencies = [
- "core-foundation-sys",
+ "cc",
  "libc",
 ]
 
 [[package]]
 name = "tagu"
 version = "0.1.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
@@ -3489,32 +3807,38 @@
 name = "tap"
 version = "1.0.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "55937e1799185b12863d447f42597ed69d9928686b8d88a1df17376a097d8369"
 
 [[package]]
 name = "target-lexicon"
-version = "0.12.13"
+version = "0.12.14"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "69758bda2e78f098e4ccb393021a0963bb3442eac05f135c30f61b7370bbafae"
+checksum = "e1fc403891a21bcfb7c37834ba66a547a8f402146eba7265b5a6d88059c9ff2f"
 
 [[package]]
-name = "task-local-extensions"
-version = "0.1.4"
+name = "temp-dir"
+version = "0.1.13"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "1f227968ec00f0e5322f9b8173c7a0cbcff6181a0a5b28e9892491c286277231"
+
+[[package]]
+name = "temp-env"
+version = "0.3.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "ba323866e5d033818e3240feeb9f7db2c4296674e4d9e16b97b7bf8f490434e8"
+checksum = "96374855068f47402c3121c6eed88d29cb1de8f3ab27090e273e420bdabcf050"
 dependencies = [
- "pin-utils",
+ "parking_lot 0.12.2",
 ]
 
 [[package]]
 name = "tempfile"
-version = "3.10.0"
+version = "3.10.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "a365e8cd18e44762ef95d87f284f4b5cd04107fec2ff3052bd6a3e6069669e67"
+checksum = "85b77fafb263dd9d05cbeac119526425676db3784113aa9295c88498cbf8bff1"
 dependencies = [
  "cfg-if",
  "fastrand",
  "rustix",
  "windows-sys 0.52.0",
 ]
 
@@ -3548,74 +3872,95 @@
 version = "3.3.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "adcb7fd841cd518e279be3d5a3eb0636409487998a4aff22f3de87b81e88384f"
 dependencies = [
  "cfg-if",
  "proc-macro2",
  "quote",
- "syn 2.0.48",
+ "syn 2.0.64",
 ]
 
 [[package]]
 name = "test-case-macros"
 version = "3.3.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "5c89e72a01ed4c579669add59014b9a524d609c0c88c6a585ce37485879f6ffb"
 dependencies = [
  "proc-macro2",
  "quote",
- "syn 2.0.48",
+ "syn 2.0.64",
  "test-case-core",
 ]
 
 [[package]]
+name = "test-log"
+version = "0.2.16"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "3dffced63c2b5c7be278154d76b479f9f9920ed34e7574201407f0b14e2bbb93"
+dependencies = [
+ "test-log-macros",
+ "tracing-subscriber",
+]
+
+[[package]]
+name = "test-log-macros"
+version = "0.2.16"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "5999e24eaa32083191ba4e425deb75cdf25efefabe5aaccb7446dd0d4122a3f5"
+dependencies = [
+ "proc-macro2",
+ "quote",
+ "syn 2.0.64",
+]
+
+[[package]]
 name = "testing_logger"
 version = "0.1.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "6d92b727cb45d33ae956f7f46b966b25f1bc712092aeef9dba5ac798fc89f720"
 dependencies = [
  "log",
 ]
 
 [[package]]
 name = "textwrap"
-version = "0.16.0"
+version = "0.16.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "222a222a5bfe1bba4a77b45ec488a741b3cb8872e5e499451fd7d0129c9c7c3d"
+checksum = "23d434d3f8967a09480fb04132ebe0a3e088c173e6d0ee7897abbdf4eab0f8b9"
 dependencies = [
  "smawk",
  "unicode-linebreak",
  "unicode-width",
 ]
 
 [[package]]
 name = "thiserror"
-version = "1.0.57"
+version = "1.0.61"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "1e45bcbe8ed29775f228095caf2cd67af7a4ccf756ebff23a306bf3e8b47b24b"
+checksum = "c546c80d6be4bc6a00c0f01730c08df82eaa7a7a61f11d656526506112cc1709"
 dependencies = [
  "thiserror-impl",
 ]
 
 [[package]]
 name = "thiserror-impl"
-version = "1.0.57"
+version = "1.0.61"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "a953cb265bef375dae3de6663da4d3804eee9682ea80d8e2542529b73c531c81"
+checksum = "46c3384250002a6d5af4d114f2845d37b57521033f30d5c3f46c4d70e1197533"
 dependencies = [
  "proc-macro2",
  "quote",
- "syn 2.0.48",
+ "syn 2.0.64",
 ]
 
 [[package]]
 name = "thread_local"
-version = "1.1.7"
+version = "1.1.8"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "3fdd6f064ccff2d6567adcb3873ca630700f00b5ad3f060c25b5dcfd9a4ce152"
+checksum = "8b9ef9bad013ada3808854ceac7b46812a6465ba368859a37e2100283d2d719c"
 dependencies = [
  "cfg-if",
  "once_cell",
 ]
 
 [[package]]
 name = "tikv-jemalloc-sys"
@@ -3634,45 +3979,14 @@
 checksum = "965fe0c26be5c56c94e38ba547249074803efd52adfb66de62107d95aab3eaca"
 dependencies = [
  "libc",
  "tikv-jemalloc-sys",
 ]
 
 [[package]]
-name = "time"
-version = "0.3.34"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "c8248b6521bb14bc45b4067159b9b6ad792e2d6d754d6c41fb50e29fefe38749"
-dependencies = [
- "deranged",
- "itoa",
- "num-conv",
- "powerfmt",
- "serde",
- "time-core",
- "time-macros",
-]
-
-[[package]]
-name = "time-core"
-version = "0.1.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "ef927ca75afb808a4d64dd374f00a2adf8d0fcff8e7b184af886c3c87ec4a3f3"
-
-[[package]]
-name = "time-macros"
-version = "0.2.17"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "7ba3a3ef41e6672a2f0f001392bb5dcd3ff0a9992d618ca761a11c3121547774"
-dependencies = [
- "num-conv",
- "time-core",
-]
-
-[[package]]
 name = "tiny-skia"
 version = "0.8.4"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "df8493a203431061e901613751931f047d1971337153f96d0e5e363d6dbf6a67"
 dependencies = [
  "arrayref",
  "arrayvec",
@@ -3722,23 +4036,24 @@
 name = "tl"
 version = "0.7.8"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "b130bd8a58c163224b44e217b4239ca7b927d82bf6cc2fea1fc561d15056e3f7"
 
 [[package]]
 name = "tokio"
-version = "1.36.0"
+version = "1.37.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "61285f6515fa018fb2d1e46eb21223fff441ee8db5d0f1435e8ab4f5cdb80931"
+checksum = "1adbebffeca75fcfd058afa480fb6c0b81e165a0323f9c9d39c9697e37c46787"
 dependencies = [
  "backtrace",
  "bytes",
  "libc",
  "mio",
  "num_cpus",
+ "parking_lot 0.12.2",
  "pin-project-lite",
  "signal-hook-registry",
  "socket2",
  "tokio-macros",
  "windows-sys 0.48.0",
 ]
 
@@ -3746,32 +4061,33 @@
 name = "tokio-macros"
 version = "2.2.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "5b8a1e28f2deaa14e508979454cb3a223b10b938b45af148bc0986de36f1923b"
 dependencies = [
  "proc-macro2",
  "quote",
- "syn 2.0.48",
+ "syn 2.0.64",
 ]
 
 [[package]]
 name = "tokio-rustls"
-version = "0.24.1"
+version = "0.25.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "c28327cf380ac148141087fbfb9de9d7bd4e84ab5d2c28fbc911d753de8a7081"
+checksum = "775e0c0f0adb3a2f22a00c4745d728b479985fc15ee7ca6a2608388c5569860f"
 dependencies = [
  "rustls",
+ "rustls-pki-types",
  "tokio",
 ]
 
 [[package]]
 name = "tokio-stream"
-version = "0.1.14"
+version = "0.1.15"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "397c988d37662c7dda6d2208364a706264bf3d6138b11d436cbac0ad38832842"
+checksum = "267ac89e0bec6e691e5813911606935d77c476ff49024f98abcea3e7b15e37af"
 dependencies = [
  "futures-core",
  "pin-project-lite",
  "tokio",
  "tokio-util",
 ]
 
@@ -3788,62 +4104,83 @@
  "tokio",
  "tokio-stream",
  "xattr",
 ]
 
 [[package]]
 name = "tokio-util"
-version = "0.7.10"
+version = "0.7.11"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "5419f34732d9eb6ee4c3578b7989078579b7f039cbbb9ca2c4da015749371e15"
+checksum = "9cf6b47b3771c49ac75ad09a6162f53ad4b8088b76ac60e8ec1455b31a189fe1"
 dependencies = [
  "bytes",
  "futures-core",
  "futures-io",
  "futures-sink",
  "pin-project-lite",
  "tokio",
- "tracing",
 ]
 
 [[package]]
 name = "toml"
-version = "0.8.10"
+version = "0.8.13"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "9a9aad4a3066010876e8dcf5a8a06e70a558751117a145c6ce2b82c2e2054290"
+checksum = "a4e43f8cc456c9704c851ae29c67e17ef65d2c30017c17a9765b89c382dc8bba"
 dependencies = [
  "serde",
  "serde_spanned",
  "toml_datetime",
  "toml_edit",
 ]
 
 [[package]]
 name = "toml_datetime"
-version = "0.6.5"
+version = "0.6.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "3550f4e9685620ac18a50ed434eb3aec30db8ba93b0287467bca5826ea25baf1"
+checksum = "4badfd56924ae69bcc9039335b2e017639ce3f9b001c393c1b2d1ef846ce2cbf"
 dependencies = [
  "serde",
 ]
 
 [[package]]
 name = "toml_edit"
-version = "0.22.5"
+version = "0.22.13"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "99e68c159e8f5ba8a28c4eb7b0c0c190d77bb479047ca713270048145a9ad28a"
+checksum = "c127785850e8c20836d49732ae6abfa47616e60bf9d9f57c43c250361a9db96c"
 dependencies = [
- "indexmap 2.2.3",
+ "indexmap",
  "serde",
  "serde_spanned",
  "toml_datetime",
  "winnow",
 ]
 
 [[package]]
+name = "tower"
+version = "0.4.13"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "b8fa9be0de6cf49e536ce1851f987bd21a43b771b09473c3549a6c853db37c1c"
+dependencies = [
+ "futures-core",
+ "futures-util",
+ "pin-project",
+ "pin-project-lite",
+ "tokio",
+ "tower-layer",
+ "tower-service",
+ "tracing",
+]
+
+[[package]]
+name = "tower-layer"
+version = "0.3.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "c20c8dbed6283a09604c3e69b4b7eeb54e298b8a600d4d5ecb5ad39de609f1d0"
+
+[[package]]
 name = "tower-service"
 version = "0.3.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "b6bc1c9ce2b5135ac7f93c72918fc37feb872bdc6a5533a8b85eb4b86bfdae52"
 
 [[package]]
 name = "tracing"
@@ -3861,15 +4198,15 @@
 name = "tracing-attributes"
 version = "0.1.27"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "34704c8d6ebcbc939824180af020566b01a7c01f80641264eba0999f6c2b6be7"
 dependencies = [
  "proc-macro2",
  "quote",
- "syn 2.0.48",
+ "syn 2.0.64",
 ]
 
 [[package]]
 name = "tracing-core"
 version = "0.1.32"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "c06d3da6113f116aaee68e4d601191614c9053067f9ab7f6edbcb161237daa54"
@@ -3916,29 +4253,42 @@
 dependencies = [
  "log",
  "once_cell",
  "tracing-core",
 ]
 
 [[package]]
+name = "tracing-serde"
+version = "0.1.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "bc6b213177105856957181934e4920de57730fc69bf42c37ee5bb664d406d9e1"
+dependencies = [
+ "serde",
+ "tracing-core",
+]
+
+[[package]]
 name = "tracing-subscriber"
 version = "0.3.18"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "ad0f048c97dbd9faa9b7df56362b8ebcaa52adb06b498c050d2f4e32f90a7a8b"
 dependencies = [
  "matchers",
  "nu-ansi-term 0.46.0",
  "once_cell",
  "regex",
+ "serde",
+ "serde_json",
  "sharded-slab",
  "smallvec",
  "thread_local",
  "tracing",
  "tracing-core",
  "tracing-log",
+ "tracing-serde",
 ]
 
 [[package]]
 name = "tracing-tree"
 version = "0.3.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "65139ecd2c3f6484c3b99bc01c77afe21e95473630747c7aca525e78b0666675"
@@ -3964,21 +4314,18 @@
 [[package]]
 name = "typenum"
 version = "1.17.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "42ff0bf0c66b8238c6f3b578df37d0b7848e55df8577b3f74f92a69acceeb825"
 
 [[package]]
-name = "unicase"
-version = "2.7.0"
+name = "ucd-trie"
+version = "0.1.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "f7d2d4dafb69621809a81864c9c1b864479e1235c0dd4e199924b9742439ed89"
-dependencies = [
- "version_check",
-]
+checksum = "ed646292ffc8188ef8ea4d1e0e0150fb15a5c2e12ad9b8fc191ae7a8a7f3c4b9"
 
 [[package]]
 name = "unicode-bidi"
 version = "0.3.15"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "08f95100a766bf4f8f28f90d77e0a5461bbdb219042e7679bebe79004fed8d75"
 
@@ -4010,38 +4357,38 @@
 name = "unicode-linebreak"
 version = "0.1.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "3b09c83c3c29d37506a3e260c08c03743a6bb66a9cd432c6934ab501a190571f"
 
 [[package]]
 name = "unicode-normalization"
-version = "0.1.22"
+version = "0.1.23"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "5c5713f0fc4b5db668a2ac63cdb7bb4469d8c9fed047b1d0292cc7b0ce2ba921"
+checksum = "a56d1686db2308d901306f92a263857ef59ea39678a5458e7cb17f01415101f5"
 dependencies = [
  "tinyvec",
 ]
 
 [[package]]
 name = "unicode-script"
-version = "0.5.5"
+version = "0.5.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "7d817255e1bed6dfd4ca47258685d14d2bdcfbc64fdc9e3819bd5848057b8ecc"
+checksum = "ad8d71f5726e5f285a935e9fe8edfd53f0491eb6e9a5774097fdabee7cd8c9cd"
 
 [[package]]
 name = "unicode-vo"
 version = "0.1.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "b1d386ff53b415b7fe27b50bb44679e2cc4660272694b7b6f3326d8480823a94"
 
 [[package]]
 name = "unicode-width"
-version = "0.1.11"
+version = "0.1.12"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "e51733f11c9c4f72aa0c160008246859e340b00807569a0da0e7a1079b27ba85"
+checksum = "68f5e5f3158ecfd4b8ff6fe086db7c8467a2dfdac97fe420f2b7c4aa97af66d6"
 
 [[package]]
 name = "unindent"
 version = "0.2.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "c7de7d73e1754487cb58364ee906a499937a0dfabd86bcb980fa99ec8c8fa2ce"
 
@@ -4118,136 +4465,153 @@
 name = "utf8parse"
 version = "0.2.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "711b9620af191e0cdc7468a8d14e709c3dcdb115b36f838e601583af800a370a"
 
 [[package]]
 name = "uuid"
-version = "1.7.0"
+version = "1.8.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "f00cc9702ca12d3c81455259621e676d0f7251cec66a21e98fe2e9a37db93b2a"
+checksum = "a183cf7feeba97b4dd1c0d46788634f6221d87fa961b305bed08c851829efcc0"
 
 [[package]]
 name = "uv"
-version = "0.1.9"
+version = "0.2.0"
 dependencies = [
  "anstream",
  "anyhow",
  "assert_cmd",
  "assert_fs",
- "base64 0.21.7",
+ "axoupdater",
+ "base64 0.22.1",
+ "byteorder",
  "chrono",
  "clap",
  "clap_complete_command",
- "console",
- "ctrlc",
- "distribution-filename",
  "distribution-types",
- "dunce",
  "filetime",
  "flate2",
  "fs-err",
- "futures",
- "gourgeist",
+ "indexmap",
  "indicatif",
  "indoc",
  "insta",
  "install-wheel-rs",
- "itertools 0.12.1",
+ "itertools 0.13.0",
  "miette",
  "mimalloc",
- "owo-colors 4.0.0",
+ "owo-colors",
  "pep440_rs",
  "pep508_rs",
- "platform-host",
  "platform-tags",
  "predicates",
- "pubgrub",
  "pypi-types",
- "pyproject-toml",
+ "rayon",
  "regex",
  "requirements-txt",
  "reqwest",
  "rustc-hash",
+ "serde",
+ "serde_json",
  "tempfile",
  "textwrap",
  "thiserror",
  "tikv-jemallocator",
  "tokio",
  "toml",
  "tracing",
  "tracing-durations-export",
  "tracing-subscriber",
  "tracing-tree",
+ "unicode-width",
  "url",
- "uv-build",
+ "uv-auth",
  "uv-cache",
  "uv-client",
+ "uv-configuration",
  "uv-dispatch",
  "uv-distribution",
  "uv-fs",
  "uv-installer",
  "uv-interpreter",
  "uv-normalize",
+ "uv-requirements",
  "uv-resolver",
- "uv-traits",
+ "uv-types",
+ "uv-virtualenv",
  "uv-warnings",
- "which",
+ "uv-workspace",
 ]
 
 [[package]]
 name = "uv-auth"
 version = "0.0.1"
 dependencies = [
+ "anyhow",
+ "async-trait",
+ "base64 0.22.1",
+ "futures",
+ "http",
+ "insta",
+ "once-map",
+ "once_cell",
+ "reqwest",
+ "reqwest-middleware",
+ "rust-netrc",
+ "tempfile",
+ "test-log",
+ "tokio",
  "tracing",
  "url",
+ "urlencoding",
+ "wiremock",
 ]
 
 [[package]]
 name = "uv-build"
 version = "0.0.1"
 dependencies = [
  "anyhow",
  "distribution-types",
  "fs-err",
- "gourgeist",
  "indoc",
  "insta",
- "itertools 0.12.1",
+ "itertools 0.13.0",
  "once_cell",
+ "pep440_rs",
  "pep508_rs",
- "platform-host",
- "pypi-types",
- "pyproject-toml",
  "regex",
+ "rustc-hash",
  "serde",
  "serde_json",
  "tempfile",
  "thiserror",
  "tokio",
  "toml",
  "tracing",
- "uv-extract",
+ "uv-configuration",
  "uv-fs",
  "uv-interpreter",
- "uv-traits",
+ "uv-types",
+ "uv-virtualenv",
 ]
 
 [[package]]
 name = "uv-cache"
 version = "0.0.1"
 dependencies = [
  "cache-key",
- "cachedir",
  "clap",
  "directories",
  "distribution-types",
  "fs-err",
  "nanoid",
  "pypi-types",
+ "rmp-serde",
+ "rustc-hash",
  "serde",
  "tempfile",
  "tracing",
  "url",
  "uv-fs",
  "uv-normalize",
  "walkdir",
@@ -4265,284 +4629,365 @@
  "chrono",
  "distribution-filename",
  "distribution-types",
  "fs-err",
  "futures",
  "html-escape",
  "http",
+ "http-body-util",
+ "hyper",
+ "hyper-util",
  "insta",
  "install-wheel-rs",
  "pep440_rs",
  "pep508_rs",
  "platform-tags",
  "pypi-types",
  "reqwest",
  "reqwest-middleware",
  "reqwest-retry",
  "rkyv",
  "rmp-serde",
- "rustc-hash",
  "serde",
  "serde_json",
- "sha2",
- "task-local-extensions",
+ "sys-info",
  "tempfile",
  "thiserror",
  "tl",
  "tokio",
  "tokio-util",
  "tracing",
  "url",
  "urlencoding",
  "uv-auth",
  "uv-cache",
+ "uv-configuration",
  "uv-fs",
  "uv-normalize",
+ "uv-version",
  "uv-warnings",
 ]
 
 [[package]]
+name = "uv-configuration"
+version = "0.0.1"
+dependencies = [
+ "anyhow",
+ "clap",
+ "distribution-types",
+ "itertools 0.13.0",
+ "pep508_rs",
+ "platform-tags",
+ "rustc-hash",
+ "schemars",
+ "serde",
+ "serde_json",
+ "tracing",
+ "uv-auth",
+ "uv-normalize",
+]
+
+[[package]]
 name = "uv-dev"
 version = "0.0.1"
 dependencies = [
  "anstream",
  "anyhow",
- "chrono",
  "clap",
  "distribution-filename",
  "distribution-types",
  "fs-err",
  "futures",
- "gourgeist",
- "indicatif",
  "install-wheel-rs",
- "itertools 0.12.1",
+ "itertools 0.13.0",
  "mimalloc",
- "owo-colors 4.0.0",
- "pep440_rs",
+ "owo-colors",
  "pep508_rs",
- "petgraph",
- "platform-host",
- "platform-tags",
  "poloto",
- "pypi-types",
+ "pretty_assertions",
  "resvg",
  "rustc-hash",
+ "schemars",
  "serde",
  "serde_json",
  "tagu",
- "tempfile",
  "tikv-jemallocator",
  "tokio",
  "tracing",
  "tracing-durations-export",
  "tracing-indicatif",
  "tracing-subscriber",
- "url",
  "uv-build",
  "uv-cache",
  "uv-client",
+ "uv-configuration",
  "uv-dispatch",
- "uv-distribution",
+ "uv-fs",
  "uv-installer",
  "uv-interpreter",
- "uv-normalize",
+ "uv-requirements",
  "uv-resolver",
- "uv-traits",
- "which",
+ "uv-types",
+ "uv-workspace",
+ "walkdir",
 ]
 
 [[package]]
 name = "uv-dispatch"
 version = "0.0.1"
 dependencies = [
  "anyhow",
  "distribution-types",
- "fs-err",
  "futures",
- "gourgeist",
- "itertools 0.12.1",
- "pep508_rs",
- "platform-host",
- "platform-tags",
- "pypi-types",
- "tempfile",
- "tokio",
+ "install-wheel-rs",
+ "itertools 0.13.0",
+ "rustc-hash",
  "tracing",
  "uv-build",
  "uv-cache",
  "uv-client",
+ "uv-configuration",
  "uv-distribution",
  "uv-installer",
  "uv-interpreter",
  "uv-resolver",
- "uv-traits",
+ "uv-types",
 ]
 
 [[package]]
 name = "uv-distribution"
 version = "0.0.1"
 dependencies = [
  "anyhow",
  "cache-key",
  "distribution-filename",
  "distribution-types",
  "fs-err",
  "futures",
  "install-wheel-rs",
  "nanoid",
+ "once_cell",
  "pep440_rs",
  "pep508_rs",
  "platform-tags",
  "pypi-types",
  "reqwest",
+ "reqwest-middleware",
  "rmp-serde",
  "rustc-hash",
  "serde",
  "tempfile",
  "thiserror",
  "tokio",
  "tokio-util",
  "tracing",
  "url",
  "uv-cache",
  "uv-client",
+ "uv-configuration",
  "uv-extract",
  "uv-fs",
  "uv-git",
  "uv-normalize",
- "uv-traits",
+ "uv-types",
  "zip",
 ]
 
 [[package]]
 name = "uv-extract"
 version = "0.0.1"
 dependencies = [
  "async-compression",
  "async_zip",
- "flate2",
  "fs-err",
  "futures",
+ "md-5",
+ "pypi-types",
  "rayon",
  "rustc-hash",
+ "sha2",
  "thiserror",
  "tokio",
  "tokio-tar",
  "tokio-util",
+ "tracing",
  "zip",
 ]
 
 [[package]]
 name = "uv-fs"
 version = "0.0.1"
 dependencies = [
+ "backoff",
+ "cachedir",
  "dunce",
+ "encoding_rs_io",
  "fs-err",
  "fs2",
  "junction",
+ "once_cell",
+ "path-absolutize",
  "tempfile",
  "tracing",
  "urlencoding",
  "uv-warnings",
 ]
 
 [[package]]
 name = "uv-git"
 version = "0.0.1"
 dependencies = [
  "anyhow",
- "base64 0.21.7",
+ "base64 0.22.1",
  "cache-key",
  "cargo-util",
+ "fs-err",
  "git2",
  "glob",
- "hex",
  "hmac",
  "home",
- "once_cell",
  "rand",
  "reqwest",
- "serde",
  "sha1",
  "tokio",
  "tracing",
  "url",
  "uv-fs",
 ]
 
 [[package]]
 name = "uv-installer"
 version = "0.0.1"
 dependencies = [
  "anyhow",
+ "async-channel",
+ "cache-key",
  "distribution-filename",
  "distribution-types",
  "fs-err",
  "futures",
  "install-wheel-rs",
- "once-map",
  "pep440_rs",
  "pep508_rs",
  "platform-tags",
  "pypi-types",
  "rayon",
  "requirements-txt",
  "rustc-hash",
+ "serde",
  "tempfile",
  "thiserror",
  "tokio",
+ "toml",
  "tracing",
  "url",
  "uv-cache",
- "uv-client",
+ "uv-configuration",
  "uv-distribution",
  "uv-extract",
  "uv-fs",
  "uv-git",
  "uv-interpreter",
  "uv-normalize",
- "uv-traits",
+ "uv-types",
+ "uv-warnings",
+ "walkdir",
 ]
 
 [[package]]
 name = "uv-interpreter"
 version = "0.0.1"
 dependencies = [
  "anyhow",
+ "assert_fs",
  "cache-key",
+ "configparser",
  "fs-err",
+ "futures",
  "indoc",
- "insta",
- "itertools 0.12.1",
+ "install-wheel-rs",
+ "itertools 0.13.0",
  "once_cell",
  "pep440_rs",
  "pep508_rs",
- "platform-host",
  "platform-tags",
+ "pypi-types",
  "regex",
+ "reqwest",
+ "reqwest-middleware",
  "rmp-serde",
  "same-file",
+ "schemars",
  "serde",
  "serde_json",
+ "temp-env",
  "tempfile",
+ "test-log",
  "thiserror",
- "tokio",
+ "tokio-util",
  "tracing",
+ "url",
  "uv-cache",
+ "uv-client",
+ "uv-extract",
  "uv-fs",
+ "uv-warnings",
  "which",
+ "winapi",
 ]
 
 [[package]]
 name = "uv-normalize"
 version = "0.0.1"
 dependencies = [
  "rkyv",
+ "schemars",
+ "serde",
+]
+
+[[package]]
+name = "uv-requirements"
+version = "0.1.0"
+dependencies = [
+ "anyhow",
+ "cache-key",
+ "configparser",
+ "console",
+ "ctrlc",
+ "distribution-filename",
+ "distribution-types",
+ "fs-err",
+ "futures",
+ "glob",
+ "indexmap",
+ "indoc",
+ "insta",
+ "itertools 0.13.0",
+ "path-absolutize",
+ "pep440_rs",
+ "pep508_rs",
+ "pypi-types",
+ "regex",
+ "requirements-txt",
+ "rustc-hash",
+ "schemars",
  "serde",
+ "thiserror",
+ "toml",
+ "tracing",
+ "url",
+ "uv-client",
+ "uv-configuration",
+ "uv-distribution",
+ "uv-fs",
+ "uv-git",
+ "uv-normalize",
+ "uv-resolver",
+ "uv-types",
+ "uv-warnings",
 ]
 
 [[package]]
 name = "uv-resolver"
 version = "0.0.1"
 dependencies = [
  "anstream",
@@ -4551,82 +4996,122 @@
  "chrono",
  "clap",
  "dashmap",
  "derivative",
  "distribution-filename",
  "distribution-types",
  "either",
- "fs-err",
  "futures",
- "gourgeist",
- "indexmap 2.2.3",
+ "indexmap",
  "insta",
  "install-wheel-rs",
- "itertools 0.12.1",
+ "itertools 0.13.0",
  "once-map",
  "once_cell",
- "owo-colors 4.0.0",
+ "owo-colors",
  "pep440_rs",
  "pep508_rs",
  "petgraph",
- "platform-host",
  "platform-tags",
  "pubgrub",
  "pypi-types",
- "reqwest",
+ "requirements-txt",
  "rkyv",
  "rustc-hash",
- "serde_json",
- "sha2",
- "tempfile",
+ "schemars",
+ "serde",
+ "textwrap",
  "thiserror",
  "tokio",
  "tokio-stream",
- "tokio-util",
+ "toml",
  "tracing",
  "url",
  "uv-cache",
  "uv-client",
+ "uv-configuration",
  "uv-distribution",
  "uv-git",
  "uv-interpreter",
  "uv-normalize",
- "uv-traits",
+ "uv-types",
  "uv-warnings",
- "zip",
 ]
 
 [[package]]
-name = "uv-traits"
+name = "uv-types"
 version = "0.0.1"
 dependencies = [
  "anyhow",
- "clap",
  "distribution-types",
  "once-map",
+ "pep440_rs",
  "pep508_rs",
- "serde",
- "serde_json",
- "tokio",
+ "pypi-types",
+ "rustc-hash",
+ "thiserror",
+ "url",
  "uv-cache",
+ "uv-configuration",
  "uv-interpreter",
  "uv-normalize",
 ]
 
 [[package]]
+name = "uv-version"
+version = "0.2.0"
+
+[[package]]
+name = "uv-virtualenv"
+version = "0.0.4"
+dependencies = [
+ "fs-err",
+ "itertools 0.13.0",
+ "pathdiff",
+ "platform-tags",
+ "pypi-types",
+ "thiserror",
+ "tracing",
+ "uv-fs",
+ "uv-interpreter",
+ "uv-version",
+]
+
+[[package]]
 name = "uv-warnings"
 version = "0.0.1"
 dependencies = [
  "anstream",
  "once_cell",
- "owo-colors 4.0.0",
+ "owo-colors",
  "rustc-hash",
 ]
 
 [[package]]
+name = "uv-workspace"
+version = "0.0.1"
+dependencies = [
+ "dirs-sys",
+ "distribution-types",
+ "fs-err",
+ "install-wheel-rs",
+ "schemars",
+ "serde",
+ "thiserror",
+ "toml",
+ "tracing",
+ "uv-configuration",
+ "uv-fs",
+ "uv-interpreter",
+ "uv-normalize",
+ "uv-resolver",
+ "uv-warnings",
+]
+
+[[package]]
 name = "valuable"
 version = "0.1.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "830b7e5d4d90034032940e4ace0d9a9a057e7a45cd94e6c007832e39edb82f6d"
 
 [[package]]
 name = "vcpkg"
@@ -4680,17 +5165,17 @@
 checksum = "9f200f5b12eb75f8c1ed65abd4b2db8a6e1b138a20de009dacee265a2498f3f6"
 dependencies = [
  "libc",
 ]
 
 [[package]]
 name = "walkdir"
-version = "2.4.0"
+version = "2.5.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "d71d857dc86794ca4c280d616f7da00d2dbfd8cd788846559a6813e6aa4b54ee"
+checksum = "29790946404f91d9c5d06f9874efddea1dc06c5efe94541a7d6863108e3a5e4b"
 dependencies = [
  "same-file",
  "winapi-util",
 ]
 
 [[package]]
 name = "want"
@@ -4705,77 +5190,77 @@
 name = "wasi"
 version = "0.11.0+wasi-snapshot-preview1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "9c8d87e72b64a3b4db28d11ce29237c246188f4f51057d65a7eab63b7987e423"
 
 [[package]]
 name = "wasm-bindgen"
-version = "0.2.91"
+version = "0.2.92"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "c1e124130aee3fb58c5bdd6b639a0509486b0338acaaae0c84a5124b0f588b7f"
+checksum = "4be2531df63900aeb2bca0daaaddec08491ee64ceecbee5076636a3b026795a8"
 dependencies = [
  "cfg-if",
  "wasm-bindgen-macro",
 ]
 
 [[package]]
 name = "wasm-bindgen-backend"
-version = "0.2.91"
+version = "0.2.92"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "c9e7e1900c352b609c8488ad12639a311045f40a35491fb69ba8c12f758af70b"
+checksum = "614d787b966d3989fa7bb98a654e369c762374fd3213d212cfc0251257e747da"
 dependencies = [
  "bumpalo",
  "log",
  "once_cell",
  "proc-macro2",
  "quote",
- "syn 2.0.48",
+ "syn 2.0.64",
  "wasm-bindgen-shared",
 ]
 
 [[package]]
 name = "wasm-bindgen-futures"
-version = "0.4.41"
+version = "0.4.42"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "877b9c3f61ceea0e56331985743b13f3d25c406a7098d45180fb5f09bc19ed97"
+checksum = "76bc14366121efc8dbb487ab05bcc9d346b3b5ec0eaa76e46594cabbe51762c0"
 dependencies = [
  "cfg-if",
  "js-sys",
  "wasm-bindgen",
  "web-sys",
 ]
 
 [[package]]
 name = "wasm-bindgen-macro"
-version = "0.2.91"
+version = "0.2.92"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "b30af9e2d358182b5c7449424f017eba305ed32a7010509ede96cdc4696c46ed"
+checksum = "a1f8823de937b71b9460c0c34e25f3da88250760bec0ebac694b49997550d726"
 dependencies = [
  "quote",
  "wasm-bindgen-macro-support",
 ]
 
 [[package]]
 name = "wasm-bindgen-macro-support"
-version = "0.2.91"
+version = "0.2.92"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "642f325be6301eb8107a83d12a8ac6c1e1c54345a7ef1a9261962dfefda09e66"
+checksum = "e94f17b526d0a461a191c78ea52bbce64071ed5c04c9ffe424dcb38f74171bb7"
 dependencies = [
  "proc-macro2",
  "quote",
- "syn 2.0.48",
+ "syn 2.0.64",
  "wasm-bindgen-backend",
  "wasm-bindgen-shared",
 ]
 
 [[package]]
 name = "wasm-bindgen-shared"
-version = "0.2.91"
+version = "0.2.92"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "4f186bd2dcf04330886ce82d6f33dd75a7bfcf69ecf5763b89fcde53b6ac9838"
+checksum = "af190c94f2773fdb3729c55b007a722abb5384da03bc0986df4c289bf5567e96"
 
 [[package]]
 name = "wasm-streams"
 version = "0.4.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "b65dc4c90b63b118468cf747d8bf3566c1913ef60be765b5730ead9e0a3ba129"
 dependencies = [
@@ -4790,51 +5275,65 @@
 name = "wasm-timer"
 version = "0.2.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "be0ecb0db480561e9a7642b5d3e4187c128914e58aa84330b9493e3eb68c5e7f"
 dependencies = [
  "futures",
  "js-sys",
- "parking_lot",
+ "parking_lot 0.11.2",
  "pin-utils",
  "wasm-bindgen",
  "wasm-bindgen-futures",
  "web-sys",
 ]
 
 [[package]]
 name = "web-sys"
-version = "0.3.68"
+version = "0.3.69"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "96565907687f7aceb35bc5fc03770a8a0471d82e479f25832f54a0e3f4b28446"
+checksum = "77afa9a11836342370f4817622a2f0f418b134426d91a82dfb48f532d2ec13ef"
 dependencies = [
  "js-sys",
  "wasm-bindgen",
 ]
 
 [[package]]
+name = "webpki-roots"
+version = "0.26.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "b3de34ae270483955a94f4b21bdaaeb83d508bb84a01435f393818edb0012009"
+dependencies = [
+ "rustls-pki-types",
+]
+
+[[package]]
 name = "weezl"
 version = "0.1.8"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "53a85b86a771b1c87058196170769dd264f66c0782acf1ae6cc51bfd64b39082"
 
 [[package]]
 name = "which"
-version = "6.0.0"
+version = "6.0.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "7fa5e0c10bf77f44aac573e498d1a82d5fbd5e91f6fc0a99e7be4b38e85e101c"
+checksum = "8211e4f58a2b2805adfbefbc07bab82958fc91e3836339b1ab7ae32465dce0d7"
 dependencies = [
  "either",
  "home",
- "once_cell",
  "rustix",
- "windows-sys 0.52.0",
+ "winsafe",
 ]
 
 [[package]]
+name = "widestring"
+version = "1.1.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "7219d36b6eac893fa81e84ebe06485e7dcbb616177469b142df14f1f4deb1311"
+
+[[package]]
 name = "winapi"
 version = "0.3.9"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "5c839a674fcd7a98952e593242ea400abe93992746761e38641405d28b00f419"
 dependencies = [
  "winapi-i686-pc-windows-gnu",
  "winapi-x86_64-pc-windows-gnu",
@@ -4844,44 +5343,121 @@
 name = "winapi-i686-pc-windows-gnu"
 version = "0.4.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "ac3b87c63620426dd9b991e5ce0329eff545bccbbb34f3be09ff6fb6ab51b7b6"
 
 [[package]]
 name = "winapi-util"
-version = "0.1.6"
+version = "0.1.8"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "f29e6f9198ba0d26b4c9f07dbe6f9ed633e1f3d5b8b414090084349e46a52596"
+checksum = "4d4cc384e1e73b93bafa6fb4f1df8c41695c8a91cf9c4c64358067d15a7b6c6b"
 dependencies = [
- "winapi",
+ "windows-sys 0.52.0",
 ]
 
 [[package]]
 name = "winapi-x86_64-pc-windows-gnu"
 version = "0.4.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "712e227841d057c1ee1cd2fb22fa7e5a5461ae8e48fa2ca79ec42cfc1931183f"
 
 [[package]]
 name = "windows"
 version = "0.52.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "e48a53791691ab099e5e2ad123536d0fff50652600abaf43bbf952894110d0be"
 dependencies = [
- "windows-core",
- "windows-targets 0.52.0",
+ "windows-core 0.52.0",
+ "windows-implement 0.52.0",
+ "windows-interface 0.52.0",
+ "windows-targets 0.52.5",
+]
+
+[[package]]
+name = "windows"
+version = "0.56.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "1de69df01bdf1ead2f4ac895dc77c9351aefff65b2f3db429a343f9cbf05e132"
+dependencies = [
+ "windows-core 0.56.0",
+ "windows-targets 0.52.5",
 ]
 
 [[package]]
 name = "windows-core"
 version = "0.52.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "33ab640c8d7e35bf8ba19b884ba838ceb4fba93a4e8c65a9059d08afcfc683d9"
 dependencies = [
- "windows-targets 0.52.0",
+ "windows-targets 0.52.5",
+]
+
+[[package]]
+name = "windows-core"
+version = "0.56.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "4698e52ed2d08f8658ab0c39512a7c00ee5fe2688c65f8c0a4f06750d729f2a6"
+dependencies = [
+ "windows-implement 0.56.0",
+ "windows-interface 0.56.0",
+ "windows-result",
+ "windows-targets 0.52.5",
+]
+
+[[package]]
+name = "windows-implement"
+version = "0.52.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "12168c33176773b86799be25e2a2ba07c7aab9968b37541f1094dbd7a60c8946"
+dependencies = [
+ "proc-macro2",
+ "quote",
+ "syn 2.0.64",
+]
+
+[[package]]
+name = "windows-implement"
+version = "0.56.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "f6fc35f58ecd95a9b71c4f2329b911016e6bec66b3f2e6a4aad86bd2e99e2f9b"
+dependencies = [
+ "proc-macro2",
+ "quote",
+ "syn 2.0.64",
+]
+
+[[package]]
+name = "windows-interface"
+version = "0.52.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "9d8dc32e0095a7eeccebd0e3f09e9509365ecb3fc6ac4d6f5f14a3f6392942d1"
+dependencies = [
+ "proc-macro2",
+ "quote",
+ "syn 2.0.64",
+]
+
+[[package]]
+name = "windows-interface"
+version = "0.56.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "08990546bf4edef8f431fa6326e032865f27138718c587dc21bc0265bbcb57cc"
+dependencies = [
+ "proc-macro2",
+ "quote",
+ "syn 2.0.64",
+]
+
+[[package]]
+name = "windows-result"
+version = "0.1.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "749f0da9cc72d82e600d8d2e44cadd0b9eedb9038f71a1c58556ac1c5791813b"
+dependencies = [
+ "windows-targets 0.52.5",
 ]
 
 [[package]]
 name = "windows-sys"
 version = "0.48.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "677d2418bec65e3338edb076e806bc1ec15693c5d0104683f2efe857f61056a9"
@@ -4891,15 +5467,15 @@
 
 [[package]]
 name = "windows-sys"
 version = "0.52.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "282be5f36a8ce781fad8c8ae18fa3f9beff57ec1b52cb3de0789201425d9a33d"
 dependencies = [
- "windows-targets 0.52.0",
+ "windows-targets 0.52.5",
 ]
 
 [[package]]
 name = "windows-targets"
 version = "0.48.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "9a2fa6e2155d7247be68c096456083145c183cbbbc2764150dda45a87197940c"
@@ -4911,131 +5487,182 @@
  "windows_x86_64_gnu 0.48.5",
  "windows_x86_64_gnullvm 0.48.5",
  "windows_x86_64_msvc 0.48.5",
 ]
 
 [[package]]
 name = "windows-targets"
-version = "0.52.0"
+version = "0.52.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "8a18201040b24831fbb9e4eb208f8892e1f50a37feb53cc7ff887feb8f50e7cd"
+checksum = "6f0713a46559409d202e70e28227288446bf7841d3211583a4b53e3f6d96e7eb"
 dependencies = [
- "windows_aarch64_gnullvm 0.52.0",
- "windows_aarch64_msvc 0.52.0",
- "windows_i686_gnu 0.52.0",
- "windows_i686_msvc 0.52.0",
- "windows_x86_64_gnu 0.52.0",
- "windows_x86_64_gnullvm 0.52.0",
- "windows_x86_64_msvc 0.52.0",
+ "windows_aarch64_gnullvm 0.52.5",
+ "windows_aarch64_msvc 0.52.5",
+ "windows_i686_gnu 0.52.5",
+ "windows_i686_gnullvm",
+ "windows_i686_msvc 0.52.5",
+ "windows_x86_64_gnu 0.52.5",
+ "windows_x86_64_gnullvm 0.52.5",
+ "windows_x86_64_msvc 0.52.5",
 ]
 
 [[package]]
 name = "windows_aarch64_gnullvm"
 version = "0.48.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "2b38e32f0abccf9987a4e3079dfb67dcd799fb61361e53e2882c3cbaf0d905d8"
 
 [[package]]
 name = "windows_aarch64_gnullvm"
-version = "0.52.0"
+version = "0.52.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "cb7764e35d4db8a7921e09562a0304bf2f93e0a51bfccee0bd0bb0b666b015ea"
+checksum = "7088eed71e8b8dda258ecc8bac5fb1153c5cffaf2578fc8ff5d61e23578d3263"
 
 [[package]]
 name = "windows_aarch64_msvc"
 version = "0.48.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "dc35310971f3b2dbbf3f0690a219f40e2d9afcf64f9ab7cc1be722937c26b4bc"
 
 [[package]]
 name = "windows_aarch64_msvc"
-version = "0.52.0"
+version = "0.52.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "bbaa0368d4f1d2aaefc55b6fcfee13f41544ddf36801e793edbbfd7d7df075ef"
+checksum = "9985fd1504e250c615ca5f281c3f7a6da76213ebd5ccc9561496568a2752afb6"
 
 [[package]]
 name = "windows_i686_gnu"
 version = "0.48.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "a75915e7def60c94dcef72200b9a8e58e5091744960da64ec734a6c6e9b3743e"
 
 [[package]]
 name = "windows_i686_gnu"
-version = "0.52.0"
+version = "0.52.5"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "88ba073cf16d5372720ec942a8ccbf61626074c6d4dd2e745299726ce8b89670"
+
+[[package]]
+name = "windows_i686_gnullvm"
+version = "0.52.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "a28637cb1fa3560a16915793afb20081aba2c92ee8af57b4d5f28e4b3e7df313"
+checksum = "87f4261229030a858f36b459e748ae97545d6f1ec60e5e0d6a3d32e0dc232ee9"
 
 [[package]]
 name = "windows_i686_msvc"
 version = "0.48.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "8f55c233f70c4b27f66c523580f78f1004e8b5a8b659e05a4eb49d4166cca406"
 
 [[package]]
 name = "windows_i686_msvc"
-version = "0.52.0"
+version = "0.52.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "ffe5e8e31046ce6230cc7215707b816e339ff4d4d67c65dffa206fd0f7aa7b9a"
+checksum = "db3c2bf3d13d5b658be73463284eaf12830ac9a26a90c717b7f771dfe97487bf"
 
 [[package]]
 name = "windows_x86_64_gnu"
 version = "0.48.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "53d40abd2583d23e4718fddf1ebec84dbff8381c07cae67ff7768bbf19c6718e"
 
 [[package]]
 name = "windows_x86_64_gnu"
-version = "0.52.0"
+version = "0.52.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "3d6fa32db2bc4a2f5abeacf2b69f7992cd09dca97498da74a151a3132c26befd"
+checksum = "4e4246f76bdeff09eb48875a0fd3e2af6aada79d409d33011886d3e1581517d9"
 
 [[package]]
 name = "windows_x86_64_gnullvm"
 version = "0.48.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "0b7b52767868a23d5bab768e390dc5f5c55825b6d30b86c844ff2dc7414044cc"
 
 [[package]]
 name = "windows_x86_64_gnullvm"
-version = "0.52.0"
+version = "0.52.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "1a657e1e9d3f514745a572a6846d3c7aa7dbe1658c056ed9c3344c4109a6949e"
+checksum = "852298e482cd67c356ddd9570386e2862b5673c85bd5f88df9ab6802b334c596"
 
 [[package]]
 name = "windows_x86_64_msvc"
 version = "0.48.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "ed94fce61571a4006852b7389a063ab983c02eb1bb37b47f8272ce92d06d9538"
 
 [[package]]
 name = "windows_x86_64_msvc"
-version = "0.52.0"
+version = "0.52.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "dff9641d1cd4be8d1a070daf9e3773c5f67e78b4d9d42263020c057706765c04"
+checksum = "bec47e5bfd1bff0eeaf6d8b485cc1074891a197ab4225d504cb7a1ab88b02bf0"
 
 [[package]]
 name = "winnow"
-version = "0.6.1"
+version = "0.6.8"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "d90f4e0f530c4c69f62b80d839e9ef3855edc9cba471a160c4d692deed62b401"
+checksum = "c3c52e9c97a68071b23e836c9380edae937f17b9c4667bd021973efc689f618d"
 dependencies = [
  "memchr",
 ]
 
 [[package]]
 name = "winreg"
-version = "0.50.0"
+version = "0.52.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "524e57b2c537c0f9b1e69f1965311ec12182b4122e45035b1508cd24d2adadb1"
+checksum = "a277a57398d4bfa075df44f501a17cfdf8542d224f0d36095a2adc7aee4ef0a5"
 dependencies = [
  "cfg-if",
  "windows-sys 0.48.0",
 ]
 
 [[package]]
+name = "winsafe"
+version = "0.0.19"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "d135d17ab770252ad95e9a872d365cf3090e3be864a34ab46f48555993efc904"
+
+[[package]]
+name = "wiremock"
+version = "0.6.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "ec874e1eef0df2dcac546057fe5e29186f09c378181cd7b635b4b7bcc98e9d81"
+dependencies = [
+ "assert-json-diff",
+ "async-trait",
+ "base64 0.21.7",
+ "deadpool",
+ "futures",
+ "http",
+ "http-body-util",
+ "hyper",
+ "hyper-util",
+ "log",
+ "once_cell",
+ "regex",
+ "serde",
+ "serde_json",
+ "tokio",
+ "url",
+]
+
+[[package]]
+name = "wmi"
+version = "0.13.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "fc2f0a4062ca522aad4705a2948fd4061b3857537990202a8ddd5af21607f79a"
+dependencies = [
+ "chrono",
+ "futures",
+ "log",
+ "serde",
+ "thiserror",
+ "windows 0.52.0",
+]
+
+[[package]]
 name = "wyz"
 version = "0.5.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "05f360fc0b24296329c78fda852a1e9ae82de9cf7b27dae4b7f62f118f77b9ed"
 dependencies = [
  "tap",
 ]
@@ -5054,26 +5681,57 @@
 [[package]]
 name = "xmlparser"
 version = "0.13.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "66fee0b777b0f5ac1c69bb06d361268faafa61cd4682ae064a171c16c433e9e4"
 
 [[package]]
-name = "yaml-rust"
-version = "0.4.5"
+name = "yansi"
+version = "0.5.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "56c1936c4cc7a1c9ab21a1ebb602eb942ba868cbd44a99cb7cdc5892335e1c85"
-dependencies = [
- "linked-hash-map",
-]
+checksum = "09041cd90cf85f7f8b2df60c646f853b7f535ce68f85244eb6731cf89fa498ec"
+
+[[package]]
+name = "zeroize"
+version = "1.7.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "525b4ec142c6b68a2d10f01f7bbf6755599ca3f81ea53b8431b7dd348f5fdb2d"
 
 [[package]]
 name = "zip"
 version = "0.6.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "760394e246e4c28189f19d488c058bf16f564016aefac5d32bb1f3b51d5e9261"
 dependencies = [
  "byteorder",
  "crc32fast",
  "crossbeam-utils",
  "flate2",
 ]
+
+[[package]]
+name = "zstd"
+version = "0.13.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "2d789b1514203a1120ad2429eae43a7bd32b90976a7bb8a05f7ec02fa88cc23a"
+dependencies = [
+ "zstd-safe",
+]
+
+[[package]]
+name = "zstd-safe"
+version = "7.1.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "1cd99b45c6bc03a018c8b8a86025678c87e55526064e38f9df301989dce7ec0a"
+dependencies = [
+ "zstd-sys",
+]
+
+[[package]]
+name = "zstd-sys"
+version = "2.0.10+zstd.1.5.6"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "c253a4914af5bafc8fa8c86ee400827e83cf6ec01195ec1f1ed8441bf00d65aa"
+dependencies = [
+ "cc",
+ "pkg-config",
+]
```

### Comparing `uv-0.1.9/pyproject.toml` & `uv-0.2.0/pyproject.toml`

 * *Files 10% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 [build-system]
 requires = ["maturin>=1.0,<2.0"]
 build-backend = "maturin"
 
 [project]
 name = "uv"
-version = "0.1.9"
+version = "0.2.0"
 description = "An extremely fast Python package installer and resolver, written in Rust."
 authors = [{ name = "Astral Software Inc.", email = "hey@astral.sh" }]
 requires-python = ">=3.8"
 keywords = [
   "uv", "requirements", "packaging"
 ]
 classifiers = [
@@ -37,31 +37,33 @@
 
 [tool.maturin]
 bindings = "bin"
 manifest-path = "crates/uv/Cargo.toml"
 module-name = "uv"
 python-source = "python"
 strip = true
-include = ["rust-toolchain.toml"]
+include = [{ path = "rust-toolchain.toml", format = ["sdist", "wheel"] }, { path = "LICENSE-APACHE", format = "sdist" }, { path = "LICENSE-MIT", format = "sdist" }]
 
 [tool.rooster]
-major_labels = []  # We do not use the major version number
-minor_labels = ["breaking"]   # Bump the minor version on breaking changes
+major_labels = []  # We do not use the major version number yet
+minor_labels = ["breaking"]
 changelog_ignore_labels = ["internal", "ci", "testing"]
 changelog_sections.breaking = "Breaking changes"
 changelog_sections.enhancement = "Enhancements"
 changelog_sections.compatibility = "Enhancements"
 changelog_sections.cli = "CLI"
 changelog_sections.configuration = "Configuration"
+changelog_sections.performance = "Performance"
 changelog_sections.bug = "Bug fixes"
 changelog_sections.rustlib = "Rust API"
 changelog_sections.documentation = "Documentation"
 changelog_sections.__unknown__ = "Other changes"
 
 # We exclude contributors from the CHANGELOG file
 # Generate separately with `rooster contributors` for the GitHub release page
 changelog_contributors = false
 
 version_files = [
   "README.md",
   "crates/uv/Cargo.toml",
+  "crates/uv-version/Cargo.toml",
 ]
```

### Comparing `uv-0.1.9/python/uv/__main__.py` & `uv-0.2.0/python/uv/__main__.py`

 * *Files 11% similar despite different names*

```diff
@@ -18,27 +18,30 @@
     venv_marker = os.path.join(sys.prefix, "pyvenv.cfg")
 
     if os.path.exists(venv_marker):
         return sys.prefix
 
     return ""
 
+
 def _run() -> None:
     uv = os.fsdecode(find_uv_bin())
 
     env = os.environ.copy()
     venv = _detect_virtualenv()
     if venv:
         env.setdefault("VIRTUAL_ENV", venv)
 
+    # Let `uv` know that it was spawned by this Python interpreter
+    env["UV_INTERNAL__PARENT_INTERPRETER"] = sys.executable
+
     if sys.platform == "win32":
         import subprocess
 
         completed_process = subprocess.run([uv, *sys.argv[1:]], env=env)
         sys.exit(completed_process.returncode)
     else:
         os.execvpe(uv, [uv, *sys.argv[1:]], env=env)
 
 
-
 if __name__ == "__main__":
     _run()
```

### Comparing `uv-0.1.9/python/uv/__init__.py` & `uv-0.2.0/python/uv/__init__.py`

 * *Files identical despite different names*

